{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.6002979145978156,
  "eval_steps": 500,
  "global_step": 21753,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016550810989738498,
      "grad_norm": 97.4415512084961,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 3.2653,
      "step": 10
    },
    {
      "epoch": 0.0033101621979476996,
      "grad_norm": 121.34355926513672,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 3.5017,
      "step": 20
    },
    {
      "epoch": 0.004965243296921549,
      "grad_norm": 48.178489685058594,
      "learning_rate": 3e-06,
      "loss": 2.8151,
      "step": 30
    },
    {
      "epoch": 0.006620324395895399,
      "grad_norm": 50.83437728881836,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.6973,
      "step": 40
    },
    {
      "epoch": 0.00827540549486925,
      "grad_norm": 39.91733169555664,
      "learning_rate": 5e-06,
      "loss": 1.5072,
      "step": 50
    },
    {
      "epoch": 0.009930486593843098,
      "grad_norm": 33.2450065612793,
      "learning_rate": 6e-06,
      "loss": 1.8742,
      "step": 60
    },
    {
      "epoch": 0.011585567692816948,
      "grad_norm": 32.60800552368164,
      "learning_rate": 7.000000000000001e-06,
      "loss": 1.6284,
      "step": 70
    },
    {
      "epoch": 0.013240648791790799,
      "grad_norm": 34.24906539916992,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.1873,
      "step": 80
    },
    {
      "epoch": 0.014895729890764648,
      "grad_norm": 50.064056396484375,
      "learning_rate": 9e-06,
      "loss": 1.1691,
      "step": 90
    },
    {
      "epoch": 0.0165508109897385,
      "grad_norm": 47.91412353515625,
      "learning_rate": 1e-05,
      "loss": 1.0641,
      "step": 100
    },
    {
      "epoch": 0.018205892088712348,
      "grad_norm": 43.95795822143555,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.0573,
      "step": 110
    },
    {
      "epoch": 0.019860973187686197,
      "grad_norm": 42.00776672363281,
      "learning_rate": 1.2e-05,
      "loss": 1.0241,
      "step": 120
    },
    {
      "epoch": 0.021516054286660046,
      "grad_norm": 28.95277214050293,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.17,
      "step": 130
    },
    {
      "epoch": 0.023171135385633895,
      "grad_norm": 17.930635452270508,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.0029,
      "step": 140
    },
    {
      "epoch": 0.024826216484607744,
      "grad_norm": 10.3660249710083,
      "learning_rate": 1.5e-05,
      "loss": 1.1087,
      "step": 150
    },
    {
      "epoch": 0.026481297583581597,
      "grad_norm": 40.463050842285156,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.9597,
      "step": 160
    },
    {
      "epoch": 0.028136378682555446,
      "grad_norm": 23.88897132873535,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 1.0799,
      "step": 170
    },
    {
      "epoch": 0.029791459781529295,
      "grad_norm": 23.142667770385742,
      "learning_rate": 1.8e-05,
      "loss": 1.0207,
      "step": 180
    },
    {
      "epoch": 0.031446540880503145,
      "grad_norm": 41.67078399658203,
      "learning_rate": 1.9e-05,
      "loss": 1.2081,
      "step": 190
    },
    {
      "epoch": 0.033101621979477,
      "grad_norm": 28.011249542236328,
      "learning_rate": 2e-05,
      "loss": 1.0128,
      "step": 200
    },
    {
      "epoch": 0.03475670307845084,
      "grad_norm": 22.017784118652344,
      "learning_rate": 2.1e-05,
      "loss": 0.9923,
      "step": 210
    },
    {
      "epoch": 0.036411784177424696,
      "grad_norm": 34.73788070678711,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.8604,
      "step": 220
    },
    {
      "epoch": 0.03806686527639854,
      "grad_norm": 27.730981826782227,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.8893,
      "step": 230
    },
    {
      "epoch": 0.039721946375372394,
      "grad_norm": 16.18294906616211,
      "learning_rate": 2.4e-05,
      "loss": 1.2433,
      "step": 240
    },
    {
      "epoch": 0.04137702747434624,
      "grad_norm": 36.50089645385742,
      "learning_rate": 2.5e-05,
      "loss": 0.9493,
      "step": 250
    },
    {
      "epoch": 0.04303210857332009,
      "grad_norm": 34.64004898071289,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.3145,
      "step": 260
    },
    {
      "epoch": 0.044687189672293945,
      "grad_norm": 35.67694854736328,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.12,
      "step": 270
    },
    {
      "epoch": 0.04634227077126779,
      "grad_norm": 14.698933601379395,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.9535,
      "step": 280
    },
    {
      "epoch": 0.04799735187024164,
      "grad_norm": 81.7050552368164,
      "learning_rate": 2.9e-05,
      "loss": 0.8433,
      "step": 290
    },
    {
      "epoch": 0.04965243296921549,
      "grad_norm": 29.671131134033203,
      "learning_rate": 3e-05,
      "loss": 1.5111,
      "step": 300
    },
    {
      "epoch": 0.05130751406818934,
      "grad_norm": 54.395599365234375,
      "learning_rate": 3.1e-05,
      "loss": 1.3558,
      "step": 310
    },
    {
      "epoch": 0.052962595167163194,
      "grad_norm": 24.62464141845703,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.4381,
      "step": 320
    },
    {
      "epoch": 0.05461767626613704,
      "grad_norm": 38.02064514160156,
      "learning_rate": 3.3e-05,
      "loss": 0.9403,
      "step": 330
    },
    {
      "epoch": 0.05627275736511089,
      "grad_norm": 44.62522506713867,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.9171,
      "step": 340
    },
    {
      "epoch": 0.05792783846408474,
      "grad_norm": 14.072800636291504,
      "learning_rate": 3.5e-05,
      "loss": 1.0232,
      "step": 350
    },
    {
      "epoch": 0.05958291956305859,
      "grad_norm": 14.406139373779297,
      "learning_rate": 3.6e-05,
      "loss": 0.7947,
      "step": 360
    },
    {
      "epoch": 0.06123800066203244,
      "grad_norm": 24.86500358581543,
      "learning_rate": 3.7e-05,
      "loss": 1.4377,
      "step": 370
    },
    {
      "epoch": 0.06289308176100629,
      "grad_norm": 18.5371036529541,
      "learning_rate": 3.8e-05,
      "loss": 0.9852,
      "step": 380
    },
    {
      "epoch": 0.06454816285998013,
      "grad_norm": 35.14235305786133,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.4349,
      "step": 390
    },
    {
      "epoch": 0.066203243958954,
      "grad_norm": 20.02771759033203,
      "learning_rate": 4e-05,
      "loss": 1.3746,
      "step": 400
    },
    {
      "epoch": 0.06785832505792784,
      "grad_norm": 28.747251510620117,
      "learning_rate": 4.1e-05,
      "loss": 1.3989,
      "step": 410
    },
    {
      "epoch": 0.06951340615690169,
      "grad_norm": 23.673721313476562,
      "learning_rate": 4.2e-05,
      "loss": 1.2095,
      "step": 420
    },
    {
      "epoch": 0.07116848725587553,
      "grad_norm": 3.3561432361602783,
      "learning_rate": 4.3e-05,
      "loss": 1.0675,
      "step": 430
    },
    {
      "epoch": 0.07282356835484939,
      "grad_norm": 51.90268325805664,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.0865,
      "step": 440
    },
    {
      "epoch": 0.07447864945382324,
      "grad_norm": 34.15875244140625,
      "learning_rate": 4.5e-05,
      "loss": 1.5736,
      "step": 450
    },
    {
      "epoch": 0.07613373055279708,
      "grad_norm": 26.975828170776367,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.2819,
      "step": 460
    },
    {
      "epoch": 0.07778881165177094,
      "grad_norm": 27.665348052978516,
      "learning_rate": 4.7e-05,
      "loss": 0.6944,
      "step": 470
    },
    {
      "epoch": 0.07944389275074479,
      "grad_norm": 29.756916046142578,
      "learning_rate": 4.8e-05,
      "loss": 1.8494,
      "step": 480
    },
    {
      "epoch": 0.08109897384971863,
      "grad_norm": 55.00706481933594,
      "learning_rate": 4.9e-05,
      "loss": 1.2432,
      "step": 490
    },
    {
      "epoch": 0.08275405494869248,
      "grad_norm": 45.52133560180664,
      "learning_rate": 5e-05,
      "loss": 1.1219,
      "step": 500
    },
    {
      "epoch": 0.08440913604766634,
      "grad_norm": 27.663555145263672,
      "learning_rate": 4.99788744296096e-05,
      "loss": 1.0477,
      "step": 510
    },
    {
      "epoch": 0.08606421714664018,
      "grad_norm": 32.310359954833984,
      "learning_rate": 4.99577488592192e-05,
      "loss": 1.1151,
      "step": 520
    },
    {
      "epoch": 0.08771929824561403,
      "grad_norm": 37.946659088134766,
      "learning_rate": 4.99366232888288e-05,
      "loss": 1.2722,
      "step": 530
    },
    {
      "epoch": 0.08937437934458789,
      "grad_norm": 27.8206729888916,
      "learning_rate": 4.9915497718438396e-05,
      "loss": 1.3977,
      "step": 540
    },
    {
      "epoch": 0.09102946044356174,
      "grad_norm": 22.341928482055664,
      "learning_rate": 4.9894372148048e-05,
      "loss": 1.1873,
      "step": 550
    },
    {
      "epoch": 0.09268454154253558,
      "grad_norm": 20.493959426879883,
      "learning_rate": 4.98732465776576e-05,
      "loss": 0.9783,
      "step": 560
    },
    {
      "epoch": 0.09433962264150944,
      "grad_norm": 36.496646881103516,
      "learning_rate": 4.98521210072672e-05,
      "loss": 1.2776,
      "step": 570
    },
    {
      "epoch": 0.09599470374048329,
      "grad_norm": 44.31249237060547,
      "learning_rate": 4.9830995436876796e-05,
      "loss": 1.2635,
      "step": 580
    },
    {
      "epoch": 0.09764978483945713,
      "grad_norm": 35.21307373046875,
      "learning_rate": 4.9809869866486395e-05,
      "loss": 1.4967,
      "step": 590
    },
    {
      "epoch": 0.09930486593843098,
      "grad_norm": 36.36589431762695,
      "learning_rate": 4.978874429609599e-05,
      "loss": 1.3594,
      "step": 600
    },
    {
      "epoch": 0.10095994703740484,
      "grad_norm": 24.680246353149414,
      "learning_rate": 4.97676187257056e-05,
      "loss": 1.499,
      "step": 610
    },
    {
      "epoch": 0.10261502813637868,
      "grad_norm": 30.076444625854492,
      "learning_rate": 4.9746493155315197e-05,
      "loss": 1.0537,
      "step": 620
    },
    {
      "epoch": 0.10427010923535253,
      "grad_norm": 18.924583435058594,
      "learning_rate": 4.9725367584924795e-05,
      "loss": 1.0623,
      "step": 630
    },
    {
      "epoch": 0.10592519033432639,
      "grad_norm": 35.29458999633789,
      "learning_rate": 4.97042420145344e-05,
      "loss": 1.3919,
      "step": 640
    },
    {
      "epoch": 0.10758027143330023,
      "grad_norm": 26.997940063476562,
      "learning_rate": 4.9683116444144e-05,
      "loss": 1.2173,
      "step": 650
    },
    {
      "epoch": 0.10923535253227408,
      "grad_norm": 23.493989944458008,
      "learning_rate": 4.96619908737536e-05,
      "loss": 1.2526,
      "step": 660
    },
    {
      "epoch": 0.11089043363124793,
      "grad_norm": 21.540573120117188,
      "learning_rate": 4.9640865303363195e-05,
      "loss": 1.2347,
      "step": 670
    },
    {
      "epoch": 0.11254551473022179,
      "grad_norm": 54.90599060058594,
      "learning_rate": 4.9619739732972794e-05,
      "loss": 1.2998,
      "step": 680
    },
    {
      "epoch": 0.11420059582919563,
      "grad_norm": 17.971019744873047,
      "learning_rate": 4.959861416258239e-05,
      "loss": 1.225,
      "step": 690
    },
    {
      "epoch": 0.11585567692816948,
      "grad_norm": 23.759201049804688,
      "learning_rate": 4.957748859219199e-05,
      "loss": 1.1704,
      "step": 700
    },
    {
      "epoch": 0.11751075802714334,
      "grad_norm": 18.30558967590332,
      "learning_rate": 4.955636302180159e-05,
      "loss": 1.1507,
      "step": 710
    },
    {
      "epoch": 0.11916583912611718,
      "grad_norm": 41.52193832397461,
      "learning_rate": 4.9535237451411194e-05,
      "loss": 1.0947,
      "step": 720
    },
    {
      "epoch": 0.12082092022509103,
      "grad_norm": 18.925830841064453,
      "learning_rate": 4.951411188102079e-05,
      "loss": 1.5544,
      "step": 730
    },
    {
      "epoch": 0.12247600132406487,
      "grad_norm": 58.37614059448242,
      "learning_rate": 4.949298631063039e-05,
      "loss": 1.3196,
      "step": 740
    },
    {
      "epoch": 0.12413108242303873,
      "grad_norm": 20.034914016723633,
      "learning_rate": 4.947186074023999e-05,
      "loss": 1.1119,
      "step": 750
    },
    {
      "epoch": 0.12578616352201258,
      "grad_norm": 42.30554962158203,
      "learning_rate": 4.945073516984959e-05,
      "loss": 0.9168,
      "step": 760
    },
    {
      "epoch": 0.12744124462098644,
      "grad_norm": 50.14617156982422,
      "learning_rate": 4.9429609599459186e-05,
      "loss": 1.0796,
      "step": 770
    },
    {
      "epoch": 0.12909632571996027,
      "grad_norm": 69.02271270751953,
      "learning_rate": 4.9408484029068784e-05,
      "loss": 1.2964,
      "step": 780
    },
    {
      "epoch": 0.13075140681893413,
      "grad_norm": 33.62277603149414,
      "learning_rate": 4.938735845867838e-05,
      "loss": 1.3134,
      "step": 790
    },
    {
      "epoch": 0.132406487917908,
      "grad_norm": 83.8985595703125,
      "learning_rate": 4.936623288828799e-05,
      "loss": 1.36,
      "step": 800
    },
    {
      "epoch": 0.13406156901688182,
      "grad_norm": 28.49913787841797,
      "learning_rate": 4.9345107317897586e-05,
      "loss": 1.2215,
      "step": 810
    },
    {
      "epoch": 0.13571665011585568,
      "grad_norm": 44.60554122924805,
      "learning_rate": 4.9323981747507185e-05,
      "loss": 1.2679,
      "step": 820
    },
    {
      "epoch": 0.13737173121482954,
      "grad_norm": 57.60272216796875,
      "learning_rate": 4.930285617711678e-05,
      "loss": 1.0022,
      "step": 830
    },
    {
      "epoch": 0.13902681231380337,
      "grad_norm": 13.666054725646973,
      "learning_rate": 4.928173060672638e-05,
      "loss": 0.9676,
      "step": 840
    },
    {
      "epoch": 0.14068189341277723,
      "grad_norm": 17.73228645324707,
      "learning_rate": 4.926060503633598e-05,
      "loss": 1.0089,
      "step": 850
    },
    {
      "epoch": 0.14233697451175106,
      "grad_norm": 38.72350311279297,
      "learning_rate": 4.9239479465945585e-05,
      "loss": 1.1762,
      "step": 860
    },
    {
      "epoch": 0.14399205561072492,
      "grad_norm": 48.70363235473633,
      "learning_rate": 4.921835389555518e-05,
      "loss": 1.4009,
      "step": 870
    },
    {
      "epoch": 0.14564713670969878,
      "grad_norm": 50.8752555847168,
      "learning_rate": 4.919722832516478e-05,
      "loss": 1.0995,
      "step": 880
    },
    {
      "epoch": 0.14730221780867261,
      "grad_norm": 44.483604431152344,
      "learning_rate": 4.917610275477439e-05,
      "loss": 1.1234,
      "step": 890
    },
    {
      "epoch": 0.14895729890764647,
      "grad_norm": 29.918901443481445,
      "learning_rate": 4.9154977184383985e-05,
      "loss": 1.0949,
      "step": 900
    },
    {
      "epoch": 0.15061238000662033,
      "grad_norm": 18.013593673706055,
      "learning_rate": 4.9133851613993584e-05,
      "loss": 1.0986,
      "step": 910
    },
    {
      "epoch": 0.15226746110559417,
      "grad_norm": 17.03639030456543,
      "learning_rate": 4.911272604360318e-05,
      "loss": 1.0965,
      "step": 920
    },
    {
      "epoch": 0.15392254220456802,
      "grad_norm": 32.880088806152344,
      "learning_rate": 4.909160047321278e-05,
      "loss": 1.1738,
      "step": 930
    },
    {
      "epoch": 0.15557762330354188,
      "grad_norm": 17.56507110595703,
      "learning_rate": 4.907047490282238e-05,
      "loss": 1.2153,
      "step": 940
    },
    {
      "epoch": 0.15723270440251572,
      "grad_norm": 26.05997085571289,
      "learning_rate": 4.904934933243198e-05,
      "loss": 0.9936,
      "step": 950
    },
    {
      "epoch": 0.15888778550148958,
      "grad_norm": 38.58293914794922,
      "learning_rate": 4.9028223762041575e-05,
      "loss": 1.39,
      "step": 960
    },
    {
      "epoch": 0.16054286660046344,
      "grad_norm": 12.222036361694336,
      "learning_rate": 4.900709819165118e-05,
      "loss": 1.0561,
      "step": 970
    },
    {
      "epoch": 0.16219794769943727,
      "grad_norm": 24.692991256713867,
      "learning_rate": 4.898597262126078e-05,
      "loss": 1.3424,
      "step": 980
    },
    {
      "epoch": 0.16385302879841113,
      "grad_norm": 67.37031555175781,
      "learning_rate": 4.896484705087038e-05,
      "loss": 1.1594,
      "step": 990
    },
    {
      "epoch": 0.16550810989738496,
      "grad_norm": 23.87232780456543,
      "learning_rate": 4.8943721480479976e-05,
      "loss": 1.191,
      "step": 1000
    },
    {
      "epoch": 0.16716319099635882,
      "grad_norm": 21.083709716796875,
      "learning_rate": 4.8922595910089574e-05,
      "loss": 1.1213,
      "step": 1010
    },
    {
      "epoch": 0.16881827209533268,
      "grad_norm": 21.680368423461914,
      "learning_rate": 4.890147033969917e-05,
      "loss": 1.0167,
      "step": 1020
    },
    {
      "epoch": 0.1704733531943065,
      "grad_norm": 18.98822593688965,
      "learning_rate": 4.888034476930877e-05,
      "loss": 1.2131,
      "step": 1030
    },
    {
      "epoch": 0.17212843429328037,
      "grad_norm": 40.81112289428711,
      "learning_rate": 4.885921919891837e-05,
      "loss": 1.0608,
      "step": 1040
    },
    {
      "epoch": 0.17378351539225423,
      "grad_norm": 27.607023239135742,
      "learning_rate": 4.883809362852797e-05,
      "loss": 1.4412,
      "step": 1050
    },
    {
      "epoch": 0.17543859649122806,
      "grad_norm": 42.07017517089844,
      "learning_rate": 4.881696805813757e-05,
      "loss": 1.1374,
      "step": 1060
    },
    {
      "epoch": 0.17709367759020192,
      "grad_norm": 35.417564392089844,
      "learning_rate": 4.879584248774717e-05,
      "loss": 1.3788,
      "step": 1070
    },
    {
      "epoch": 0.17874875868917578,
      "grad_norm": 33.0713005065918,
      "learning_rate": 4.877471691735677e-05,
      "loss": 1.1659,
      "step": 1080
    },
    {
      "epoch": 0.1804038397881496,
      "grad_norm": 32.4471549987793,
      "learning_rate": 4.875359134696637e-05,
      "loss": 1.0727,
      "step": 1090
    },
    {
      "epoch": 0.18205892088712347,
      "grad_norm": 38.011470794677734,
      "learning_rate": 4.8732465776575966e-05,
      "loss": 0.9315,
      "step": 1100
    },
    {
      "epoch": 0.18371400198609733,
      "grad_norm": 19.309675216674805,
      "learning_rate": 4.871134020618557e-05,
      "loss": 1.5556,
      "step": 1110
    },
    {
      "epoch": 0.18536908308507116,
      "grad_norm": 60.10500717163086,
      "learning_rate": 4.869021463579517e-05,
      "loss": 0.6319,
      "step": 1120
    },
    {
      "epoch": 0.18702416418404502,
      "grad_norm": 13.00697135925293,
      "learning_rate": 4.866908906540477e-05,
      "loss": 1.4658,
      "step": 1130
    },
    {
      "epoch": 0.18867924528301888,
      "grad_norm": 25.067235946655273,
      "learning_rate": 4.8647963495014373e-05,
      "loss": 1.3293,
      "step": 1140
    },
    {
      "epoch": 0.1903343263819927,
      "grad_norm": 14.16656494140625,
      "learning_rate": 4.862683792462397e-05,
      "loss": 0.7232,
      "step": 1150
    },
    {
      "epoch": 0.19198940748096657,
      "grad_norm": 31.454057693481445,
      "learning_rate": 4.860571235423357e-05,
      "loss": 1.3936,
      "step": 1160
    },
    {
      "epoch": 0.1936444885799404,
      "grad_norm": 45.0728874206543,
      "learning_rate": 4.858458678384317e-05,
      "loss": 1.3844,
      "step": 1170
    },
    {
      "epoch": 0.19529956967891426,
      "grad_norm": 31.794769287109375,
      "learning_rate": 4.856346121345277e-05,
      "loss": 1.0689,
      "step": 1180
    },
    {
      "epoch": 0.19695465077788812,
      "grad_norm": 21.14841651916504,
      "learning_rate": 4.8542335643062365e-05,
      "loss": 1.4044,
      "step": 1190
    },
    {
      "epoch": 0.19860973187686196,
      "grad_norm": 45.91816329956055,
      "learning_rate": 4.8521210072671964e-05,
      "loss": 1.1661,
      "step": 1200
    },
    {
      "epoch": 0.20026481297583582,
      "grad_norm": 18.18951988220215,
      "learning_rate": 4.850008450228156e-05,
      "loss": 1.0786,
      "step": 1210
    },
    {
      "epoch": 0.20191989407480967,
      "grad_norm": 36.60145568847656,
      "learning_rate": 4.847895893189116e-05,
      "loss": 1.443,
      "step": 1220
    },
    {
      "epoch": 0.2035749751737835,
      "grad_norm": 36.871280670166016,
      "learning_rate": 4.8457833361500766e-05,
      "loss": 1.3718,
      "step": 1230
    },
    {
      "epoch": 0.20523005627275737,
      "grad_norm": 18.16054916381836,
      "learning_rate": 4.8436707791110364e-05,
      "loss": 0.9796,
      "step": 1240
    },
    {
      "epoch": 0.20688513737173123,
      "grad_norm": 44.73019790649414,
      "learning_rate": 4.841558222071996e-05,
      "loss": 1.0275,
      "step": 1250
    },
    {
      "epoch": 0.20854021847070506,
      "grad_norm": 17.73753547668457,
      "learning_rate": 4.839445665032956e-05,
      "loss": 0.7922,
      "step": 1260
    },
    {
      "epoch": 0.21019529956967892,
      "grad_norm": 42.6441650390625,
      "learning_rate": 4.837333107993916e-05,
      "loss": 1.4442,
      "step": 1270
    },
    {
      "epoch": 0.21185038066865278,
      "grad_norm": 24.02442741394043,
      "learning_rate": 4.835220550954876e-05,
      "loss": 1.0102,
      "step": 1280
    },
    {
      "epoch": 0.2135054617676266,
      "grad_norm": 48.59553146362305,
      "learning_rate": 4.8331079939158356e-05,
      "loss": 1.2905,
      "step": 1290
    },
    {
      "epoch": 0.21516054286660047,
      "grad_norm": 17.703311920166016,
      "learning_rate": 4.8309954368767954e-05,
      "loss": 0.8522,
      "step": 1300
    },
    {
      "epoch": 0.2168156239655743,
      "grad_norm": 64.39567565917969,
      "learning_rate": 4.828882879837756e-05,
      "loss": 1.1265,
      "step": 1310
    },
    {
      "epoch": 0.21847070506454816,
      "grad_norm": 23.48064613342285,
      "learning_rate": 4.826770322798716e-05,
      "loss": 1.3915,
      "step": 1320
    },
    {
      "epoch": 0.22012578616352202,
      "grad_norm": 41.70143127441406,
      "learning_rate": 4.8246577657596756e-05,
      "loss": 1.3118,
      "step": 1330
    },
    {
      "epoch": 0.22178086726249585,
      "grad_norm": 69.24468994140625,
      "learning_rate": 4.8225452087206355e-05,
      "loss": 0.8991,
      "step": 1340
    },
    {
      "epoch": 0.2234359483614697,
      "grad_norm": 43.01826477050781,
      "learning_rate": 4.820432651681595e-05,
      "loss": 0.8805,
      "step": 1350
    },
    {
      "epoch": 0.22509102946044357,
      "grad_norm": 47.81785202026367,
      "learning_rate": 4.818320094642556e-05,
      "loss": 0.8368,
      "step": 1360
    },
    {
      "epoch": 0.2267461105594174,
      "grad_norm": 30.941341400146484,
      "learning_rate": 4.816207537603516e-05,
      "loss": 1.5406,
      "step": 1370
    },
    {
      "epoch": 0.22840119165839126,
      "grad_norm": 39.646812438964844,
      "learning_rate": 4.8140949805644755e-05,
      "loss": 0.9361,
      "step": 1380
    },
    {
      "epoch": 0.23005627275736512,
      "grad_norm": 13.443039894104004,
      "learning_rate": 4.811982423525435e-05,
      "loss": 1.2759,
      "step": 1390
    },
    {
      "epoch": 0.23171135385633895,
      "grad_norm": 26.6287899017334,
      "learning_rate": 4.809869866486396e-05,
      "loss": 1.072,
      "step": 1400
    },
    {
      "epoch": 0.2333664349553128,
      "grad_norm": 63.24628448486328,
      "learning_rate": 4.807757309447356e-05,
      "loss": 1.4455,
      "step": 1410
    },
    {
      "epoch": 0.23502151605428667,
      "grad_norm": 24.016357421875,
      "learning_rate": 4.8056447524083155e-05,
      "loss": 1.3908,
      "step": 1420
    },
    {
      "epoch": 0.2366765971532605,
      "grad_norm": 12.46517276763916,
      "learning_rate": 4.8035321953692754e-05,
      "loss": 0.7688,
      "step": 1430
    },
    {
      "epoch": 0.23833167825223436,
      "grad_norm": 10.827034950256348,
      "learning_rate": 4.801419638330235e-05,
      "loss": 1.0949,
      "step": 1440
    },
    {
      "epoch": 0.2399867593512082,
      "grad_norm": 136.52789306640625,
      "learning_rate": 4.799307081291195e-05,
      "loss": 0.7759,
      "step": 1450
    },
    {
      "epoch": 0.24164184045018205,
      "grad_norm": 35.291507720947266,
      "learning_rate": 4.797194524252155e-05,
      "loss": 1.319,
      "step": 1460
    },
    {
      "epoch": 0.24329692154915591,
      "grad_norm": 25.31490135192871,
      "learning_rate": 4.795081967213115e-05,
      "loss": 1.2102,
      "step": 1470
    },
    {
      "epoch": 0.24495200264812975,
      "grad_norm": 5.311086177825928,
      "learning_rate": 4.792969410174075e-05,
      "loss": 0.8988,
      "step": 1480
    },
    {
      "epoch": 0.2466070837471036,
      "grad_norm": 28.51626968383789,
      "learning_rate": 4.790856853135035e-05,
      "loss": 1.0968,
      "step": 1490
    },
    {
      "epoch": 0.24826216484607747,
      "grad_norm": 54.17319869995117,
      "learning_rate": 4.788744296095995e-05,
      "loss": 1.0518,
      "step": 1500
    },
    {
      "epoch": 0.2499172459450513,
      "grad_norm": 26.058807373046875,
      "learning_rate": 4.786631739056955e-05,
      "loss": 1.3213,
      "step": 1510
    },
    {
      "epoch": 0.25157232704402516,
      "grad_norm": 23.86149787902832,
      "learning_rate": 4.7845191820179146e-05,
      "loss": 1.3633,
      "step": 1520
    },
    {
      "epoch": 0.253227408142999,
      "grad_norm": 36.164154052734375,
      "learning_rate": 4.7824066249788744e-05,
      "loss": 1.0884,
      "step": 1530
    },
    {
      "epoch": 0.2548824892419729,
      "grad_norm": 55.85585403442383,
      "learning_rate": 4.780294067939834e-05,
      "loss": 0.8493,
      "step": 1540
    },
    {
      "epoch": 0.2565375703409467,
      "grad_norm": 27.191936492919922,
      "learning_rate": 4.778181510900794e-05,
      "loss": 0.9305,
      "step": 1550
    },
    {
      "epoch": 0.25819265143992054,
      "grad_norm": 40.60062026977539,
      "learning_rate": 4.776068953861754e-05,
      "loss": 1.1499,
      "step": 1560
    },
    {
      "epoch": 0.2598477325388944,
      "grad_norm": 30.577640533447266,
      "learning_rate": 4.7739563968227145e-05,
      "loss": 1.0569,
      "step": 1570
    },
    {
      "epoch": 0.26150281363786826,
      "grad_norm": 18.682828903198242,
      "learning_rate": 4.771843839783674e-05,
      "loss": 0.8512,
      "step": 1580
    },
    {
      "epoch": 0.2631578947368421,
      "grad_norm": 17.841514587402344,
      "learning_rate": 4.769731282744634e-05,
      "loss": 1.2974,
      "step": 1590
    },
    {
      "epoch": 0.264812975835816,
      "grad_norm": 28.501638412475586,
      "learning_rate": 4.767618725705594e-05,
      "loss": 1.141,
      "step": 1600
    },
    {
      "epoch": 0.2664680569347898,
      "grad_norm": 37.53023910522461,
      "learning_rate": 4.7655061686665545e-05,
      "loss": 0.7923,
      "step": 1610
    },
    {
      "epoch": 0.26812313803376364,
      "grad_norm": 22.709646224975586,
      "learning_rate": 4.763393611627514e-05,
      "loss": 1.319,
      "step": 1620
    },
    {
      "epoch": 0.26977821913273753,
      "grad_norm": 27.045103073120117,
      "learning_rate": 4.761281054588474e-05,
      "loss": 0.7483,
      "step": 1630
    },
    {
      "epoch": 0.27143330023171136,
      "grad_norm": 55.98433303833008,
      "learning_rate": 4.759168497549434e-05,
      "loss": 1.3818,
      "step": 1640
    },
    {
      "epoch": 0.2730883813306852,
      "grad_norm": 28.643022537231445,
      "learning_rate": 4.7570559405103945e-05,
      "loss": 1.173,
      "step": 1650
    },
    {
      "epoch": 0.2747434624296591,
      "grad_norm": 26.441133499145508,
      "learning_rate": 4.7549433834713544e-05,
      "loss": 1.2022,
      "step": 1660
    },
    {
      "epoch": 0.2763985435286329,
      "grad_norm": 46.60511016845703,
      "learning_rate": 4.752830826432314e-05,
      "loss": 1.0439,
      "step": 1670
    },
    {
      "epoch": 0.27805362462760674,
      "grad_norm": 20.532909393310547,
      "learning_rate": 4.750718269393274e-05,
      "loss": 0.997,
      "step": 1680
    },
    {
      "epoch": 0.2797087057265806,
      "grad_norm": 37.974918365478516,
      "learning_rate": 4.748605712354234e-05,
      "loss": 0.9507,
      "step": 1690
    },
    {
      "epoch": 0.28136378682555446,
      "grad_norm": 58.13558578491211,
      "learning_rate": 4.746493155315194e-05,
      "loss": 1.2208,
      "step": 1700
    },
    {
      "epoch": 0.2830188679245283,
      "grad_norm": 43.653099060058594,
      "learning_rate": 4.7443805982761536e-05,
      "loss": 1.4586,
      "step": 1710
    },
    {
      "epoch": 0.2846739490235021,
      "grad_norm": 11.706361770629883,
      "learning_rate": 4.7422680412371134e-05,
      "loss": 1.1155,
      "step": 1720
    },
    {
      "epoch": 0.286329030122476,
      "grad_norm": 24.717708587646484,
      "learning_rate": 4.740155484198073e-05,
      "loss": 1.1153,
      "step": 1730
    },
    {
      "epoch": 0.28798411122144985,
      "grad_norm": 28.47773551940918,
      "learning_rate": 4.738042927159034e-05,
      "loss": 1.3297,
      "step": 1740
    },
    {
      "epoch": 0.2896391923204237,
      "grad_norm": 55.19663619995117,
      "learning_rate": 4.7359303701199936e-05,
      "loss": 1.4315,
      "step": 1750
    },
    {
      "epoch": 0.29129427341939756,
      "grad_norm": 20.91109848022461,
      "learning_rate": 4.7338178130809534e-05,
      "loss": 1.0999,
      "step": 1760
    },
    {
      "epoch": 0.2929493545183714,
      "grad_norm": 26.083189010620117,
      "learning_rate": 4.731705256041913e-05,
      "loss": 1.0293,
      "step": 1770
    },
    {
      "epoch": 0.29460443561734523,
      "grad_norm": 42.47421646118164,
      "learning_rate": 4.729592699002873e-05,
      "loss": 1.2907,
      "step": 1780
    },
    {
      "epoch": 0.2962595167163191,
      "grad_norm": 24.587677001953125,
      "learning_rate": 4.727480141963833e-05,
      "loss": 1.0462,
      "step": 1790
    },
    {
      "epoch": 0.29791459781529295,
      "grad_norm": 54.55634307861328,
      "learning_rate": 4.725367584924793e-05,
      "loss": 0.9033,
      "step": 1800
    },
    {
      "epoch": 0.2995696789142668,
      "grad_norm": 24.767593383789062,
      "learning_rate": 4.7232550278857526e-05,
      "loss": 1.2028,
      "step": 1810
    },
    {
      "epoch": 0.30122476001324067,
      "grad_norm": 44.51886749267578,
      "learning_rate": 4.721142470846713e-05,
      "loss": 1.2678,
      "step": 1820
    },
    {
      "epoch": 0.3028798411122145,
      "grad_norm": 51.98725509643555,
      "learning_rate": 4.719029913807673e-05,
      "loss": 1.0257,
      "step": 1830
    },
    {
      "epoch": 0.30453492221118833,
      "grad_norm": 33.271419525146484,
      "learning_rate": 4.716917356768633e-05,
      "loss": 1.2286,
      "step": 1840
    },
    {
      "epoch": 0.3061900033101622,
      "grad_norm": 33.29391098022461,
      "learning_rate": 4.714804799729593e-05,
      "loss": 1.1303,
      "step": 1850
    },
    {
      "epoch": 0.30784508440913605,
      "grad_norm": 33.18928146362305,
      "learning_rate": 4.712692242690553e-05,
      "loss": 1.1277,
      "step": 1860
    },
    {
      "epoch": 0.3095001655081099,
      "grad_norm": 23.810184478759766,
      "learning_rate": 4.710579685651513e-05,
      "loss": 1.1335,
      "step": 1870
    },
    {
      "epoch": 0.31115524660708377,
      "grad_norm": 99.15519714355469,
      "learning_rate": 4.708467128612473e-05,
      "loss": 1.1959,
      "step": 1880
    },
    {
      "epoch": 0.3128103277060576,
      "grad_norm": 29.15110206604004,
      "learning_rate": 4.706354571573433e-05,
      "loss": 1.4197,
      "step": 1890
    },
    {
      "epoch": 0.31446540880503143,
      "grad_norm": 35.4796028137207,
      "learning_rate": 4.7042420145343925e-05,
      "loss": 1.0968,
      "step": 1900
    },
    {
      "epoch": 0.3161204899040053,
      "grad_norm": 33.30064010620117,
      "learning_rate": 4.702129457495353e-05,
      "loss": 1.0341,
      "step": 1910
    },
    {
      "epoch": 0.31777557100297915,
      "grad_norm": 33.59657287597656,
      "learning_rate": 4.700016900456313e-05,
      "loss": 1.1013,
      "step": 1920
    },
    {
      "epoch": 0.319430652101953,
      "grad_norm": 40.53507614135742,
      "learning_rate": 4.697904343417273e-05,
      "loss": 1.3462,
      "step": 1930
    },
    {
      "epoch": 0.32108573320092687,
      "grad_norm": 31.782255172729492,
      "learning_rate": 4.6957917863782325e-05,
      "loss": 1.1407,
      "step": 1940
    },
    {
      "epoch": 0.3227408142999007,
      "grad_norm": 21.611434936523438,
      "learning_rate": 4.6936792293391924e-05,
      "loss": 1.0156,
      "step": 1950
    },
    {
      "epoch": 0.32439589539887453,
      "grad_norm": 104.43136596679688,
      "learning_rate": 4.691566672300152e-05,
      "loss": 1.1035,
      "step": 1960
    },
    {
      "epoch": 0.3260509764978484,
      "grad_norm": 36.75350570678711,
      "learning_rate": 4.689454115261112e-05,
      "loss": 0.9366,
      "step": 1970
    },
    {
      "epoch": 0.32770605759682225,
      "grad_norm": 36.79549789428711,
      "learning_rate": 4.687341558222072e-05,
      "loss": 1.1395,
      "step": 1980
    },
    {
      "epoch": 0.3293611386957961,
      "grad_norm": 24.88180160522461,
      "learning_rate": 4.6852290011830324e-05,
      "loss": 0.856,
      "step": 1990
    },
    {
      "epoch": 0.3310162197947699,
      "grad_norm": 34.89110565185547,
      "learning_rate": 4.683116444143992e-05,
      "loss": 0.9102,
      "step": 2000
    },
    {
      "epoch": 0.3326713008937438,
      "grad_norm": 20.741331100463867,
      "learning_rate": 4.681003887104952e-05,
      "loss": 1.192,
      "step": 2010
    },
    {
      "epoch": 0.33432638199271764,
      "grad_norm": 36.0693244934082,
      "learning_rate": 4.678891330065912e-05,
      "loss": 1.1723,
      "step": 2020
    },
    {
      "epoch": 0.33598146309169147,
      "grad_norm": 69.0107650756836,
      "learning_rate": 4.676778773026872e-05,
      "loss": 0.9863,
      "step": 2030
    },
    {
      "epoch": 0.33763654419066536,
      "grad_norm": 29.610151290893555,
      "learning_rate": 4.6746662159878316e-05,
      "loss": 1.2898,
      "step": 2040
    },
    {
      "epoch": 0.3392916252896392,
      "grad_norm": 15.492105484008789,
      "learning_rate": 4.6725536589487914e-05,
      "loss": 1.0231,
      "step": 2050
    },
    {
      "epoch": 0.340946706388613,
      "grad_norm": 25.773880004882812,
      "learning_rate": 4.670441101909751e-05,
      "loss": 1.5425,
      "step": 2060
    },
    {
      "epoch": 0.3426017874875869,
      "grad_norm": 47.44260025024414,
      "learning_rate": 4.668328544870712e-05,
      "loss": 1.2476,
      "step": 2070
    },
    {
      "epoch": 0.34425686858656074,
      "grad_norm": 77.98101806640625,
      "learning_rate": 4.6662159878316716e-05,
      "loss": 1.378,
      "step": 2080
    },
    {
      "epoch": 0.34591194968553457,
      "grad_norm": 43.10260009765625,
      "learning_rate": 4.6641034307926315e-05,
      "loss": 1.2092,
      "step": 2090
    },
    {
      "epoch": 0.34756703078450846,
      "grad_norm": 38.76762771606445,
      "learning_rate": 4.661990873753592e-05,
      "loss": 1.0358,
      "step": 2100
    },
    {
      "epoch": 0.3492221118834823,
      "grad_norm": 27.373626708984375,
      "learning_rate": 4.659878316714552e-05,
      "loss": 0.8094,
      "step": 2110
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 24.611284255981445,
      "learning_rate": 4.657765759675512e-05,
      "loss": 0.9375,
      "step": 2120
    },
    {
      "epoch": 0.35253227408143,
      "grad_norm": 24.617549896240234,
      "learning_rate": 4.6556532026364715e-05,
      "loss": 1.377,
      "step": 2130
    },
    {
      "epoch": 0.35418735518040384,
      "grad_norm": 25.508323669433594,
      "learning_rate": 4.6535406455974313e-05,
      "loss": 1.0572,
      "step": 2140
    },
    {
      "epoch": 0.35584243627937767,
      "grad_norm": 34.27818298339844,
      "learning_rate": 4.651428088558391e-05,
      "loss": 1.2413,
      "step": 2150
    },
    {
      "epoch": 0.35749751737835156,
      "grad_norm": 86.87638854980469,
      "learning_rate": 4.649315531519352e-05,
      "loss": 1.1934,
      "step": 2160
    },
    {
      "epoch": 0.3591525984773254,
      "grad_norm": 11.199602127075195,
      "learning_rate": 4.6472029744803115e-05,
      "loss": 1.3045,
      "step": 2170
    },
    {
      "epoch": 0.3608076795762992,
      "grad_norm": 13.859945297241211,
      "learning_rate": 4.6450904174412714e-05,
      "loss": 0.9475,
      "step": 2180
    },
    {
      "epoch": 0.3624627606752731,
      "grad_norm": 31.423864364624023,
      "learning_rate": 4.642977860402231e-05,
      "loss": 1.1896,
      "step": 2190
    },
    {
      "epoch": 0.36411784177424694,
      "grad_norm": 29.091333389282227,
      "learning_rate": 4.640865303363191e-05,
      "loss": 1.1421,
      "step": 2200
    },
    {
      "epoch": 0.3657729228732208,
      "grad_norm": 40.34096145629883,
      "learning_rate": 4.638752746324151e-05,
      "loss": 1.0777,
      "step": 2210
    },
    {
      "epoch": 0.36742800397219466,
      "grad_norm": 21.893680572509766,
      "learning_rate": 4.636640189285111e-05,
      "loss": 1.0392,
      "step": 2220
    },
    {
      "epoch": 0.3690830850711685,
      "grad_norm": 29.696298599243164,
      "learning_rate": 4.6345276322460706e-05,
      "loss": 1.0009,
      "step": 2230
    },
    {
      "epoch": 0.3707381661701423,
      "grad_norm": 59.434635162353516,
      "learning_rate": 4.6324150752070304e-05,
      "loss": 1.1633,
      "step": 2240
    },
    {
      "epoch": 0.3723932472691162,
      "grad_norm": 10.650955200195312,
      "learning_rate": 4.630302518167991e-05,
      "loss": 1.1615,
      "step": 2250
    },
    {
      "epoch": 0.37404832836809004,
      "grad_norm": 35.60771179199219,
      "learning_rate": 4.628189961128951e-05,
      "loss": 0.9056,
      "step": 2260
    },
    {
      "epoch": 0.3757034094670639,
      "grad_norm": 32.109474182128906,
      "learning_rate": 4.6260774040899106e-05,
      "loss": 0.9758,
      "step": 2270
    },
    {
      "epoch": 0.37735849056603776,
      "grad_norm": 27.079797744750977,
      "learning_rate": 4.6239648470508704e-05,
      "loss": 0.8164,
      "step": 2280
    },
    {
      "epoch": 0.3790135716650116,
      "grad_norm": 52.39609146118164,
      "learning_rate": 4.62185229001183e-05,
      "loss": 1.3938,
      "step": 2290
    },
    {
      "epoch": 0.3806686527639854,
      "grad_norm": 38.14473342895508,
      "learning_rate": 4.61973973297279e-05,
      "loss": 1.2163,
      "step": 2300
    },
    {
      "epoch": 0.38232373386295926,
      "grad_norm": 28.690258026123047,
      "learning_rate": 4.61762717593375e-05,
      "loss": 1.1468,
      "step": 2310
    },
    {
      "epoch": 0.38397881496193315,
      "grad_norm": 67.65990447998047,
      "learning_rate": 4.6155146188947105e-05,
      "loss": 0.9443,
      "step": 2320
    },
    {
      "epoch": 0.385633896060907,
      "grad_norm": 35.638084411621094,
      "learning_rate": 4.61340206185567e-05,
      "loss": 0.7823,
      "step": 2330
    },
    {
      "epoch": 0.3872889771598808,
      "grad_norm": 262.21441650390625,
      "learning_rate": 4.61128950481663e-05,
      "loss": 1.3207,
      "step": 2340
    },
    {
      "epoch": 0.3889440582588547,
      "grad_norm": 31.428504943847656,
      "learning_rate": 4.6091769477775907e-05,
      "loss": 0.9319,
      "step": 2350
    },
    {
      "epoch": 0.39059913935782853,
      "grad_norm": 31.62726593017578,
      "learning_rate": 4.6070643907385505e-05,
      "loss": 1.1942,
      "step": 2360
    },
    {
      "epoch": 0.39225422045680236,
      "grad_norm": 44.401485443115234,
      "learning_rate": 4.60495183369951e-05,
      "loss": 0.8836,
      "step": 2370
    },
    {
      "epoch": 0.39390930155577625,
      "grad_norm": 58.04596710205078,
      "learning_rate": 4.60283927666047e-05,
      "loss": 1.3717,
      "step": 2380
    },
    {
      "epoch": 0.3955643826547501,
      "grad_norm": 92.31807708740234,
      "learning_rate": 4.60072671962143e-05,
      "loss": 1.1065,
      "step": 2390
    },
    {
      "epoch": 0.3972194637537239,
      "grad_norm": 14.041876792907715,
      "learning_rate": 4.59861416258239e-05,
      "loss": 0.9633,
      "step": 2400
    },
    {
      "epoch": 0.3988745448526978,
      "grad_norm": 18.356443405151367,
      "learning_rate": 4.59650160554335e-05,
      "loss": 0.921,
      "step": 2410
    },
    {
      "epoch": 0.40052962595167163,
      "grad_norm": 31.046247482299805,
      "learning_rate": 4.59438904850431e-05,
      "loss": 0.9325,
      "step": 2420
    },
    {
      "epoch": 0.40218470705064546,
      "grad_norm": 31.712236404418945,
      "learning_rate": 4.59227649146527e-05,
      "loss": 1.3867,
      "step": 2430
    },
    {
      "epoch": 0.40383978814961935,
      "grad_norm": 17.54230499267578,
      "learning_rate": 4.59016393442623e-05,
      "loss": 0.8318,
      "step": 2440
    },
    {
      "epoch": 0.4054948692485932,
      "grad_norm": 36.878849029541016,
      "learning_rate": 4.58805137738719e-05,
      "loss": 0.8369,
      "step": 2450
    },
    {
      "epoch": 0.407149950347567,
      "grad_norm": 35.157989501953125,
      "learning_rate": 4.5859388203481496e-05,
      "loss": 1.2673,
      "step": 2460
    },
    {
      "epoch": 0.4088050314465409,
      "grad_norm": 6.213328838348389,
      "learning_rate": 4.5838262633091094e-05,
      "loss": 0.6922,
      "step": 2470
    },
    {
      "epoch": 0.41046011254551473,
      "grad_norm": 18.879789352416992,
      "learning_rate": 4.581713706270069e-05,
      "loss": 1.0078,
      "step": 2480
    },
    {
      "epoch": 0.41211519364448856,
      "grad_norm": 31.578876495361328,
      "learning_rate": 4.579601149231029e-05,
      "loss": 1.0876,
      "step": 2490
    },
    {
      "epoch": 0.41377027474346245,
      "grad_norm": 21.7144718170166,
      "learning_rate": 4.5774885921919896e-05,
      "loss": 1.0685,
      "step": 2500
    },
    {
      "epoch": 0.4154253558424363,
      "grad_norm": 30.247915267944336,
      "learning_rate": 4.5753760351529494e-05,
      "loss": 1.0728,
      "step": 2510
    },
    {
      "epoch": 0.4170804369414101,
      "grad_norm": 30.390466690063477,
      "learning_rate": 4.573263478113909e-05,
      "loss": 1.0073,
      "step": 2520
    },
    {
      "epoch": 0.418735518040384,
      "grad_norm": 28.40842628479004,
      "learning_rate": 4.571150921074869e-05,
      "loss": 1.0378,
      "step": 2530
    },
    {
      "epoch": 0.42039059913935783,
      "grad_norm": 42.324851989746094,
      "learning_rate": 4.569038364035829e-05,
      "loss": 0.8659,
      "step": 2540
    },
    {
      "epoch": 0.42204568023833167,
      "grad_norm": 33.81584548950195,
      "learning_rate": 4.566925806996789e-05,
      "loss": 0.9631,
      "step": 2550
    },
    {
      "epoch": 0.42370076133730555,
      "grad_norm": 65.20732116699219,
      "learning_rate": 4.5648132499577486e-05,
      "loss": 1.0285,
      "step": 2560
    },
    {
      "epoch": 0.4253558424362794,
      "grad_norm": 32.36189270019531,
      "learning_rate": 4.562700692918709e-05,
      "loss": 0.9369,
      "step": 2570
    },
    {
      "epoch": 0.4270109235352532,
      "grad_norm": 34.64073944091797,
      "learning_rate": 4.560588135879669e-05,
      "loss": 1.0328,
      "step": 2580
    },
    {
      "epoch": 0.4286660046342271,
      "grad_norm": 47.48529052734375,
      "learning_rate": 4.558475578840629e-05,
      "loss": 1.1025,
      "step": 2590
    },
    {
      "epoch": 0.43032108573320094,
      "grad_norm": 19.65098762512207,
      "learning_rate": 4.556363021801589e-05,
      "loss": 0.9529,
      "step": 2600
    },
    {
      "epoch": 0.43197616683217477,
      "grad_norm": 32.361305236816406,
      "learning_rate": 4.554250464762549e-05,
      "loss": 1.0927,
      "step": 2610
    },
    {
      "epoch": 0.4336312479311486,
      "grad_norm": 18.194734573364258,
      "learning_rate": 4.552137907723509e-05,
      "loss": 1.1426,
      "step": 2620
    },
    {
      "epoch": 0.4352863290301225,
      "grad_norm": 37.406986236572266,
      "learning_rate": 4.550025350684469e-05,
      "loss": 1.2334,
      "step": 2630
    },
    {
      "epoch": 0.4369414101290963,
      "grad_norm": 66.44937133789062,
      "learning_rate": 4.547912793645429e-05,
      "loss": 1.1419,
      "step": 2640
    },
    {
      "epoch": 0.43859649122807015,
      "grad_norm": 163.84278869628906,
      "learning_rate": 4.5458002366063885e-05,
      "loss": 1.1752,
      "step": 2650
    },
    {
      "epoch": 0.44025157232704404,
      "grad_norm": 40.34383010864258,
      "learning_rate": 4.5436876795673484e-05,
      "loss": 1.0691,
      "step": 2660
    },
    {
      "epoch": 0.44190665342601787,
      "grad_norm": 59.469512939453125,
      "learning_rate": 4.541575122528309e-05,
      "loss": 1.2363,
      "step": 2670
    },
    {
      "epoch": 0.4435617345249917,
      "grad_norm": 29.32086753845215,
      "learning_rate": 4.539462565489269e-05,
      "loss": 1.2027,
      "step": 2680
    },
    {
      "epoch": 0.4452168156239656,
      "grad_norm": 30.00234603881836,
      "learning_rate": 4.5373500084502286e-05,
      "loss": 0.9681,
      "step": 2690
    },
    {
      "epoch": 0.4468718967229394,
      "grad_norm": 39.90174865722656,
      "learning_rate": 4.5352374514111884e-05,
      "loss": 1.0401,
      "step": 2700
    },
    {
      "epoch": 0.44852697782191325,
      "grad_norm": 48.75502395629883,
      "learning_rate": 4.533124894372148e-05,
      "loss": 1.2143,
      "step": 2710
    },
    {
      "epoch": 0.45018205892088714,
      "grad_norm": 70.19911193847656,
      "learning_rate": 4.531012337333108e-05,
      "loss": 0.7516,
      "step": 2720
    },
    {
      "epoch": 0.45183714001986097,
      "grad_norm": 64.30555725097656,
      "learning_rate": 4.528899780294068e-05,
      "loss": 0.8186,
      "step": 2730
    },
    {
      "epoch": 0.4534922211188348,
      "grad_norm": 43.67957305908203,
      "learning_rate": 4.526787223255028e-05,
      "loss": 0.8879,
      "step": 2740
    },
    {
      "epoch": 0.4551473022178087,
      "grad_norm": 23.11676597595215,
      "learning_rate": 4.5246746662159876e-05,
      "loss": 1.4472,
      "step": 2750
    },
    {
      "epoch": 0.4568023833167825,
      "grad_norm": 31.11016845703125,
      "learning_rate": 4.522562109176948e-05,
      "loss": 1.0396,
      "step": 2760
    },
    {
      "epoch": 0.45845746441575635,
      "grad_norm": 12.208727836608887,
      "learning_rate": 4.520449552137908e-05,
      "loss": 0.7298,
      "step": 2770
    },
    {
      "epoch": 0.46011254551473024,
      "grad_norm": 43.10711669921875,
      "learning_rate": 4.518336995098868e-05,
      "loss": 1.3463,
      "step": 2780
    },
    {
      "epoch": 0.4617676266137041,
      "grad_norm": 16.272186279296875,
      "learning_rate": 4.5162244380598276e-05,
      "loss": 1.42,
      "step": 2790
    },
    {
      "epoch": 0.4634227077126779,
      "grad_norm": 37.473304748535156,
      "learning_rate": 4.5141118810207875e-05,
      "loss": 0.9635,
      "step": 2800
    },
    {
      "epoch": 0.4650777888116518,
      "grad_norm": 24.73524284362793,
      "learning_rate": 4.511999323981747e-05,
      "loss": 0.9983,
      "step": 2810
    },
    {
      "epoch": 0.4667328699106256,
      "grad_norm": 58.638614654541016,
      "learning_rate": 4.509886766942708e-05,
      "loss": 0.8488,
      "step": 2820
    },
    {
      "epoch": 0.46838795100959946,
      "grad_norm": 76.90815734863281,
      "learning_rate": 4.5077742099036676e-05,
      "loss": 0.7436,
      "step": 2830
    },
    {
      "epoch": 0.47004303210857334,
      "grad_norm": 58.285491943359375,
      "learning_rate": 4.505661652864628e-05,
      "loss": 0.6724,
      "step": 2840
    },
    {
      "epoch": 0.4716981132075472,
      "grad_norm": 55.37185287475586,
      "learning_rate": 4.503549095825588e-05,
      "loss": 1.0724,
      "step": 2850
    },
    {
      "epoch": 0.473353194306521,
      "grad_norm": 28.99238395690918,
      "learning_rate": 4.501436538786548e-05,
      "loss": 1.1103,
      "step": 2860
    },
    {
      "epoch": 0.4750082754054949,
      "grad_norm": 24.56254768371582,
      "learning_rate": 4.499323981747508e-05,
      "loss": 1.0626,
      "step": 2870
    },
    {
      "epoch": 0.4766633565044687,
      "grad_norm": 27.78445816040039,
      "learning_rate": 4.4972114247084675e-05,
      "loss": 0.9471,
      "step": 2880
    },
    {
      "epoch": 0.47831843760344256,
      "grad_norm": 30.564992904663086,
      "learning_rate": 4.4950988676694274e-05,
      "loss": 1.0842,
      "step": 2890
    },
    {
      "epoch": 0.4799735187024164,
      "grad_norm": 38.38351821899414,
      "learning_rate": 4.492986310630387e-05,
      "loss": 1.1071,
      "step": 2900
    },
    {
      "epoch": 0.4816285998013903,
      "grad_norm": 11.51467514038086,
      "learning_rate": 4.490873753591347e-05,
      "loss": 0.8578,
      "step": 2910
    },
    {
      "epoch": 0.4832836809003641,
      "grad_norm": 48.67621612548828,
      "learning_rate": 4.488761196552307e-05,
      "loss": 0.9966,
      "step": 2920
    },
    {
      "epoch": 0.48493876199933794,
      "grad_norm": 62.043243408203125,
      "learning_rate": 4.4866486395132674e-05,
      "loss": 1.3475,
      "step": 2930
    },
    {
      "epoch": 0.48659384309831183,
      "grad_norm": 18.47925567626953,
      "learning_rate": 4.484536082474227e-05,
      "loss": 0.986,
      "step": 2940
    },
    {
      "epoch": 0.48824892419728566,
      "grad_norm": 23.822511672973633,
      "learning_rate": 4.482423525435187e-05,
      "loss": 0.9373,
      "step": 2950
    },
    {
      "epoch": 0.4899040052962595,
      "grad_norm": 32.23102569580078,
      "learning_rate": 4.480310968396147e-05,
      "loss": 1.0518,
      "step": 2960
    },
    {
      "epoch": 0.4915590863952334,
      "grad_norm": 12.534701347351074,
      "learning_rate": 4.478198411357107e-05,
      "loss": 0.7396,
      "step": 2970
    },
    {
      "epoch": 0.4932141674942072,
      "grad_norm": 35.98786544799805,
      "learning_rate": 4.4760858543180666e-05,
      "loss": 1.0152,
      "step": 2980
    },
    {
      "epoch": 0.49486924859318104,
      "grad_norm": 11.84727668762207,
      "learning_rate": 4.4739732972790264e-05,
      "loss": 1.1239,
      "step": 2990
    },
    {
      "epoch": 0.49652432969215493,
      "grad_norm": 71.0889892578125,
      "learning_rate": 4.471860740239986e-05,
      "loss": 1.108,
      "step": 3000
    },
    {
      "epoch": 0.49817941079112876,
      "grad_norm": 28.665138244628906,
      "learning_rate": 4.469748183200947e-05,
      "loss": 1.1014,
      "step": 3010
    },
    {
      "epoch": 0.4998344918901026,
      "grad_norm": 24.225772857666016,
      "learning_rate": 4.4676356261619066e-05,
      "loss": 1.0068,
      "step": 3020
    },
    {
      "epoch": 0.5014895729890765,
      "grad_norm": 33.060577392578125,
      "learning_rate": 4.4655230691228664e-05,
      "loss": 1.1641,
      "step": 3030
    },
    {
      "epoch": 0.5031446540880503,
      "grad_norm": 22.529266357421875,
      "learning_rate": 4.463410512083826e-05,
      "loss": 0.8886,
      "step": 3040
    },
    {
      "epoch": 0.5047997351870241,
      "grad_norm": 36.67279052734375,
      "learning_rate": 4.461297955044786e-05,
      "loss": 1.0461,
      "step": 3050
    },
    {
      "epoch": 0.506454816285998,
      "grad_norm": 58.06285858154297,
      "learning_rate": 4.4591853980057466e-05,
      "loss": 1.0847,
      "step": 3060
    },
    {
      "epoch": 0.5081098973849718,
      "grad_norm": 44.26525115966797,
      "learning_rate": 4.4570728409667065e-05,
      "loss": 1.1326,
      "step": 3070
    },
    {
      "epoch": 0.5097649784839458,
      "grad_norm": 39.28578186035156,
      "learning_rate": 4.454960283927666e-05,
      "loss": 1.1133,
      "step": 3080
    },
    {
      "epoch": 0.5114200595829196,
      "grad_norm": 50.24728775024414,
      "learning_rate": 4.452847726888626e-05,
      "loss": 0.8941,
      "step": 3090
    },
    {
      "epoch": 0.5130751406818934,
      "grad_norm": 11.468934059143066,
      "learning_rate": 4.450735169849587e-05,
      "loss": 0.8624,
      "step": 3100
    },
    {
      "epoch": 0.5147302217808672,
      "grad_norm": 38.07728958129883,
      "learning_rate": 4.4486226128105465e-05,
      "loss": 1.0228,
      "step": 3110
    },
    {
      "epoch": 0.5163853028798411,
      "grad_norm": 46.164031982421875,
      "learning_rate": 4.4465100557715063e-05,
      "loss": 0.976,
      "step": 3120
    },
    {
      "epoch": 0.5180403839788149,
      "grad_norm": 53.20150375366211,
      "learning_rate": 4.444397498732466e-05,
      "loss": 1.1195,
      "step": 3130
    },
    {
      "epoch": 0.5196954650777889,
      "grad_norm": 21.280811309814453,
      "learning_rate": 4.442284941693426e-05,
      "loss": 0.9734,
      "step": 3140
    },
    {
      "epoch": 0.5213505461767627,
      "grad_norm": 3.9784908294677734,
      "learning_rate": 4.440172384654386e-05,
      "loss": 0.8636,
      "step": 3150
    },
    {
      "epoch": 0.5230056272757365,
      "grad_norm": 33.209293365478516,
      "learning_rate": 4.438059827615346e-05,
      "loss": 1.2984,
      "step": 3160
    },
    {
      "epoch": 0.5246607083747103,
      "grad_norm": 42.088531494140625,
      "learning_rate": 4.4359472705763055e-05,
      "loss": 1.206,
      "step": 3170
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 24.37703514099121,
      "learning_rate": 4.433834713537266e-05,
      "loss": 1.2237,
      "step": 3180
    },
    {
      "epoch": 0.527970870572658,
      "grad_norm": 122.7417984008789,
      "learning_rate": 4.431722156498226e-05,
      "loss": 0.8702,
      "step": 3190
    },
    {
      "epoch": 0.529625951671632,
      "grad_norm": 54.10929870605469,
      "learning_rate": 4.429609599459186e-05,
      "loss": 1.3202,
      "step": 3200
    },
    {
      "epoch": 0.5312810327706058,
      "grad_norm": 65.12776184082031,
      "learning_rate": 4.4274970424201456e-05,
      "loss": 0.9525,
      "step": 3210
    },
    {
      "epoch": 0.5329361138695796,
      "grad_norm": 47.61336898803711,
      "learning_rate": 4.4253844853811054e-05,
      "loss": 1.0271,
      "step": 3220
    },
    {
      "epoch": 0.5345911949685535,
      "grad_norm": 27.298389434814453,
      "learning_rate": 4.423271928342065e-05,
      "loss": 1.219,
      "step": 3230
    },
    {
      "epoch": 0.5362462760675273,
      "grad_norm": 43.79317092895508,
      "learning_rate": 4.421159371303025e-05,
      "loss": 1.0317,
      "step": 3240
    },
    {
      "epoch": 0.5379013571665011,
      "grad_norm": 27.77593994140625,
      "learning_rate": 4.419046814263985e-05,
      "loss": 1.1975,
      "step": 3250
    },
    {
      "epoch": 0.5395564382654751,
      "grad_norm": 3.324611186981201,
      "learning_rate": 4.416934257224945e-05,
      "loss": 0.7069,
      "step": 3260
    },
    {
      "epoch": 0.5412115193644489,
      "grad_norm": 24.413789749145508,
      "learning_rate": 4.414821700185905e-05,
      "loss": 1.3402,
      "step": 3270
    },
    {
      "epoch": 0.5428666004634227,
      "grad_norm": 33.32161331176758,
      "learning_rate": 4.412709143146865e-05,
      "loss": 1.2173,
      "step": 3280
    },
    {
      "epoch": 0.5445216815623966,
      "grad_norm": 51.53544998168945,
      "learning_rate": 4.410596586107825e-05,
      "loss": 1.2179,
      "step": 3290
    },
    {
      "epoch": 0.5461767626613704,
      "grad_norm": 18.494131088256836,
      "learning_rate": 4.408484029068785e-05,
      "loss": 0.6608,
      "step": 3300
    },
    {
      "epoch": 0.5478318437603442,
      "grad_norm": 17.877620697021484,
      "learning_rate": 4.406371472029745e-05,
      "loss": 0.8259,
      "step": 3310
    },
    {
      "epoch": 0.5494869248593182,
      "grad_norm": 33.14328384399414,
      "learning_rate": 4.404258914990705e-05,
      "loss": 0.989,
      "step": 3320
    },
    {
      "epoch": 0.551142005958292,
      "grad_norm": 46.79896926879883,
      "learning_rate": 4.402146357951665e-05,
      "loss": 1.2171,
      "step": 3330
    },
    {
      "epoch": 0.5527970870572658,
      "grad_norm": 14.918326377868652,
      "learning_rate": 4.400033800912625e-05,
      "loss": 0.9357,
      "step": 3340
    },
    {
      "epoch": 0.5544521681562397,
      "grad_norm": 11.87060260772705,
      "learning_rate": 4.397921243873585e-05,
      "loss": 0.656,
      "step": 3350
    },
    {
      "epoch": 0.5561072492552135,
      "grad_norm": 39.53138732910156,
      "learning_rate": 4.395808686834545e-05,
      "loss": 1.0588,
      "step": 3360
    },
    {
      "epoch": 0.5577623303541873,
      "grad_norm": 77.6053695678711,
      "learning_rate": 4.393696129795505e-05,
      "loss": 0.856,
      "step": 3370
    },
    {
      "epoch": 0.5594174114531612,
      "grad_norm": 15.92467212677002,
      "learning_rate": 4.391583572756465e-05,
      "loss": 0.985,
      "step": 3380
    },
    {
      "epoch": 0.5610724925521351,
      "grad_norm": 26.090070724487305,
      "learning_rate": 4.389471015717425e-05,
      "loss": 0.9932,
      "step": 3390
    },
    {
      "epoch": 0.5627275736511089,
      "grad_norm": 52.27733612060547,
      "learning_rate": 4.3873584586783845e-05,
      "loss": 0.7681,
      "step": 3400
    },
    {
      "epoch": 0.5643826547500828,
      "grad_norm": 62.16057586669922,
      "learning_rate": 4.3852459016393444e-05,
      "loss": 1.2947,
      "step": 3410
    },
    {
      "epoch": 0.5660377358490566,
      "grad_norm": 38.06724166870117,
      "learning_rate": 4.383133344600304e-05,
      "loss": 1.0936,
      "step": 3420
    },
    {
      "epoch": 0.5676928169480304,
      "grad_norm": 17.531719207763672,
      "learning_rate": 4.381020787561264e-05,
      "loss": 0.8624,
      "step": 3430
    },
    {
      "epoch": 0.5693478980470043,
      "grad_norm": 24.88350486755371,
      "learning_rate": 4.3789082305222246e-05,
      "loss": 1.0214,
      "step": 3440
    },
    {
      "epoch": 0.5710029791459782,
      "grad_norm": 55.01778793334961,
      "learning_rate": 4.3767956734831844e-05,
      "loss": 1.106,
      "step": 3450
    },
    {
      "epoch": 0.572658060244952,
      "grad_norm": 27.850460052490234,
      "learning_rate": 4.374683116444144e-05,
      "loss": 1.0337,
      "step": 3460
    },
    {
      "epoch": 0.5743131413439259,
      "grad_norm": 46.280765533447266,
      "learning_rate": 4.372570559405104e-05,
      "loss": 1.2113,
      "step": 3470
    },
    {
      "epoch": 0.5759682224428997,
      "grad_norm": 29.35649299621582,
      "learning_rate": 4.370458002366064e-05,
      "loss": 1.4945,
      "step": 3480
    },
    {
      "epoch": 0.5776233035418735,
      "grad_norm": 27.5532283782959,
      "learning_rate": 4.368345445327024e-05,
      "loss": 0.8778,
      "step": 3490
    },
    {
      "epoch": 0.5792783846408474,
      "grad_norm": 30.413143157958984,
      "learning_rate": 4.3662328882879836e-05,
      "loss": 0.7766,
      "step": 3500
    },
    {
      "epoch": 0.5809334657398213,
      "grad_norm": 48.63931655883789,
      "learning_rate": 4.3641203312489434e-05,
      "loss": 0.7546,
      "step": 3510
    },
    {
      "epoch": 0.5825885468387951,
      "grad_norm": 26.924457550048828,
      "learning_rate": 4.362007774209904e-05,
      "loss": 0.9927,
      "step": 3520
    },
    {
      "epoch": 0.584243627937769,
      "grad_norm": 23.608797073364258,
      "learning_rate": 4.359895217170864e-05,
      "loss": 1.1612,
      "step": 3530
    },
    {
      "epoch": 0.5858987090367428,
      "grad_norm": 30.908000946044922,
      "learning_rate": 4.3577826601318236e-05,
      "loss": 1.1725,
      "step": 3540
    },
    {
      "epoch": 0.5875537901357166,
      "grad_norm": 24.111806869506836,
      "learning_rate": 4.3556701030927835e-05,
      "loss": 1.139,
      "step": 3550
    },
    {
      "epoch": 0.5892088712346905,
      "grad_norm": 24.285385131835938,
      "learning_rate": 4.353557546053744e-05,
      "loss": 1.1201,
      "step": 3560
    },
    {
      "epoch": 0.5908639523336644,
      "grad_norm": 35.23525619506836,
      "learning_rate": 4.351444989014704e-05,
      "loss": 0.8126,
      "step": 3570
    },
    {
      "epoch": 0.5925190334326382,
      "grad_norm": 25.917625427246094,
      "learning_rate": 4.3493324319756637e-05,
      "loss": 0.6303,
      "step": 3580
    },
    {
      "epoch": 0.5941741145316121,
      "grad_norm": 20.659053802490234,
      "learning_rate": 4.3472198749366235e-05,
      "loss": 1.1397,
      "step": 3590
    },
    {
      "epoch": 0.5958291956305859,
      "grad_norm": 21.246225357055664,
      "learning_rate": 4.345107317897583e-05,
      "loss": 1.0096,
      "step": 3600
    },
    {
      "epoch": 0.5974842767295597,
      "grad_norm": 53.92167663574219,
      "learning_rate": 4.342994760858544e-05,
      "loss": 0.6409,
      "step": 3610
    },
    {
      "epoch": 0.5991393578285336,
      "grad_norm": 58.40272903442383,
      "learning_rate": 4.340882203819504e-05,
      "loss": 1.1804,
      "step": 3620
    },
    {
      "epoch": 0.6007944389275075,
      "grad_norm": 49.088958740234375,
      "learning_rate": 4.3387696467804635e-05,
      "loss": 1.1878,
      "step": 3630
    },
    {
      "epoch": 0.6024495200264813,
      "grad_norm": 61.187171936035156,
      "learning_rate": 4.3366570897414234e-05,
      "loss": 1.1599,
      "step": 3640
    },
    {
      "epoch": 0.6041046011254552,
      "grad_norm": 32.11727523803711,
      "learning_rate": 4.334544532702383e-05,
      "loss": 0.9688,
      "step": 3650
    },
    {
      "epoch": 0.605759682224429,
      "grad_norm": 40.61669158935547,
      "learning_rate": 4.332431975663343e-05,
      "loss": 0.9756,
      "step": 3660
    },
    {
      "epoch": 0.6074147633234028,
      "grad_norm": 30.08391761779785,
      "learning_rate": 4.330319418624303e-05,
      "loss": 1.0081,
      "step": 3670
    },
    {
      "epoch": 0.6090698444223767,
      "grad_norm": 30.464418411254883,
      "learning_rate": 4.328206861585263e-05,
      "loss": 0.8904,
      "step": 3680
    },
    {
      "epoch": 0.6107249255213505,
      "grad_norm": 61.88846969604492,
      "learning_rate": 4.326094304546223e-05,
      "loss": 0.9793,
      "step": 3690
    },
    {
      "epoch": 0.6123800066203244,
      "grad_norm": 40.16982650756836,
      "learning_rate": 4.323981747507183e-05,
      "loss": 0.8866,
      "step": 3700
    },
    {
      "epoch": 0.6140350877192983,
      "grad_norm": 35.0702018737793,
      "learning_rate": 4.321869190468143e-05,
      "loss": 0.9236,
      "step": 3710
    },
    {
      "epoch": 0.6156901688182721,
      "grad_norm": 108.08277893066406,
      "learning_rate": 4.319756633429103e-05,
      "loss": 0.9811,
      "step": 3720
    },
    {
      "epoch": 0.6173452499172459,
      "grad_norm": 14.359704971313477,
      "learning_rate": 4.3176440763900626e-05,
      "loss": 1.0931,
      "step": 3730
    },
    {
      "epoch": 0.6190003310162198,
      "grad_norm": 44.02459716796875,
      "learning_rate": 4.3155315193510224e-05,
      "loss": 0.9219,
      "step": 3740
    },
    {
      "epoch": 0.6206554121151936,
      "grad_norm": 33.16947937011719,
      "learning_rate": 4.313418962311982e-05,
      "loss": 1.0206,
      "step": 3750
    },
    {
      "epoch": 0.6223104932141675,
      "grad_norm": 33.4658203125,
      "learning_rate": 4.311306405272942e-05,
      "loss": 1.187,
      "step": 3760
    },
    {
      "epoch": 0.6239655743131414,
      "grad_norm": 21.02299690246582,
      "learning_rate": 4.309193848233902e-05,
      "loss": 0.8634,
      "step": 3770
    },
    {
      "epoch": 0.6256206554121152,
      "grad_norm": 14.111098289489746,
      "learning_rate": 4.3070812911948625e-05,
      "loss": 0.9826,
      "step": 3780
    },
    {
      "epoch": 0.627275736511089,
      "grad_norm": 36.8897819519043,
      "learning_rate": 4.304968734155822e-05,
      "loss": 1.0663,
      "step": 3790
    },
    {
      "epoch": 0.6289308176100629,
      "grad_norm": 28.036787033081055,
      "learning_rate": 4.302856177116782e-05,
      "loss": 0.9095,
      "step": 3800
    },
    {
      "epoch": 0.6305858987090367,
      "grad_norm": 40.62337875366211,
      "learning_rate": 4.3007436200777426e-05,
      "loss": 1.0071,
      "step": 3810
    },
    {
      "epoch": 0.6322409798080106,
      "grad_norm": 18.000431060791016,
      "learning_rate": 4.2986310630387025e-05,
      "loss": 0.8433,
      "step": 3820
    },
    {
      "epoch": 0.6338960609069845,
      "grad_norm": 29.96490478515625,
      "learning_rate": 4.296518505999662e-05,
      "loss": 1.0397,
      "step": 3830
    },
    {
      "epoch": 0.6355511420059583,
      "grad_norm": 48.65170669555664,
      "learning_rate": 4.294405948960622e-05,
      "loss": 0.8542,
      "step": 3840
    },
    {
      "epoch": 0.6372062231049321,
      "grad_norm": 28.370372772216797,
      "learning_rate": 4.292293391921582e-05,
      "loss": 1.4465,
      "step": 3850
    },
    {
      "epoch": 0.638861304203906,
      "grad_norm": 23.024667739868164,
      "learning_rate": 4.2901808348825425e-05,
      "loss": 0.8225,
      "step": 3860
    },
    {
      "epoch": 0.6405163853028798,
      "grad_norm": 22.802209854125977,
      "learning_rate": 4.2880682778435024e-05,
      "loss": 0.9894,
      "step": 3870
    },
    {
      "epoch": 0.6421714664018537,
      "grad_norm": 43.88615798950195,
      "learning_rate": 4.285955720804462e-05,
      "loss": 0.9835,
      "step": 3880
    },
    {
      "epoch": 0.6438265475008276,
      "grad_norm": 27.256874084472656,
      "learning_rate": 4.283843163765422e-05,
      "loss": 0.6628,
      "step": 3890
    },
    {
      "epoch": 0.6454816285998014,
      "grad_norm": 30.44654083251953,
      "learning_rate": 4.281730606726382e-05,
      "loss": 0.9703,
      "step": 3900
    },
    {
      "epoch": 0.6471367096987752,
      "grad_norm": 28.35403060913086,
      "learning_rate": 4.279618049687342e-05,
      "loss": 1.0148,
      "step": 3910
    },
    {
      "epoch": 0.6487917907977491,
      "grad_norm": 55.94342803955078,
      "learning_rate": 4.2775054926483015e-05,
      "loss": 0.9131,
      "step": 3920
    },
    {
      "epoch": 0.6504468718967229,
      "grad_norm": 30.77705192565918,
      "learning_rate": 4.2753929356092614e-05,
      "loss": 1.1561,
      "step": 3930
    },
    {
      "epoch": 0.6521019529956968,
      "grad_norm": 21.770986557006836,
      "learning_rate": 4.273280378570221e-05,
      "loss": 1.1906,
      "step": 3940
    },
    {
      "epoch": 0.6537570340946707,
      "grad_norm": 33.03350830078125,
      "learning_rate": 4.271167821531182e-05,
      "loss": 1.3382,
      "step": 3950
    },
    {
      "epoch": 0.6554121151936445,
      "grad_norm": 34.91267013549805,
      "learning_rate": 4.2690552644921416e-05,
      "loss": 0.9367,
      "step": 3960
    },
    {
      "epoch": 0.6570671962926183,
      "grad_norm": 40.27107238769531,
      "learning_rate": 4.2669427074531014e-05,
      "loss": 1.0694,
      "step": 3970
    },
    {
      "epoch": 0.6587222773915922,
      "grad_norm": 23.485074996948242,
      "learning_rate": 4.264830150414061e-05,
      "loss": 1.2919,
      "step": 3980
    },
    {
      "epoch": 0.660377358490566,
      "grad_norm": 73.80374908447266,
      "learning_rate": 4.262717593375021e-05,
      "loss": 0.9429,
      "step": 3990
    },
    {
      "epoch": 0.6620324395895398,
      "grad_norm": 29.777626037597656,
      "learning_rate": 4.260605036335981e-05,
      "loss": 1.5669,
      "step": 4000
    },
    {
      "epoch": 0.6636875206885138,
      "grad_norm": 35.779815673828125,
      "learning_rate": 4.258492479296941e-05,
      "loss": 0.9587,
      "step": 4010
    },
    {
      "epoch": 0.6653426017874876,
      "grad_norm": 30.935335159301758,
      "learning_rate": 4.2563799222579006e-05,
      "loss": 0.8064,
      "step": 4020
    },
    {
      "epoch": 0.6669976828864614,
      "grad_norm": 19.49384117126465,
      "learning_rate": 4.254267365218861e-05,
      "loss": 0.94,
      "step": 4030
    },
    {
      "epoch": 0.6686527639854353,
      "grad_norm": 37.684349060058594,
      "learning_rate": 4.252154808179821e-05,
      "loss": 1.1691,
      "step": 4040
    },
    {
      "epoch": 0.6703078450844091,
      "grad_norm": 47.306121826171875,
      "learning_rate": 4.2500422511407815e-05,
      "loss": 0.8677,
      "step": 4050
    },
    {
      "epoch": 0.6719629261833829,
      "grad_norm": 25.551767349243164,
      "learning_rate": 4.247929694101741e-05,
      "loss": 1.3394,
      "step": 4060
    },
    {
      "epoch": 0.6736180072823569,
      "grad_norm": 65.29220581054688,
      "learning_rate": 4.245817137062701e-05,
      "loss": 1.1042,
      "step": 4070
    },
    {
      "epoch": 0.6752730883813307,
      "grad_norm": 32.81867599487305,
      "learning_rate": 4.243704580023661e-05,
      "loss": 0.9489,
      "step": 4080
    },
    {
      "epoch": 0.6769281694803045,
      "grad_norm": 50.70890808105469,
      "learning_rate": 4.241592022984621e-05,
      "loss": 0.729,
      "step": 4090
    },
    {
      "epoch": 0.6785832505792784,
      "grad_norm": 35.11899948120117,
      "learning_rate": 4.239479465945581e-05,
      "loss": 1.2056,
      "step": 4100
    },
    {
      "epoch": 0.6802383316782522,
      "grad_norm": 16.49527931213379,
      "learning_rate": 4.2373669089065405e-05,
      "loss": 1.2025,
      "step": 4110
    },
    {
      "epoch": 0.681893412777226,
      "grad_norm": 24.486730575561523,
      "learning_rate": 4.235254351867501e-05,
      "loss": 0.9311,
      "step": 4120
    },
    {
      "epoch": 0.6835484938762,
      "grad_norm": 24.59572410583496,
      "learning_rate": 4.233141794828461e-05,
      "loss": 1.1472,
      "step": 4130
    },
    {
      "epoch": 0.6852035749751738,
      "grad_norm": 15.923365592956543,
      "learning_rate": 4.231029237789421e-05,
      "loss": 0.9433,
      "step": 4140
    },
    {
      "epoch": 0.6868586560741476,
      "grad_norm": 45.41712951660156,
      "learning_rate": 4.2289166807503805e-05,
      "loss": 1.0075,
      "step": 4150
    },
    {
      "epoch": 0.6885137371731215,
      "grad_norm": 29.924531936645508,
      "learning_rate": 4.2268041237113404e-05,
      "loss": 0.9492,
      "step": 4160
    },
    {
      "epoch": 0.6901688182720953,
      "grad_norm": 55.1723518371582,
      "learning_rate": 4.2246915666723e-05,
      "loss": 0.9912,
      "step": 4170
    },
    {
      "epoch": 0.6918238993710691,
      "grad_norm": 22.32375717163086,
      "learning_rate": 4.22257900963326e-05,
      "loss": 0.9665,
      "step": 4180
    },
    {
      "epoch": 0.6934789804700431,
      "grad_norm": 8.212151527404785,
      "learning_rate": 4.22046645259422e-05,
      "loss": 0.5996,
      "step": 4190
    },
    {
      "epoch": 0.6951340615690169,
      "grad_norm": 17.61923599243164,
      "learning_rate": 4.2183538955551804e-05,
      "loss": 0.9646,
      "step": 4200
    },
    {
      "epoch": 0.6967891426679907,
      "grad_norm": 24.540983200073242,
      "learning_rate": 4.21624133851614e-05,
      "loss": 1.1525,
      "step": 4210
    },
    {
      "epoch": 0.6984442237669646,
      "grad_norm": 20.798940658569336,
      "learning_rate": 4.2141287814771e-05,
      "loss": 0.9483,
      "step": 4220
    },
    {
      "epoch": 0.7000993048659384,
      "grad_norm": 21.86621856689453,
      "learning_rate": 4.21201622443806e-05,
      "loss": 0.9096,
      "step": 4230
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 49.925994873046875,
      "learning_rate": 4.20990366739902e-05,
      "loss": 0.7554,
      "step": 4240
    },
    {
      "epoch": 0.7034094670638862,
      "grad_norm": 16.87919044494629,
      "learning_rate": 4.2077911103599796e-05,
      "loss": 1.0287,
      "step": 4250
    },
    {
      "epoch": 0.70506454816286,
      "grad_norm": 41.30405807495117,
      "learning_rate": 4.2056785533209394e-05,
      "loss": 1.0356,
      "step": 4260
    },
    {
      "epoch": 0.7067196292618338,
      "grad_norm": 21.572566986083984,
      "learning_rate": 4.2035659962819e-05,
      "loss": 0.7868,
      "step": 4270
    },
    {
      "epoch": 0.7083747103608077,
      "grad_norm": 115.95557403564453,
      "learning_rate": 4.20145343924286e-05,
      "loss": 1.1376,
      "step": 4280
    },
    {
      "epoch": 0.7100297914597815,
      "grad_norm": 18.10597801208496,
      "learning_rate": 4.1993408822038196e-05,
      "loss": 0.84,
      "step": 4290
    },
    {
      "epoch": 0.7116848725587553,
      "grad_norm": 23.530176162719727,
      "learning_rate": 4.19722832516478e-05,
      "loss": 1.0425,
      "step": 4300
    },
    {
      "epoch": 0.7133399536577292,
      "grad_norm": 18.510290145874023,
      "learning_rate": 4.19511576812574e-05,
      "loss": 0.8814,
      "step": 4310
    },
    {
      "epoch": 0.7149950347567031,
      "grad_norm": 29.641555786132812,
      "learning_rate": 4.1930032110867e-05,
      "loss": 0.9694,
      "step": 4320
    },
    {
      "epoch": 0.716650115855677,
      "grad_norm": 25.45354461669922,
      "learning_rate": 4.1908906540476597e-05,
      "loss": 0.9587,
      "step": 4330
    },
    {
      "epoch": 0.7183051969546508,
      "grad_norm": 54.723060607910156,
      "learning_rate": 4.1887780970086195e-05,
      "loss": 0.7461,
      "step": 4340
    },
    {
      "epoch": 0.7199602780536246,
      "grad_norm": 19.017486572265625,
      "learning_rate": 4.186665539969579e-05,
      "loss": 0.5914,
      "step": 4350
    },
    {
      "epoch": 0.7216153591525984,
      "grad_norm": 33.76288604736328,
      "learning_rate": 4.184552982930539e-05,
      "loss": 0.9077,
      "step": 4360
    },
    {
      "epoch": 0.7232704402515723,
      "grad_norm": 10.511170387268066,
      "learning_rate": 4.1824404258915e-05,
      "loss": 1.1006,
      "step": 4370
    },
    {
      "epoch": 0.7249255213505462,
      "grad_norm": 27.957937240600586,
      "learning_rate": 4.1803278688524595e-05,
      "loss": 0.6678,
      "step": 4380
    },
    {
      "epoch": 0.72658060244952,
      "grad_norm": 28.102434158325195,
      "learning_rate": 4.1782153118134194e-05,
      "loss": 0.8939,
      "step": 4390
    },
    {
      "epoch": 0.7282356835484939,
      "grad_norm": 33.85995101928711,
      "learning_rate": 4.176102754774379e-05,
      "loss": 1.0002,
      "step": 4400
    },
    {
      "epoch": 0.7298907646474677,
      "grad_norm": 23.318620681762695,
      "learning_rate": 4.173990197735339e-05,
      "loss": 0.8716,
      "step": 4410
    },
    {
      "epoch": 0.7315458457464415,
      "grad_norm": 39.76333236694336,
      "learning_rate": 4.171877640696299e-05,
      "loss": 0.773,
      "step": 4420
    },
    {
      "epoch": 0.7332009268454154,
      "grad_norm": 32.521297454833984,
      "learning_rate": 4.169765083657259e-05,
      "loss": 0.8318,
      "step": 4430
    },
    {
      "epoch": 0.7348560079443893,
      "grad_norm": 42.59047317504883,
      "learning_rate": 4.1676525266182186e-05,
      "loss": 1.2121,
      "step": 4440
    },
    {
      "epoch": 0.7365110890433632,
      "grad_norm": 20.450075149536133,
      "learning_rate": 4.1655399695791784e-05,
      "loss": 1.0118,
      "step": 4450
    },
    {
      "epoch": 0.738166170142337,
      "grad_norm": 41.27296447753906,
      "learning_rate": 4.163427412540139e-05,
      "loss": 1.0816,
      "step": 4460
    },
    {
      "epoch": 0.7398212512413108,
      "grad_norm": 81.6081314086914,
      "learning_rate": 4.161314855501099e-05,
      "loss": 0.9787,
      "step": 4470
    },
    {
      "epoch": 0.7414763323402846,
      "grad_norm": 8.307270050048828,
      "learning_rate": 4.1592022984620586e-05,
      "loss": 0.9335,
      "step": 4480
    },
    {
      "epoch": 0.7431314134392585,
      "grad_norm": 37.8497314453125,
      "learning_rate": 4.1570897414230184e-05,
      "loss": 0.9319,
      "step": 4490
    },
    {
      "epoch": 0.7447864945382324,
      "grad_norm": 16.711477279663086,
      "learning_rate": 4.154977184383978e-05,
      "loss": 0.8074,
      "step": 4500
    },
    {
      "epoch": 0.7464415756372063,
      "grad_norm": 26.266727447509766,
      "learning_rate": 4.152864627344938e-05,
      "loss": 1.0552,
      "step": 4510
    },
    {
      "epoch": 0.7480966567361801,
      "grad_norm": 23.072359085083008,
      "learning_rate": 4.1507520703058986e-05,
      "loss": 0.8194,
      "step": 4520
    },
    {
      "epoch": 0.7497517378351539,
      "grad_norm": 16.523712158203125,
      "learning_rate": 4.1486395132668585e-05,
      "loss": 1.0683,
      "step": 4530
    },
    {
      "epoch": 0.7514068189341278,
      "grad_norm": 65.1489486694336,
      "learning_rate": 4.146526956227818e-05,
      "loss": 0.5869,
      "step": 4540
    },
    {
      "epoch": 0.7530619000331016,
      "grad_norm": 30.347267150878906,
      "learning_rate": 4.144414399188779e-05,
      "loss": 1.1597,
      "step": 4550
    },
    {
      "epoch": 0.7547169811320755,
      "grad_norm": 36.275550842285156,
      "learning_rate": 4.1423018421497387e-05,
      "loss": 0.9384,
      "step": 4560
    },
    {
      "epoch": 0.7563720622310494,
      "grad_norm": 47.993350982666016,
      "learning_rate": 4.1401892851106985e-05,
      "loss": 0.7494,
      "step": 4570
    },
    {
      "epoch": 0.7580271433300232,
      "grad_norm": 58.59830856323242,
      "learning_rate": 4.138076728071658e-05,
      "loss": 0.9838,
      "step": 4580
    },
    {
      "epoch": 0.759682224428997,
      "grad_norm": 41.1681022644043,
      "learning_rate": 4.135964171032618e-05,
      "loss": 0.8667,
      "step": 4590
    },
    {
      "epoch": 0.7613373055279709,
      "grad_norm": 42.74210739135742,
      "learning_rate": 4.133851613993578e-05,
      "loss": 1.3909,
      "step": 4600
    },
    {
      "epoch": 0.7629923866269447,
      "grad_norm": 17.520587921142578,
      "learning_rate": 4.131739056954538e-05,
      "loss": 1.1192,
      "step": 4610
    },
    {
      "epoch": 0.7646474677259185,
      "grad_norm": 25.94455909729004,
      "learning_rate": 4.129626499915498e-05,
      "loss": 0.6019,
      "step": 4620
    },
    {
      "epoch": 0.7663025488248925,
      "grad_norm": 23.717060089111328,
      "learning_rate": 4.127513942876458e-05,
      "loss": 1.2677,
      "step": 4630
    },
    {
      "epoch": 0.7679576299238663,
      "grad_norm": 35.15800476074219,
      "learning_rate": 4.125401385837418e-05,
      "loss": 1.3152,
      "step": 4640
    },
    {
      "epoch": 0.7696127110228401,
      "grad_norm": 24.99532127380371,
      "learning_rate": 4.123288828798378e-05,
      "loss": 0.6985,
      "step": 4650
    },
    {
      "epoch": 0.771267792121814,
      "grad_norm": 65.27812194824219,
      "learning_rate": 4.121176271759338e-05,
      "loss": 0.845,
      "step": 4660
    },
    {
      "epoch": 0.7729228732207878,
      "grad_norm": 18.420948028564453,
      "learning_rate": 4.1190637147202976e-05,
      "loss": 0.9871,
      "step": 4670
    },
    {
      "epoch": 0.7745779543197616,
      "grad_norm": 12.860443115234375,
      "learning_rate": 4.1169511576812574e-05,
      "loss": 0.9623,
      "step": 4680
    },
    {
      "epoch": 0.7762330354187356,
      "grad_norm": 5.086736679077148,
      "learning_rate": 4.114838600642217e-05,
      "loss": 0.4524,
      "step": 4690
    },
    {
      "epoch": 0.7778881165177094,
      "grad_norm": 16.871694564819336,
      "learning_rate": 4.112726043603177e-05,
      "loss": 0.9623,
      "step": 4700
    },
    {
      "epoch": 0.7795431976166832,
      "grad_norm": 45.56202697753906,
      "learning_rate": 4.1106134865641376e-05,
      "loss": 0.9394,
      "step": 4710
    },
    {
      "epoch": 0.7811982787156571,
      "grad_norm": 51.37690734863281,
      "learning_rate": 4.1085009295250974e-05,
      "loss": 1.1341,
      "step": 4720
    },
    {
      "epoch": 0.7828533598146309,
      "grad_norm": 29.63262367248535,
      "learning_rate": 4.106388372486057e-05,
      "loss": 0.742,
      "step": 4730
    },
    {
      "epoch": 0.7845084409136047,
      "grad_norm": 39.99495315551758,
      "learning_rate": 4.104275815447017e-05,
      "loss": 1.1854,
      "step": 4740
    },
    {
      "epoch": 0.7861635220125787,
      "grad_norm": 27.609437942504883,
      "learning_rate": 4.102163258407977e-05,
      "loss": 1.0299,
      "step": 4750
    },
    {
      "epoch": 0.7878186031115525,
      "grad_norm": 67.61388397216797,
      "learning_rate": 4.100050701368937e-05,
      "loss": 1.0392,
      "step": 4760
    },
    {
      "epoch": 0.7894736842105263,
      "grad_norm": 25.84587860107422,
      "learning_rate": 4.097938144329897e-05,
      "loss": 1.1141,
      "step": 4770
    },
    {
      "epoch": 0.7911287653095002,
      "grad_norm": 41.50617599487305,
      "learning_rate": 4.095825587290857e-05,
      "loss": 1.3059,
      "step": 4780
    },
    {
      "epoch": 0.792783846408474,
      "grad_norm": 23.353759765625,
      "learning_rate": 4.093713030251817e-05,
      "loss": 0.9132,
      "step": 4790
    },
    {
      "epoch": 0.7944389275074478,
      "grad_norm": 16.491046905517578,
      "learning_rate": 4.0916004732127775e-05,
      "loss": 0.7818,
      "step": 4800
    },
    {
      "epoch": 0.7960940086064218,
      "grad_norm": 24.868755340576172,
      "learning_rate": 4.089487916173737e-05,
      "loss": 0.8768,
      "step": 4810
    },
    {
      "epoch": 0.7977490897053956,
      "grad_norm": 37.912105560302734,
      "learning_rate": 4.087375359134697e-05,
      "loss": 1.1469,
      "step": 4820
    },
    {
      "epoch": 0.7994041708043694,
      "grad_norm": 47.86464309692383,
      "learning_rate": 4.085262802095657e-05,
      "loss": 1.1507,
      "step": 4830
    },
    {
      "epoch": 0.8010592519033433,
      "grad_norm": 30.63715362548828,
      "learning_rate": 4.083150245056617e-05,
      "loss": 0.7953,
      "step": 4840
    },
    {
      "epoch": 0.8027143330023171,
      "grad_norm": 37.060062408447266,
      "learning_rate": 4.081037688017577e-05,
      "loss": 1.2063,
      "step": 4850
    },
    {
      "epoch": 0.8043694141012909,
      "grad_norm": 24.586227416992188,
      "learning_rate": 4.0789251309785365e-05,
      "loss": 0.6582,
      "step": 4860
    },
    {
      "epoch": 0.8060244952002649,
      "grad_norm": 17.097469329833984,
      "learning_rate": 4.0768125739394963e-05,
      "loss": 0.8553,
      "step": 4870
    },
    {
      "epoch": 0.8076795762992387,
      "grad_norm": 26.19289207458496,
      "learning_rate": 4.074700016900457e-05,
      "loss": 0.7912,
      "step": 4880
    },
    {
      "epoch": 0.8093346573982125,
      "grad_norm": 33.3283805847168,
      "learning_rate": 4.072587459861417e-05,
      "loss": 0.8896,
      "step": 4890
    },
    {
      "epoch": 0.8109897384971864,
      "grad_norm": 25.70931053161621,
      "learning_rate": 4.0704749028223765e-05,
      "loss": 1.046,
      "step": 4900
    },
    {
      "epoch": 0.8126448195961602,
      "grad_norm": 31.056930541992188,
      "learning_rate": 4.0683623457833364e-05,
      "loss": 0.8603,
      "step": 4910
    },
    {
      "epoch": 0.814299900695134,
      "grad_norm": 34.41746139526367,
      "learning_rate": 4.066249788744296e-05,
      "loss": 1.0313,
      "step": 4920
    },
    {
      "epoch": 0.8159549817941079,
      "grad_norm": 22.61565399169922,
      "learning_rate": 4.064137231705256e-05,
      "loss": 0.6345,
      "step": 4930
    },
    {
      "epoch": 0.8176100628930818,
      "grad_norm": 57.303367614746094,
      "learning_rate": 4.062024674666216e-05,
      "loss": 0.8763,
      "step": 4940
    },
    {
      "epoch": 0.8192651439920556,
      "grad_norm": 14.989606857299805,
      "learning_rate": 4.059912117627176e-05,
      "loss": 1.01,
      "step": 4950
    },
    {
      "epoch": 0.8209202250910295,
      "grad_norm": 41.18373107910156,
      "learning_rate": 4.0577995605881356e-05,
      "loss": 1.171,
      "step": 4960
    },
    {
      "epoch": 0.8225753061900033,
      "grad_norm": 8.303975105285645,
      "learning_rate": 4.055687003549096e-05,
      "loss": 0.6651,
      "step": 4970
    },
    {
      "epoch": 0.8242303872889771,
      "grad_norm": 60.517173767089844,
      "learning_rate": 4.053574446510056e-05,
      "loss": 0.7118,
      "step": 4980
    },
    {
      "epoch": 0.825885468387951,
      "grad_norm": 37.4803352355957,
      "learning_rate": 4.051461889471016e-05,
      "loss": 1.1228,
      "step": 4990
    },
    {
      "epoch": 0.8275405494869249,
      "grad_norm": 12.022649765014648,
      "learning_rate": 4.0493493324319756e-05,
      "loss": 0.812,
      "step": 5000
    },
    {
      "epoch": 0.8291956305858987,
      "grad_norm": 18.875782012939453,
      "learning_rate": 4.0472367753929354e-05,
      "loss": 1.2018,
      "step": 5010
    },
    {
      "epoch": 0.8308507116848726,
      "grad_norm": 45.75765609741211,
      "learning_rate": 4.045124218353896e-05,
      "loss": 0.7898,
      "step": 5020
    },
    {
      "epoch": 0.8325057927838464,
      "grad_norm": 33.374141693115234,
      "learning_rate": 4.043011661314856e-05,
      "loss": 1.0045,
      "step": 5030
    },
    {
      "epoch": 0.8341608738828202,
      "grad_norm": 26.64209747314453,
      "learning_rate": 4.0408991042758156e-05,
      "loss": 1.0374,
      "step": 5040
    },
    {
      "epoch": 0.8358159549817941,
      "grad_norm": 26.986963272094727,
      "learning_rate": 4.038786547236776e-05,
      "loss": 0.7405,
      "step": 5050
    },
    {
      "epoch": 0.837471036080768,
      "grad_norm": 29.854278564453125,
      "learning_rate": 4.036673990197736e-05,
      "loss": 1.088,
      "step": 5060
    },
    {
      "epoch": 0.8391261171797418,
      "grad_norm": 31.561147689819336,
      "learning_rate": 4.034561433158696e-05,
      "loss": 0.9318,
      "step": 5070
    },
    {
      "epoch": 0.8407811982787157,
      "grad_norm": 55.01606750488281,
      "learning_rate": 4.032448876119656e-05,
      "loss": 1.0969,
      "step": 5080
    },
    {
      "epoch": 0.8424362793776895,
      "grad_norm": 22.20467758178711,
      "learning_rate": 4.0303363190806155e-05,
      "loss": 0.9399,
      "step": 5090
    },
    {
      "epoch": 0.8440913604766633,
      "grad_norm": 16.404600143432617,
      "learning_rate": 4.0282237620415753e-05,
      "loss": 1.0694,
      "step": 5100
    },
    {
      "epoch": 0.8457464415756372,
      "grad_norm": 49.124752044677734,
      "learning_rate": 4.026111205002535e-05,
      "loss": 0.9494,
      "step": 5110
    },
    {
      "epoch": 0.8474015226746111,
      "grad_norm": 23.652305603027344,
      "learning_rate": 4.023998647963495e-05,
      "loss": 1.4924,
      "step": 5120
    },
    {
      "epoch": 0.8490566037735849,
      "grad_norm": 35.03337860107422,
      "learning_rate": 4.021886090924455e-05,
      "loss": 0.8561,
      "step": 5130
    },
    {
      "epoch": 0.8507116848725588,
      "grad_norm": 10.916654586791992,
      "learning_rate": 4.0197735338854154e-05,
      "loss": 0.6841,
      "step": 5140
    },
    {
      "epoch": 0.8523667659715326,
      "grad_norm": 29.743663787841797,
      "learning_rate": 4.017660976846375e-05,
      "loss": 0.8381,
      "step": 5150
    },
    {
      "epoch": 0.8540218470705064,
      "grad_norm": 48.18903732299805,
      "learning_rate": 4.015548419807335e-05,
      "loss": 1.1105,
      "step": 5160
    },
    {
      "epoch": 0.8556769281694803,
      "grad_norm": 15.480923652648926,
      "learning_rate": 4.013435862768295e-05,
      "loss": 0.5612,
      "step": 5170
    },
    {
      "epoch": 0.8573320092684542,
      "grad_norm": 54.044368743896484,
      "learning_rate": 4.011323305729255e-05,
      "loss": 1.2448,
      "step": 5180
    },
    {
      "epoch": 0.858987090367428,
      "grad_norm": 39.72895050048828,
      "learning_rate": 4.0092107486902146e-05,
      "loss": 0.7588,
      "step": 5190
    },
    {
      "epoch": 0.8606421714664019,
      "grad_norm": 44.82255172729492,
      "learning_rate": 4.0070981916511744e-05,
      "loss": 0.68,
      "step": 5200
    },
    {
      "epoch": 0.8622972525653757,
      "grad_norm": 42.09828186035156,
      "learning_rate": 4.004985634612134e-05,
      "loss": 1.152,
      "step": 5210
    },
    {
      "epoch": 0.8639523336643495,
      "grad_norm": 12.558135032653809,
      "learning_rate": 4.002873077573095e-05,
      "loss": 0.974,
      "step": 5220
    },
    {
      "epoch": 0.8656074147633234,
      "grad_norm": 68.55506134033203,
      "learning_rate": 4.0007605205340546e-05,
      "loss": 0.998,
      "step": 5230
    },
    {
      "epoch": 0.8672624958622972,
      "grad_norm": 31.091636657714844,
      "learning_rate": 3.9986479634950144e-05,
      "loss": 0.7319,
      "step": 5240
    },
    {
      "epoch": 0.8689175769612711,
      "grad_norm": 66.26123809814453,
      "learning_rate": 3.996535406455974e-05,
      "loss": 1.0208,
      "step": 5250
    },
    {
      "epoch": 0.870572658060245,
      "grad_norm": 24.81817626953125,
      "learning_rate": 3.994422849416935e-05,
      "loss": 0.9526,
      "step": 5260
    },
    {
      "epoch": 0.8722277391592188,
      "grad_norm": 23.893579483032227,
      "learning_rate": 3.9923102923778946e-05,
      "loss": 0.7943,
      "step": 5270
    },
    {
      "epoch": 0.8738828202581926,
      "grad_norm": 22.225414276123047,
      "learning_rate": 3.9901977353388545e-05,
      "loss": 0.7798,
      "step": 5280
    },
    {
      "epoch": 0.8755379013571665,
      "grad_norm": 124.0031967163086,
      "learning_rate": 3.988085178299814e-05,
      "loss": 0.7872,
      "step": 5290
    },
    {
      "epoch": 0.8771929824561403,
      "grad_norm": 32.75639724731445,
      "learning_rate": 3.985972621260774e-05,
      "loss": 1.0514,
      "step": 5300
    },
    {
      "epoch": 0.8788480635551142,
      "grad_norm": 34.68301773071289,
      "learning_rate": 3.9838600642217347e-05,
      "loss": 1.1533,
      "step": 5310
    },
    {
      "epoch": 0.8805031446540881,
      "grad_norm": 41.72272491455078,
      "learning_rate": 3.9817475071826945e-05,
      "loss": 0.8542,
      "step": 5320
    },
    {
      "epoch": 0.8821582257530619,
      "grad_norm": 36.96218490600586,
      "learning_rate": 3.979634950143654e-05,
      "loss": 0.9289,
      "step": 5330
    },
    {
      "epoch": 0.8838133068520357,
      "grad_norm": 25.352962493896484,
      "learning_rate": 3.977522393104614e-05,
      "loss": 0.7114,
      "step": 5340
    },
    {
      "epoch": 0.8854683879510096,
      "grad_norm": 29.200422286987305,
      "learning_rate": 3.975409836065574e-05,
      "loss": 0.8159,
      "step": 5350
    },
    {
      "epoch": 0.8871234690499834,
      "grad_norm": 37.4744987487793,
      "learning_rate": 3.973297279026534e-05,
      "loss": 1.1216,
      "step": 5360
    },
    {
      "epoch": 0.8887785501489573,
      "grad_norm": 107.9002685546875,
      "learning_rate": 3.971184721987494e-05,
      "loss": 1.135,
      "step": 5370
    },
    {
      "epoch": 0.8904336312479312,
      "grad_norm": 10.678826332092285,
      "learning_rate": 3.9690721649484535e-05,
      "loss": 0.7345,
      "step": 5380
    },
    {
      "epoch": 0.892088712346905,
      "grad_norm": 20.760501861572266,
      "learning_rate": 3.966959607909414e-05,
      "loss": 0.9014,
      "step": 5390
    },
    {
      "epoch": 0.8937437934458788,
      "grad_norm": 45.45418930053711,
      "learning_rate": 3.964847050870374e-05,
      "loss": 0.8495,
      "step": 5400
    },
    {
      "epoch": 0.8953988745448527,
      "grad_norm": 10.497682571411133,
      "learning_rate": 3.962734493831334e-05,
      "loss": 0.6992,
      "step": 5410
    },
    {
      "epoch": 0.8970539556438265,
      "grad_norm": 48.815982818603516,
      "learning_rate": 3.9606219367922936e-05,
      "loss": 0.9699,
      "step": 5420
    },
    {
      "epoch": 0.8987090367428004,
      "grad_norm": 26.05363655090332,
      "learning_rate": 3.9585093797532534e-05,
      "loss": 0.778,
      "step": 5430
    },
    {
      "epoch": 0.9003641178417743,
      "grad_norm": 11.353460311889648,
      "learning_rate": 3.956396822714213e-05,
      "loss": 0.7267,
      "step": 5440
    },
    {
      "epoch": 0.9020191989407481,
      "grad_norm": 25.102746963500977,
      "learning_rate": 3.954284265675173e-05,
      "loss": 0.6099,
      "step": 5450
    },
    {
      "epoch": 0.9036742800397219,
      "grad_norm": 25.125328063964844,
      "learning_rate": 3.952171708636133e-05,
      "loss": 0.6322,
      "step": 5460
    },
    {
      "epoch": 0.9053293611386958,
      "grad_norm": 54.90968704223633,
      "learning_rate": 3.950059151597093e-05,
      "loss": 0.9016,
      "step": 5470
    },
    {
      "epoch": 0.9069844422376696,
      "grad_norm": 33.970218658447266,
      "learning_rate": 3.947946594558053e-05,
      "loss": 1.0876,
      "step": 5480
    },
    {
      "epoch": 0.9086395233366436,
      "grad_norm": 15.54452133178711,
      "learning_rate": 3.945834037519013e-05,
      "loss": 0.6732,
      "step": 5490
    },
    {
      "epoch": 0.9102946044356174,
      "grad_norm": 27.127317428588867,
      "learning_rate": 3.943721480479973e-05,
      "loss": 1.1233,
      "step": 5500
    },
    {
      "epoch": 0.9119496855345912,
      "grad_norm": 37.726463317871094,
      "learning_rate": 3.9416089234409335e-05,
      "loss": 1.0778,
      "step": 5510
    },
    {
      "epoch": 0.913604766633565,
      "grad_norm": 17.310577392578125,
      "learning_rate": 3.939496366401893e-05,
      "loss": 0.7675,
      "step": 5520
    },
    {
      "epoch": 0.9152598477325389,
      "grad_norm": 17.762847900390625,
      "learning_rate": 3.937383809362853e-05,
      "loss": 0.9618,
      "step": 5530
    },
    {
      "epoch": 0.9169149288315127,
      "grad_norm": 38.27465057373047,
      "learning_rate": 3.935271252323813e-05,
      "loss": 0.9575,
      "step": 5540
    },
    {
      "epoch": 0.9185700099304865,
      "grad_norm": 16.514741897583008,
      "learning_rate": 3.933158695284773e-05,
      "loss": 0.9325,
      "step": 5550
    },
    {
      "epoch": 0.9202250910294605,
      "grad_norm": 21.655864715576172,
      "learning_rate": 3.931046138245733e-05,
      "loss": 0.926,
      "step": 5560
    },
    {
      "epoch": 0.9218801721284343,
      "grad_norm": 46.66102600097656,
      "learning_rate": 3.928933581206693e-05,
      "loss": 0.7173,
      "step": 5570
    },
    {
      "epoch": 0.9235352532274081,
      "grad_norm": 7.062456130981445,
      "learning_rate": 3.926821024167653e-05,
      "loss": 0.6103,
      "step": 5580
    },
    {
      "epoch": 0.925190334326382,
      "grad_norm": 39.740440368652344,
      "learning_rate": 3.924708467128613e-05,
      "loss": 0.9664,
      "step": 5590
    },
    {
      "epoch": 0.9268454154253558,
      "grad_norm": 14.52615737915039,
      "learning_rate": 3.922595910089573e-05,
      "loss": 1.0266,
      "step": 5600
    },
    {
      "epoch": 0.9285004965243296,
      "grad_norm": 15.054244995117188,
      "learning_rate": 3.9204833530505325e-05,
      "loss": 0.842,
      "step": 5610
    },
    {
      "epoch": 0.9301555776233036,
      "grad_norm": 17.36724853515625,
      "learning_rate": 3.9183707960114924e-05,
      "loss": 0.5741,
      "step": 5620
    },
    {
      "epoch": 0.9318106587222774,
      "grad_norm": 40.45450973510742,
      "learning_rate": 3.916258238972452e-05,
      "loss": 1.0123,
      "step": 5630
    },
    {
      "epoch": 0.9334657398212513,
      "grad_norm": 23.949005126953125,
      "learning_rate": 3.914145681933412e-05,
      "loss": 0.7034,
      "step": 5640
    },
    {
      "epoch": 0.9351208209202251,
      "grad_norm": 34.041748046875,
      "learning_rate": 3.9120331248943725e-05,
      "loss": 0.8579,
      "step": 5650
    },
    {
      "epoch": 0.9367759020191989,
      "grad_norm": 30.048744201660156,
      "learning_rate": 3.9099205678553324e-05,
      "loss": 1.0491,
      "step": 5660
    },
    {
      "epoch": 0.9384309831181727,
      "grad_norm": 39.627445220947266,
      "learning_rate": 3.907808010816292e-05,
      "loss": 0.7466,
      "step": 5670
    },
    {
      "epoch": 0.9400860642171467,
      "grad_norm": 18.481027603149414,
      "learning_rate": 3.905695453777252e-05,
      "loss": 0.7082,
      "step": 5680
    },
    {
      "epoch": 0.9417411453161205,
      "grad_norm": 26.240121841430664,
      "learning_rate": 3.903582896738212e-05,
      "loss": 0.7483,
      "step": 5690
    },
    {
      "epoch": 0.9433962264150944,
      "grad_norm": 38.89731216430664,
      "learning_rate": 3.901470339699172e-05,
      "loss": 1.0431,
      "step": 5700
    },
    {
      "epoch": 0.9450513075140682,
      "grad_norm": 40.050323486328125,
      "learning_rate": 3.8993577826601316e-05,
      "loss": 1.0079,
      "step": 5710
    },
    {
      "epoch": 0.946706388613042,
      "grad_norm": 16.640865325927734,
      "learning_rate": 3.8972452256210914e-05,
      "loss": 0.833,
      "step": 5720
    },
    {
      "epoch": 0.9483614697120158,
      "grad_norm": 32.167396545410156,
      "learning_rate": 3.895132668582052e-05,
      "loss": 0.9518,
      "step": 5730
    },
    {
      "epoch": 0.9500165508109898,
      "grad_norm": 18.119890213012695,
      "learning_rate": 3.893020111543012e-05,
      "loss": 0.9064,
      "step": 5740
    },
    {
      "epoch": 0.9516716319099636,
      "grad_norm": 26.22325325012207,
      "learning_rate": 3.8909075545039716e-05,
      "loss": 0.807,
      "step": 5750
    },
    {
      "epoch": 0.9533267130089375,
      "grad_norm": 49.05755615234375,
      "learning_rate": 3.888794997464932e-05,
      "loss": 1.0806,
      "step": 5760
    },
    {
      "epoch": 0.9549817941079113,
      "grad_norm": 20.40446662902832,
      "learning_rate": 3.886682440425892e-05,
      "loss": 1.1333,
      "step": 5770
    },
    {
      "epoch": 0.9566368752068851,
      "grad_norm": 29.72589874267578,
      "learning_rate": 3.884569883386852e-05,
      "loss": 0.6462,
      "step": 5780
    },
    {
      "epoch": 0.958291956305859,
      "grad_norm": 21.21282386779785,
      "learning_rate": 3.8824573263478116e-05,
      "loss": 1.1139,
      "step": 5790
    },
    {
      "epoch": 0.9599470374048328,
      "grad_norm": 37.28199005126953,
      "learning_rate": 3.8803447693087715e-05,
      "loss": 0.6569,
      "step": 5800
    },
    {
      "epoch": 0.9616021185038067,
      "grad_norm": 41.15958786010742,
      "learning_rate": 3.878232212269731e-05,
      "loss": 0.8854,
      "step": 5810
    },
    {
      "epoch": 0.9632571996027806,
      "grad_norm": 27.973587036132812,
      "learning_rate": 3.876119655230692e-05,
      "loss": 1.0271,
      "step": 5820
    },
    {
      "epoch": 0.9649122807017544,
      "grad_norm": 16.447437286376953,
      "learning_rate": 3.874007098191652e-05,
      "loss": 0.9797,
      "step": 5830
    },
    {
      "epoch": 0.9665673618007282,
      "grad_norm": 20.319849014282227,
      "learning_rate": 3.8718945411526115e-05,
      "loss": 1.0463,
      "step": 5840
    },
    {
      "epoch": 0.968222442899702,
      "grad_norm": 54.23350524902344,
      "learning_rate": 3.8697819841135713e-05,
      "loss": 1.0145,
      "step": 5850
    },
    {
      "epoch": 0.9698775239986759,
      "grad_norm": 13.759140968322754,
      "learning_rate": 3.867669427074531e-05,
      "loss": 0.8332,
      "step": 5860
    },
    {
      "epoch": 0.9715326050976498,
      "grad_norm": 10.593096733093262,
      "learning_rate": 3.865556870035491e-05,
      "loss": 0.679,
      "step": 5870
    },
    {
      "epoch": 0.9731876861966237,
      "grad_norm": 60.50212097167969,
      "learning_rate": 3.863444312996451e-05,
      "loss": 0.8477,
      "step": 5880
    },
    {
      "epoch": 0.9748427672955975,
      "grad_norm": 37.749107360839844,
      "learning_rate": 3.861331755957411e-05,
      "loss": 0.8214,
      "step": 5890
    },
    {
      "epoch": 0.9764978483945713,
      "grad_norm": 19.80242919921875,
      "learning_rate": 3.859219198918371e-05,
      "loss": 0.7335,
      "step": 5900
    },
    {
      "epoch": 0.9781529294935452,
      "grad_norm": 30.348848342895508,
      "learning_rate": 3.857106641879331e-05,
      "loss": 1.0282,
      "step": 5910
    },
    {
      "epoch": 0.979808010592519,
      "grad_norm": 53.1911506652832,
      "learning_rate": 3.854994084840291e-05,
      "loss": 0.5311,
      "step": 5920
    },
    {
      "epoch": 0.9814630916914929,
      "grad_norm": 62.07038497924805,
      "learning_rate": 3.852881527801251e-05,
      "loss": 1.258,
      "step": 5930
    },
    {
      "epoch": 0.9831181727904668,
      "grad_norm": 29.08894157409668,
      "learning_rate": 3.8507689707622106e-05,
      "loss": 0.7495,
      "step": 5940
    },
    {
      "epoch": 0.9847732538894406,
      "grad_norm": 16.66305160522461,
      "learning_rate": 3.8486564137231704e-05,
      "loss": 1.0708,
      "step": 5950
    },
    {
      "epoch": 0.9864283349884144,
      "grad_norm": 17.30808448791504,
      "learning_rate": 3.84654385668413e-05,
      "loss": 0.8558,
      "step": 5960
    },
    {
      "epoch": 0.9880834160873883,
      "grad_norm": 111.9420166015625,
      "learning_rate": 3.84443129964509e-05,
      "loss": 0.7558,
      "step": 5970
    },
    {
      "epoch": 0.9897384971863621,
      "grad_norm": 18.218891143798828,
      "learning_rate": 3.8423187426060506e-05,
      "loss": 0.8794,
      "step": 5980
    },
    {
      "epoch": 0.991393578285336,
      "grad_norm": 59.237060546875,
      "learning_rate": 3.8402061855670104e-05,
      "loss": 1.0428,
      "step": 5990
    },
    {
      "epoch": 0.9930486593843099,
      "grad_norm": 90.12430572509766,
      "learning_rate": 3.83809362852797e-05,
      "loss": 0.7552,
      "step": 6000
    },
    {
      "epoch": 0.9947037404832837,
      "grad_norm": 75.5633544921875,
      "learning_rate": 3.835981071488931e-05,
      "loss": 0.9478,
      "step": 6010
    },
    {
      "epoch": 0.9963588215822575,
      "grad_norm": 20.95497703552246,
      "learning_rate": 3.8338685144498906e-05,
      "loss": 0.7591,
      "step": 6020
    },
    {
      "epoch": 0.9980139026812314,
      "grad_norm": 16.415205001831055,
      "learning_rate": 3.8317559574108505e-05,
      "loss": 0.8699,
      "step": 6030
    },
    {
      "epoch": 0.9996689837802052,
      "grad_norm": 48.7807731628418,
      "learning_rate": 3.82964340037181e-05,
      "loss": 0.7141,
      "step": 6040
    },
    {
      "epoch": 1.001324064879179,
      "grad_norm": 23.99674415588379,
      "learning_rate": 3.82753084333277e-05,
      "loss": 0.7393,
      "step": 6050
    },
    {
      "epoch": 1.002979145978153,
      "grad_norm": 2.6606550216674805,
      "learning_rate": 3.82541828629373e-05,
      "loss": 0.4528,
      "step": 6060
    },
    {
      "epoch": 1.0046342270771267,
      "grad_norm": 22.87383270263672,
      "learning_rate": 3.8233057292546905e-05,
      "loss": 0.8134,
      "step": 6070
    },
    {
      "epoch": 1.0062893081761006,
      "grad_norm": 35.86394500732422,
      "learning_rate": 3.8211931722156503e-05,
      "loss": 0.7342,
      "step": 6080
    },
    {
      "epoch": 1.0079443892750746,
      "grad_norm": 27.55295181274414,
      "learning_rate": 3.81908061517661e-05,
      "loss": 0.4937,
      "step": 6090
    },
    {
      "epoch": 1.0095994703740483,
      "grad_norm": 11.351792335510254,
      "learning_rate": 3.81696805813757e-05,
      "loss": 0.5858,
      "step": 6100
    },
    {
      "epoch": 1.0112545514730222,
      "grad_norm": 28.147884368896484,
      "learning_rate": 3.81485550109853e-05,
      "loss": 0.5892,
      "step": 6110
    },
    {
      "epoch": 1.012909632571996,
      "grad_norm": 25.35367202758789,
      "learning_rate": 3.81274294405949e-05,
      "loss": 0.6338,
      "step": 6120
    },
    {
      "epoch": 1.01456471367097,
      "grad_norm": 2.7460508346557617,
      "learning_rate": 3.8106303870204495e-05,
      "loss": 0.4591,
      "step": 6130
    },
    {
      "epoch": 1.0162197947699436,
      "grad_norm": 30.059175491333008,
      "learning_rate": 3.8085178299814094e-05,
      "loss": 0.5005,
      "step": 6140
    },
    {
      "epoch": 1.0178748758689176,
      "grad_norm": 5.3687639236450195,
      "learning_rate": 3.806405272942369e-05,
      "loss": 0.5319,
      "step": 6150
    },
    {
      "epoch": 1.0195299569678915,
      "grad_norm": 33.88230895996094,
      "learning_rate": 3.80429271590333e-05,
      "loss": 0.9129,
      "step": 6160
    },
    {
      "epoch": 1.0211850380668652,
      "grad_norm": 12.727429389953613,
      "learning_rate": 3.8021801588642896e-05,
      "loss": 1.0625,
      "step": 6170
    },
    {
      "epoch": 1.0228401191658392,
      "grad_norm": 54.05946350097656,
      "learning_rate": 3.8000676018252494e-05,
      "loss": 0.7673,
      "step": 6180
    },
    {
      "epoch": 1.0244952002648129,
      "grad_norm": 20.86033058166504,
      "learning_rate": 3.797955044786209e-05,
      "loss": 0.7079,
      "step": 6190
    },
    {
      "epoch": 1.0261502813637868,
      "grad_norm": 21.361989974975586,
      "learning_rate": 3.795842487747169e-05,
      "loss": 0.8103,
      "step": 6200
    },
    {
      "epoch": 1.0278053624627608,
      "grad_norm": 27.68878746032715,
      "learning_rate": 3.793729930708129e-05,
      "loss": 0.6271,
      "step": 6210
    },
    {
      "epoch": 1.0294604435617345,
      "grad_norm": 40.39561080932617,
      "learning_rate": 3.791617373669089e-05,
      "loss": 0.5853,
      "step": 6220
    },
    {
      "epoch": 1.0311155246607084,
      "grad_norm": 24.63524627685547,
      "learning_rate": 3.789504816630049e-05,
      "loss": 0.4345,
      "step": 6230
    },
    {
      "epoch": 1.0327706057596822,
      "grad_norm": 61.5844612121582,
      "learning_rate": 3.787392259591009e-05,
      "loss": 0.8169,
      "step": 6240
    },
    {
      "epoch": 1.034425686858656,
      "grad_norm": 29.03615951538086,
      "learning_rate": 3.785279702551969e-05,
      "loss": 0.8474,
      "step": 6250
    },
    {
      "epoch": 1.0360807679576298,
      "grad_norm": 24.305652618408203,
      "learning_rate": 3.7831671455129295e-05,
      "loss": 0.5968,
      "step": 6260
    },
    {
      "epoch": 1.0377358490566038,
      "grad_norm": 24.659137725830078,
      "learning_rate": 3.781054588473889e-05,
      "loss": 0.6915,
      "step": 6270
    },
    {
      "epoch": 1.0393909301555777,
      "grad_norm": 40.681617736816406,
      "learning_rate": 3.778942031434849e-05,
      "loss": 0.4404,
      "step": 6280
    },
    {
      "epoch": 1.0410460112545514,
      "grad_norm": 11.239706993103027,
      "learning_rate": 3.776829474395809e-05,
      "loss": 0.6377,
      "step": 6290
    },
    {
      "epoch": 1.0427010923535254,
      "grad_norm": 28.902429580688477,
      "learning_rate": 3.774716917356769e-05,
      "loss": 0.3951,
      "step": 6300
    },
    {
      "epoch": 1.044356173452499,
      "grad_norm": 32.730438232421875,
      "learning_rate": 3.7726043603177287e-05,
      "loss": 0.6843,
      "step": 6310
    },
    {
      "epoch": 1.046011254551473,
      "grad_norm": 45.89207077026367,
      "learning_rate": 3.7704918032786885e-05,
      "loss": 0.5394,
      "step": 6320
    },
    {
      "epoch": 1.047666335650447,
      "grad_norm": 19.825157165527344,
      "learning_rate": 3.768379246239649e-05,
      "loss": 0.6372,
      "step": 6330
    },
    {
      "epoch": 1.0493214167494207,
      "grad_norm": 10.737863540649414,
      "learning_rate": 3.766266689200609e-05,
      "loss": 0.6095,
      "step": 6340
    },
    {
      "epoch": 1.0509764978483946,
      "grad_norm": 35.61298370361328,
      "learning_rate": 3.764154132161569e-05,
      "loss": 0.4843,
      "step": 6350
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 41.52722930908203,
      "learning_rate": 3.7620415751225285e-05,
      "loss": 0.5839,
      "step": 6360
    },
    {
      "epoch": 1.0542866600463423,
      "grad_norm": 38.534664154052734,
      "learning_rate": 3.7599290180834884e-05,
      "loss": 0.5889,
      "step": 6370
    },
    {
      "epoch": 1.055941741145316,
      "grad_norm": 6.589816570281982,
      "learning_rate": 3.757816461044448e-05,
      "loss": 0.4791,
      "step": 6380
    },
    {
      "epoch": 1.05759682224429,
      "grad_norm": 10.527118682861328,
      "learning_rate": 3.755703904005408e-05,
      "loss": 0.6583,
      "step": 6390
    },
    {
      "epoch": 1.059251903343264,
      "grad_norm": 17.307870864868164,
      "learning_rate": 3.753591346966368e-05,
      "loss": 0.6156,
      "step": 6400
    },
    {
      "epoch": 1.0609069844422376,
      "grad_norm": 15.441558837890625,
      "learning_rate": 3.7514787899273284e-05,
      "loss": 0.9004,
      "step": 6410
    },
    {
      "epoch": 1.0625620655412116,
      "grad_norm": 27.434940338134766,
      "learning_rate": 3.749366232888288e-05,
      "loss": 0.5577,
      "step": 6420
    },
    {
      "epoch": 1.0642171466401853,
      "grad_norm": 69.26449584960938,
      "learning_rate": 3.747253675849248e-05,
      "loss": 0.5812,
      "step": 6430
    },
    {
      "epoch": 1.0658722277391592,
      "grad_norm": 38.147804260253906,
      "learning_rate": 3.745141118810208e-05,
      "loss": 0.8755,
      "step": 6440
    },
    {
      "epoch": 1.067527308838133,
      "grad_norm": 9.738122940063477,
      "learning_rate": 3.743028561771168e-05,
      "loss": 0.6124,
      "step": 6450
    },
    {
      "epoch": 1.069182389937107,
      "grad_norm": 21.58034324645996,
      "learning_rate": 3.7409160047321276e-05,
      "loss": 0.7137,
      "step": 6460
    },
    {
      "epoch": 1.0708374710360808,
      "grad_norm": 24.178417205810547,
      "learning_rate": 3.738803447693088e-05,
      "loss": 0.7232,
      "step": 6470
    },
    {
      "epoch": 1.0724925521350546,
      "grad_norm": 37.47402572631836,
      "learning_rate": 3.736690890654048e-05,
      "loss": 0.7478,
      "step": 6480
    },
    {
      "epoch": 1.0741476332340285,
      "grad_norm": 12.621219635009766,
      "learning_rate": 3.734578333615008e-05,
      "loss": 0.5389,
      "step": 6490
    },
    {
      "epoch": 1.0758027143330022,
      "grad_norm": 19.407360076904297,
      "learning_rate": 3.732465776575968e-05,
      "loss": 0.61,
      "step": 6500
    },
    {
      "epoch": 1.0774577954319762,
      "grad_norm": 90.82967376708984,
      "learning_rate": 3.730353219536928e-05,
      "loss": 1.0033,
      "step": 6510
    },
    {
      "epoch": 1.0791128765309501,
      "grad_norm": 21.24578285217285,
      "learning_rate": 3.728240662497888e-05,
      "loss": 0.8119,
      "step": 6520
    },
    {
      "epoch": 1.0807679576299238,
      "grad_norm": 29.143354415893555,
      "learning_rate": 3.726128105458848e-05,
      "loss": 0.5768,
      "step": 6530
    },
    {
      "epoch": 1.0824230387288978,
      "grad_norm": 37.799713134765625,
      "learning_rate": 3.7240155484198076e-05,
      "loss": 0.6227,
      "step": 6540
    },
    {
      "epoch": 1.0840781198278715,
      "grad_norm": 32.8206672668457,
      "learning_rate": 3.7219029913807675e-05,
      "loss": 0.8603,
      "step": 6550
    },
    {
      "epoch": 1.0857332009268454,
      "grad_norm": 35.560325622558594,
      "learning_rate": 3.719790434341727e-05,
      "loss": 0.7908,
      "step": 6560
    },
    {
      "epoch": 1.0873882820258192,
      "grad_norm": 13.880935668945312,
      "learning_rate": 3.717677877302687e-05,
      "loss": 0.4804,
      "step": 6570
    },
    {
      "epoch": 1.089043363124793,
      "grad_norm": 5.424874305725098,
      "learning_rate": 3.715565320263648e-05,
      "loss": 0.4581,
      "step": 6580
    },
    {
      "epoch": 1.090698444223767,
      "grad_norm": 47.82488250732422,
      "learning_rate": 3.7134527632246075e-05,
      "loss": 0.6467,
      "step": 6590
    },
    {
      "epoch": 1.0923535253227408,
      "grad_norm": 47.30984878540039,
      "learning_rate": 3.7113402061855674e-05,
      "loss": 0.6121,
      "step": 6600
    },
    {
      "epoch": 1.0940086064217147,
      "grad_norm": 29.692768096923828,
      "learning_rate": 3.709227649146527e-05,
      "loss": 0.6096,
      "step": 6610
    },
    {
      "epoch": 1.0956636875206884,
      "grad_norm": 14.344732284545898,
      "learning_rate": 3.707115092107487e-05,
      "loss": 0.4658,
      "step": 6620
    },
    {
      "epoch": 1.0973187686196624,
      "grad_norm": 17.299856185913086,
      "learning_rate": 3.705002535068447e-05,
      "loss": 0.6885,
      "step": 6630
    },
    {
      "epoch": 1.0989738497186363,
      "grad_norm": 31.473751068115234,
      "learning_rate": 3.702889978029407e-05,
      "loss": 0.9624,
      "step": 6640
    },
    {
      "epoch": 1.10062893081761,
      "grad_norm": 6.045357704162598,
      "learning_rate": 3.7007774209903665e-05,
      "loss": 0.577,
      "step": 6650
    },
    {
      "epoch": 1.102284011916584,
      "grad_norm": 24.454242706298828,
      "learning_rate": 3.6986648639513264e-05,
      "loss": 0.6653,
      "step": 6660
    },
    {
      "epoch": 1.1039390930155577,
      "grad_norm": 16.46118927001953,
      "learning_rate": 3.696552306912287e-05,
      "loss": 0.7013,
      "step": 6670
    },
    {
      "epoch": 1.1055941741145316,
      "grad_norm": 41.0910530090332,
      "learning_rate": 3.694439749873247e-05,
      "loss": 0.7117,
      "step": 6680
    },
    {
      "epoch": 1.1072492552135054,
      "grad_norm": 108.03185272216797,
      "learning_rate": 3.6923271928342066e-05,
      "loss": 0.5915,
      "step": 6690
    },
    {
      "epoch": 1.1089043363124793,
      "grad_norm": 26.44959831237793,
      "learning_rate": 3.6902146357951664e-05,
      "loss": 0.4162,
      "step": 6700
    },
    {
      "epoch": 1.1105594174114533,
      "grad_norm": 46.18669128417969,
      "learning_rate": 3.688102078756126e-05,
      "loss": 0.772,
      "step": 6710
    },
    {
      "epoch": 1.112214498510427,
      "grad_norm": 11.763246536254883,
      "learning_rate": 3.685989521717087e-05,
      "loss": 0.7397,
      "step": 6720
    },
    {
      "epoch": 1.113869579609401,
      "grad_norm": 26.21137809753418,
      "learning_rate": 3.6838769646780466e-05,
      "loss": 0.5436,
      "step": 6730
    },
    {
      "epoch": 1.1155246607083746,
      "grad_norm": 35.607383728027344,
      "learning_rate": 3.6817644076390064e-05,
      "loss": 0.6109,
      "step": 6740
    },
    {
      "epoch": 1.1171797418073486,
      "grad_norm": 33.45698165893555,
      "learning_rate": 3.679651850599967e-05,
      "loss": 0.7481,
      "step": 6750
    },
    {
      "epoch": 1.1188348229063223,
      "grad_norm": 22.97040557861328,
      "learning_rate": 3.677539293560927e-05,
      "loss": 0.9309,
      "step": 6760
    },
    {
      "epoch": 1.1204899040052962,
      "grad_norm": 23.57598876953125,
      "learning_rate": 3.6754267365218866e-05,
      "loss": 0.5564,
      "step": 6770
    },
    {
      "epoch": 1.1221449851042702,
      "grad_norm": 40.754241943359375,
      "learning_rate": 3.6733141794828465e-05,
      "loss": 0.6838,
      "step": 6780
    },
    {
      "epoch": 1.123800066203244,
      "grad_norm": 194.5100555419922,
      "learning_rate": 3.671201622443806e-05,
      "loss": 0.4433,
      "step": 6790
    },
    {
      "epoch": 1.1254551473022179,
      "grad_norm": 70.14537048339844,
      "learning_rate": 3.669089065404766e-05,
      "loss": 0.5632,
      "step": 6800
    },
    {
      "epoch": 1.1271102284011916,
      "grad_norm": 45.88896179199219,
      "learning_rate": 3.666976508365726e-05,
      "loss": 0.4323,
      "step": 6810
    },
    {
      "epoch": 1.1287653095001655,
      "grad_norm": 30.459667205810547,
      "learning_rate": 3.664863951326686e-05,
      "loss": 0.8318,
      "step": 6820
    },
    {
      "epoch": 1.1304203905991392,
      "grad_norm": 30.067577362060547,
      "learning_rate": 3.662751394287646e-05,
      "loss": 0.4547,
      "step": 6830
    },
    {
      "epoch": 1.1320754716981132,
      "grad_norm": 25.983854293823242,
      "learning_rate": 3.660638837248606e-05,
      "loss": 0.5291,
      "step": 6840
    },
    {
      "epoch": 1.1337305527970871,
      "grad_norm": 37.30104446411133,
      "learning_rate": 3.658526280209566e-05,
      "loss": 0.8222,
      "step": 6850
    },
    {
      "epoch": 1.1353856338960608,
      "grad_norm": 13.5311918258667,
      "learning_rate": 3.656413723170526e-05,
      "loss": 0.6255,
      "step": 6860
    },
    {
      "epoch": 1.1370407149950348,
      "grad_norm": 14.898466110229492,
      "learning_rate": 3.654301166131486e-05,
      "loss": 0.6047,
      "step": 6870
    },
    {
      "epoch": 1.1386957960940087,
      "grad_norm": 89.04261779785156,
      "learning_rate": 3.6521886090924455e-05,
      "loss": 0.7569,
      "step": 6880
    },
    {
      "epoch": 1.1403508771929824,
      "grad_norm": 25.644994735717773,
      "learning_rate": 3.6500760520534054e-05,
      "loss": 0.7446,
      "step": 6890
    },
    {
      "epoch": 1.1420059582919564,
      "grad_norm": 18.464025497436523,
      "learning_rate": 3.647963495014365e-05,
      "loss": 0.5555,
      "step": 6900
    },
    {
      "epoch": 1.1436610393909301,
      "grad_norm": 27.158897399902344,
      "learning_rate": 3.645850937975325e-05,
      "loss": 0.6505,
      "step": 6910
    },
    {
      "epoch": 1.145316120489904,
      "grad_norm": 24.003767013549805,
      "learning_rate": 3.6437383809362856e-05,
      "loss": 0.5802,
      "step": 6920
    },
    {
      "epoch": 1.1469712015888778,
      "grad_norm": 29.795583724975586,
      "learning_rate": 3.6416258238972454e-05,
      "loss": 0.8738,
      "step": 6930
    },
    {
      "epoch": 1.1486262826878517,
      "grad_norm": 13.572277069091797,
      "learning_rate": 3.639513266858205e-05,
      "loss": 0.5452,
      "step": 6940
    },
    {
      "epoch": 1.1502813637868257,
      "grad_norm": 15.532824516296387,
      "learning_rate": 3.637400709819165e-05,
      "loss": 0.5303,
      "step": 6950
    },
    {
      "epoch": 1.1519364448857994,
      "grad_norm": 27.94902229309082,
      "learning_rate": 3.635288152780125e-05,
      "loss": 0.9961,
      "step": 6960
    },
    {
      "epoch": 1.1535915259847733,
      "grad_norm": 27.398401260375977,
      "learning_rate": 3.6331755957410854e-05,
      "loss": 0.3987,
      "step": 6970
    },
    {
      "epoch": 1.155246607083747,
      "grad_norm": 19.338653564453125,
      "learning_rate": 3.631063038702045e-05,
      "loss": 0.7439,
      "step": 6980
    },
    {
      "epoch": 1.156901688182721,
      "grad_norm": 36.364017486572266,
      "learning_rate": 3.628950481663005e-05,
      "loss": 0.6934,
      "step": 6990
    },
    {
      "epoch": 1.1585567692816947,
      "grad_norm": 45.0477409362793,
      "learning_rate": 3.626837924623965e-05,
      "loss": 0.6027,
      "step": 7000
    },
    {
      "epoch": 1.1602118503806687,
      "grad_norm": 32.046730041503906,
      "learning_rate": 3.6247253675849255e-05,
      "loss": 0.6637,
      "step": 7010
    },
    {
      "epoch": 1.1618669314796426,
      "grad_norm": 30.81036949157715,
      "learning_rate": 3.622612810545885e-05,
      "loss": 0.8079,
      "step": 7020
    },
    {
      "epoch": 1.1635220125786163,
      "grad_norm": 38.34925079345703,
      "learning_rate": 3.620500253506845e-05,
      "loss": 0.4865,
      "step": 7030
    },
    {
      "epoch": 1.1651770936775903,
      "grad_norm": 17.540435791015625,
      "learning_rate": 3.618387696467805e-05,
      "loss": 0.5234,
      "step": 7040
    },
    {
      "epoch": 1.166832174776564,
      "grad_norm": 41.68998336791992,
      "learning_rate": 3.616275139428765e-05,
      "loss": 0.5734,
      "step": 7050
    },
    {
      "epoch": 1.168487255875538,
      "grad_norm": 28.82371711730957,
      "learning_rate": 3.614162582389725e-05,
      "loss": 0.6707,
      "step": 7060
    },
    {
      "epoch": 1.1701423369745116,
      "grad_norm": 58.00960159301758,
      "learning_rate": 3.6120500253506845e-05,
      "loss": 0.7818,
      "step": 7070
    },
    {
      "epoch": 1.1717974180734856,
      "grad_norm": 32.1250114440918,
      "learning_rate": 3.6099374683116443e-05,
      "loss": 0.8632,
      "step": 7080
    },
    {
      "epoch": 1.1734524991724595,
      "grad_norm": 20.238388061523438,
      "learning_rate": 3.607824911272605e-05,
      "loss": 0.3957,
      "step": 7090
    },
    {
      "epoch": 1.1751075802714332,
      "grad_norm": 41.980316162109375,
      "learning_rate": 3.605712354233565e-05,
      "loss": 0.7519,
      "step": 7100
    },
    {
      "epoch": 1.1767626613704072,
      "grad_norm": 80.91207122802734,
      "learning_rate": 3.6035997971945245e-05,
      "loss": 0.5851,
      "step": 7110
    },
    {
      "epoch": 1.178417742469381,
      "grad_norm": 34.86690139770508,
      "learning_rate": 3.6014872401554844e-05,
      "loss": 0.7178,
      "step": 7120
    },
    {
      "epoch": 1.1800728235683549,
      "grad_norm": 49.62925720214844,
      "learning_rate": 3.599374683116444e-05,
      "loss": 0.4759,
      "step": 7130
    },
    {
      "epoch": 1.1817279046673286,
      "grad_norm": 18.404447555541992,
      "learning_rate": 3.597262126077404e-05,
      "loss": 0.4374,
      "step": 7140
    },
    {
      "epoch": 1.1833829857663025,
      "grad_norm": 19.61612892150879,
      "learning_rate": 3.595149569038364e-05,
      "loss": 0.7304,
      "step": 7150
    },
    {
      "epoch": 1.1850380668652765,
      "grad_norm": 7.475507736206055,
      "learning_rate": 3.593037011999324e-05,
      "loss": 0.5899,
      "step": 7160
    },
    {
      "epoch": 1.1866931479642502,
      "grad_norm": 42.93541717529297,
      "learning_rate": 3.5909244549602836e-05,
      "loss": 0.663,
      "step": 7170
    },
    {
      "epoch": 1.1883482290632241,
      "grad_norm": 37.30464553833008,
      "learning_rate": 3.588811897921244e-05,
      "loss": 0.678,
      "step": 7180
    },
    {
      "epoch": 1.190003310162198,
      "grad_norm": 33.16069793701172,
      "learning_rate": 3.586699340882204e-05,
      "loss": 0.4303,
      "step": 7190
    },
    {
      "epoch": 1.1916583912611718,
      "grad_norm": 13.771459579467773,
      "learning_rate": 3.584586783843164e-05,
      "loss": 0.4089,
      "step": 7200
    },
    {
      "epoch": 1.1933134723601457,
      "grad_norm": 15.139047622680664,
      "learning_rate": 3.5824742268041236e-05,
      "loss": 0.6308,
      "step": 7210
    },
    {
      "epoch": 1.1949685534591195,
      "grad_norm": 8.202455520629883,
      "learning_rate": 3.580361669765084e-05,
      "loss": 0.471,
      "step": 7220
    },
    {
      "epoch": 1.1966236345580934,
      "grad_norm": 18.460041046142578,
      "learning_rate": 3.578249112726044e-05,
      "loss": 0.7535,
      "step": 7230
    },
    {
      "epoch": 1.1982787156570671,
      "grad_norm": 39.69636535644531,
      "learning_rate": 3.576136555687004e-05,
      "loss": 0.7314,
      "step": 7240
    },
    {
      "epoch": 1.199933796756041,
      "grad_norm": 32.82419204711914,
      "learning_rate": 3.5740239986479636e-05,
      "loss": 0.6163,
      "step": 7250
    },
    {
      "epoch": 1.201588877855015,
      "grad_norm": 27.068750381469727,
      "learning_rate": 3.571911441608924e-05,
      "loss": 0.694,
      "step": 7260
    },
    {
      "epoch": 1.2032439589539887,
      "grad_norm": 13.804849624633789,
      "learning_rate": 3.569798884569884e-05,
      "loss": 0.7274,
      "step": 7270
    },
    {
      "epoch": 1.2048990400529627,
      "grad_norm": 20.708019256591797,
      "learning_rate": 3.567686327530844e-05,
      "loss": 0.5979,
      "step": 7280
    },
    {
      "epoch": 1.2065541211519364,
      "grad_norm": 86.74909210205078,
      "learning_rate": 3.5655737704918037e-05,
      "loss": 0.4508,
      "step": 7290
    },
    {
      "epoch": 1.2082092022509103,
      "grad_norm": 44.51918411254883,
      "learning_rate": 3.5634612134527635e-05,
      "loss": 0.7182,
      "step": 7300
    },
    {
      "epoch": 1.209864283349884,
      "grad_norm": 17.876062393188477,
      "learning_rate": 3.561348656413723e-05,
      "loss": 0.4915,
      "step": 7310
    },
    {
      "epoch": 1.211519364448858,
      "grad_norm": 25.45119285583496,
      "learning_rate": 3.559236099374683e-05,
      "loss": 0.5699,
      "step": 7320
    },
    {
      "epoch": 1.213174445547832,
      "grad_norm": 10.18854808807373,
      "learning_rate": 3.557123542335643e-05,
      "loss": 0.8281,
      "step": 7330
    },
    {
      "epoch": 1.2148295266468057,
      "grad_norm": 35.668155670166016,
      "learning_rate": 3.555010985296603e-05,
      "loss": 0.9047,
      "step": 7340
    },
    {
      "epoch": 1.2164846077457796,
      "grad_norm": 23.68388557434082,
      "learning_rate": 3.5528984282575634e-05,
      "loss": 0.4687,
      "step": 7350
    },
    {
      "epoch": 1.2181396888447533,
      "grad_norm": 44.19721221923828,
      "learning_rate": 3.550785871218523e-05,
      "loss": 0.4742,
      "step": 7360
    },
    {
      "epoch": 1.2197947699437273,
      "grad_norm": 50.87058639526367,
      "learning_rate": 3.548673314179483e-05,
      "loss": 0.8039,
      "step": 7370
    },
    {
      "epoch": 1.221449851042701,
      "grad_norm": 25.51121711730957,
      "learning_rate": 3.546560757140443e-05,
      "loss": 0.3098,
      "step": 7380
    },
    {
      "epoch": 1.223104932141675,
      "grad_norm": 37.21236038208008,
      "learning_rate": 3.544448200101403e-05,
      "loss": 0.5097,
      "step": 7390
    },
    {
      "epoch": 1.2247600132406489,
      "grad_norm": 97.08671569824219,
      "learning_rate": 3.5423356430623626e-05,
      "loss": 0.7174,
      "step": 7400
    },
    {
      "epoch": 1.2264150943396226,
      "grad_norm": 5.381474494934082,
      "learning_rate": 3.5402230860233224e-05,
      "loss": 0.6402,
      "step": 7410
    },
    {
      "epoch": 1.2280701754385965,
      "grad_norm": 98.58634948730469,
      "learning_rate": 3.538110528984282e-05,
      "loss": 0.7539,
      "step": 7420
    },
    {
      "epoch": 1.2297252565375703,
      "grad_norm": 46.36783981323242,
      "learning_rate": 3.535997971945243e-05,
      "loss": 0.4939,
      "step": 7430
    },
    {
      "epoch": 1.2313803376365442,
      "grad_norm": 25.866992950439453,
      "learning_rate": 3.5338854149062026e-05,
      "loss": 0.6772,
      "step": 7440
    },
    {
      "epoch": 1.233035418735518,
      "grad_norm": 34.2402229309082,
      "learning_rate": 3.5317728578671624e-05,
      "loss": 0.6252,
      "step": 7450
    },
    {
      "epoch": 1.2346904998344919,
      "grad_norm": 13.783380508422852,
      "learning_rate": 3.529660300828123e-05,
      "loss": 0.5038,
      "step": 7460
    },
    {
      "epoch": 1.2363455809334658,
      "grad_norm": 29.829383850097656,
      "learning_rate": 3.527547743789083e-05,
      "loss": 0.7647,
      "step": 7470
    },
    {
      "epoch": 1.2380006620324395,
      "grad_norm": 25.98597526550293,
      "learning_rate": 3.5254351867500426e-05,
      "loss": 0.565,
      "step": 7480
    },
    {
      "epoch": 1.2396557431314135,
      "grad_norm": 20.713289260864258,
      "learning_rate": 3.5233226297110025e-05,
      "loss": 0.4374,
      "step": 7490
    },
    {
      "epoch": 1.2413108242303874,
      "grad_norm": 39.58167266845703,
      "learning_rate": 3.521210072671962e-05,
      "loss": 0.6064,
      "step": 7500
    },
    {
      "epoch": 1.2429659053293611,
      "grad_norm": 48.65592575073242,
      "learning_rate": 3.519097515632922e-05,
      "loss": 0.7463,
      "step": 7510
    },
    {
      "epoch": 1.244620986428335,
      "grad_norm": 29.788589477539062,
      "learning_rate": 3.5169849585938826e-05,
      "loss": 0.6325,
      "step": 7520
    },
    {
      "epoch": 1.2462760675273088,
      "grad_norm": 15.075490951538086,
      "learning_rate": 3.5148724015548425e-05,
      "loss": 0.6657,
      "step": 7530
    },
    {
      "epoch": 1.2479311486262827,
      "grad_norm": 51.05302810668945,
      "learning_rate": 3.512759844515802e-05,
      "loss": 0.9273,
      "step": 7540
    },
    {
      "epoch": 1.2495862297252565,
      "grad_norm": 10.648987770080566,
      "learning_rate": 3.510647287476762e-05,
      "loss": 0.7212,
      "step": 7550
    },
    {
      "epoch": 1.2512413108242304,
      "grad_norm": 15.426338195800781,
      "learning_rate": 3.508534730437722e-05,
      "loss": 0.6026,
      "step": 7560
    },
    {
      "epoch": 1.2528963919232043,
      "grad_norm": 20.050052642822266,
      "learning_rate": 3.506422173398682e-05,
      "loss": 0.6087,
      "step": 7570
    },
    {
      "epoch": 1.254551473022178,
      "grad_norm": 49.580352783203125,
      "learning_rate": 3.504309616359642e-05,
      "loss": 0.4843,
      "step": 7580
    },
    {
      "epoch": 1.256206554121152,
      "grad_norm": 23.522798538208008,
      "learning_rate": 3.5021970593206015e-05,
      "loss": 0.805,
      "step": 7590
    },
    {
      "epoch": 1.2578616352201257,
      "grad_norm": 16.689361572265625,
      "learning_rate": 3.500084502281562e-05,
      "loss": 0.6471,
      "step": 7600
    },
    {
      "epoch": 1.2595167163190997,
      "grad_norm": 5.364867210388184,
      "learning_rate": 3.497971945242522e-05,
      "loss": 0.3531,
      "step": 7610
    },
    {
      "epoch": 1.2611717974180734,
      "grad_norm": 45.445430755615234,
      "learning_rate": 3.495859388203482e-05,
      "loss": 0.9353,
      "step": 7620
    },
    {
      "epoch": 1.2628268785170473,
      "grad_norm": 60.19402313232422,
      "learning_rate": 3.4937468311644415e-05,
      "loss": 0.571,
      "step": 7630
    },
    {
      "epoch": 1.2644819596160213,
      "grad_norm": 7.6822967529296875,
      "learning_rate": 3.4916342741254014e-05,
      "loss": 0.8742,
      "step": 7640
    },
    {
      "epoch": 1.266137040714995,
      "grad_norm": 54.923805236816406,
      "learning_rate": 3.489521717086361e-05,
      "loss": 0.5599,
      "step": 7650
    },
    {
      "epoch": 1.267792121813969,
      "grad_norm": 54.36650466918945,
      "learning_rate": 3.487409160047321e-05,
      "loss": 0.5213,
      "step": 7660
    },
    {
      "epoch": 1.2694472029129427,
      "grad_norm": 62.123775482177734,
      "learning_rate": 3.485296603008281e-05,
      "loss": 0.6604,
      "step": 7670
    },
    {
      "epoch": 1.2711022840119166,
      "grad_norm": 34.18712615966797,
      "learning_rate": 3.4831840459692414e-05,
      "loss": 0.4916,
      "step": 7680
    },
    {
      "epoch": 1.2727573651108903,
      "grad_norm": 77.72297668457031,
      "learning_rate": 3.481071488930201e-05,
      "loss": 0.494,
      "step": 7690
    },
    {
      "epoch": 1.2744124462098643,
      "grad_norm": 37.85313415527344,
      "learning_rate": 3.478958931891161e-05,
      "loss": 0.785,
      "step": 7700
    },
    {
      "epoch": 1.2760675273088382,
      "grad_norm": 58.1658821105957,
      "learning_rate": 3.4768463748521216e-05,
      "loss": 0.8302,
      "step": 7710
    },
    {
      "epoch": 1.277722608407812,
      "grad_norm": 51.591182708740234,
      "learning_rate": 3.4747338178130814e-05,
      "loss": 0.2979,
      "step": 7720
    },
    {
      "epoch": 1.2793776895067859,
      "grad_norm": 57.990150451660156,
      "learning_rate": 3.472621260774041e-05,
      "loss": 0.6638,
      "step": 7730
    },
    {
      "epoch": 1.2810327706057598,
      "grad_norm": 21.427705764770508,
      "learning_rate": 3.470508703735001e-05,
      "loss": 0.8636,
      "step": 7740
    },
    {
      "epoch": 1.2826878517047335,
      "grad_norm": 9.190192222595215,
      "learning_rate": 3.468396146695961e-05,
      "loss": 0.437,
      "step": 7750
    },
    {
      "epoch": 1.2843429328037073,
      "grad_norm": 21.447153091430664,
      "learning_rate": 3.466283589656921e-05,
      "loss": 0.826,
      "step": 7760
    },
    {
      "epoch": 1.2859980139026812,
      "grad_norm": 17.203527450561523,
      "learning_rate": 3.464171032617881e-05,
      "loss": 0.4666,
      "step": 7770
    },
    {
      "epoch": 1.2876530950016551,
      "grad_norm": 35.42194747924805,
      "learning_rate": 3.462058475578841e-05,
      "loss": 0.6275,
      "step": 7780
    },
    {
      "epoch": 1.2893081761006289,
      "grad_norm": 24.349218368530273,
      "learning_rate": 3.459945918539801e-05,
      "loss": 0.7362,
      "step": 7790
    },
    {
      "epoch": 1.2909632571996028,
      "grad_norm": 62.74641799926758,
      "learning_rate": 3.457833361500761e-05,
      "loss": 0.3824,
      "step": 7800
    },
    {
      "epoch": 1.2926183382985768,
      "grad_norm": 57.760250091552734,
      "learning_rate": 3.455720804461721e-05,
      "loss": 0.7394,
      "step": 7810
    },
    {
      "epoch": 1.2942734193975505,
      "grad_norm": 31.35919189453125,
      "learning_rate": 3.4536082474226805e-05,
      "loss": 0.6486,
      "step": 7820
    },
    {
      "epoch": 1.2959285004965242,
      "grad_norm": 19.552099227905273,
      "learning_rate": 3.4514956903836403e-05,
      "loss": 0.5991,
      "step": 7830
    },
    {
      "epoch": 1.2975835815954981,
      "grad_norm": 27.119735717773438,
      "learning_rate": 3.4493831333446e-05,
      "loss": 0.4756,
      "step": 7840
    },
    {
      "epoch": 1.299238662694472,
      "grad_norm": 15.046406745910645,
      "learning_rate": 3.44727057630556e-05,
      "loss": 0.5557,
      "step": 7850
    },
    {
      "epoch": 1.3008937437934458,
      "grad_norm": 59.926658630371094,
      "learning_rate": 3.4451580192665205e-05,
      "loss": 0.4217,
      "step": 7860
    },
    {
      "epoch": 1.3025488248924197,
      "grad_norm": 1.7295507192611694,
      "learning_rate": 3.4430454622274804e-05,
      "loss": 0.5906,
      "step": 7870
    },
    {
      "epoch": 1.3042039059913937,
      "grad_norm": 10.3943510055542,
      "learning_rate": 3.44093290518844e-05,
      "loss": 0.2218,
      "step": 7880
    },
    {
      "epoch": 1.3058589870903674,
      "grad_norm": 19.361141204833984,
      "learning_rate": 3.4388203481494e-05,
      "loss": 0.9833,
      "step": 7890
    },
    {
      "epoch": 1.3075140681893413,
      "grad_norm": 9.771082878112793,
      "learning_rate": 3.43670779111036e-05,
      "loss": 0.4198,
      "step": 7900
    },
    {
      "epoch": 1.309169149288315,
      "grad_norm": 26.844057083129883,
      "learning_rate": 3.43459523407132e-05,
      "loss": 0.674,
      "step": 7910
    },
    {
      "epoch": 1.310824230387289,
      "grad_norm": 67.22569274902344,
      "learning_rate": 3.4324826770322796e-05,
      "loss": 0.6934,
      "step": 7920
    },
    {
      "epoch": 1.3124793114862627,
      "grad_norm": 32.03361892700195,
      "learning_rate": 3.43037011999324e-05,
      "loss": 0.627,
      "step": 7930
    },
    {
      "epoch": 1.3141343925852367,
      "grad_norm": 40.793785095214844,
      "learning_rate": 3.4282575629542e-05,
      "loss": 0.5268,
      "step": 7940
    },
    {
      "epoch": 1.3157894736842106,
      "grad_norm": 29.476661682128906,
      "learning_rate": 3.42614500591516e-05,
      "loss": 0.6516,
      "step": 7950
    },
    {
      "epoch": 1.3174445547831843,
      "grad_norm": 25.695566177368164,
      "learning_rate": 3.42403244887612e-05,
      "loss": 0.6099,
      "step": 7960
    },
    {
      "epoch": 1.3190996358821583,
      "grad_norm": 41.39927673339844,
      "learning_rate": 3.42191989183708e-05,
      "loss": 0.614,
      "step": 7970
    },
    {
      "epoch": 1.320754716981132,
      "grad_norm": 59.47270584106445,
      "learning_rate": 3.41980733479804e-05,
      "loss": 0.8386,
      "step": 7980
    },
    {
      "epoch": 1.322409798080106,
      "grad_norm": 30.169767379760742,
      "learning_rate": 3.417694777759e-05,
      "loss": 0.6558,
      "step": 7990
    },
    {
      "epoch": 1.3240648791790797,
      "grad_norm": 41.621551513671875,
      "learning_rate": 3.4155822207199596e-05,
      "loss": 0.9552,
      "step": 8000
    },
    {
      "epoch": 1.3257199602780536,
      "grad_norm": 29.71756362915039,
      "learning_rate": 3.4134696636809195e-05,
      "loss": 0.7129,
      "step": 8010
    },
    {
      "epoch": 1.3273750413770276,
      "grad_norm": 9.66351318359375,
      "learning_rate": 3.411357106641879e-05,
      "loss": 0.9169,
      "step": 8020
    },
    {
      "epoch": 1.3290301224760013,
      "grad_norm": 39.547183990478516,
      "learning_rate": 3.40924454960284e-05,
      "loss": 0.4972,
      "step": 8030
    },
    {
      "epoch": 1.3306852035749752,
      "grad_norm": 59.44026184082031,
      "learning_rate": 3.4071319925638e-05,
      "loss": 0.5513,
      "step": 8040
    },
    {
      "epoch": 1.3323402846739492,
      "grad_norm": 25.111730575561523,
      "learning_rate": 3.4050194355247595e-05,
      "loss": 0.7256,
      "step": 8050
    },
    {
      "epoch": 1.3339953657729229,
      "grad_norm": 16.908447265625,
      "learning_rate": 3.402906878485719e-05,
      "loss": 0.5171,
      "step": 8060
    },
    {
      "epoch": 1.3356504468718966,
      "grad_norm": 55.91846466064453,
      "learning_rate": 3.400794321446679e-05,
      "loss": 0.6413,
      "step": 8070
    },
    {
      "epoch": 1.3373055279708705,
      "grad_norm": 80.03086853027344,
      "learning_rate": 3.398681764407639e-05,
      "loss": 0.7492,
      "step": 8080
    },
    {
      "epoch": 1.3389606090698445,
      "grad_norm": 76.32337951660156,
      "learning_rate": 3.396569207368599e-05,
      "loss": 0.6072,
      "step": 8090
    },
    {
      "epoch": 1.3406156901688182,
      "grad_norm": 20.380544662475586,
      "learning_rate": 3.394456650329559e-05,
      "loss": 0.5263,
      "step": 8100
    },
    {
      "epoch": 1.3422707712677922,
      "grad_norm": 8.669727325439453,
      "learning_rate": 3.392344093290519e-05,
      "loss": 0.812,
      "step": 8110
    },
    {
      "epoch": 1.343925852366766,
      "grad_norm": 13.086383819580078,
      "learning_rate": 3.390231536251479e-05,
      "loss": 0.6629,
      "step": 8120
    },
    {
      "epoch": 1.3455809334657398,
      "grad_norm": 123.90145874023438,
      "learning_rate": 3.388118979212439e-05,
      "loss": 0.7059,
      "step": 8130
    },
    {
      "epoch": 1.3472360145647135,
      "grad_norm": 50.16926574707031,
      "learning_rate": 3.386006422173399e-05,
      "loss": 0.6248,
      "step": 8140
    },
    {
      "epoch": 1.3488910956636875,
      "grad_norm": 25.21413230895996,
      "learning_rate": 3.3838938651343586e-05,
      "loss": 0.5408,
      "step": 8150
    },
    {
      "epoch": 1.3505461767626614,
      "grad_norm": 24.537731170654297,
      "learning_rate": 3.3817813080953184e-05,
      "loss": 0.589,
      "step": 8160
    },
    {
      "epoch": 1.3522012578616351,
      "grad_norm": 35.68827438354492,
      "learning_rate": 3.379668751056278e-05,
      "loss": 0.8553,
      "step": 8170
    },
    {
      "epoch": 1.353856338960609,
      "grad_norm": 48.56003952026367,
      "learning_rate": 3.377556194017239e-05,
      "loss": 0.7354,
      "step": 8180
    },
    {
      "epoch": 1.355511420059583,
      "grad_norm": 36.26851272583008,
      "learning_rate": 3.3754436369781986e-05,
      "loss": 0.5568,
      "step": 8190
    },
    {
      "epoch": 1.3571665011585567,
      "grad_norm": 7.772048473358154,
      "learning_rate": 3.3733310799391584e-05,
      "loss": 0.394,
      "step": 8200
    },
    {
      "epoch": 1.3588215822575307,
      "grad_norm": 23.651857376098633,
      "learning_rate": 3.371218522900119e-05,
      "loss": 0.5855,
      "step": 8210
    },
    {
      "epoch": 1.3604766633565044,
      "grad_norm": 31.869905471801758,
      "learning_rate": 3.369105965861079e-05,
      "loss": 1.0781,
      "step": 8220
    },
    {
      "epoch": 1.3621317444554784,
      "grad_norm": 52.5668830871582,
      "learning_rate": 3.3669934088220386e-05,
      "loss": 0.7639,
      "step": 8230
    },
    {
      "epoch": 1.363786825554452,
      "grad_norm": 14.778186798095703,
      "learning_rate": 3.3648808517829985e-05,
      "loss": 0.8815,
      "step": 8240
    },
    {
      "epoch": 1.365441906653426,
      "grad_norm": 58.815765380859375,
      "learning_rate": 3.362768294743958e-05,
      "loss": 0.566,
      "step": 8250
    },
    {
      "epoch": 1.3670969877524,
      "grad_norm": 37.205787658691406,
      "learning_rate": 3.360655737704918e-05,
      "loss": 0.3333,
      "step": 8260
    },
    {
      "epoch": 1.3687520688513737,
      "grad_norm": 34.678627014160156,
      "learning_rate": 3.358543180665878e-05,
      "loss": 0.6066,
      "step": 8270
    },
    {
      "epoch": 1.3704071499503476,
      "grad_norm": 46.48219680786133,
      "learning_rate": 3.3564306236268385e-05,
      "loss": 0.6812,
      "step": 8280
    },
    {
      "epoch": 1.3720622310493213,
      "grad_norm": 86.53052520751953,
      "learning_rate": 3.354318066587798e-05,
      "loss": 0.5797,
      "step": 8290
    },
    {
      "epoch": 1.3737173121482953,
      "grad_norm": 11.134310722351074,
      "learning_rate": 3.352205509548758e-05,
      "loss": 0.7021,
      "step": 8300
    },
    {
      "epoch": 1.375372393247269,
      "grad_norm": 17.571714401245117,
      "learning_rate": 3.350092952509718e-05,
      "loss": 0.4105,
      "step": 8310
    },
    {
      "epoch": 1.377027474346243,
      "grad_norm": 73.83724212646484,
      "learning_rate": 3.347980395470678e-05,
      "loss": 0.6573,
      "step": 8320
    },
    {
      "epoch": 1.378682555445217,
      "grad_norm": 54.51940155029297,
      "learning_rate": 3.345867838431638e-05,
      "loss": 0.6516,
      "step": 8330
    },
    {
      "epoch": 1.3803376365441906,
      "grad_norm": 25.041194915771484,
      "learning_rate": 3.3437552813925975e-05,
      "loss": 0.6231,
      "step": 8340
    },
    {
      "epoch": 1.3819927176431646,
      "grad_norm": 30.485050201416016,
      "learning_rate": 3.3416427243535574e-05,
      "loss": 0.578,
      "step": 8350
    },
    {
      "epoch": 1.3836477987421385,
      "grad_norm": 95.35092163085938,
      "learning_rate": 3.339530167314517e-05,
      "loss": 0.695,
      "step": 8360
    },
    {
      "epoch": 1.3853028798411122,
      "grad_norm": 39.1922607421875,
      "learning_rate": 3.337417610275478e-05,
      "loss": 0.8307,
      "step": 8370
    },
    {
      "epoch": 1.386957960940086,
      "grad_norm": 24.229183197021484,
      "learning_rate": 3.3353050532364376e-05,
      "loss": 0.5491,
      "step": 8380
    },
    {
      "epoch": 1.3886130420390599,
      "grad_norm": 24.319957733154297,
      "learning_rate": 3.3331924961973974e-05,
      "loss": 0.5026,
      "step": 8390
    },
    {
      "epoch": 1.3902681231380338,
      "grad_norm": 1.4365705251693726,
      "learning_rate": 3.331079939158357e-05,
      "loss": 0.454,
      "step": 8400
    },
    {
      "epoch": 1.3919232042370075,
      "grad_norm": 19.758359909057617,
      "learning_rate": 3.328967382119317e-05,
      "loss": 0.9188,
      "step": 8410
    },
    {
      "epoch": 1.3935782853359815,
      "grad_norm": 26.6937313079834,
      "learning_rate": 3.326854825080277e-05,
      "loss": 0.6399,
      "step": 8420
    },
    {
      "epoch": 1.3952333664349554,
      "grad_norm": 26.592512130737305,
      "learning_rate": 3.3247422680412374e-05,
      "loss": 0.8049,
      "step": 8430
    },
    {
      "epoch": 1.3968884475339292,
      "grad_norm": 3.4461278915405273,
      "learning_rate": 3.322629711002197e-05,
      "loss": 0.7795,
      "step": 8440
    },
    {
      "epoch": 1.3985435286329029,
      "grad_norm": 49.00618362426758,
      "learning_rate": 3.320517153963157e-05,
      "loss": 0.6297,
      "step": 8450
    },
    {
      "epoch": 1.4001986097318768,
      "grad_norm": 37.74024200439453,
      "learning_rate": 3.3184045969241176e-05,
      "loss": 0.8044,
      "step": 8460
    },
    {
      "epoch": 1.4018536908308508,
      "grad_norm": 58.21160888671875,
      "learning_rate": 3.3162920398850775e-05,
      "loss": 0.676,
      "step": 8470
    },
    {
      "epoch": 1.4035087719298245,
      "grad_norm": 11.393818855285645,
      "learning_rate": 3.314179482846037e-05,
      "loss": 0.5305,
      "step": 8480
    },
    {
      "epoch": 1.4051638530287984,
      "grad_norm": 42.05527114868164,
      "learning_rate": 3.312066925806997e-05,
      "loss": 0.4555,
      "step": 8490
    },
    {
      "epoch": 1.4068189341277724,
      "grad_norm": 67.26050567626953,
      "learning_rate": 3.309954368767957e-05,
      "loss": 0.6629,
      "step": 8500
    },
    {
      "epoch": 1.408474015226746,
      "grad_norm": 15.186110496520996,
      "learning_rate": 3.307841811728917e-05,
      "loss": 0.5151,
      "step": 8510
    },
    {
      "epoch": 1.41012909632572,
      "grad_norm": 9.584951400756836,
      "learning_rate": 3.3057292546898766e-05,
      "loss": 0.5331,
      "step": 8520
    },
    {
      "epoch": 1.4117841774246938,
      "grad_norm": 101.70578002929688,
      "learning_rate": 3.3036166976508365e-05,
      "loss": 0.3978,
      "step": 8530
    },
    {
      "epoch": 1.4134392585236677,
      "grad_norm": 95.73712921142578,
      "learning_rate": 3.301504140611797e-05,
      "loss": 0.5473,
      "step": 8540
    },
    {
      "epoch": 1.4150943396226414,
      "grad_norm": 4.075655937194824,
      "learning_rate": 3.299391583572757e-05,
      "loss": 0.4768,
      "step": 8550
    },
    {
      "epoch": 1.4167494207216154,
      "grad_norm": 13.103824615478516,
      "learning_rate": 3.297279026533717e-05,
      "loss": 0.4872,
      "step": 8560
    },
    {
      "epoch": 1.4184045018205893,
      "grad_norm": 16.777809143066406,
      "learning_rate": 3.2951664694946765e-05,
      "loss": 0.6486,
      "step": 8570
    },
    {
      "epoch": 1.420059582919563,
      "grad_norm": 10.220786094665527,
      "learning_rate": 3.2930539124556364e-05,
      "loss": 0.663,
      "step": 8580
    },
    {
      "epoch": 1.421714664018537,
      "grad_norm": 22.435876846313477,
      "learning_rate": 3.290941355416596e-05,
      "loss": 0.956,
      "step": 8590
    },
    {
      "epoch": 1.4233697451175107,
      "grad_norm": 6.458202362060547,
      "learning_rate": 3.288828798377556e-05,
      "loss": 0.471,
      "step": 8600
    },
    {
      "epoch": 1.4250248262164846,
      "grad_norm": 31.732744216918945,
      "learning_rate": 3.286716241338516e-05,
      "loss": 0.872,
      "step": 8610
    },
    {
      "epoch": 1.4266799073154584,
      "grad_norm": 7.74770450592041,
      "learning_rate": 3.2846036842994764e-05,
      "loss": 0.5478,
      "step": 8620
    },
    {
      "epoch": 1.4283349884144323,
      "grad_norm": 13.017435073852539,
      "learning_rate": 3.282491127260436e-05,
      "loss": 0.4509,
      "step": 8630
    },
    {
      "epoch": 1.4299900695134062,
      "grad_norm": 54.090858459472656,
      "learning_rate": 3.280378570221396e-05,
      "loss": 0.5639,
      "step": 8640
    },
    {
      "epoch": 1.43164515061238,
      "grad_norm": 46.6500358581543,
      "learning_rate": 3.278266013182356e-05,
      "loss": 0.4266,
      "step": 8650
    },
    {
      "epoch": 1.433300231711354,
      "grad_norm": 32.937015533447266,
      "learning_rate": 3.276153456143316e-05,
      "loss": 0.9757,
      "step": 8660
    },
    {
      "epoch": 1.4349553128103278,
      "grad_norm": 31.470365524291992,
      "learning_rate": 3.274040899104276e-05,
      "loss": 0.4901,
      "step": 8670
    },
    {
      "epoch": 1.4366103939093016,
      "grad_norm": 7.1170973777771,
      "learning_rate": 3.271928342065236e-05,
      "loss": 0.6605,
      "step": 8680
    },
    {
      "epoch": 1.4382654750082753,
      "grad_norm": 14.390827178955078,
      "learning_rate": 3.269815785026196e-05,
      "loss": 0.3847,
      "step": 8690
    },
    {
      "epoch": 1.4399205561072492,
      "grad_norm": 20.143524169921875,
      "learning_rate": 3.267703227987156e-05,
      "loss": 0.482,
      "step": 8700
    },
    {
      "epoch": 1.4415756372062232,
      "grad_norm": 18.338457107543945,
      "learning_rate": 3.265590670948116e-05,
      "loss": 0.4701,
      "step": 8710
    },
    {
      "epoch": 1.443230718305197,
      "grad_norm": 37.5656852722168,
      "learning_rate": 3.263478113909076e-05,
      "loss": 0.5971,
      "step": 8720
    },
    {
      "epoch": 1.4448857994041708,
      "grad_norm": 67.01133728027344,
      "learning_rate": 3.261365556870036e-05,
      "loss": 0.5971,
      "step": 8730
    },
    {
      "epoch": 1.4465408805031448,
      "grad_norm": 51.83787155151367,
      "learning_rate": 3.259252999830996e-05,
      "loss": 0.6175,
      "step": 8740
    },
    {
      "epoch": 1.4481959616021185,
      "grad_norm": 15.789324760437012,
      "learning_rate": 3.2571404427919556e-05,
      "loss": 0.8937,
      "step": 8750
    },
    {
      "epoch": 1.4498510427010922,
      "grad_norm": 44.99809646606445,
      "learning_rate": 3.2550278857529155e-05,
      "loss": 0.7993,
      "step": 8760
    },
    {
      "epoch": 1.4515061238000662,
      "grad_norm": 10.544629096984863,
      "learning_rate": 3.252915328713875e-05,
      "loss": 0.3489,
      "step": 8770
    },
    {
      "epoch": 1.45316120489904,
      "grad_norm": 60.939544677734375,
      "learning_rate": 3.250802771674835e-05,
      "loss": 0.4646,
      "step": 8780
    },
    {
      "epoch": 1.4548162859980138,
      "grad_norm": 29.269969940185547,
      "learning_rate": 3.248690214635796e-05,
      "loss": 0.5927,
      "step": 8790
    },
    {
      "epoch": 1.4564713670969878,
      "grad_norm": 44.66840362548828,
      "learning_rate": 3.2465776575967555e-05,
      "loss": 0.6669,
      "step": 8800
    },
    {
      "epoch": 1.4581264481959617,
      "grad_norm": 29.14904022216797,
      "learning_rate": 3.2444651005577153e-05,
      "loss": 0.5756,
      "step": 8810
    },
    {
      "epoch": 1.4597815292949354,
      "grad_norm": 19.601720809936523,
      "learning_rate": 3.242352543518675e-05,
      "loss": 0.5734,
      "step": 8820
    },
    {
      "epoch": 1.4614366103939094,
      "grad_norm": 31.914987564086914,
      "learning_rate": 3.240239986479635e-05,
      "loss": 0.4395,
      "step": 8830
    },
    {
      "epoch": 1.463091691492883,
      "grad_norm": 34.41419982910156,
      "learning_rate": 3.238127429440595e-05,
      "loss": 0.6304,
      "step": 8840
    },
    {
      "epoch": 1.464746772591857,
      "grad_norm": 2.672409772872925,
      "learning_rate": 3.236014872401555e-05,
      "loss": 0.6812,
      "step": 8850
    },
    {
      "epoch": 1.4664018536908308,
      "grad_norm": 29.685413360595703,
      "learning_rate": 3.2339023153625145e-05,
      "loss": 0.8919,
      "step": 8860
    },
    {
      "epoch": 1.4680569347898047,
      "grad_norm": 3.2807388305664062,
      "learning_rate": 3.2317897583234744e-05,
      "loss": 0.3255,
      "step": 8870
    },
    {
      "epoch": 1.4697120158887786,
      "grad_norm": 57.28415298461914,
      "learning_rate": 3.229677201284435e-05,
      "loss": 0.7602,
      "step": 8880
    },
    {
      "epoch": 1.4713670969877524,
      "grad_norm": 116.7706069946289,
      "learning_rate": 3.227564644245395e-05,
      "loss": 0.5619,
      "step": 8890
    },
    {
      "epoch": 1.4730221780867263,
      "grad_norm": 44.44943618774414,
      "learning_rate": 3.2254520872063546e-05,
      "loss": 0.8346,
      "step": 8900
    },
    {
      "epoch": 1.4746772591857,
      "grad_norm": 65.38809967041016,
      "learning_rate": 3.2233395301673144e-05,
      "loss": 0.8983,
      "step": 8910
    },
    {
      "epoch": 1.476332340284674,
      "grad_norm": 33.51227569580078,
      "learning_rate": 3.221226973128275e-05,
      "loss": 0.5711,
      "step": 8920
    },
    {
      "epoch": 1.4779874213836477,
      "grad_norm": 35.03468704223633,
      "learning_rate": 3.219114416089235e-05,
      "loss": 0.5884,
      "step": 8930
    },
    {
      "epoch": 1.4796425024826216,
      "grad_norm": 7.029872417449951,
      "learning_rate": 3.2170018590501946e-05,
      "loss": 0.3541,
      "step": 8940
    },
    {
      "epoch": 1.4812975835815956,
      "grad_norm": 12.61609935760498,
      "learning_rate": 3.2148893020111544e-05,
      "loss": 0.3129,
      "step": 8950
    },
    {
      "epoch": 1.4829526646805693,
      "grad_norm": 10.268706321716309,
      "learning_rate": 3.212776744972115e-05,
      "loss": 0.5844,
      "step": 8960
    },
    {
      "epoch": 1.4846077457795432,
      "grad_norm": 16.197996139526367,
      "learning_rate": 3.210664187933075e-05,
      "loss": 0.7023,
      "step": 8970
    },
    {
      "epoch": 1.4862628268785172,
      "grad_norm": 24.211841583251953,
      "learning_rate": 3.2085516308940346e-05,
      "loss": 0.5818,
      "step": 8980
    },
    {
      "epoch": 1.487917907977491,
      "grad_norm": 28.82326889038086,
      "learning_rate": 3.2064390738549945e-05,
      "loss": 0.6874,
      "step": 8990
    },
    {
      "epoch": 1.4895729890764646,
      "grad_norm": 32.24673843383789,
      "learning_rate": 3.204326516815954e-05,
      "loss": 0.5749,
      "step": 9000
    },
    {
      "epoch": 1.4912280701754386,
      "grad_norm": 22.990232467651367,
      "learning_rate": 3.202213959776914e-05,
      "loss": 0.5347,
      "step": 9010
    },
    {
      "epoch": 1.4928831512744125,
      "grad_norm": 43.5405387878418,
      "learning_rate": 3.200101402737874e-05,
      "loss": 0.5467,
      "step": 9020
    },
    {
      "epoch": 1.4945382323733862,
      "grad_norm": 25.683395385742188,
      "learning_rate": 3.197988845698834e-05,
      "loss": 0.6819,
      "step": 9030
    },
    {
      "epoch": 1.4961933134723602,
      "grad_norm": 34.148529052734375,
      "learning_rate": 3.1958762886597937e-05,
      "loss": 0.8178,
      "step": 9040
    },
    {
      "epoch": 1.4978483945713341,
      "grad_norm": 21.778461456298828,
      "learning_rate": 3.193763731620754e-05,
      "loss": 0.4577,
      "step": 9050
    },
    {
      "epoch": 1.4995034756703078,
      "grad_norm": 28.194236755371094,
      "learning_rate": 3.191651174581714e-05,
      "loss": 0.4455,
      "step": 9060
    },
    {
      "epoch": 1.5011585567692816,
      "grad_norm": 40.204368591308594,
      "learning_rate": 3.189538617542674e-05,
      "loss": 0.6206,
      "step": 9070
    },
    {
      "epoch": 1.5028136378682555,
      "grad_norm": 16.206832885742188,
      "learning_rate": 3.187426060503634e-05,
      "loss": 0.6492,
      "step": 9080
    },
    {
      "epoch": 1.5044687189672294,
      "grad_norm": 9.411066055297852,
      "learning_rate": 3.1853135034645935e-05,
      "loss": 0.9514,
      "step": 9090
    },
    {
      "epoch": 1.5061238000662032,
      "grad_norm": 5.627820014953613,
      "learning_rate": 3.1832009464255534e-05,
      "loss": 0.7044,
      "step": 9100
    },
    {
      "epoch": 1.507778881165177,
      "grad_norm": 70.86753845214844,
      "learning_rate": 3.181088389386513e-05,
      "loss": 0.6505,
      "step": 9110
    },
    {
      "epoch": 1.509433962264151,
      "grad_norm": 4.1608734130859375,
      "learning_rate": 3.178975832347473e-05,
      "loss": 0.2963,
      "step": 9120
    },
    {
      "epoch": 1.5110890433631248,
      "grad_norm": 52.4428825378418,
      "learning_rate": 3.1768632753084336e-05,
      "loss": 0.6361,
      "step": 9130
    },
    {
      "epoch": 1.5127441244620985,
      "grad_norm": 21.825822830200195,
      "learning_rate": 3.1747507182693934e-05,
      "loss": 0.3431,
      "step": 9140
    },
    {
      "epoch": 1.5143992055610727,
      "grad_norm": 45.61469268798828,
      "learning_rate": 3.172638161230353e-05,
      "loss": 0.8964,
      "step": 9150
    },
    {
      "epoch": 1.5160542866600464,
      "grad_norm": 9.846871376037598,
      "learning_rate": 3.170525604191313e-05,
      "loss": 0.4893,
      "step": 9160
    },
    {
      "epoch": 1.51770936775902,
      "grad_norm": 85.58138275146484,
      "learning_rate": 3.1684130471522736e-05,
      "loss": 0.65,
      "step": 9170
    },
    {
      "epoch": 1.519364448857994,
      "grad_norm": 37.59318161010742,
      "learning_rate": 3.1663004901132334e-05,
      "loss": 0.5762,
      "step": 9180
    },
    {
      "epoch": 1.521019529956968,
      "grad_norm": 22.9913387298584,
      "learning_rate": 3.164187933074193e-05,
      "loss": 0.2699,
      "step": 9190
    },
    {
      "epoch": 1.5226746110559417,
      "grad_norm": 44.0463752746582,
      "learning_rate": 3.162075376035153e-05,
      "loss": 0.5141,
      "step": 9200
    },
    {
      "epoch": 1.5243296921549154,
      "grad_norm": 30.60729217529297,
      "learning_rate": 3.159962818996113e-05,
      "loss": 0.4922,
      "step": 9210
    },
    {
      "epoch": 1.5259847732538896,
      "grad_norm": 51.889896392822266,
      "learning_rate": 3.1578502619570735e-05,
      "loss": 0.4561,
      "step": 9220
    },
    {
      "epoch": 1.5276398543528633,
      "grad_norm": 28.701465606689453,
      "learning_rate": 3.155737704918033e-05,
      "loss": 0.6716,
      "step": 9230
    },
    {
      "epoch": 1.529294935451837,
      "grad_norm": 52.00653076171875,
      "learning_rate": 3.153625147878993e-05,
      "loss": 0.5267,
      "step": 9240
    },
    {
      "epoch": 1.530950016550811,
      "grad_norm": 17.409868240356445,
      "learning_rate": 3.151512590839953e-05,
      "loss": 0.7417,
      "step": 9250
    },
    {
      "epoch": 1.532605097649785,
      "grad_norm": 29.054611206054688,
      "learning_rate": 3.149400033800913e-05,
      "loss": 0.4541,
      "step": 9260
    },
    {
      "epoch": 1.5342601787487586,
      "grad_norm": 13.986786842346191,
      "learning_rate": 3.1472874767618727e-05,
      "loss": 0.7004,
      "step": 9270
    },
    {
      "epoch": 1.5359152598477326,
      "grad_norm": 31.387351989746094,
      "learning_rate": 3.1451749197228325e-05,
      "loss": 0.5786,
      "step": 9280
    },
    {
      "epoch": 1.5375703409467065,
      "grad_norm": 33.96162796020508,
      "learning_rate": 3.143062362683792e-05,
      "loss": 0.6437,
      "step": 9290
    },
    {
      "epoch": 1.5392254220456802,
      "grad_norm": 8.957194328308105,
      "learning_rate": 3.140949805644753e-05,
      "loss": 0.6246,
      "step": 9300
    },
    {
      "epoch": 1.540880503144654,
      "grad_norm": 32.73854446411133,
      "learning_rate": 3.138837248605713e-05,
      "loss": 0.5088,
      "step": 9310
    },
    {
      "epoch": 1.542535584243628,
      "grad_norm": 84.86222076416016,
      "learning_rate": 3.1367246915666725e-05,
      "loss": 0.3885,
      "step": 9320
    },
    {
      "epoch": 1.5441906653426019,
      "grad_norm": 50.55561447143555,
      "learning_rate": 3.1346121345276324e-05,
      "loss": 0.7482,
      "step": 9330
    },
    {
      "epoch": 1.5458457464415756,
      "grad_norm": 67.17286682128906,
      "learning_rate": 3.132499577488592e-05,
      "loss": 0.5577,
      "step": 9340
    },
    {
      "epoch": 1.5475008275405495,
      "grad_norm": 21.51382064819336,
      "learning_rate": 3.130387020449552e-05,
      "loss": 0.5201,
      "step": 9350
    },
    {
      "epoch": 1.5491559086395235,
      "grad_norm": 110.96090698242188,
      "learning_rate": 3.128274463410512e-05,
      "loss": 0.671,
      "step": 9360
    },
    {
      "epoch": 1.5508109897384972,
      "grad_norm": 18.40187644958496,
      "learning_rate": 3.126161906371472e-05,
      "loss": 0.2737,
      "step": 9370
    },
    {
      "epoch": 1.552466070837471,
      "grad_norm": 22.279857635498047,
      "learning_rate": 3.1240493493324316e-05,
      "loss": 0.845,
      "step": 9380
    },
    {
      "epoch": 1.5541211519364448,
      "grad_norm": 6.536747455596924,
      "learning_rate": 3.121936792293392e-05,
      "loss": 0.4976,
      "step": 9390
    },
    {
      "epoch": 1.5557762330354188,
      "grad_norm": 22.796611785888672,
      "learning_rate": 3.119824235254352e-05,
      "loss": 0.4937,
      "step": 9400
    },
    {
      "epoch": 1.5574313141343925,
      "grad_norm": 149.44989013671875,
      "learning_rate": 3.117711678215312e-05,
      "loss": 0.335,
      "step": 9410
    },
    {
      "epoch": 1.5590863952333665,
      "grad_norm": 55.38862991333008,
      "learning_rate": 3.115599121176272e-05,
      "loss": 1.1109,
      "step": 9420
    },
    {
      "epoch": 1.5607414763323404,
      "grad_norm": 3.127471923828125,
      "learning_rate": 3.113486564137232e-05,
      "loss": 0.4247,
      "step": 9430
    },
    {
      "epoch": 1.5623965574313141,
      "grad_norm": 60.76400375366211,
      "learning_rate": 3.111374007098192e-05,
      "loss": 0.6916,
      "step": 9440
    },
    {
      "epoch": 1.5640516385302878,
      "grad_norm": 19.881383895874023,
      "learning_rate": 3.109261450059152e-05,
      "loss": 0.6574,
      "step": 9450
    },
    {
      "epoch": 1.5657067196292618,
      "grad_norm": 50.8070068359375,
      "learning_rate": 3.1071488930201116e-05,
      "loss": 0.3729,
      "step": 9460
    },
    {
      "epoch": 1.5673618007282357,
      "grad_norm": 19.472368240356445,
      "learning_rate": 3.105036335981072e-05,
      "loss": 0.8023,
      "step": 9470
    },
    {
      "epoch": 1.5690168818272094,
      "grad_norm": 15.579330444335938,
      "learning_rate": 3.102923778942032e-05,
      "loss": 0.8027,
      "step": 9480
    },
    {
      "epoch": 1.5706719629261834,
      "grad_norm": 17.91141700744629,
      "learning_rate": 3.100811221902992e-05,
      "loss": 0.7519,
      "step": 9490
    },
    {
      "epoch": 1.5723270440251573,
      "grad_norm": 35.47119903564453,
      "learning_rate": 3.0986986648639516e-05,
      "loss": 0.4759,
      "step": 9500
    },
    {
      "epoch": 1.573982125124131,
      "grad_norm": 8.241140365600586,
      "learning_rate": 3.0965861078249115e-05,
      "loss": 0.6297,
      "step": 9510
    },
    {
      "epoch": 1.5756372062231048,
      "grad_norm": 34.762821197509766,
      "learning_rate": 3.094473550785871e-05,
      "loss": 0.5475,
      "step": 9520
    },
    {
      "epoch": 1.577292287322079,
      "grad_norm": 30.46901512145996,
      "learning_rate": 3.092360993746831e-05,
      "loss": 0.5997,
      "step": 9530
    },
    {
      "epoch": 1.5789473684210527,
      "grad_norm": 35.649784088134766,
      "learning_rate": 3.090248436707791e-05,
      "loss": 0.7205,
      "step": 9540
    },
    {
      "epoch": 1.5806024495200264,
      "grad_norm": 91.74974822998047,
      "learning_rate": 3.088135879668751e-05,
      "loss": 0.6053,
      "step": 9550
    },
    {
      "epoch": 1.5822575306190003,
      "grad_norm": 29.002336502075195,
      "learning_rate": 3.0860233226297114e-05,
      "loss": 0.5731,
      "step": 9560
    },
    {
      "epoch": 1.5839126117179743,
      "grad_norm": 43.867374420166016,
      "learning_rate": 3.083910765590671e-05,
      "loss": 0.4108,
      "step": 9570
    },
    {
      "epoch": 1.585567692816948,
      "grad_norm": 32.04021072387695,
      "learning_rate": 3.081798208551631e-05,
      "loss": 0.4387,
      "step": 9580
    },
    {
      "epoch": 1.587222773915922,
      "grad_norm": 92.9800033569336,
      "learning_rate": 3.079685651512591e-05,
      "loss": 0.7612,
      "step": 9590
    },
    {
      "epoch": 1.5888778550148959,
      "grad_norm": 44.01927947998047,
      "learning_rate": 3.077573094473551e-05,
      "loss": 0.5129,
      "step": 9600
    },
    {
      "epoch": 1.5905329361138696,
      "grad_norm": 44.888916015625,
      "learning_rate": 3.0754605374345105e-05,
      "loss": 0.5017,
      "step": 9610
    },
    {
      "epoch": 1.5921880172128433,
      "grad_norm": 38.77029800415039,
      "learning_rate": 3.0733479803954704e-05,
      "loss": 0.4969,
      "step": 9620
    },
    {
      "epoch": 1.5938430983118173,
      "grad_norm": 22.316669464111328,
      "learning_rate": 3.07123542335643e-05,
      "loss": 0.642,
      "step": 9630
    },
    {
      "epoch": 1.5954981794107912,
      "grad_norm": 72.05184173583984,
      "learning_rate": 3.069122866317391e-05,
      "loss": 0.4821,
      "step": 9640
    },
    {
      "epoch": 1.597153260509765,
      "grad_norm": 12.957145690917969,
      "learning_rate": 3.0670103092783506e-05,
      "loss": 0.4255,
      "step": 9650
    },
    {
      "epoch": 1.5988083416087389,
      "grad_norm": 39.423824310302734,
      "learning_rate": 3.0648977522393104e-05,
      "loss": 0.5218,
      "step": 9660
    },
    {
      "epoch": 1.6004634227077128,
      "grad_norm": 66.96188354492188,
      "learning_rate": 3.062785195200271e-05,
      "loss": 0.6767,
      "step": 9670
    },
    {
      "epoch": 1.6021185038066865,
      "grad_norm": 12.80077075958252,
      "learning_rate": 3.060672638161231e-05,
      "loss": 0.8469,
      "step": 9680
    },
    {
      "epoch": 1.6037735849056602,
      "grad_norm": 23.78473663330078,
      "learning_rate": 3.0585600811221906e-05,
      "loss": 0.5575,
      "step": 9690
    },
    {
      "epoch": 1.6054286660046342,
      "grad_norm": 21.829099655151367,
      "learning_rate": 3.0564475240831504e-05,
      "loss": 0.286,
      "step": 9700
    },
    {
      "epoch": 1.6070837471036081,
      "grad_norm": 25.35091781616211,
      "learning_rate": 3.05433496704411e-05,
      "loss": 0.5869,
      "step": 9710
    },
    {
      "epoch": 1.6087388282025818,
      "grad_norm": 17.384611129760742,
      "learning_rate": 3.05222241000507e-05,
      "loss": 0.7723,
      "step": 9720
    },
    {
      "epoch": 1.6103939093015558,
      "grad_norm": 26.734222412109375,
      "learning_rate": 3.0501098529660306e-05,
      "loss": 0.6866,
      "step": 9730
    },
    {
      "epoch": 1.6120489904005297,
      "grad_norm": 8.61534309387207,
      "learning_rate": 3.0479972959269905e-05,
      "loss": 0.3586,
      "step": 9740
    },
    {
      "epoch": 1.6137040714995035,
      "grad_norm": 15.616613388061523,
      "learning_rate": 3.0458847388879503e-05,
      "loss": 0.8698,
      "step": 9750
    },
    {
      "epoch": 1.6153591525984772,
      "grad_norm": 23.286623001098633,
      "learning_rate": 3.04377218184891e-05,
      "loss": 0.7149,
      "step": 9760
    },
    {
      "epoch": 1.6170142336974511,
      "grad_norm": 28.393083572387695,
      "learning_rate": 3.04165962480987e-05,
      "loss": 0.3979,
      "step": 9770
    },
    {
      "epoch": 1.618669314796425,
      "grad_norm": 6.332217693328857,
      "learning_rate": 3.0395470677708298e-05,
      "loss": 0.6687,
      "step": 9780
    },
    {
      "epoch": 1.6203243958953988,
      "grad_norm": 29.016908645629883,
      "learning_rate": 3.0374345107317897e-05,
      "loss": 0.7332,
      "step": 9790
    },
    {
      "epoch": 1.6219794769943727,
      "grad_norm": 38.44139862060547,
      "learning_rate": 3.0353219536927495e-05,
      "loss": 0.6106,
      "step": 9800
    },
    {
      "epoch": 1.6236345580933467,
      "grad_norm": 20.401287078857422,
      "learning_rate": 3.03320939665371e-05,
      "loss": 0.6339,
      "step": 9810
    },
    {
      "epoch": 1.6252896391923204,
      "grad_norm": 8.870390892028809,
      "learning_rate": 3.03109683961467e-05,
      "loss": 0.593,
      "step": 9820
    },
    {
      "epoch": 1.626944720291294,
      "grad_norm": 41.36398696899414,
      "learning_rate": 3.0289842825756297e-05,
      "loss": 0.6334,
      "step": 9830
    },
    {
      "epoch": 1.6285998013902683,
      "grad_norm": 21.159461975097656,
      "learning_rate": 3.02687172553659e-05,
      "loss": 0.46,
      "step": 9840
    },
    {
      "epoch": 1.630254882489242,
      "grad_norm": 39.25425338745117,
      "learning_rate": 3.0247591684975497e-05,
      "loss": 0.7403,
      "step": 9850
    },
    {
      "epoch": 1.6319099635882157,
      "grad_norm": 54.87135696411133,
      "learning_rate": 3.0226466114585096e-05,
      "loss": 0.5604,
      "step": 9860
    },
    {
      "epoch": 1.6335650446871897,
      "grad_norm": 26.803190231323242,
      "learning_rate": 3.0205340544194694e-05,
      "loss": 0.785,
      "step": 9870
    },
    {
      "epoch": 1.6352201257861636,
      "grad_norm": 13.269815444946289,
      "learning_rate": 3.0184214973804292e-05,
      "loss": 0.5769,
      "step": 9880
    },
    {
      "epoch": 1.6368752068851373,
      "grad_norm": 6.998653888702393,
      "learning_rate": 3.016308940341389e-05,
      "loss": 0.6431,
      "step": 9890
    },
    {
      "epoch": 1.6385302879841113,
      "grad_norm": 13.511797904968262,
      "learning_rate": 3.0141963833023496e-05,
      "loss": 0.5748,
      "step": 9900
    },
    {
      "epoch": 1.6401853690830852,
      "grad_norm": 43.419158935546875,
      "learning_rate": 3.0120838262633094e-05,
      "loss": 0.5672,
      "step": 9910
    },
    {
      "epoch": 1.641840450182059,
      "grad_norm": 17.68165397644043,
      "learning_rate": 3.0099712692242693e-05,
      "loss": 0.7234,
      "step": 9920
    },
    {
      "epoch": 1.6434955312810327,
      "grad_norm": 14.90223217010498,
      "learning_rate": 3.007858712185229e-05,
      "loss": 0.7044,
      "step": 9930
    },
    {
      "epoch": 1.6451506123800066,
      "grad_norm": 39.667442321777344,
      "learning_rate": 3.005746155146189e-05,
      "loss": 0.642,
      "step": 9940
    },
    {
      "epoch": 1.6468056934789805,
      "grad_norm": 12.217744827270508,
      "learning_rate": 3.003633598107149e-05,
      "loss": 0.4655,
      "step": 9950
    },
    {
      "epoch": 1.6484607745779543,
      "grad_norm": 24.09513282775879,
      "learning_rate": 3.001521041068109e-05,
      "loss": 0.4373,
      "step": 9960
    },
    {
      "epoch": 1.6501158556769282,
      "grad_norm": 33.06611251831055,
      "learning_rate": 2.9994084840290688e-05,
      "loss": 0.5267,
      "step": 9970
    },
    {
      "epoch": 1.6517709367759021,
      "grad_norm": 105.32717895507812,
      "learning_rate": 2.9972959269900293e-05,
      "loss": 0.4994,
      "step": 9980
    },
    {
      "epoch": 1.6534260178748759,
      "grad_norm": 47.62408447265625,
      "learning_rate": 2.995183369950989e-05,
      "loss": 0.8017,
      "step": 9990
    },
    {
      "epoch": 1.6550810989738496,
      "grad_norm": 43.02900314331055,
      "learning_rate": 2.993070812911949e-05,
      "loss": 0.6853,
      "step": 10000
    },
    {
      "epoch": 1.6567361800728235,
      "grad_norm": 17.933164596557617,
      "learning_rate": 2.9909582558729088e-05,
      "loss": 0.6629,
      "step": 10010
    },
    {
      "epoch": 1.6583912611717975,
      "grad_norm": 33.135047912597656,
      "learning_rate": 2.9888456988338687e-05,
      "loss": 0.3543,
      "step": 10020
    },
    {
      "epoch": 1.6600463422707712,
      "grad_norm": 40.104270935058594,
      "learning_rate": 2.9867331417948285e-05,
      "loss": 0.6864,
      "step": 10030
    },
    {
      "epoch": 1.6617014233697451,
      "grad_norm": 18.148244857788086,
      "learning_rate": 2.9846205847557883e-05,
      "loss": 0.488,
      "step": 10040
    },
    {
      "epoch": 1.663356504468719,
      "grad_norm": 48.31261444091797,
      "learning_rate": 2.9825080277167482e-05,
      "loss": 0.4914,
      "step": 10050
    },
    {
      "epoch": 1.6650115855676928,
      "grad_norm": 19.23989486694336,
      "learning_rate": 2.9803954706777084e-05,
      "loss": 0.7973,
      "step": 10060
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 45.016326904296875,
      "learning_rate": 2.9782829136386685e-05,
      "loss": 0.5164,
      "step": 10070
    },
    {
      "epoch": 1.6683217477656405,
      "grad_norm": 18.105581283569336,
      "learning_rate": 2.9761703565996284e-05,
      "loss": 0.4304,
      "step": 10080
    },
    {
      "epoch": 1.6699768288646144,
      "grad_norm": 18.286935806274414,
      "learning_rate": 2.9740577995605885e-05,
      "loss": 0.6697,
      "step": 10090
    },
    {
      "epoch": 1.6716319099635881,
      "grad_norm": 34.672420501708984,
      "learning_rate": 2.9719452425215484e-05,
      "loss": 0.4025,
      "step": 10100
    },
    {
      "epoch": 1.673286991062562,
      "grad_norm": 14.003779411315918,
      "learning_rate": 2.9698326854825082e-05,
      "loss": 0.8246,
      "step": 10110
    },
    {
      "epoch": 1.674942072161536,
      "grad_norm": 6.566058158874512,
      "learning_rate": 2.967720128443468e-05,
      "loss": 0.5245,
      "step": 10120
    },
    {
      "epoch": 1.6765971532605097,
      "grad_norm": 12.537371635437012,
      "learning_rate": 2.965607571404428e-05,
      "loss": 0.5137,
      "step": 10130
    },
    {
      "epoch": 1.6782522343594835,
      "grad_norm": 78.28679656982422,
      "learning_rate": 2.9634950143653877e-05,
      "loss": 0.4989,
      "step": 10140
    },
    {
      "epoch": 1.6799073154584576,
      "grad_norm": 33.779300689697266,
      "learning_rate": 2.9613824573263483e-05,
      "loss": 0.3598,
      "step": 10150
    },
    {
      "epoch": 1.6815623965574313,
      "grad_norm": 10.070023536682129,
      "learning_rate": 2.959269900287308e-05,
      "loss": 0.6152,
      "step": 10160
    },
    {
      "epoch": 1.683217477656405,
      "grad_norm": 33.583763122558594,
      "learning_rate": 2.957157343248268e-05,
      "loss": 0.7675,
      "step": 10170
    },
    {
      "epoch": 1.684872558755379,
      "grad_norm": 6.353926181793213,
      "learning_rate": 2.9550447862092278e-05,
      "loss": 0.5996,
      "step": 10180
    },
    {
      "epoch": 1.686527639854353,
      "grad_norm": 33.85573959350586,
      "learning_rate": 2.9529322291701876e-05,
      "loss": 0.446,
      "step": 10190
    },
    {
      "epoch": 1.6881827209533267,
      "grad_norm": 27.597007751464844,
      "learning_rate": 2.9508196721311478e-05,
      "loss": 0.5155,
      "step": 10200
    },
    {
      "epoch": 1.6898378020523006,
      "grad_norm": 24.514604568481445,
      "learning_rate": 2.9487071150921076e-05,
      "loss": 0.3707,
      "step": 10210
    },
    {
      "epoch": 1.6914928831512746,
      "grad_norm": 26.552688598632812,
      "learning_rate": 2.9465945580530675e-05,
      "loss": 0.6233,
      "step": 10220
    },
    {
      "epoch": 1.6931479642502483,
      "grad_norm": 42.96650314331055,
      "learning_rate": 2.9444820010140273e-05,
      "loss": 0.5748,
      "step": 10230
    },
    {
      "epoch": 1.694803045349222,
      "grad_norm": 22.856124877929688,
      "learning_rate": 2.9423694439749878e-05,
      "loss": 0.5541,
      "step": 10240
    },
    {
      "epoch": 1.696458126448196,
      "grad_norm": 56.14107131958008,
      "learning_rate": 2.9402568869359477e-05,
      "loss": 0.4592,
      "step": 10250
    },
    {
      "epoch": 1.6981132075471699,
      "grad_norm": 50.81303787231445,
      "learning_rate": 2.9381443298969075e-05,
      "loss": 0.4757,
      "step": 10260
    },
    {
      "epoch": 1.6997682886461436,
      "grad_norm": 30.20767593383789,
      "learning_rate": 2.9360317728578673e-05,
      "loss": 0.7143,
      "step": 10270
    },
    {
      "epoch": 1.7014233697451175,
      "grad_norm": 24.21530532836914,
      "learning_rate": 2.933919215818827e-05,
      "loss": 0.4274,
      "step": 10280
    },
    {
      "epoch": 1.7030784508440915,
      "grad_norm": 38.4283332824707,
      "learning_rate": 2.931806658779787e-05,
      "loss": 0.8154,
      "step": 10290
    },
    {
      "epoch": 1.7047335319430652,
      "grad_norm": 21.94261360168457,
      "learning_rate": 2.929694101740747e-05,
      "loss": 0.643,
      "step": 10300
    },
    {
      "epoch": 1.706388613042039,
      "grad_norm": 69.01811218261719,
      "learning_rate": 2.927581544701707e-05,
      "loss": 0.4984,
      "step": 10310
    },
    {
      "epoch": 1.7080436941410129,
      "grad_norm": 138.8223876953125,
      "learning_rate": 2.9254689876626672e-05,
      "loss": 0.2822,
      "step": 10320
    },
    {
      "epoch": 1.7096987752399868,
      "grad_norm": 33.17416763305664,
      "learning_rate": 2.923356430623627e-05,
      "loss": 0.5535,
      "step": 10330
    },
    {
      "epoch": 1.7113538563389605,
      "grad_norm": 33.64805221557617,
      "learning_rate": 2.9212438735845872e-05,
      "loss": 0.8512,
      "step": 10340
    },
    {
      "epoch": 1.7130089374379345,
      "grad_norm": 5.563132286071777,
      "learning_rate": 2.919131316545547e-05,
      "loss": 0.5524,
      "step": 10350
    },
    {
      "epoch": 1.7146640185369084,
      "grad_norm": 56.04706573486328,
      "learning_rate": 2.917018759506507e-05,
      "loss": 0.6878,
      "step": 10360
    },
    {
      "epoch": 1.7163190996358821,
      "grad_norm": 15.07618522644043,
      "learning_rate": 2.9149062024674667e-05,
      "loss": 0.6239,
      "step": 10370
    },
    {
      "epoch": 1.7179741807348559,
      "grad_norm": 35.05037307739258,
      "learning_rate": 2.9127936454284266e-05,
      "loss": 0.4407,
      "step": 10380
    },
    {
      "epoch": 1.7196292618338298,
      "grad_norm": 45.98164367675781,
      "learning_rate": 2.9106810883893864e-05,
      "loss": 0.3514,
      "step": 10390
    },
    {
      "epoch": 1.7212843429328037,
      "grad_norm": 4.852276802062988,
      "learning_rate": 2.9085685313503462e-05,
      "loss": 0.3803,
      "step": 10400
    },
    {
      "epoch": 1.7229394240317775,
      "grad_norm": 48.74020767211914,
      "learning_rate": 2.9064559743113068e-05,
      "loss": 0.3946,
      "step": 10410
    },
    {
      "epoch": 1.7245945051307514,
      "grad_norm": 27.56412696838379,
      "learning_rate": 2.9043434172722666e-05,
      "loss": 0.5623,
      "step": 10420
    },
    {
      "epoch": 1.7262495862297254,
      "grad_norm": 70.70027923583984,
      "learning_rate": 2.9022308602332264e-05,
      "loss": 0.533,
      "step": 10430
    },
    {
      "epoch": 1.727904667328699,
      "grad_norm": 28.41685676574707,
      "learning_rate": 2.9001183031941863e-05,
      "loss": 0.7168,
      "step": 10440
    },
    {
      "epoch": 1.7295597484276728,
      "grad_norm": 25.81745147705078,
      "learning_rate": 2.8980057461551465e-05,
      "loss": 0.8068,
      "step": 10450
    },
    {
      "epoch": 1.731214829526647,
      "grad_norm": 15.77867317199707,
      "learning_rate": 2.8958931891161063e-05,
      "loss": 0.6744,
      "step": 10460
    },
    {
      "epoch": 1.7328699106256207,
      "grad_norm": 46.17994689941406,
      "learning_rate": 2.893780632077066e-05,
      "loss": 0.6447,
      "step": 10470
    },
    {
      "epoch": 1.7345249917245944,
      "grad_norm": 28.83710289001465,
      "learning_rate": 2.891668075038026e-05,
      "loss": 0.6237,
      "step": 10480
    },
    {
      "epoch": 1.7361800728235683,
      "grad_norm": 23.391328811645508,
      "learning_rate": 2.8895555179989865e-05,
      "loss": 0.5173,
      "step": 10490
    },
    {
      "epoch": 1.7378351539225423,
      "grad_norm": 56.16194152832031,
      "learning_rate": 2.8874429609599463e-05,
      "loss": 0.7228,
      "step": 10500
    },
    {
      "epoch": 1.739490235021516,
      "grad_norm": 31.28583335876465,
      "learning_rate": 2.885330403920906e-05,
      "loss": 0.4784,
      "step": 10510
    },
    {
      "epoch": 1.74114531612049,
      "grad_norm": 84.77526092529297,
      "learning_rate": 2.883217846881866e-05,
      "loss": 0.7413,
      "step": 10520
    },
    {
      "epoch": 1.742800397219464,
      "grad_norm": 18.585098266601562,
      "learning_rate": 2.881105289842826e-05,
      "loss": 0.3818,
      "step": 10530
    },
    {
      "epoch": 1.7444554783184376,
      "grad_norm": 73.18435668945312,
      "learning_rate": 2.8789927328037857e-05,
      "loss": 0.5592,
      "step": 10540
    },
    {
      "epoch": 1.7461105594174113,
      "grad_norm": 12.261223793029785,
      "learning_rate": 2.8768801757647455e-05,
      "loss": 0.4367,
      "step": 10550
    },
    {
      "epoch": 1.7477656405163853,
      "grad_norm": 38.87782669067383,
      "learning_rate": 2.8747676187257057e-05,
      "loss": 0.3726,
      "step": 10560
    },
    {
      "epoch": 1.7494207216153592,
      "grad_norm": 7.270809650421143,
      "learning_rate": 2.8726550616866655e-05,
      "loss": 0.5006,
      "step": 10570
    },
    {
      "epoch": 1.751075802714333,
      "grad_norm": 18.295150756835938,
      "learning_rate": 2.8705425046476257e-05,
      "loss": 0.7156,
      "step": 10580
    },
    {
      "epoch": 1.7527308838133069,
      "grad_norm": 24.1942081451416,
      "learning_rate": 2.868429947608586e-05,
      "loss": 0.6048,
      "step": 10590
    },
    {
      "epoch": 1.7543859649122808,
      "grad_norm": 11.1830415725708,
      "learning_rate": 2.8663173905695457e-05,
      "loss": 0.7379,
      "step": 10600
    },
    {
      "epoch": 1.7560410460112545,
      "grad_norm": 2.218465805053711,
      "learning_rate": 2.8642048335305056e-05,
      "loss": 0.5624,
      "step": 10610
    },
    {
      "epoch": 1.7576961271102283,
      "grad_norm": 38.09602355957031,
      "learning_rate": 2.8620922764914654e-05,
      "loss": 0.4813,
      "step": 10620
    },
    {
      "epoch": 1.7593512082092022,
      "grad_norm": 27.335697174072266,
      "learning_rate": 2.8599797194524252e-05,
      "loss": 0.6323,
      "step": 10630
    },
    {
      "epoch": 1.7610062893081762,
      "grad_norm": 51.33543395996094,
      "learning_rate": 2.857867162413385e-05,
      "loss": 0.4755,
      "step": 10640
    },
    {
      "epoch": 1.7626613704071499,
      "grad_norm": 3.864112138748169,
      "learning_rate": 2.855754605374345e-05,
      "loss": 0.4848,
      "step": 10650
    },
    {
      "epoch": 1.7643164515061238,
      "grad_norm": 34.530452728271484,
      "learning_rate": 2.8536420483353054e-05,
      "loss": 0.6591,
      "step": 10660
    },
    {
      "epoch": 1.7659715326050978,
      "grad_norm": 51.761722564697266,
      "learning_rate": 2.8515294912962653e-05,
      "loss": 0.4216,
      "step": 10670
    },
    {
      "epoch": 1.7676266137040715,
      "grad_norm": 59.398284912109375,
      "learning_rate": 2.849416934257225e-05,
      "loss": 0.4966,
      "step": 10680
    },
    {
      "epoch": 1.7692816948030452,
      "grad_norm": 6.5447773933410645,
      "learning_rate": 2.847304377218185e-05,
      "loss": 0.2915,
      "step": 10690
    },
    {
      "epoch": 1.7709367759020191,
      "grad_norm": 89.8686752319336,
      "learning_rate": 2.845191820179145e-05,
      "loss": 0.9066,
      "step": 10700
    },
    {
      "epoch": 1.772591857000993,
      "grad_norm": 44.180110931396484,
      "learning_rate": 2.843079263140105e-05,
      "loss": 0.3618,
      "step": 10710
    },
    {
      "epoch": 1.7742469380999668,
      "grad_norm": 56.36697769165039,
      "learning_rate": 2.8409667061010648e-05,
      "loss": 0.5867,
      "step": 10720
    },
    {
      "epoch": 1.7759020191989408,
      "grad_norm": 53.58796310424805,
      "learning_rate": 2.8388541490620246e-05,
      "loss": 0.2637,
      "step": 10730
    },
    {
      "epoch": 1.7775571002979147,
      "grad_norm": 39.26431655883789,
      "learning_rate": 2.8367415920229845e-05,
      "loss": 0.34,
      "step": 10740
    },
    {
      "epoch": 1.7792121813968884,
      "grad_norm": 9.081877708435059,
      "learning_rate": 2.834629034983945e-05,
      "loss": 0.5367,
      "step": 10750
    },
    {
      "epoch": 1.7808672624958621,
      "grad_norm": 20.416353225708008,
      "learning_rate": 2.8325164779449048e-05,
      "loss": 0.6035,
      "step": 10760
    },
    {
      "epoch": 1.7825223435948363,
      "grad_norm": 18.86355972290039,
      "learning_rate": 2.8304039209058647e-05,
      "loss": 0.1797,
      "step": 10770
    },
    {
      "epoch": 1.78417742469381,
      "grad_norm": 16.877546310424805,
      "learning_rate": 2.8282913638668245e-05,
      "loss": 0.7375,
      "step": 10780
    },
    {
      "epoch": 1.7858325057927837,
      "grad_norm": 54.301605224609375,
      "learning_rate": 2.8261788068277843e-05,
      "loss": 0.4666,
      "step": 10790
    },
    {
      "epoch": 1.7874875868917577,
      "grad_norm": 29.1569766998291,
      "learning_rate": 2.8240662497887442e-05,
      "loss": 0.9711,
      "step": 10800
    },
    {
      "epoch": 1.7891426679907316,
      "grad_norm": 9.11926555633545,
      "learning_rate": 2.8219536927497044e-05,
      "loss": 0.4525,
      "step": 10810
    },
    {
      "epoch": 1.7907977490897053,
      "grad_norm": 79.45528411865234,
      "learning_rate": 2.8198411357106642e-05,
      "loss": 0.485,
      "step": 10820
    },
    {
      "epoch": 1.7924528301886793,
      "grad_norm": 12.362090110778809,
      "learning_rate": 2.8177285786716244e-05,
      "loss": 0.5108,
      "step": 10830
    },
    {
      "epoch": 1.7941079112876532,
      "grad_norm": 40.02168273925781,
      "learning_rate": 2.8156160216325846e-05,
      "loss": 0.3898,
      "step": 10840
    },
    {
      "epoch": 1.795762992386627,
      "grad_norm": 2.478203535079956,
      "learning_rate": 2.8135034645935444e-05,
      "loss": 0.3868,
      "step": 10850
    },
    {
      "epoch": 1.7974180734856007,
      "grad_norm": 17.051368713378906,
      "learning_rate": 2.8113909075545042e-05,
      "loss": 0.5977,
      "step": 10860
    },
    {
      "epoch": 1.7990731545845746,
      "grad_norm": 16.939308166503906,
      "learning_rate": 2.809278350515464e-05,
      "loss": 0.304,
      "step": 10870
    },
    {
      "epoch": 1.8007282356835486,
      "grad_norm": 46.70264434814453,
      "learning_rate": 2.807165793476424e-05,
      "loss": 0.7867,
      "step": 10880
    },
    {
      "epoch": 1.8023833167825223,
      "grad_norm": 13.951534271240234,
      "learning_rate": 2.8050532364373837e-05,
      "loss": 0.3382,
      "step": 10890
    },
    {
      "epoch": 1.8040383978814962,
      "grad_norm": 3.723099946975708,
      "learning_rate": 2.8029406793983436e-05,
      "loss": 0.7351,
      "step": 10900
    },
    {
      "epoch": 1.8056934789804702,
      "grad_norm": 25.21904945373535,
      "learning_rate": 2.8008281223593034e-05,
      "loss": 1.0417,
      "step": 10910
    },
    {
      "epoch": 1.8073485600794439,
      "grad_norm": 8.551405906677246,
      "learning_rate": 2.798715565320264e-05,
      "loss": 0.7471,
      "step": 10920
    },
    {
      "epoch": 1.8090036411784176,
      "grad_norm": 1.8303372859954834,
      "learning_rate": 2.7966030082812238e-05,
      "loss": 0.5508,
      "step": 10930
    },
    {
      "epoch": 1.8106587222773916,
      "grad_norm": 21.150949478149414,
      "learning_rate": 2.794490451242184e-05,
      "loss": 0.4825,
      "step": 10940
    },
    {
      "epoch": 1.8123138033763655,
      "grad_norm": 44.063236236572266,
      "learning_rate": 2.7923778942031438e-05,
      "loss": 0.426,
      "step": 10950
    },
    {
      "epoch": 1.8139688844753392,
      "grad_norm": 70.1872787475586,
      "learning_rate": 2.7902653371641036e-05,
      "loss": 0.4368,
      "step": 10960
    },
    {
      "epoch": 1.8156239655743132,
      "grad_norm": 35.09783935546875,
      "learning_rate": 2.7881527801250635e-05,
      "loss": 0.544,
      "step": 10970
    },
    {
      "epoch": 1.817279046673287,
      "grad_norm": 61.11764144897461,
      "learning_rate": 2.7860402230860233e-05,
      "loss": 0.5381,
      "step": 10980
    },
    {
      "epoch": 1.8189341277722608,
      "grad_norm": 5.007161617279053,
      "learning_rate": 2.783927666046983e-05,
      "loss": 0.3841,
      "step": 10990
    },
    {
      "epoch": 1.8205892088712345,
      "grad_norm": 4.660142421722412,
      "learning_rate": 2.7818151090079437e-05,
      "loss": 0.8121,
      "step": 11000
    },
    {
      "epoch": 1.8222442899702085,
      "grad_norm": 11.88615608215332,
      "learning_rate": 2.7797025519689035e-05,
      "loss": 0.2849,
      "step": 11010
    },
    {
      "epoch": 1.8238993710691824,
      "grad_norm": 37.269981384277344,
      "learning_rate": 2.7775899949298633e-05,
      "loss": 0.5023,
      "step": 11020
    },
    {
      "epoch": 1.8255544521681561,
      "grad_norm": 11.874856948852539,
      "learning_rate": 2.7754774378908232e-05,
      "loss": 0.3421,
      "step": 11030
    },
    {
      "epoch": 1.82720953326713,
      "grad_norm": 16.902467727661133,
      "learning_rate": 2.773364880851783e-05,
      "loss": 0.5782,
      "step": 11040
    },
    {
      "epoch": 1.828864614366104,
      "grad_norm": 87.80493927001953,
      "learning_rate": 2.7712523238127432e-05,
      "loss": 0.495,
      "step": 11050
    },
    {
      "epoch": 1.8305196954650778,
      "grad_norm": 3.193943977355957,
      "learning_rate": 2.769139766773703e-05,
      "loss": 0.4447,
      "step": 11060
    },
    {
      "epoch": 1.8321747765640515,
      "grad_norm": 61.60831069946289,
      "learning_rate": 2.767027209734663e-05,
      "loss": 0.5849,
      "step": 11070
    },
    {
      "epoch": 1.8338298576630256,
      "grad_norm": 35.075401306152344,
      "learning_rate": 2.7649146526956227e-05,
      "loss": 0.5074,
      "step": 11080
    },
    {
      "epoch": 1.8354849387619994,
      "grad_norm": 23.852928161621094,
      "learning_rate": 2.7628020956565832e-05,
      "loss": 0.468,
      "step": 11090
    },
    {
      "epoch": 1.837140019860973,
      "grad_norm": 28.535785675048828,
      "learning_rate": 2.760689538617543e-05,
      "loss": 0.4483,
      "step": 11100
    },
    {
      "epoch": 1.838795100959947,
      "grad_norm": 34.89313507080078,
      "learning_rate": 2.758576981578503e-05,
      "loss": 0.4155,
      "step": 11110
    },
    {
      "epoch": 1.840450182058921,
      "grad_norm": 40.52228546142578,
      "learning_rate": 2.7564644245394627e-05,
      "loss": 0.7367,
      "step": 11120
    },
    {
      "epoch": 1.8421052631578947,
      "grad_norm": 11.133609771728516,
      "learning_rate": 2.7543518675004226e-05,
      "loss": 0.3939,
      "step": 11130
    },
    {
      "epoch": 1.8437603442568686,
      "grad_norm": 41.47711944580078,
      "learning_rate": 2.7522393104613824e-05,
      "loss": 0.4598,
      "step": 11140
    },
    {
      "epoch": 1.8454154253558426,
      "grad_norm": 42.7108039855957,
      "learning_rate": 2.7501267534223423e-05,
      "loss": 0.4401,
      "step": 11150
    },
    {
      "epoch": 1.8470705064548163,
      "grad_norm": 10.279467582702637,
      "learning_rate": 2.7480141963833024e-05,
      "loss": 0.6368,
      "step": 11160
    },
    {
      "epoch": 1.84872558755379,
      "grad_norm": 45.331886291503906,
      "learning_rate": 2.7459016393442626e-05,
      "loss": 0.6688,
      "step": 11170
    },
    {
      "epoch": 1.850380668652764,
      "grad_norm": 2.5222151279449463,
      "learning_rate": 2.7437890823052224e-05,
      "loss": 0.2976,
      "step": 11180
    },
    {
      "epoch": 1.852035749751738,
      "grad_norm": 34.95054626464844,
      "learning_rate": 2.7416765252661826e-05,
      "loss": 0.5398,
      "step": 11190
    },
    {
      "epoch": 1.8536908308507116,
      "grad_norm": 36.394683837890625,
      "learning_rate": 2.7395639682271425e-05,
      "loss": 0.3849,
      "step": 11200
    },
    {
      "epoch": 1.8553459119496856,
      "grad_norm": 72.46415710449219,
      "learning_rate": 2.7374514111881023e-05,
      "loss": 0.3695,
      "step": 11210
    },
    {
      "epoch": 1.8570009930486595,
      "grad_norm": 30.89885139465332,
      "learning_rate": 2.735338854149062e-05,
      "loss": 0.6288,
      "step": 11220
    },
    {
      "epoch": 1.8586560741476332,
      "grad_norm": 27.147977828979492,
      "learning_rate": 2.733226297110022e-05,
      "loss": 0.5784,
      "step": 11230
    },
    {
      "epoch": 1.860311155246607,
      "grad_norm": 117.77691650390625,
      "learning_rate": 2.7311137400709818e-05,
      "loss": 0.6413,
      "step": 11240
    },
    {
      "epoch": 1.861966236345581,
      "grad_norm": 32.540401458740234,
      "learning_rate": 2.7290011830319416e-05,
      "loss": 0.6368,
      "step": 11250
    },
    {
      "epoch": 1.8636213174445548,
      "grad_norm": 4.737954616546631,
      "learning_rate": 2.726888625992902e-05,
      "loss": 0.6132,
      "step": 11260
    },
    {
      "epoch": 1.8652763985435286,
      "grad_norm": 23.33702278137207,
      "learning_rate": 2.724776068953862e-05,
      "loss": 0.4325,
      "step": 11270
    },
    {
      "epoch": 1.8669314796425025,
      "grad_norm": 11.611008644104004,
      "learning_rate": 2.722663511914822e-05,
      "loss": 0.4252,
      "step": 11280
    },
    {
      "epoch": 1.8685865607414764,
      "grad_norm": 14.756309509277344,
      "learning_rate": 2.7205509548757817e-05,
      "loss": 0.4658,
      "step": 11290
    },
    {
      "epoch": 1.8702416418404502,
      "grad_norm": 13.506721496582031,
      "learning_rate": 2.718438397836742e-05,
      "loss": 0.5284,
      "step": 11300
    },
    {
      "epoch": 1.8718967229394239,
      "grad_norm": 35.79391860961914,
      "learning_rate": 2.7163258407977017e-05,
      "loss": 0.6275,
      "step": 11310
    },
    {
      "epoch": 1.8735518040383978,
      "grad_norm": 11.447168350219727,
      "learning_rate": 2.7142132837586615e-05,
      "loss": 0.488,
      "step": 11320
    },
    {
      "epoch": 1.8752068851373718,
      "grad_norm": 19.104969024658203,
      "learning_rate": 2.7121007267196214e-05,
      "loss": 0.4796,
      "step": 11330
    },
    {
      "epoch": 1.8768619662363455,
      "grad_norm": 10.781807899475098,
      "learning_rate": 2.709988169680582e-05,
      "loss": 0.56,
      "step": 11340
    },
    {
      "epoch": 1.8785170473353194,
      "grad_norm": 22.842500686645508,
      "learning_rate": 2.7078756126415417e-05,
      "loss": 0.6512,
      "step": 11350
    },
    {
      "epoch": 1.8801721284342934,
      "grad_norm": 89.2745590209961,
      "learning_rate": 2.7057630556025016e-05,
      "loss": 0.7707,
      "step": 11360
    },
    {
      "epoch": 1.881827209533267,
      "grad_norm": 43.24648666381836,
      "learning_rate": 2.7036504985634614e-05,
      "loss": 0.6088,
      "step": 11370
    },
    {
      "epoch": 1.8834822906322408,
      "grad_norm": 42.92900848388672,
      "learning_rate": 2.7015379415244212e-05,
      "loss": 0.4021,
      "step": 11380
    },
    {
      "epoch": 1.885137371731215,
      "grad_norm": 14.681112289428711,
      "learning_rate": 2.699425384485381e-05,
      "loss": 0.4396,
      "step": 11390
    },
    {
      "epoch": 1.8867924528301887,
      "grad_norm": 42.22365188598633,
      "learning_rate": 2.697312827446341e-05,
      "loss": 0.5551,
      "step": 11400
    },
    {
      "epoch": 1.8884475339291624,
      "grad_norm": 102.96427917480469,
      "learning_rate": 2.695200270407301e-05,
      "loss": 0.3079,
      "step": 11410
    },
    {
      "epoch": 1.8901026150281364,
      "grad_norm": 26.1923885345459,
      "learning_rate": 2.693087713368261e-05,
      "loss": 0.5411,
      "step": 11420
    },
    {
      "epoch": 1.8917576961271103,
      "grad_norm": 19.822668075561523,
      "learning_rate": 2.690975156329221e-05,
      "loss": 0.5784,
      "step": 11430
    },
    {
      "epoch": 1.893412777226084,
      "grad_norm": 29.183250427246094,
      "learning_rate": 2.6888625992901813e-05,
      "loss": 0.5748,
      "step": 11440
    },
    {
      "epoch": 1.895067858325058,
      "grad_norm": 6.200109481811523,
      "learning_rate": 2.686750042251141e-05,
      "loss": 0.4769,
      "step": 11450
    },
    {
      "epoch": 1.896722939424032,
      "grad_norm": 30.480045318603516,
      "learning_rate": 2.684637485212101e-05,
      "loss": 0.4874,
      "step": 11460
    },
    {
      "epoch": 1.8983780205230056,
      "grad_norm": 24.052433013916016,
      "learning_rate": 2.6825249281730608e-05,
      "loss": 0.4173,
      "step": 11470
    },
    {
      "epoch": 1.9000331016219794,
      "grad_norm": 15.859445571899414,
      "learning_rate": 2.6804123711340206e-05,
      "loss": 0.2196,
      "step": 11480
    },
    {
      "epoch": 1.9016881827209533,
      "grad_norm": 58.839111328125,
      "learning_rate": 2.6782998140949805e-05,
      "loss": 0.5296,
      "step": 11490
    },
    {
      "epoch": 1.9033432638199272,
      "grad_norm": 61.02309799194336,
      "learning_rate": 2.6761872570559403e-05,
      "loss": 0.6825,
      "step": 11500
    },
    {
      "epoch": 1.904998344918901,
      "grad_norm": 25.421823501586914,
      "learning_rate": 2.674074700016901e-05,
      "loss": 0.3649,
      "step": 11510
    },
    {
      "epoch": 1.906653426017875,
      "grad_norm": 23.666240692138672,
      "learning_rate": 2.6719621429778607e-05,
      "loss": 0.5905,
      "step": 11520
    },
    {
      "epoch": 1.9083085071168489,
      "grad_norm": 48.46137619018555,
      "learning_rate": 2.6698495859388205e-05,
      "loss": 0.5629,
      "step": 11530
    },
    {
      "epoch": 1.9099635882158226,
      "grad_norm": 27.753480911254883,
      "learning_rate": 2.6677370288997803e-05,
      "loss": 0.4893,
      "step": 11540
    },
    {
      "epoch": 1.9116186693147963,
      "grad_norm": 32.94865798950195,
      "learning_rate": 2.6656244718607405e-05,
      "loss": 0.6498,
      "step": 11550
    },
    {
      "epoch": 1.9132737504137702,
      "grad_norm": 24.55036735534668,
      "learning_rate": 2.6635119148217004e-05,
      "loss": 0.6468,
      "step": 11560
    },
    {
      "epoch": 1.9149288315127442,
      "grad_norm": 55.14248275756836,
      "learning_rate": 2.6613993577826602e-05,
      "loss": 0.4684,
      "step": 11570
    },
    {
      "epoch": 1.916583912611718,
      "grad_norm": 30.24213409423828,
      "learning_rate": 2.65928680074362e-05,
      "loss": 0.397,
      "step": 11580
    },
    {
      "epoch": 1.9182389937106918,
      "grad_norm": 27.777132034301758,
      "learning_rate": 2.65717424370458e-05,
      "loss": 0.6249,
      "step": 11590
    },
    {
      "epoch": 1.9198940748096658,
      "grad_norm": 14.326662063598633,
      "learning_rate": 2.6550616866655404e-05,
      "loss": 0.5606,
      "step": 11600
    },
    {
      "epoch": 1.9215491559086395,
      "grad_norm": 52.248809814453125,
      "learning_rate": 2.6529491296265002e-05,
      "loss": 0.3039,
      "step": 11610
    },
    {
      "epoch": 1.9232042370076132,
      "grad_norm": 15.207871437072754,
      "learning_rate": 2.65083657258746e-05,
      "loss": 0.6986,
      "step": 11620
    },
    {
      "epoch": 1.9248593181065872,
      "grad_norm": 49.15470504760742,
      "learning_rate": 2.64872401554842e-05,
      "loss": 0.5575,
      "step": 11630
    },
    {
      "epoch": 1.9265143992055611,
      "grad_norm": 2.221754789352417,
      "learning_rate": 2.6466114585093797e-05,
      "loss": 0.6849,
      "step": 11640
    },
    {
      "epoch": 1.9281694803045348,
      "grad_norm": 117.50939178466797,
      "learning_rate": 2.6444989014703396e-05,
      "loss": 0.5286,
      "step": 11650
    },
    {
      "epoch": 1.9298245614035088,
      "grad_norm": 23.71174430847168,
      "learning_rate": 2.6423863444312998e-05,
      "loss": 0.3617,
      "step": 11660
    },
    {
      "epoch": 1.9314796425024827,
      "grad_norm": 53.292236328125,
      "learning_rate": 2.6402737873922596e-05,
      "loss": 0.4647,
      "step": 11670
    },
    {
      "epoch": 1.9331347236014564,
      "grad_norm": 35.42875671386719,
      "learning_rate": 2.6381612303532198e-05,
      "loss": 0.4802,
      "step": 11680
    },
    {
      "epoch": 1.9347898047004302,
      "grad_norm": 78.36019897460938,
      "learning_rate": 2.63604867331418e-05,
      "loss": 0.5337,
      "step": 11690
    },
    {
      "epoch": 1.9364448857994043,
      "grad_norm": 9.104009628295898,
      "learning_rate": 2.6339361162751398e-05,
      "loss": 0.4373,
      "step": 11700
    },
    {
      "epoch": 1.938099966898378,
      "grad_norm": 28.362924575805664,
      "learning_rate": 2.6318235592360996e-05,
      "loss": 0.5937,
      "step": 11710
    },
    {
      "epoch": 1.9397550479973518,
      "grad_norm": 21.59025764465332,
      "learning_rate": 2.6297110021970595e-05,
      "loss": 0.6506,
      "step": 11720
    },
    {
      "epoch": 1.9414101290963257,
      "grad_norm": 6.504013538360596,
      "learning_rate": 2.6275984451580193e-05,
      "loss": 0.3815,
      "step": 11730
    },
    {
      "epoch": 1.9430652101952997,
      "grad_norm": 55.06075668334961,
      "learning_rate": 2.625485888118979e-05,
      "loss": 0.4028,
      "step": 11740
    },
    {
      "epoch": 1.9447202912942734,
      "grad_norm": 39.980796813964844,
      "learning_rate": 2.623373331079939e-05,
      "loss": 0.4995,
      "step": 11750
    },
    {
      "epoch": 1.9463753723932473,
      "grad_norm": 89.19711303710938,
      "learning_rate": 2.6212607740408988e-05,
      "loss": 0.6884,
      "step": 11760
    },
    {
      "epoch": 1.9480304534922213,
      "grad_norm": 7.868603706359863,
      "learning_rate": 2.6191482170018593e-05,
      "loss": 0.5477,
      "step": 11770
    },
    {
      "epoch": 1.949685534591195,
      "grad_norm": 25.509000778198242,
      "learning_rate": 2.6170356599628192e-05,
      "loss": 0.5626,
      "step": 11780
    },
    {
      "epoch": 1.9513406156901687,
      "grad_norm": 47.245365142822266,
      "learning_rate": 2.614923102923779e-05,
      "loss": 0.5393,
      "step": 11790
    },
    {
      "epoch": 1.9529956967891426,
      "grad_norm": 6.843713760375977,
      "learning_rate": 2.6128105458847392e-05,
      "loss": 0.8341,
      "step": 11800
    },
    {
      "epoch": 1.9546507778881166,
      "grad_norm": 50.04123306274414,
      "learning_rate": 2.610697988845699e-05,
      "loss": 0.2368,
      "step": 11810
    },
    {
      "epoch": 1.9563058589870903,
      "grad_norm": 58.39591598510742,
      "learning_rate": 2.608585431806659e-05,
      "loss": 0.4353,
      "step": 11820
    },
    {
      "epoch": 1.9579609400860642,
      "grad_norm": 15.875720024108887,
      "learning_rate": 2.6064728747676187e-05,
      "loss": 0.358,
      "step": 11830
    },
    {
      "epoch": 1.9596160211850382,
      "grad_norm": 28.916526794433594,
      "learning_rate": 2.6043603177285785e-05,
      "loss": 0.6089,
      "step": 11840
    },
    {
      "epoch": 1.961271102284012,
      "grad_norm": 34.021793365478516,
      "learning_rate": 2.602247760689539e-05,
      "loss": 0.4402,
      "step": 11850
    },
    {
      "epoch": 1.9629261833829856,
      "grad_norm": 0.9247215390205383,
      "learning_rate": 2.600135203650499e-05,
      "loss": 0.4472,
      "step": 11860
    },
    {
      "epoch": 1.9645812644819596,
      "grad_norm": 63.7453498840332,
      "learning_rate": 2.5980226466114587e-05,
      "loss": 0.6386,
      "step": 11870
    },
    {
      "epoch": 1.9662363455809335,
      "grad_norm": 123.2784652709961,
      "learning_rate": 2.5959100895724186e-05,
      "loss": 0.4922,
      "step": 11880
    },
    {
      "epoch": 1.9678914266799072,
      "grad_norm": 44.14190673828125,
      "learning_rate": 2.5937975325333784e-05,
      "loss": 0.5261,
      "step": 11890
    },
    {
      "epoch": 1.9695465077788812,
      "grad_norm": 60.91446304321289,
      "learning_rate": 2.5916849754943383e-05,
      "loss": 0.8063,
      "step": 11900
    },
    {
      "epoch": 1.9712015888778551,
      "grad_norm": 21.818790435791016,
      "learning_rate": 2.5895724184552984e-05,
      "loss": 0.4838,
      "step": 11910
    },
    {
      "epoch": 1.9728566699768288,
      "grad_norm": 28.13170623779297,
      "learning_rate": 2.5874598614162583e-05,
      "loss": 0.34,
      "step": 11920
    },
    {
      "epoch": 1.9745117510758026,
      "grad_norm": 19.04758071899414,
      "learning_rate": 2.585347304377218e-05,
      "loss": 0.3707,
      "step": 11930
    },
    {
      "epoch": 1.9761668321747765,
      "grad_norm": 6.9725446701049805,
      "learning_rate": 2.5832347473381786e-05,
      "loss": 0.4444,
      "step": 11940
    },
    {
      "epoch": 1.9778219132737505,
      "grad_norm": 26.61187744140625,
      "learning_rate": 2.5811221902991385e-05,
      "loss": 0.5786,
      "step": 11950
    },
    {
      "epoch": 1.9794769943727242,
      "grad_norm": 16.179914474487305,
      "learning_rate": 2.5790096332600983e-05,
      "loss": 0.2951,
      "step": 11960
    },
    {
      "epoch": 1.9811320754716981,
      "grad_norm": 21.33866310119629,
      "learning_rate": 2.576897076221058e-05,
      "loss": 0.3004,
      "step": 11970
    },
    {
      "epoch": 1.982787156570672,
      "grad_norm": 18.29104232788086,
      "learning_rate": 2.574784519182018e-05,
      "loss": 0.458,
      "step": 11980
    },
    {
      "epoch": 1.9844422376696458,
      "grad_norm": 81.75030517578125,
      "learning_rate": 2.5726719621429778e-05,
      "loss": 0.6137,
      "step": 11990
    },
    {
      "epoch": 1.9860973187686195,
      "grad_norm": 61.353824615478516,
      "learning_rate": 2.5705594051039377e-05,
      "loss": 0.5957,
      "step": 12000
    },
    {
      "epoch": 1.9877523998675937,
      "grad_norm": 20.287988662719727,
      "learning_rate": 2.5684468480648975e-05,
      "loss": 0.5103,
      "step": 12010
    },
    {
      "epoch": 1.9894074809665674,
      "grad_norm": 18.4429874420166,
      "learning_rate": 2.566334291025858e-05,
      "loss": 0.5067,
      "step": 12020
    },
    {
      "epoch": 1.991062562065541,
      "grad_norm": 18.82564353942871,
      "learning_rate": 2.564221733986818e-05,
      "loss": 0.4996,
      "step": 12030
    },
    {
      "epoch": 1.992717643164515,
      "grad_norm": 3.0229499340057373,
      "learning_rate": 2.5621091769477777e-05,
      "loss": 0.5388,
      "step": 12040
    },
    {
      "epoch": 1.994372724263489,
      "grad_norm": 45.43217849731445,
      "learning_rate": 2.559996619908738e-05,
      "loss": 0.3161,
      "step": 12050
    },
    {
      "epoch": 1.9960278053624627,
      "grad_norm": 50.68543243408203,
      "learning_rate": 2.5578840628696977e-05,
      "loss": 0.3982,
      "step": 12060
    },
    {
      "epoch": 1.9976828864614367,
      "grad_norm": 52.44258499145508,
      "learning_rate": 2.5557715058306575e-05,
      "loss": 0.5432,
      "step": 12070
    },
    {
      "epoch": 1.9993379675604106,
      "grad_norm": 1.132646918296814,
      "learning_rate": 2.5536589487916174e-05,
      "loss": 0.3893,
      "step": 12080
    },
    {
      "epoch": 2.0009930486593843,
      "grad_norm": 5.234786033630371,
      "learning_rate": 2.5515463917525772e-05,
      "loss": 0.6674,
      "step": 12090
    },
    {
      "epoch": 2.002648129758358,
      "grad_norm": 14.69287109375,
      "learning_rate": 2.549433834713537e-05,
      "loss": 0.2046,
      "step": 12100
    },
    {
      "epoch": 2.004303210857332,
      "grad_norm": 3.327281951904297,
      "learning_rate": 2.5473212776744976e-05,
      "loss": 0.3901,
      "step": 12110
    },
    {
      "epoch": 2.005958291956306,
      "grad_norm": 27.242910385131836,
      "learning_rate": 2.5452087206354574e-05,
      "loss": 0.185,
      "step": 12120
    },
    {
      "epoch": 2.0076133730552796,
      "grad_norm": 24.372013092041016,
      "learning_rate": 2.5430961635964172e-05,
      "loss": 0.3887,
      "step": 12130
    },
    {
      "epoch": 2.0092684541542534,
      "grad_norm": 15.625655174255371,
      "learning_rate": 2.540983606557377e-05,
      "loss": 0.5303,
      "step": 12140
    },
    {
      "epoch": 2.0109235352532275,
      "grad_norm": 7.695705413818359,
      "learning_rate": 2.5388710495183373e-05,
      "loss": 0.2251,
      "step": 12150
    },
    {
      "epoch": 2.0125786163522013,
      "grad_norm": 19.003725051879883,
      "learning_rate": 2.536758492479297e-05,
      "loss": 0.344,
      "step": 12160
    },
    {
      "epoch": 2.014233697451175,
      "grad_norm": 1.7984496355056763,
      "learning_rate": 2.534645935440257e-05,
      "loss": 0.5304,
      "step": 12170
    },
    {
      "epoch": 2.015888778550149,
      "grad_norm": 5.032491683959961,
      "learning_rate": 2.5325333784012168e-05,
      "loss": 0.36,
      "step": 12180
    },
    {
      "epoch": 2.017543859649123,
      "grad_norm": 21.549575805664062,
      "learning_rate": 2.5304208213621773e-05,
      "loss": 0.3526,
      "step": 12190
    },
    {
      "epoch": 2.0191989407480966,
      "grad_norm": 33.95366668701172,
      "learning_rate": 2.528308264323137e-05,
      "loss": 0.3073,
      "step": 12200
    },
    {
      "epoch": 2.0208540218470703,
      "grad_norm": 62.7857780456543,
      "learning_rate": 2.526195707284097e-05,
      "loss": 0.4155,
      "step": 12210
    },
    {
      "epoch": 2.0225091029460445,
      "grad_norm": 3.166184186935425,
      "learning_rate": 2.5240831502450568e-05,
      "loss": 0.2535,
      "step": 12220
    },
    {
      "epoch": 2.024164184045018,
      "grad_norm": 19.562816619873047,
      "learning_rate": 2.5219705932060166e-05,
      "loss": 0.1831,
      "step": 12230
    },
    {
      "epoch": 2.025819265143992,
      "grad_norm": 54.216426849365234,
      "learning_rate": 2.5198580361669765e-05,
      "loss": 0.5242,
      "step": 12240
    },
    {
      "epoch": 2.027474346242966,
      "grad_norm": 24.793542861938477,
      "learning_rate": 2.5177454791279363e-05,
      "loss": 0.1493,
      "step": 12250
    },
    {
      "epoch": 2.02912942734194,
      "grad_norm": 96.75383758544922,
      "learning_rate": 2.5156329220888965e-05,
      "loss": 0.4036,
      "step": 12260
    },
    {
      "epoch": 2.0307845084409135,
      "grad_norm": 113.43352508544922,
      "learning_rate": 2.5135203650498563e-05,
      "loss": 0.6239,
      "step": 12270
    },
    {
      "epoch": 2.0324395895398872,
      "grad_norm": 44.52779006958008,
      "learning_rate": 2.5114078080108165e-05,
      "loss": 0.1346,
      "step": 12280
    },
    {
      "epoch": 2.0340946706388614,
      "grad_norm": 52.117637634277344,
      "learning_rate": 2.5092952509717767e-05,
      "loss": 0.3256,
      "step": 12290
    },
    {
      "epoch": 2.035749751737835,
      "grad_norm": 0.4121800363063812,
      "learning_rate": 2.5071826939327365e-05,
      "loss": 0.1787,
      "step": 12300
    },
    {
      "epoch": 2.037404832836809,
      "grad_norm": 27.071002960205078,
      "learning_rate": 2.5050701368936964e-05,
      "loss": 0.3318,
      "step": 12310
    },
    {
      "epoch": 2.039059913935783,
      "grad_norm": 44.08222961425781,
      "learning_rate": 2.5029575798546562e-05,
      "loss": 0.2839,
      "step": 12320
    },
    {
      "epoch": 2.0407149950347567,
      "grad_norm": 77.24282836914062,
      "learning_rate": 2.500845022815616e-05,
      "loss": 0.3934,
      "step": 12330
    },
    {
      "epoch": 2.0423700761337304,
      "grad_norm": 7.149777889251709,
      "learning_rate": 2.4987324657765762e-05,
      "loss": 0.4021,
      "step": 12340
    },
    {
      "epoch": 2.0440251572327046,
      "grad_norm": 60.8967399597168,
      "learning_rate": 2.496619908737536e-05,
      "loss": 0.4903,
      "step": 12350
    },
    {
      "epoch": 2.0456802383316783,
      "grad_norm": 46.11470413208008,
      "learning_rate": 2.494507351698496e-05,
      "loss": 0.1967,
      "step": 12360
    },
    {
      "epoch": 2.047335319430652,
      "grad_norm": 90.87523651123047,
      "learning_rate": 2.4923947946594557e-05,
      "loss": 0.2948,
      "step": 12370
    },
    {
      "epoch": 2.0489904005296258,
      "grad_norm": 95.86457824707031,
      "learning_rate": 2.4902822376204156e-05,
      "loss": 0.3956,
      "step": 12380
    },
    {
      "epoch": 2.0506454816286,
      "grad_norm": 93.4114761352539,
      "learning_rate": 2.4881696805813758e-05,
      "loss": 0.279,
      "step": 12390
    },
    {
      "epoch": 2.0523005627275737,
      "grad_norm": 3.799722909927368,
      "learning_rate": 2.486057123542336e-05,
      "loss": 0.4066,
      "step": 12400
    },
    {
      "epoch": 2.0539556438265474,
      "grad_norm": 6.939272403717041,
      "learning_rate": 2.4839445665032958e-05,
      "loss": 0.411,
      "step": 12410
    },
    {
      "epoch": 2.0556107249255215,
      "grad_norm": 253.56979370117188,
      "learning_rate": 2.4818320094642556e-05,
      "loss": 0.3552,
      "step": 12420
    },
    {
      "epoch": 2.0572658060244953,
      "grad_norm": 95.88117218017578,
      "learning_rate": 2.4797194524252158e-05,
      "loss": 0.4235,
      "step": 12430
    },
    {
      "epoch": 2.058920887123469,
      "grad_norm": 88.13639068603516,
      "learning_rate": 2.4776068953861756e-05,
      "loss": 0.4206,
      "step": 12440
    },
    {
      "epoch": 2.0605759682224427,
      "grad_norm": 156.16653442382812,
      "learning_rate": 2.4754943383471355e-05,
      "loss": 0.3031,
      "step": 12450
    },
    {
      "epoch": 2.062231049321417,
      "grad_norm": 45.62697982788086,
      "learning_rate": 2.4733817813080953e-05,
      "loss": 0.5576,
      "step": 12460
    },
    {
      "epoch": 2.0638861304203906,
      "grad_norm": 40.73452377319336,
      "learning_rate": 2.4712692242690555e-05,
      "loss": 0.5562,
      "step": 12470
    },
    {
      "epoch": 2.0655412115193643,
      "grad_norm": 1.7098522186279297,
      "learning_rate": 2.4691566672300153e-05,
      "loss": 0.1423,
      "step": 12480
    },
    {
      "epoch": 2.0671962926183385,
      "grad_norm": 7.455072402954102,
      "learning_rate": 2.467044110190975e-05,
      "loss": 0.1845,
      "step": 12490
    },
    {
      "epoch": 2.068851373717312,
      "grad_norm": 1.5340931415557861,
      "learning_rate": 2.464931553151935e-05,
      "loss": 0.271,
      "step": 12500
    },
    {
      "epoch": 2.070506454816286,
      "grad_norm": 67.5182113647461,
      "learning_rate": 2.4628189961128952e-05,
      "loss": 0.4487,
      "step": 12510
    },
    {
      "epoch": 2.0721615359152596,
      "grad_norm": 21.770326614379883,
      "learning_rate": 2.460706439073855e-05,
      "loss": 0.204,
      "step": 12520
    },
    {
      "epoch": 2.073816617014234,
      "grad_norm": 117.31205749511719,
      "learning_rate": 2.4585938820348152e-05,
      "loss": 0.2065,
      "step": 12530
    },
    {
      "epoch": 2.0754716981132075,
      "grad_norm": 48.22764205932617,
      "learning_rate": 2.456481324995775e-05,
      "loss": 0.2877,
      "step": 12540
    },
    {
      "epoch": 2.0771267792121813,
      "grad_norm": 11.801217079162598,
      "learning_rate": 2.454368767956735e-05,
      "loss": 0.2162,
      "step": 12550
    },
    {
      "epoch": 2.0787818603111554,
      "grad_norm": 12.618889808654785,
      "learning_rate": 2.452256210917695e-05,
      "loss": 0.4111,
      "step": 12560
    },
    {
      "epoch": 2.080436941410129,
      "grad_norm": 53.88837432861328,
      "learning_rate": 2.450143653878655e-05,
      "loss": 0.439,
      "step": 12570
    },
    {
      "epoch": 2.082092022509103,
      "grad_norm": 44.589088439941406,
      "learning_rate": 2.4480310968396147e-05,
      "loss": 0.2306,
      "step": 12580
    },
    {
      "epoch": 2.0837471036080766,
      "grad_norm": 48.462493896484375,
      "learning_rate": 2.4459185398005746e-05,
      "loss": 0.3375,
      "step": 12590
    },
    {
      "epoch": 2.0854021847070507,
      "grad_norm": 20.131126403808594,
      "learning_rate": 2.4438059827615347e-05,
      "loss": 0.2903,
      "step": 12600
    },
    {
      "epoch": 2.0870572658060245,
      "grad_norm": 47.46771240234375,
      "learning_rate": 2.4416934257224946e-05,
      "loss": 0.1293,
      "step": 12610
    },
    {
      "epoch": 2.088712346904998,
      "grad_norm": 33.93312454223633,
      "learning_rate": 2.4395808686834544e-05,
      "loss": 0.3587,
      "step": 12620
    },
    {
      "epoch": 2.0903674280039723,
      "grad_norm": 0.3353501558303833,
      "learning_rate": 2.4374683116444142e-05,
      "loss": 0.3501,
      "step": 12630
    },
    {
      "epoch": 2.092022509102946,
      "grad_norm": 133.47410583496094,
      "learning_rate": 2.4353557546053744e-05,
      "loss": 0.3348,
      "step": 12640
    },
    {
      "epoch": 2.09367759020192,
      "grad_norm": 2.5563411712646484,
      "learning_rate": 2.4332431975663346e-05,
      "loss": 0.5381,
      "step": 12650
    },
    {
      "epoch": 2.095332671300894,
      "grad_norm": 30.315753936767578,
      "learning_rate": 2.4311306405272944e-05,
      "loss": 0.6892,
      "step": 12660
    },
    {
      "epoch": 2.0969877523998677,
      "grad_norm": 45.57251739501953,
      "learning_rate": 2.4290180834882543e-05,
      "loss": 0.2545,
      "step": 12670
    },
    {
      "epoch": 2.0986428334988414,
      "grad_norm": 92.68636322021484,
      "learning_rate": 2.4269055264492145e-05,
      "loss": 0.4353,
      "step": 12680
    },
    {
      "epoch": 2.100297914597815,
      "grad_norm": 12.346460342407227,
      "learning_rate": 2.4247929694101743e-05,
      "loss": 0.3443,
      "step": 12690
    },
    {
      "epoch": 2.1019529956967893,
      "grad_norm": 31.057830810546875,
      "learning_rate": 2.422680412371134e-05,
      "loss": 0.3335,
      "step": 12700
    },
    {
      "epoch": 2.103608076795763,
      "grad_norm": 48.73617935180664,
      "learning_rate": 2.420567855332094e-05,
      "loss": 0.3461,
      "step": 12710
    },
    {
      "epoch": 2.1052631578947367,
      "grad_norm": 6.1351823806762695,
      "learning_rate": 2.4184552982930538e-05,
      "loss": 0.2166,
      "step": 12720
    },
    {
      "epoch": 2.106918238993711,
      "grad_norm": 28.549463272094727,
      "learning_rate": 2.416342741254014e-05,
      "loss": 0.3359,
      "step": 12730
    },
    {
      "epoch": 2.1085733200926846,
      "grad_norm": 119.2783203125,
      "learning_rate": 2.4142301842149738e-05,
      "loss": 0.5466,
      "step": 12740
    },
    {
      "epoch": 2.1102284011916583,
      "grad_norm": 55.586177825927734,
      "learning_rate": 2.4121176271759337e-05,
      "loss": 0.4243,
      "step": 12750
    },
    {
      "epoch": 2.111883482290632,
      "grad_norm": 25.192127227783203,
      "learning_rate": 2.410005070136894e-05,
      "loss": 0.2304,
      "step": 12760
    },
    {
      "epoch": 2.113538563389606,
      "grad_norm": 12.332239151000977,
      "learning_rate": 2.407892513097854e-05,
      "loss": 0.2858,
      "step": 12770
    },
    {
      "epoch": 2.11519364448858,
      "grad_norm": 3.5269033908843994,
      "learning_rate": 2.405779956058814e-05,
      "loss": 0.1565,
      "step": 12780
    },
    {
      "epoch": 2.1168487255875537,
      "grad_norm": 77.55351257324219,
      "learning_rate": 2.4036673990197737e-05,
      "loss": 0.7642,
      "step": 12790
    },
    {
      "epoch": 2.118503806686528,
      "grad_norm": 41.00485610961914,
      "learning_rate": 2.4015548419807335e-05,
      "loss": 0.2373,
      "step": 12800
    },
    {
      "epoch": 2.1201588877855015,
      "grad_norm": 39.56200408935547,
      "learning_rate": 2.3994422849416937e-05,
      "loss": 0.5245,
      "step": 12810
    },
    {
      "epoch": 2.1218139688844753,
      "grad_norm": 0.2756171226501465,
      "learning_rate": 2.3973297279026535e-05,
      "loss": 0.2067,
      "step": 12820
    },
    {
      "epoch": 2.123469049983449,
      "grad_norm": 11.414253234863281,
      "learning_rate": 2.3952171708636134e-05,
      "loss": 0.3615,
      "step": 12830
    },
    {
      "epoch": 2.125124131082423,
      "grad_norm": 13.567072868347168,
      "learning_rate": 2.3931046138245732e-05,
      "loss": 0.5677,
      "step": 12840
    },
    {
      "epoch": 2.126779212181397,
      "grad_norm": 1.9319268465042114,
      "learning_rate": 2.3909920567855334e-05,
      "loss": 0.3692,
      "step": 12850
    },
    {
      "epoch": 2.1284342932803706,
      "grad_norm": 15.374892234802246,
      "learning_rate": 2.3888794997464932e-05,
      "loss": 0.3727,
      "step": 12860
    },
    {
      "epoch": 2.1300893743793448,
      "grad_norm": 47.594356536865234,
      "learning_rate": 2.386766942707453e-05,
      "loss": 0.5565,
      "step": 12870
    },
    {
      "epoch": 2.1317444554783185,
      "grad_norm": 13.124810218811035,
      "learning_rate": 2.3846543856684133e-05,
      "loss": 0.2043,
      "step": 12880
    },
    {
      "epoch": 2.133399536577292,
      "grad_norm": 33.48638916015625,
      "learning_rate": 2.382541828629373e-05,
      "loss": 0.3203,
      "step": 12890
    },
    {
      "epoch": 2.135054617676266,
      "grad_norm": 11.282769203186035,
      "learning_rate": 2.3804292715903333e-05,
      "loss": 0.1803,
      "step": 12900
    },
    {
      "epoch": 2.13670969877524,
      "grad_norm": 48.678367614746094,
      "learning_rate": 2.378316714551293e-05,
      "loss": 0.507,
      "step": 12910
    },
    {
      "epoch": 2.138364779874214,
      "grad_norm": 26.893184661865234,
      "learning_rate": 2.376204157512253e-05,
      "loss": 0.3192,
      "step": 12920
    },
    {
      "epoch": 2.1400198609731875,
      "grad_norm": 21.059326171875,
      "learning_rate": 2.3740916004732128e-05,
      "loss": 0.1488,
      "step": 12930
    },
    {
      "epoch": 2.1416749420721617,
      "grad_norm": 68.49398803710938,
      "learning_rate": 2.371979043434173e-05,
      "loss": 0.4229,
      "step": 12940
    },
    {
      "epoch": 2.1433300231711354,
      "grad_norm": 19.015832901000977,
      "learning_rate": 2.3698664863951328e-05,
      "loss": 0.4031,
      "step": 12950
    },
    {
      "epoch": 2.144985104270109,
      "grad_norm": 0.7281198501586914,
      "learning_rate": 2.3677539293560926e-05,
      "loss": 0.2375,
      "step": 12960
    },
    {
      "epoch": 2.1466401853690833,
      "grad_norm": 78.211181640625,
      "learning_rate": 2.3656413723170525e-05,
      "loss": 0.3599,
      "step": 12970
    },
    {
      "epoch": 2.148295266468057,
      "grad_norm": 50.87521743774414,
      "learning_rate": 2.3635288152780127e-05,
      "loss": 0.6352,
      "step": 12980
    },
    {
      "epoch": 2.1499503475670307,
      "grad_norm": 4.604019641876221,
      "learning_rate": 2.3614162582389725e-05,
      "loss": 0.2231,
      "step": 12990
    },
    {
      "epoch": 2.1516054286660045,
      "grad_norm": 71.47454071044922,
      "learning_rate": 2.3593037011999323e-05,
      "loss": 0.1383,
      "step": 13000
    },
    {
      "epoch": 2.1532605097649786,
      "grad_norm": 99.33460235595703,
      "learning_rate": 2.3571911441608925e-05,
      "loss": 0.5468,
      "step": 13010
    },
    {
      "epoch": 2.1549155908639523,
      "grad_norm": 0.5189388394355774,
      "learning_rate": 2.3550785871218527e-05,
      "loss": 0.3478,
      "step": 13020
    },
    {
      "epoch": 2.156570671962926,
      "grad_norm": 2.5766265392303467,
      "learning_rate": 2.3529660300828125e-05,
      "loss": 0.297,
      "step": 13030
    },
    {
      "epoch": 2.1582257530619002,
      "grad_norm": 3.1345653533935547,
      "learning_rate": 2.3508534730437724e-05,
      "loss": 0.1605,
      "step": 13040
    },
    {
      "epoch": 2.159880834160874,
      "grad_norm": 18.29214859008789,
      "learning_rate": 2.3487409160047322e-05,
      "loss": 0.3127,
      "step": 13050
    },
    {
      "epoch": 2.1615359152598477,
      "grad_norm": 6.5934553146362305,
      "learning_rate": 2.346628358965692e-05,
      "loss": 0.238,
      "step": 13060
    },
    {
      "epoch": 2.1631909963588214,
      "grad_norm": 55.940895080566406,
      "learning_rate": 2.3445158019266522e-05,
      "loss": 0.3467,
      "step": 13070
    },
    {
      "epoch": 2.1648460774577956,
      "grad_norm": 61.5859375,
      "learning_rate": 2.342403244887612e-05,
      "loss": 0.3404,
      "step": 13080
    },
    {
      "epoch": 2.1665011585567693,
      "grad_norm": 3.369541883468628,
      "learning_rate": 2.340290687848572e-05,
      "loss": 0.223,
      "step": 13090
    },
    {
      "epoch": 2.168156239655743,
      "grad_norm": 42.765098571777344,
      "learning_rate": 2.3381781308095317e-05,
      "loss": 0.4715,
      "step": 13100
    },
    {
      "epoch": 2.169811320754717,
      "grad_norm": 28.842084884643555,
      "learning_rate": 2.336065573770492e-05,
      "loss": 0.4437,
      "step": 13110
    },
    {
      "epoch": 2.171466401853691,
      "grad_norm": 6.670476913452148,
      "learning_rate": 2.3339530167314517e-05,
      "loss": 0.399,
      "step": 13120
    },
    {
      "epoch": 2.1731214829526646,
      "grad_norm": 62.41172790527344,
      "learning_rate": 2.331840459692412e-05,
      "loss": 0.4773,
      "step": 13130
    },
    {
      "epoch": 2.1747765640516383,
      "grad_norm": 87.00591278076172,
      "learning_rate": 2.3297279026533718e-05,
      "loss": 0.2228,
      "step": 13140
    },
    {
      "epoch": 2.1764316451506125,
      "grad_norm": 8.646007537841797,
      "learning_rate": 2.327615345614332e-05,
      "loss": 0.1606,
      "step": 13150
    },
    {
      "epoch": 2.178086726249586,
      "grad_norm": 61.00957107543945,
      "learning_rate": 2.3255027885752918e-05,
      "loss": 0.372,
      "step": 13160
    },
    {
      "epoch": 2.17974180734856,
      "grad_norm": 2.7312004566192627,
      "learning_rate": 2.3233902315362516e-05,
      "loss": 0.1365,
      "step": 13170
    },
    {
      "epoch": 2.181396888447534,
      "grad_norm": 27.985347747802734,
      "learning_rate": 2.3212776744972115e-05,
      "loss": 0.1799,
      "step": 13180
    },
    {
      "epoch": 2.183051969546508,
      "grad_norm": 57.95122528076172,
      "learning_rate": 2.3191651174581716e-05,
      "loss": 0.2166,
      "step": 13190
    },
    {
      "epoch": 2.1847070506454815,
      "grad_norm": 43.49421691894531,
      "learning_rate": 2.3170525604191315e-05,
      "loss": 0.3049,
      "step": 13200
    },
    {
      "epoch": 2.1863621317444553,
      "grad_norm": 0.2622573971748352,
      "learning_rate": 2.3149400033800913e-05,
      "loss": 0.4164,
      "step": 13210
    },
    {
      "epoch": 2.1880172128434294,
      "grad_norm": 140.30007934570312,
      "learning_rate": 2.312827446341051e-05,
      "loss": 0.3379,
      "step": 13220
    },
    {
      "epoch": 2.189672293942403,
      "grad_norm": 25.21002197265625,
      "learning_rate": 2.310714889302011e-05,
      "loss": 0.3186,
      "step": 13230
    },
    {
      "epoch": 2.191327375041377,
      "grad_norm": 46.11494827270508,
      "learning_rate": 2.308602332262971e-05,
      "loss": 0.3955,
      "step": 13240
    },
    {
      "epoch": 2.192982456140351,
      "grad_norm": 70.87294006347656,
      "learning_rate": 2.306489775223931e-05,
      "loss": 0.2673,
      "step": 13250
    },
    {
      "epoch": 2.1946375372393248,
      "grad_norm": 90.28768920898438,
      "learning_rate": 2.3043772181848912e-05,
      "loss": 0.2396,
      "step": 13260
    },
    {
      "epoch": 2.1962926183382985,
      "grad_norm": 31.45452880859375,
      "learning_rate": 2.302264661145851e-05,
      "loss": 0.9248,
      "step": 13270
    },
    {
      "epoch": 2.1979476994372726,
      "grad_norm": 9.99374008178711,
      "learning_rate": 2.3001521041068112e-05,
      "loss": 0.545,
      "step": 13280
    },
    {
      "epoch": 2.1996027805362464,
      "grad_norm": 4.824028015136719,
      "learning_rate": 2.298039547067771e-05,
      "loss": 0.3544,
      "step": 13290
    },
    {
      "epoch": 2.20125786163522,
      "grad_norm": 14.971929550170898,
      "learning_rate": 2.295926990028731e-05,
      "loss": 0.3678,
      "step": 13300
    },
    {
      "epoch": 2.202912942734194,
      "grad_norm": 109.33060455322266,
      "learning_rate": 2.2938144329896907e-05,
      "loss": 0.3649,
      "step": 13310
    },
    {
      "epoch": 2.204568023833168,
      "grad_norm": 35.090118408203125,
      "learning_rate": 2.291701875950651e-05,
      "loss": 0.4333,
      "step": 13320
    },
    {
      "epoch": 2.2062231049321417,
      "grad_norm": 65.23269653320312,
      "learning_rate": 2.2895893189116107e-05,
      "loss": 0.2485,
      "step": 13330
    },
    {
      "epoch": 2.2078781860311154,
      "grad_norm": 44.121185302734375,
      "learning_rate": 2.2874767618725706e-05,
      "loss": 0.2866,
      "step": 13340
    },
    {
      "epoch": 2.2095332671300896,
      "grad_norm": 0.721973717212677,
      "learning_rate": 2.2853642048335304e-05,
      "loss": 0.1882,
      "step": 13350
    },
    {
      "epoch": 2.2111883482290633,
      "grad_norm": 17.929140090942383,
      "learning_rate": 2.2832516477944906e-05,
      "loss": 0.5194,
      "step": 13360
    },
    {
      "epoch": 2.212843429328037,
      "grad_norm": 24.843730926513672,
      "learning_rate": 2.2811390907554504e-05,
      "loss": 0.3366,
      "step": 13370
    },
    {
      "epoch": 2.2144985104270107,
      "grad_norm": 10.89903736114502,
      "learning_rate": 2.2790265337164106e-05,
      "loss": 0.4134,
      "step": 13380
    },
    {
      "epoch": 2.216153591525985,
      "grad_norm": 17.031036376953125,
      "learning_rate": 2.2769139766773704e-05,
      "loss": 0.176,
      "step": 13390
    },
    {
      "epoch": 2.2178086726249586,
      "grad_norm": 2.8286733627319336,
      "learning_rate": 2.2748014196383303e-05,
      "loss": 0.1848,
      "step": 13400
    },
    {
      "epoch": 2.2194637537239323,
      "grad_norm": 23.61067771911621,
      "learning_rate": 2.2726888625992904e-05,
      "loss": 0.3935,
      "step": 13410
    },
    {
      "epoch": 2.2211188348229065,
      "grad_norm": 0.15322914719581604,
      "learning_rate": 2.2705763055602503e-05,
      "loss": 0.2324,
      "step": 13420
    },
    {
      "epoch": 2.2227739159218802,
      "grad_norm": 0.402432918548584,
      "learning_rate": 2.26846374852121e-05,
      "loss": 0.1672,
      "step": 13430
    },
    {
      "epoch": 2.224428997020854,
      "grad_norm": 0.11730184406042099,
      "learning_rate": 2.26635119148217e-05,
      "loss": 0.401,
      "step": 13440
    },
    {
      "epoch": 2.2260840781198277,
      "grad_norm": 3.715543746948242,
      "learning_rate": 2.26423863444313e-05,
      "loss": 0.205,
      "step": 13450
    },
    {
      "epoch": 2.227739159218802,
      "grad_norm": 36.74658966064453,
      "learning_rate": 2.26212607740409e-05,
      "loss": 0.5164,
      "step": 13460
    },
    {
      "epoch": 2.2293942403177756,
      "grad_norm": 57.740501403808594,
      "learning_rate": 2.2600135203650498e-05,
      "loss": 0.3133,
      "step": 13470
    },
    {
      "epoch": 2.2310493214167493,
      "grad_norm": 71.15342712402344,
      "learning_rate": 2.2579009633260097e-05,
      "loss": 0.3101,
      "step": 13480
    },
    {
      "epoch": 2.2327044025157234,
      "grad_norm": 93.92870330810547,
      "learning_rate": 2.25578840628697e-05,
      "loss": 0.3098,
      "step": 13490
    },
    {
      "epoch": 2.234359483614697,
      "grad_norm": 34.76979446411133,
      "learning_rate": 2.25367584924793e-05,
      "loss": 0.6014,
      "step": 13500
    },
    {
      "epoch": 2.236014564713671,
      "grad_norm": 9.3934907913208,
      "learning_rate": 2.25156329220889e-05,
      "loss": 0.453,
      "step": 13510
    },
    {
      "epoch": 2.2376696458126446,
      "grad_norm": 7.46796178817749,
      "learning_rate": 2.2494507351698497e-05,
      "loss": 0.2844,
      "step": 13520
    },
    {
      "epoch": 2.2393247269116188,
      "grad_norm": 11.432573318481445,
      "learning_rate": 2.24733817813081e-05,
      "loss": 0.2945,
      "step": 13530
    },
    {
      "epoch": 2.2409798080105925,
      "grad_norm": 65.35745239257812,
      "learning_rate": 2.2452256210917697e-05,
      "loss": 0.2183,
      "step": 13540
    },
    {
      "epoch": 2.242634889109566,
      "grad_norm": 24.275123596191406,
      "learning_rate": 2.2431130640527295e-05,
      "loss": 0.6104,
      "step": 13550
    },
    {
      "epoch": 2.2442899702085404,
      "grad_norm": 3.038806915283203,
      "learning_rate": 2.2410005070136894e-05,
      "loss": 0.2329,
      "step": 13560
    },
    {
      "epoch": 2.245945051307514,
      "grad_norm": 35.04482650756836,
      "learning_rate": 2.2388879499746492e-05,
      "loss": 0.3975,
      "step": 13570
    },
    {
      "epoch": 2.247600132406488,
      "grad_norm": 66.75386810302734,
      "learning_rate": 2.2367753929356094e-05,
      "loss": 0.3489,
      "step": 13580
    },
    {
      "epoch": 2.249255213505462,
      "grad_norm": 38.34096145629883,
      "learning_rate": 2.2346628358965692e-05,
      "loss": 0.2536,
      "step": 13590
    },
    {
      "epoch": 2.2509102946044357,
      "grad_norm": 0.9697713255882263,
      "learning_rate": 2.232550278857529e-05,
      "loss": 0.2561,
      "step": 13600
    },
    {
      "epoch": 2.2525653757034094,
      "grad_norm": 28.04800033569336,
      "learning_rate": 2.2304377218184892e-05,
      "loss": 0.2031,
      "step": 13610
    },
    {
      "epoch": 2.254220456802383,
      "grad_norm": 9.084296226501465,
      "learning_rate": 2.228325164779449e-05,
      "loss": 0.165,
      "step": 13620
    },
    {
      "epoch": 2.2558755379013573,
      "grad_norm": 26.017000198364258,
      "learning_rate": 2.2262126077404093e-05,
      "loss": 0.3078,
      "step": 13630
    },
    {
      "epoch": 2.257530619000331,
      "grad_norm": 21.698749542236328,
      "learning_rate": 2.224100050701369e-05,
      "loss": 0.4137,
      "step": 13640
    },
    {
      "epoch": 2.2591857000993047,
      "grad_norm": 18.58194923400879,
      "learning_rate": 2.221987493662329e-05,
      "loss": 0.2448,
      "step": 13650
    },
    {
      "epoch": 2.2608407811982785,
      "grad_norm": 5.563835620880127,
      "learning_rate": 2.219874936623289e-05,
      "loss": 0.2449,
      "step": 13660
    },
    {
      "epoch": 2.2624958622972526,
      "grad_norm": 2.8231871128082275,
      "learning_rate": 2.217762379584249e-05,
      "loss": 0.2633,
      "step": 13670
    },
    {
      "epoch": 2.2641509433962264,
      "grad_norm": 46.33640670776367,
      "learning_rate": 2.2156498225452088e-05,
      "loss": 0.2739,
      "step": 13680
    },
    {
      "epoch": 2.2658060244952,
      "grad_norm": 5.7718682289123535,
      "learning_rate": 2.2135372655061686e-05,
      "loss": 0.2992,
      "step": 13690
    },
    {
      "epoch": 2.2674611055941742,
      "grad_norm": 19.096349716186523,
      "learning_rate": 2.2114247084671288e-05,
      "loss": 0.2681,
      "step": 13700
    },
    {
      "epoch": 2.269116186693148,
      "grad_norm": 14.689592361450195,
      "learning_rate": 2.2093121514280886e-05,
      "loss": 0.1107,
      "step": 13710
    },
    {
      "epoch": 2.2707712677921217,
      "grad_norm": 26.382862091064453,
      "learning_rate": 2.2071995943890485e-05,
      "loss": 0.5473,
      "step": 13720
    },
    {
      "epoch": 2.272426348891096,
      "grad_norm": 69.65252685546875,
      "learning_rate": 2.2050870373500083e-05,
      "loss": 0.488,
      "step": 13730
    },
    {
      "epoch": 2.2740814299900696,
      "grad_norm": 7.002397537231445,
      "learning_rate": 2.2029744803109685e-05,
      "loss": 0.2354,
      "step": 13740
    },
    {
      "epoch": 2.2757365110890433,
      "grad_norm": 1.5102304220199585,
      "learning_rate": 2.2008619232719287e-05,
      "loss": 0.1291,
      "step": 13750
    },
    {
      "epoch": 2.2773915921880175,
      "grad_norm": 6.718581199645996,
      "learning_rate": 2.1987493662328885e-05,
      "loss": 0.3734,
      "step": 13760
    },
    {
      "epoch": 2.279046673286991,
      "grad_norm": 87.25067901611328,
      "learning_rate": 2.1966368091938484e-05,
      "loss": 0.1962,
      "step": 13770
    },
    {
      "epoch": 2.280701754385965,
      "grad_norm": 47.40670394897461,
      "learning_rate": 2.1945242521548082e-05,
      "loss": 0.4474,
      "step": 13780
    },
    {
      "epoch": 2.2823568354849386,
      "grad_norm": 113.96581268310547,
      "learning_rate": 2.1924116951157684e-05,
      "loss": 0.3161,
      "step": 13790
    },
    {
      "epoch": 2.284011916583913,
      "grad_norm": 71.7204360961914,
      "learning_rate": 2.1902991380767282e-05,
      "loss": 0.1996,
      "step": 13800
    },
    {
      "epoch": 2.2856669976828865,
      "grad_norm": 50.51991653442383,
      "learning_rate": 2.188186581037688e-05,
      "loss": 0.4621,
      "step": 13810
    },
    {
      "epoch": 2.2873220787818602,
      "grad_norm": 4.124840259552002,
      "learning_rate": 2.186074023998648e-05,
      "loss": 0.2315,
      "step": 13820
    },
    {
      "epoch": 2.288977159880834,
      "grad_norm": 124.5548095703125,
      "learning_rate": 2.183961466959608e-05,
      "loss": 0.2037,
      "step": 13830
    },
    {
      "epoch": 2.290632240979808,
      "grad_norm": 8.859611511230469,
      "learning_rate": 2.181848909920568e-05,
      "loss": 0.3908,
      "step": 13840
    },
    {
      "epoch": 2.292287322078782,
      "grad_norm": 39.44078063964844,
      "learning_rate": 2.1797363528815277e-05,
      "loss": 0.5824,
      "step": 13850
    },
    {
      "epoch": 2.2939424031777556,
      "grad_norm": 9.315308570861816,
      "learning_rate": 2.177623795842488e-05,
      "loss": 0.6593,
      "step": 13860
    },
    {
      "epoch": 2.2955974842767297,
      "grad_norm": 27.8656063079834,
      "learning_rate": 2.175511238803448e-05,
      "loss": 0.4022,
      "step": 13870
    },
    {
      "epoch": 2.2972525653757034,
      "grad_norm": 32.66128921508789,
      "learning_rate": 2.173398681764408e-05,
      "loss": 0.2803,
      "step": 13880
    },
    {
      "epoch": 2.298907646474677,
      "grad_norm": 26.5188045501709,
      "learning_rate": 2.1712861247253678e-05,
      "loss": 0.4802,
      "step": 13890
    },
    {
      "epoch": 2.3005627275736513,
      "grad_norm": 13.260534286499023,
      "learning_rate": 2.1691735676863276e-05,
      "loss": 0.4174,
      "step": 13900
    },
    {
      "epoch": 2.302217808672625,
      "grad_norm": 29.170433044433594,
      "learning_rate": 2.1670610106472874e-05,
      "loss": 0.4792,
      "step": 13910
    },
    {
      "epoch": 2.3038728897715988,
      "grad_norm": 69.29991912841797,
      "learning_rate": 2.1649484536082476e-05,
      "loss": 0.2892,
      "step": 13920
    },
    {
      "epoch": 2.3055279708705725,
      "grad_norm": 43.54744338989258,
      "learning_rate": 2.1628358965692075e-05,
      "loss": 0.5853,
      "step": 13930
    },
    {
      "epoch": 2.3071830519695466,
      "grad_norm": 0.3954694867134094,
      "learning_rate": 2.1607233395301673e-05,
      "loss": 0.2273,
      "step": 13940
    },
    {
      "epoch": 2.3088381330685204,
      "grad_norm": 7.360860347747803,
      "learning_rate": 2.158610782491127e-05,
      "loss": 0.3896,
      "step": 13950
    },
    {
      "epoch": 2.310493214167494,
      "grad_norm": 34.09373474121094,
      "learning_rate": 2.1564982254520873e-05,
      "loss": 0.4965,
      "step": 13960
    },
    {
      "epoch": 2.312148295266468,
      "grad_norm": 31.37018394470215,
      "learning_rate": 2.154385668413047e-05,
      "loss": 0.3873,
      "step": 13970
    },
    {
      "epoch": 2.313803376365442,
      "grad_norm": 63.41179656982422,
      "learning_rate": 2.1522731113740073e-05,
      "loss": 0.3553,
      "step": 13980
    },
    {
      "epoch": 2.3154584574644157,
      "grad_norm": 17.04778289794922,
      "learning_rate": 2.1501605543349672e-05,
      "loss": 0.4394,
      "step": 13990
    },
    {
      "epoch": 2.3171135385633894,
      "grad_norm": 3.070392370223999,
      "learning_rate": 2.1480479972959273e-05,
      "loss": 0.2358,
      "step": 14000
    },
    {
      "epoch": 2.3187686196623636,
      "grad_norm": 8.18575382232666,
      "learning_rate": 2.1459354402568872e-05,
      "loss": 0.3084,
      "step": 14010
    },
    {
      "epoch": 2.3204237007613373,
      "grad_norm": 21.43338966369629,
      "learning_rate": 2.143822883217847e-05,
      "loss": 0.334,
      "step": 14020
    },
    {
      "epoch": 2.322078781860311,
      "grad_norm": 55.36513137817383,
      "learning_rate": 2.141710326178807e-05,
      "loss": 0.2717,
      "step": 14030
    },
    {
      "epoch": 2.323733862959285,
      "grad_norm": 0.18100973963737488,
      "learning_rate": 2.139597769139767e-05,
      "loss": 0.1753,
      "step": 14040
    },
    {
      "epoch": 2.325388944058259,
      "grad_norm": 27.72402000427246,
      "learning_rate": 2.137485212100727e-05,
      "loss": 0.2135,
      "step": 14050
    },
    {
      "epoch": 2.3270440251572326,
      "grad_norm": 1.790158748626709,
      "learning_rate": 2.1353726550616867e-05,
      "loss": 0.2081,
      "step": 14060
    },
    {
      "epoch": 2.328699106256207,
      "grad_norm": 67.3473892211914,
      "learning_rate": 2.1332600980226466e-05,
      "loss": 0.3017,
      "step": 14070
    },
    {
      "epoch": 2.3303541873551805,
      "grad_norm": 43.657962799072266,
      "learning_rate": 2.1311475409836064e-05,
      "loss": 0.4126,
      "step": 14080
    },
    {
      "epoch": 2.3320092684541542,
      "grad_norm": 14.28260612487793,
      "learning_rate": 2.1290349839445666e-05,
      "loss": 0.8339,
      "step": 14090
    },
    {
      "epoch": 2.333664349553128,
      "grad_norm": 0.1582505851984024,
      "learning_rate": 2.1269224269055264e-05,
      "loss": 0.2645,
      "step": 14100
    },
    {
      "epoch": 2.335319430652102,
      "grad_norm": 24.42403221130371,
      "learning_rate": 2.1248098698664866e-05,
      "loss": 0.4953,
      "step": 14110
    },
    {
      "epoch": 2.336974511751076,
      "grad_norm": 1.5316617488861084,
      "learning_rate": 2.1226973128274464e-05,
      "loss": 0.3446,
      "step": 14120
    },
    {
      "epoch": 2.3386295928500496,
      "grad_norm": 34.68370819091797,
      "learning_rate": 2.1205847557884066e-05,
      "loss": 0.4942,
      "step": 14130
    },
    {
      "epoch": 2.3402846739490233,
      "grad_norm": 18.718555450439453,
      "learning_rate": 2.1184721987493664e-05,
      "loss": 0.1952,
      "step": 14140
    },
    {
      "epoch": 2.3419397550479975,
      "grad_norm": 14.770249366760254,
      "learning_rate": 2.1163596417103263e-05,
      "loss": 0.4426,
      "step": 14150
    },
    {
      "epoch": 2.343594836146971,
      "grad_norm": 171.96763610839844,
      "learning_rate": 2.114247084671286e-05,
      "loss": 0.104,
      "step": 14160
    },
    {
      "epoch": 2.345249917245945,
      "grad_norm": 84.42997741699219,
      "learning_rate": 2.1121345276322463e-05,
      "loss": 0.5889,
      "step": 14170
    },
    {
      "epoch": 2.346904998344919,
      "grad_norm": 1.7412184476852417,
      "learning_rate": 2.110021970593206e-05,
      "loss": 0.0963,
      "step": 14180
    },
    {
      "epoch": 2.348560079443893,
      "grad_norm": 16.750917434692383,
      "learning_rate": 2.107909413554166e-05,
      "loss": 0.4424,
      "step": 14190
    },
    {
      "epoch": 2.3502151605428665,
      "grad_norm": 28.162919998168945,
      "learning_rate": 2.1057968565151258e-05,
      "loss": 0.5026,
      "step": 14200
    },
    {
      "epoch": 2.3518702416418407,
      "grad_norm": 59.13865661621094,
      "learning_rate": 2.103684299476086e-05,
      "loss": 0.4244,
      "step": 14210
    },
    {
      "epoch": 2.3535253227408144,
      "grad_norm": 58.15868377685547,
      "learning_rate": 2.1015717424370458e-05,
      "loss": 0.386,
      "step": 14220
    },
    {
      "epoch": 2.355180403839788,
      "grad_norm": 56.64242172241211,
      "learning_rate": 2.099459185398006e-05,
      "loss": 0.3547,
      "step": 14230
    },
    {
      "epoch": 2.356835484938762,
      "grad_norm": 4.149704933166504,
      "learning_rate": 2.097346628358966e-05,
      "loss": 0.3646,
      "step": 14240
    },
    {
      "epoch": 2.358490566037736,
      "grad_norm": 21.128631591796875,
      "learning_rate": 2.0952340713199257e-05,
      "loss": 0.2736,
      "step": 14250
    },
    {
      "epoch": 2.3601456471367097,
      "grad_norm": 2.4360036849975586,
      "learning_rate": 2.093121514280886e-05,
      "loss": 0.1214,
      "step": 14260
    },
    {
      "epoch": 2.3618007282356834,
      "grad_norm": 12.499972343444824,
      "learning_rate": 2.0910089572418457e-05,
      "loss": 0.2489,
      "step": 14270
    },
    {
      "epoch": 2.363455809334657,
      "grad_norm": 17.711807250976562,
      "learning_rate": 2.0888964002028055e-05,
      "loss": 0.1887,
      "step": 14280
    },
    {
      "epoch": 2.3651108904336313,
      "grad_norm": 17.367237091064453,
      "learning_rate": 2.0867838431637654e-05,
      "loss": 0.2672,
      "step": 14290
    },
    {
      "epoch": 2.366765971532605,
      "grad_norm": 5.213496208190918,
      "learning_rate": 2.0846712861247255e-05,
      "loss": 0.1295,
      "step": 14300
    },
    {
      "epoch": 2.3684210526315788,
      "grad_norm": 116.6193618774414,
      "learning_rate": 2.0825587290856854e-05,
      "loss": 0.4451,
      "step": 14310
    },
    {
      "epoch": 2.370076133730553,
      "grad_norm": 203.3242645263672,
      "learning_rate": 2.0804461720466452e-05,
      "loss": 0.7714,
      "step": 14320
    },
    {
      "epoch": 2.3717312148295266,
      "grad_norm": 54.66300964355469,
      "learning_rate": 2.078333615007605e-05,
      "loss": 0.3039,
      "step": 14330
    },
    {
      "epoch": 2.3733862959285004,
      "grad_norm": 14.382818222045898,
      "learning_rate": 2.0762210579685652e-05,
      "loss": 0.2207,
      "step": 14340
    },
    {
      "epoch": 2.3750413770274745,
      "grad_norm": 52.40081787109375,
      "learning_rate": 2.074108500929525e-05,
      "loss": 0.2839,
      "step": 14350
    },
    {
      "epoch": 2.3766964581264483,
      "grad_norm": 4.7220940589904785,
      "learning_rate": 2.0719959438904853e-05,
      "loss": 0.5001,
      "step": 14360
    },
    {
      "epoch": 2.378351539225422,
      "grad_norm": 26.405914306640625,
      "learning_rate": 2.069883386851445e-05,
      "loss": 0.2283,
      "step": 14370
    },
    {
      "epoch": 2.380006620324396,
      "grad_norm": 0.8957440257072449,
      "learning_rate": 2.0677708298124053e-05,
      "loss": 0.4346,
      "step": 14380
    },
    {
      "epoch": 2.38166170142337,
      "grad_norm": 2.113913059234619,
      "learning_rate": 2.065658272773365e-05,
      "loss": 0.2679,
      "step": 14390
    },
    {
      "epoch": 2.3833167825223436,
      "grad_norm": 91.48001861572266,
      "learning_rate": 2.063545715734325e-05,
      "loss": 0.4888,
      "step": 14400
    },
    {
      "epoch": 2.3849718636213173,
      "grad_norm": 32.737937927246094,
      "learning_rate": 2.0614331586952848e-05,
      "loss": 0.2879,
      "step": 14410
    },
    {
      "epoch": 2.3866269447202915,
      "grad_norm": 32.72587585449219,
      "learning_rate": 2.0593206016562446e-05,
      "loss": 0.2341,
      "step": 14420
    },
    {
      "epoch": 2.388282025819265,
      "grad_norm": 40.87239456176758,
      "learning_rate": 2.0572080446172048e-05,
      "loss": 0.2871,
      "step": 14430
    },
    {
      "epoch": 2.389937106918239,
      "grad_norm": 0.2906323969364166,
      "learning_rate": 2.0550954875781646e-05,
      "loss": 0.2937,
      "step": 14440
    },
    {
      "epoch": 2.3915921880172126,
      "grad_norm": 44.66936111450195,
      "learning_rate": 2.0529829305391245e-05,
      "loss": 0.4308,
      "step": 14450
    },
    {
      "epoch": 2.393247269116187,
      "grad_norm": 54.737823486328125,
      "learning_rate": 2.0508703735000847e-05,
      "loss": 0.3422,
      "step": 14460
    },
    {
      "epoch": 2.3949023502151605,
      "grad_norm": 0.11754538863897324,
      "learning_rate": 2.0487578164610445e-05,
      "loss": 0.1734,
      "step": 14470
    },
    {
      "epoch": 2.3965574313141342,
      "grad_norm": 33.16493606567383,
      "learning_rate": 2.0466452594220047e-05,
      "loss": 0.2457,
      "step": 14480
    },
    {
      "epoch": 2.3982125124131084,
      "grad_norm": 42.1945915222168,
      "learning_rate": 2.0445327023829645e-05,
      "loss": 0.3075,
      "step": 14490
    },
    {
      "epoch": 2.399867593512082,
      "grad_norm": 3.5507774353027344,
      "learning_rate": 2.0424201453439243e-05,
      "loss": 0.6268,
      "step": 14500
    },
    {
      "epoch": 2.401522674611056,
      "grad_norm": 54.502166748046875,
      "learning_rate": 2.0403075883048845e-05,
      "loss": 0.7519,
      "step": 14510
    },
    {
      "epoch": 2.40317775571003,
      "grad_norm": 27.669267654418945,
      "learning_rate": 2.0381950312658444e-05,
      "loss": 0.7454,
      "step": 14520
    },
    {
      "epoch": 2.4048328368090037,
      "grad_norm": 36.43217849731445,
      "learning_rate": 2.0360824742268042e-05,
      "loss": 0.3544,
      "step": 14530
    },
    {
      "epoch": 2.4064879179079774,
      "grad_norm": 16.99609375,
      "learning_rate": 2.033969917187764e-05,
      "loss": 0.4071,
      "step": 14540
    },
    {
      "epoch": 2.408142999006951,
      "grad_norm": 26.789709091186523,
      "learning_rate": 2.0318573601487242e-05,
      "loss": 0.2948,
      "step": 14550
    },
    {
      "epoch": 2.4097980801059253,
      "grad_norm": 10.458468437194824,
      "learning_rate": 2.029744803109684e-05,
      "loss": 0.3415,
      "step": 14560
    },
    {
      "epoch": 2.411453161204899,
      "grad_norm": 52.794212341308594,
      "learning_rate": 2.027632246070644e-05,
      "loss": 0.3518,
      "step": 14570
    },
    {
      "epoch": 2.4131082423038728,
      "grad_norm": 16.142526626586914,
      "learning_rate": 2.0255196890316037e-05,
      "loss": 0.1323,
      "step": 14580
    },
    {
      "epoch": 2.4147633234028465,
      "grad_norm": 4.030161380767822,
      "learning_rate": 2.023407131992564e-05,
      "loss": 0.4612,
      "step": 14590
    },
    {
      "epoch": 2.4164184045018207,
      "grad_norm": 28.93282699584961,
      "learning_rate": 2.021294574953524e-05,
      "loss": 0.1642,
      "step": 14600
    },
    {
      "epoch": 2.4180734856007944,
      "grad_norm": 0.46015867590904236,
      "learning_rate": 2.019182017914484e-05,
      "loss": 0.1189,
      "step": 14610
    },
    {
      "epoch": 2.419728566699768,
      "grad_norm": 1.8919284343719482,
      "learning_rate": 2.0170694608754438e-05,
      "loss": 0.4175,
      "step": 14620
    },
    {
      "epoch": 2.4213836477987423,
      "grad_norm": 0.1559431254863739,
      "learning_rate": 2.0149569038364036e-05,
      "loss": 0.1822,
      "step": 14630
    },
    {
      "epoch": 2.423038728897716,
      "grad_norm": 15.282894134521484,
      "learning_rate": 2.0128443467973638e-05,
      "loss": 0.1758,
      "step": 14640
    },
    {
      "epoch": 2.4246938099966897,
      "grad_norm": 0.01492499653249979,
      "learning_rate": 2.0107317897583236e-05,
      "loss": 0.1012,
      "step": 14650
    },
    {
      "epoch": 2.426348891095664,
      "grad_norm": 85.59393310546875,
      "learning_rate": 2.0086192327192835e-05,
      "loss": 0.5765,
      "step": 14660
    },
    {
      "epoch": 2.4280039721946376,
      "grad_norm": 16.526174545288086,
      "learning_rate": 2.0065066756802433e-05,
      "loss": 0.5194,
      "step": 14670
    },
    {
      "epoch": 2.4296590532936113,
      "grad_norm": 3.440570831298828,
      "learning_rate": 2.0043941186412035e-05,
      "loss": 0.3249,
      "step": 14680
    },
    {
      "epoch": 2.4313141343925855,
      "grad_norm": 1.3722749948501587,
      "learning_rate": 2.0022815616021633e-05,
      "loss": 0.1587,
      "step": 14690
    },
    {
      "epoch": 2.432969215491559,
      "grad_norm": 68.87767791748047,
      "learning_rate": 2.000169004563123e-05,
      "loss": 0.2168,
      "step": 14700
    },
    {
      "epoch": 2.434624296590533,
      "grad_norm": 83.99150085449219,
      "learning_rate": 1.9980564475240833e-05,
      "loss": 0.112,
      "step": 14710
    },
    {
      "epoch": 2.4362793776895066,
      "grad_norm": 0.2729966342449188,
      "learning_rate": 1.995943890485043e-05,
      "loss": 0.4033,
      "step": 14720
    },
    {
      "epoch": 2.437934458788481,
      "grad_norm": 2.380955457687378,
      "learning_rate": 1.9938313334460033e-05,
      "loss": 0.3726,
      "step": 14730
    },
    {
      "epoch": 2.4395895398874545,
      "grad_norm": 0.02889109216630459,
      "learning_rate": 1.9917187764069632e-05,
      "loss": 0.289,
      "step": 14740
    },
    {
      "epoch": 2.4412446209864282,
      "grad_norm": 40.85703659057617,
      "learning_rate": 1.989606219367923e-05,
      "loss": 0.2683,
      "step": 14750
    },
    {
      "epoch": 2.442899702085402,
      "grad_norm": 18.5722713470459,
      "learning_rate": 1.987493662328883e-05,
      "loss": 0.4068,
      "step": 14760
    },
    {
      "epoch": 2.444554783184376,
      "grad_norm": 202.53538513183594,
      "learning_rate": 1.985381105289843e-05,
      "loss": 0.3086,
      "step": 14770
    },
    {
      "epoch": 2.44620986428335,
      "grad_norm": 78.62748718261719,
      "learning_rate": 1.983268548250803e-05,
      "loss": 0.2644,
      "step": 14780
    },
    {
      "epoch": 2.4478649453823236,
      "grad_norm": 60.810508728027344,
      "learning_rate": 1.9811559912117627e-05,
      "loss": 0.2528,
      "step": 14790
    },
    {
      "epoch": 2.4495200264812977,
      "grad_norm": 15.85888957977295,
      "learning_rate": 1.9790434341727225e-05,
      "loss": 0.5142,
      "step": 14800
    },
    {
      "epoch": 2.4511751075802715,
      "grad_norm": 19.03053092956543,
      "learning_rate": 1.9769308771336827e-05,
      "loss": 0.5002,
      "step": 14810
    },
    {
      "epoch": 2.452830188679245,
      "grad_norm": 4.498003005981445,
      "learning_rate": 1.9748183200946426e-05,
      "loss": 0.2622,
      "step": 14820
    },
    {
      "epoch": 2.4544852697782193,
      "grad_norm": 14.206376075744629,
      "learning_rate": 1.9727057630556024e-05,
      "loss": 0.1063,
      "step": 14830
    },
    {
      "epoch": 2.456140350877193,
      "grad_norm": 12.93790054321289,
      "learning_rate": 1.9705932060165626e-05,
      "loss": 0.3127,
      "step": 14840
    },
    {
      "epoch": 2.457795431976167,
      "grad_norm": 0.7055134177207947,
      "learning_rate": 1.9684806489775228e-05,
      "loss": 0.2289,
      "step": 14850
    },
    {
      "epoch": 2.4594505130751405,
      "grad_norm": 50.22205352783203,
      "learning_rate": 1.9663680919384826e-05,
      "loss": 0.4253,
      "step": 14860
    },
    {
      "epoch": 2.4611055941741147,
      "grad_norm": 1.7977900505065918,
      "learning_rate": 1.9642555348994424e-05,
      "loss": 0.3469,
      "step": 14870
    },
    {
      "epoch": 2.4627606752730884,
      "grad_norm": 60.81037139892578,
      "learning_rate": 1.9621429778604023e-05,
      "loss": 0.422,
      "step": 14880
    },
    {
      "epoch": 2.464415756372062,
      "grad_norm": 19.813772201538086,
      "learning_rate": 1.9600304208213624e-05,
      "loss": 0.1905,
      "step": 14890
    },
    {
      "epoch": 2.466070837471036,
      "grad_norm": 100.60957336425781,
      "learning_rate": 1.9579178637823223e-05,
      "loss": 0.3757,
      "step": 14900
    },
    {
      "epoch": 2.46772591857001,
      "grad_norm": 126.26012420654297,
      "learning_rate": 1.955805306743282e-05,
      "loss": 0.3612,
      "step": 14910
    },
    {
      "epoch": 2.4693809996689837,
      "grad_norm": 94.36631774902344,
      "learning_rate": 1.953692749704242e-05,
      "loss": 0.2254,
      "step": 14920
    },
    {
      "epoch": 2.4710360807679574,
      "grad_norm": 8.143867492675781,
      "learning_rate": 1.9515801926652018e-05,
      "loss": 0.5293,
      "step": 14930
    },
    {
      "epoch": 2.4726911618669316,
      "grad_norm": 24.239259719848633,
      "learning_rate": 1.949467635626162e-05,
      "loss": 0.1661,
      "step": 14940
    },
    {
      "epoch": 2.4743462429659053,
      "grad_norm": 35.732879638671875,
      "learning_rate": 1.9473550785871218e-05,
      "loss": 0.3481,
      "step": 14950
    },
    {
      "epoch": 2.476001324064879,
      "grad_norm": 17.889842987060547,
      "learning_rate": 1.945242521548082e-05,
      "loss": 0.4194,
      "step": 14960
    },
    {
      "epoch": 2.477656405163853,
      "grad_norm": 2.374890089035034,
      "learning_rate": 1.943129964509042e-05,
      "loss": 0.2022,
      "step": 14970
    },
    {
      "epoch": 2.479311486262827,
      "grad_norm": 3.740821123123169,
      "learning_rate": 1.941017407470002e-05,
      "loss": 0.2153,
      "step": 14980
    },
    {
      "epoch": 2.4809665673618007,
      "grad_norm": 18.38161277770996,
      "learning_rate": 1.938904850430962e-05,
      "loss": 0.4984,
      "step": 14990
    },
    {
      "epoch": 2.482621648460775,
      "grad_norm": 4.165164470672607,
      "learning_rate": 1.9367922933919217e-05,
      "loss": 0.2802,
      "step": 15000
    },
    {
      "epoch": 2.4842767295597485,
      "grad_norm": 27.747507095336914,
      "learning_rate": 1.9346797363528815e-05,
      "loss": 0.4144,
      "step": 15010
    },
    {
      "epoch": 2.4859318106587223,
      "grad_norm": 1.886864185333252,
      "learning_rate": 1.9325671793138417e-05,
      "loss": 0.3665,
      "step": 15020
    },
    {
      "epoch": 2.487586891757696,
      "grad_norm": 102.67200469970703,
      "learning_rate": 1.9304546222748015e-05,
      "loss": 0.3774,
      "step": 15030
    },
    {
      "epoch": 2.48924197285667,
      "grad_norm": 60.91831970214844,
      "learning_rate": 1.9283420652357614e-05,
      "loss": 0.1679,
      "step": 15040
    },
    {
      "epoch": 2.490897053955644,
      "grad_norm": 50.06858444213867,
      "learning_rate": 1.9262295081967212e-05,
      "loss": 0.4963,
      "step": 15050
    },
    {
      "epoch": 2.4925521350546176,
      "grad_norm": 63.511451721191406,
      "learning_rate": 1.9241169511576814e-05,
      "loss": 0.156,
      "step": 15060
    },
    {
      "epoch": 2.4942072161535913,
      "grad_norm": 52.23703384399414,
      "learning_rate": 1.9220043941186412e-05,
      "loss": 0.4154,
      "step": 15070
    },
    {
      "epoch": 2.4958622972525655,
      "grad_norm": 9.771721839904785,
      "learning_rate": 1.9198918370796014e-05,
      "loss": 0.2655,
      "step": 15080
    },
    {
      "epoch": 2.497517378351539,
      "grad_norm": 23.497568130493164,
      "learning_rate": 1.9177792800405612e-05,
      "loss": 0.2331,
      "step": 15090
    },
    {
      "epoch": 2.499172459450513,
      "grad_norm": 6.200976848602295,
      "learning_rate": 1.915666723001521e-05,
      "loss": 0.3776,
      "step": 15100
    },
    {
      "epoch": 2.500827540549487,
      "grad_norm": 100.8780288696289,
      "learning_rate": 1.9135541659624813e-05,
      "loss": 0.2023,
      "step": 15110
    },
    {
      "epoch": 2.502482621648461,
      "grad_norm": 50.70888900756836,
      "learning_rate": 1.911441608923441e-05,
      "loss": 0.2789,
      "step": 15120
    },
    {
      "epoch": 2.5041377027474345,
      "grad_norm": 83.38892364501953,
      "learning_rate": 1.909329051884401e-05,
      "loss": 0.3457,
      "step": 15130
    },
    {
      "epoch": 2.5057927838464087,
      "grad_norm": 36.346717834472656,
      "learning_rate": 1.9072164948453608e-05,
      "loss": 0.2792,
      "step": 15140
    },
    {
      "epoch": 2.5074478649453824,
      "grad_norm": 31.174158096313477,
      "learning_rate": 1.905103937806321e-05,
      "loss": 0.3276,
      "step": 15150
    },
    {
      "epoch": 2.509102946044356,
      "grad_norm": 0.19541069865226746,
      "learning_rate": 1.9029913807672808e-05,
      "loss": 0.0601,
      "step": 15160
    },
    {
      "epoch": 2.5107580271433303,
      "grad_norm": 19.039966583251953,
      "learning_rate": 1.9008788237282406e-05,
      "loss": 0.2933,
      "step": 15170
    },
    {
      "epoch": 2.512413108242304,
      "grad_norm": 23.00742530822754,
      "learning_rate": 1.8987662666892005e-05,
      "loss": 0.1821,
      "step": 15180
    },
    {
      "epoch": 2.5140681893412777,
      "grad_norm": 0.016628041863441467,
      "learning_rate": 1.8966537096501606e-05,
      "loss": 0.3016,
      "step": 15190
    },
    {
      "epoch": 2.5157232704402515,
      "grad_norm": 94.32640838623047,
      "learning_rate": 1.8945411526111205e-05,
      "loss": 0.3297,
      "step": 15200
    },
    {
      "epoch": 2.517378351539225,
      "grad_norm": 0.1943204253911972,
      "learning_rate": 1.8924285955720807e-05,
      "loss": 0.2418,
      "step": 15210
    },
    {
      "epoch": 2.5190334326381993,
      "grad_norm": 0.9861687421798706,
      "learning_rate": 1.8903160385330405e-05,
      "loss": 0.2641,
      "step": 15220
    },
    {
      "epoch": 2.520688513737173,
      "grad_norm": 82.40266418457031,
      "learning_rate": 1.8882034814940007e-05,
      "loss": 0.3211,
      "step": 15230
    },
    {
      "epoch": 2.522343594836147,
      "grad_norm": 34.48919677734375,
      "learning_rate": 1.8860909244549605e-05,
      "loss": 0.3544,
      "step": 15240
    },
    {
      "epoch": 2.523998675935121,
      "grad_norm": 104.3883056640625,
      "learning_rate": 1.8839783674159204e-05,
      "loss": 0.4909,
      "step": 15250
    },
    {
      "epoch": 2.5256537570340947,
      "grad_norm": 3.5192947387695312,
      "learning_rate": 1.8818658103768802e-05,
      "loss": 0.2044,
      "step": 15260
    },
    {
      "epoch": 2.5273088381330684,
      "grad_norm": 6.7467522621154785,
      "learning_rate": 1.87975325333784e-05,
      "loss": 0.3521,
      "step": 15270
    },
    {
      "epoch": 2.5289639192320426,
      "grad_norm": 35.79228973388672,
      "learning_rate": 1.8776406962988002e-05,
      "loss": 0.5527,
      "step": 15280
    },
    {
      "epoch": 2.5306190003310163,
      "grad_norm": 45.69153594970703,
      "learning_rate": 1.87552813925976e-05,
      "loss": 0.153,
      "step": 15290
    },
    {
      "epoch": 2.53227408142999,
      "grad_norm": 41.9891242980957,
      "learning_rate": 1.87341558222072e-05,
      "loss": 0.378,
      "step": 15300
    },
    {
      "epoch": 2.533929162528964,
      "grad_norm": 5.26572847366333,
      "learning_rate": 1.8713030251816797e-05,
      "loss": 0.2444,
      "step": 15310
    },
    {
      "epoch": 2.535584243627938,
      "grad_norm": 9.390190124511719,
      "learning_rate": 1.86919046814264e-05,
      "loss": 0.1637,
      "step": 15320
    },
    {
      "epoch": 2.5372393247269116,
      "grad_norm": 24.65130615234375,
      "learning_rate": 1.8670779111036e-05,
      "loss": 0.1582,
      "step": 15330
    },
    {
      "epoch": 2.5388944058258853,
      "grad_norm": 1.270185947418213,
      "learning_rate": 1.86496535406456e-05,
      "loss": 0.0839,
      "step": 15340
    },
    {
      "epoch": 2.540549486924859,
      "grad_norm": 6.8098530769348145,
      "learning_rate": 1.8628527970255198e-05,
      "loss": 0.4453,
      "step": 15350
    },
    {
      "epoch": 2.542204568023833,
      "grad_norm": 11.210939407348633,
      "learning_rate": 1.86074023998648e-05,
      "loss": 0.2308,
      "step": 15360
    },
    {
      "epoch": 2.543859649122807,
      "grad_norm": 0.2857091724872589,
      "learning_rate": 1.8586276829474398e-05,
      "loss": 0.2285,
      "step": 15370
    },
    {
      "epoch": 2.5455147302217807,
      "grad_norm": 3.8039751052856445,
      "learning_rate": 1.8565151259083996e-05,
      "loss": 0.2181,
      "step": 15380
    },
    {
      "epoch": 2.547169811320755,
      "grad_norm": 3.678884506225586,
      "learning_rate": 1.8544025688693594e-05,
      "loss": 0.1918,
      "step": 15390
    },
    {
      "epoch": 2.5488248924197285,
      "grad_norm": 64.30712127685547,
      "learning_rate": 1.8522900118303196e-05,
      "loss": 0.3729,
      "step": 15400
    },
    {
      "epoch": 2.5504799735187023,
      "grad_norm": 38.15809631347656,
      "learning_rate": 1.8501774547912795e-05,
      "loss": 0.3117,
      "step": 15410
    },
    {
      "epoch": 2.5521350546176764,
      "grad_norm": 8.54313850402832,
      "learning_rate": 1.8480648977522393e-05,
      "loss": 0.3228,
      "step": 15420
    },
    {
      "epoch": 2.55379013571665,
      "grad_norm": 29.081544876098633,
      "learning_rate": 1.845952340713199e-05,
      "loss": 0.4022,
      "step": 15430
    },
    {
      "epoch": 2.555445216815624,
      "grad_norm": 8.095470428466797,
      "learning_rate": 1.8438397836741593e-05,
      "loss": 0.1923,
      "step": 15440
    },
    {
      "epoch": 2.557100297914598,
      "grad_norm": 26.09986686706543,
      "learning_rate": 1.841727226635119e-05,
      "loss": 0.5448,
      "step": 15450
    },
    {
      "epoch": 2.5587553790135718,
      "grad_norm": 74.14949035644531,
      "learning_rate": 1.8396146695960793e-05,
      "loss": 0.2348,
      "step": 15460
    },
    {
      "epoch": 2.5604104601125455,
      "grad_norm": 28.699588775634766,
      "learning_rate": 1.837502112557039e-05,
      "loss": 0.1868,
      "step": 15470
    },
    {
      "epoch": 2.5620655412115196,
      "grad_norm": 82.58898162841797,
      "learning_rate": 1.835389555517999e-05,
      "loss": 0.5017,
      "step": 15480
    },
    {
      "epoch": 2.5637206223104934,
      "grad_norm": 6.295657634735107,
      "learning_rate": 1.8332769984789592e-05,
      "loss": 0.2612,
      "step": 15490
    },
    {
      "epoch": 2.565375703409467,
      "grad_norm": 50.61637878417969,
      "learning_rate": 1.831164441439919e-05,
      "loss": 0.3351,
      "step": 15500
    },
    {
      "epoch": 2.567030784508441,
      "grad_norm": 2.029797315597534,
      "learning_rate": 1.829051884400879e-05,
      "loss": 0.1327,
      "step": 15510
    },
    {
      "epoch": 2.5686858656074145,
      "grad_norm": 32.607940673828125,
      "learning_rate": 1.8269393273618387e-05,
      "loss": 0.5676,
      "step": 15520
    },
    {
      "epoch": 2.5703409467063887,
      "grad_norm": 20.5897274017334,
      "learning_rate": 1.824826770322799e-05,
      "loss": 0.3094,
      "step": 15530
    },
    {
      "epoch": 2.5719960278053624,
      "grad_norm": 2.2483019828796387,
      "learning_rate": 1.8227142132837587e-05,
      "loss": 0.189,
      "step": 15540
    },
    {
      "epoch": 2.573651108904336,
      "grad_norm": 11.311417579650879,
      "learning_rate": 1.8206016562447186e-05,
      "loss": 0.358,
      "step": 15550
    },
    {
      "epoch": 2.5753061900033103,
      "grad_norm": 17.211456298828125,
      "learning_rate": 1.8184890992056784e-05,
      "loss": 0.4079,
      "step": 15560
    },
    {
      "epoch": 2.576961271102284,
      "grad_norm": 47.36027526855469,
      "learning_rate": 1.8163765421666386e-05,
      "loss": 0.435,
      "step": 15570
    },
    {
      "epoch": 2.5786163522012577,
      "grad_norm": 23.20574951171875,
      "learning_rate": 1.8142639851275987e-05,
      "loss": 0.2075,
      "step": 15580
    },
    {
      "epoch": 2.580271433300232,
      "grad_norm": 13.479445457458496,
      "learning_rate": 1.8121514280885586e-05,
      "loss": 0.3542,
      "step": 15590
    },
    {
      "epoch": 2.5819265143992056,
      "grad_norm": 62.46315002441406,
      "learning_rate": 1.8100388710495184e-05,
      "loss": 0.3032,
      "step": 15600
    },
    {
      "epoch": 2.5835815954981793,
      "grad_norm": 5.632845878601074,
      "learning_rate": 1.8079263140104783e-05,
      "loss": 0.2325,
      "step": 15610
    },
    {
      "epoch": 2.5852366765971535,
      "grad_norm": 36.138816833496094,
      "learning_rate": 1.8058137569714384e-05,
      "loss": 0.276,
      "step": 15620
    },
    {
      "epoch": 2.5868917576961272,
      "grad_norm": 39.120906829833984,
      "learning_rate": 1.8037011999323983e-05,
      "loss": 0.1548,
      "step": 15630
    },
    {
      "epoch": 2.588546838795101,
      "grad_norm": 11.821616172790527,
      "learning_rate": 1.801588642893358e-05,
      "loss": 0.3523,
      "step": 15640
    },
    {
      "epoch": 2.5902019198940747,
      "grad_norm": 21.812196731567383,
      "learning_rate": 1.799476085854318e-05,
      "loss": 0.4346,
      "step": 15650
    },
    {
      "epoch": 2.5918570009930484,
      "grad_norm": 8.911273956298828,
      "learning_rate": 1.797363528815278e-05,
      "loss": 0.0698,
      "step": 15660
    },
    {
      "epoch": 2.5935120820920226,
      "grad_norm": 10.907587051391602,
      "learning_rate": 1.795250971776238e-05,
      "loss": 0.2961,
      "step": 15670
    },
    {
      "epoch": 2.5951671631909963,
      "grad_norm": 38.54566192626953,
      "learning_rate": 1.7931384147371978e-05,
      "loss": 0.5851,
      "step": 15680
    },
    {
      "epoch": 2.59682224428997,
      "grad_norm": 29.731563568115234,
      "learning_rate": 1.791025857698158e-05,
      "loss": 0.3351,
      "step": 15690
    },
    {
      "epoch": 2.598477325388944,
      "grad_norm": 11.710588455200195,
      "learning_rate": 1.788913300659118e-05,
      "loss": 0.1128,
      "step": 15700
    },
    {
      "epoch": 2.600132406487918,
      "grad_norm": 95.3436508178711,
      "learning_rate": 1.786800743620078e-05,
      "loss": 0.5173,
      "step": 15710
    },
    {
      "epoch": 2.6017874875868916,
      "grad_norm": 20.557191848754883,
      "learning_rate": 1.784688186581038e-05,
      "loss": 0.3874,
      "step": 15720
    },
    {
      "epoch": 2.6034425686858658,
      "grad_norm": 11.680071830749512,
      "learning_rate": 1.7825756295419977e-05,
      "loss": 0.2699,
      "step": 15730
    },
    {
      "epoch": 2.6050976497848395,
      "grad_norm": 36.43222427368164,
      "learning_rate": 1.780463072502958e-05,
      "loss": 0.2107,
      "step": 15740
    },
    {
      "epoch": 2.606752730883813,
      "grad_norm": 83.37367248535156,
      "learning_rate": 1.7783505154639177e-05,
      "loss": 0.4526,
      "step": 15750
    },
    {
      "epoch": 2.6084078119827874,
      "grad_norm": 18.101734161376953,
      "learning_rate": 1.7762379584248775e-05,
      "loss": 0.404,
      "step": 15760
    },
    {
      "epoch": 2.610062893081761,
      "grad_norm": 29.86277961730957,
      "learning_rate": 1.7741254013858374e-05,
      "loss": 0.1981,
      "step": 15770
    },
    {
      "epoch": 2.611717974180735,
      "grad_norm": 95.885009765625,
      "learning_rate": 1.7720128443467972e-05,
      "loss": 0.6765,
      "step": 15780
    },
    {
      "epoch": 2.613373055279709,
      "grad_norm": 13.254373550415039,
      "learning_rate": 1.7699002873077574e-05,
      "loss": 0.7493,
      "step": 15790
    },
    {
      "epoch": 2.6150281363786827,
      "grad_norm": 22.089107513427734,
      "learning_rate": 1.7677877302687172e-05,
      "loss": 0.2057,
      "step": 15800
    },
    {
      "epoch": 2.6166832174776564,
      "grad_norm": 10.314010620117188,
      "learning_rate": 1.7656751732296774e-05,
      "loss": 0.3212,
      "step": 15810
    },
    {
      "epoch": 2.61833829857663,
      "grad_norm": 81.7684326171875,
      "learning_rate": 1.7635626161906372e-05,
      "loss": 0.188,
      "step": 15820
    },
    {
      "epoch": 2.619993379675604,
      "grad_norm": 6.941982746124268,
      "learning_rate": 1.7614500591515974e-05,
      "loss": 0.2698,
      "step": 15830
    },
    {
      "epoch": 2.621648460774578,
      "grad_norm": 25.405813217163086,
      "learning_rate": 1.7593375021125573e-05,
      "loss": 0.4295,
      "step": 15840
    },
    {
      "epoch": 2.6233035418735517,
      "grad_norm": 11.961947441101074,
      "learning_rate": 1.757224945073517e-05,
      "loss": 0.3283,
      "step": 15850
    },
    {
      "epoch": 2.6249586229725255,
      "grad_norm": 0.17432883381843567,
      "learning_rate": 1.755112388034477e-05,
      "loss": 0.1242,
      "step": 15860
    },
    {
      "epoch": 2.6266137040714996,
      "grad_norm": 29.024240493774414,
      "learning_rate": 1.752999830995437e-05,
      "loss": 0.36,
      "step": 15870
    },
    {
      "epoch": 2.6282687851704734,
      "grad_norm": 0.21559984982013702,
      "learning_rate": 1.750887273956397e-05,
      "loss": 0.2281,
      "step": 15880
    },
    {
      "epoch": 2.629923866269447,
      "grad_norm": 2.055856466293335,
      "learning_rate": 1.7487747169173568e-05,
      "loss": 0.4612,
      "step": 15890
    },
    {
      "epoch": 2.6315789473684212,
      "grad_norm": 21.097938537597656,
      "learning_rate": 1.7466621598783166e-05,
      "loss": 0.1615,
      "step": 15900
    },
    {
      "epoch": 2.633234028467395,
      "grad_norm": 112.76007080078125,
      "learning_rate": 1.7445496028392768e-05,
      "loss": 0.2838,
      "step": 15910
    },
    {
      "epoch": 2.6348891095663687,
      "grad_norm": 4.673019886016846,
      "learning_rate": 1.7424370458002366e-05,
      "loss": 0.2313,
      "step": 15920
    },
    {
      "epoch": 2.636544190665343,
      "grad_norm": 41.109283447265625,
      "learning_rate": 1.7403244887611965e-05,
      "loss": 0.1953,
      "step": 15930
    },
    {
      "epoch": 2.6381992717643166,
      "grad_norm": 74.24070739746094,
      "learning_rate": 1.7382119317221567e-05,
      "loss": 0.8399,
      "step": 15940
    },
    {
      "epoch": 2.6398543528632903,
      "grad_norm": 51.536895751953125,
      "learning_rate": 1.7360993746831165e-05,
      "loss": 0.2507,
      "step": 15950
    },
    {
      "epoch": 2.641509433962264,
      "grad_norm": 7.951625823974609,
      "learning_rate": 1.7339868176440767e-05,
      "loss": 0.2798,
      "step": 15960
    },
    {
      "epoch": 2.6431645150612377,
      "grad_norm": 13.150014877319336,
      "learning_rate": 1.7318742606050365e-05,
      "loss": 0.5131,
      "step": 15970
    },
    {
      "epoch": 2.644819596160212,
      "grad_norm": 72.6067123413086,
      "learning_rate": 1.7297617035659963e-05,
      "loss": 0.5528,
      "step": 15980
    },
    {
      "epoch": 2.6464746772591856,
      "grad_norm": 4.474448204040527,
      "learning_rate": 1.7276491465269562e-05,
      "loss": 0.3339,
      "step": 15990
    },
    {
      "epoch": 2.6481297583581593,
      "grad_norm": 1.2252564430236816,
      "learning_rate": 1.7255365894879164e-05,
      "loss": 0.2909,
      "step": 16000
    },
    {
      "epoch": 2.6497848394571335,
      "grad_norm": 35.324462890625,
      "learning_rate": 1.7234240324488762e-05,
      "loss": 0.2877,
      "step": 16010
    },
    {
      "epoch": 2.651439920556107,
      "grad_norm": 19.841630935668945,
      "learning_rate": 1.721311475409836e-05,
      "loss": 0.1138,
      "step": 16020
    },
    {
      "epoch": 2.653095001655081,
      "grad_norm": 0.9123427867889404,
      "learning_rate": 1.719198918370796e-05,
      "loss": 0.3122,
      "step": 16030
    },
    {
      "epoch": 2.654750082754055,
      "grad_norm": 55.48956298828125,
      "learning_rate": 1.717086361331756e-05,
      "loss": 0.448,
      "step": 16040
    },
    {
      "epoch": 2.656405163853029,
      "grad_norm": 51.36769485473633,
      "learning_rate": 1.714973804292716e-05,
      "loss": 0.1546,
      "step": 16050
    },
    {
      "epoch": 2.6580602449520025,
      "grad_norm": 1.4360907077789307,
      "learning_rate": 1.712861247253676e-05,
      "loss": 0.2646,
      "step": 16060
    },
    {
      "epoch": 2.6597153260509767,
      "grad_norm": 29.45737075805664,
      "learning_rate": 1.710748690214636e-05,
      "loss": 0.5457,
      "step": 16070
    },
    {
      "epoch": 2.6613704071499504,
      "grad_norm": 13.573003768920898,
      "learning_rate": 1.708636133175596e-05,
      "loss": 0.2195,
      "step": 16080
    },
    {
      "epoch": 2.663025488248924,
      "grad_norm": 25.957944869995117,
      "learning_rate": 1.706523576136556e-05,
      "loss": 0.1995,
      "step": 16090
    },
    {
      "epoch": 2.6646805693478983,
      "grad_norm": 47.9066162109375,
      "learning_rate": 1.7044110190975158e-05,
      "loss": 0.298,
      "step": 16100
    },
    {
      "epoch": 2.666335650446872,
      "grad_norm": 24.96002769470215,
      "learning_rate": 1.7022984620584756e-05,
      "loss": 0.2992,
      "step": 16110
    },
    {
      "epoch": 2.6679907315458458,
      "grad_norm": 24.526933670043945,
      "learning_rate": 1.7001859050194354e-05,
      "loss": 0.264,
      "step": 16120
    },
    {
      "epoch": 2.6696458126448195,
      "grad_norm": 4.81766414642334,
      "learning_rate": 1.6980733479803956e-05,
      "loss": 0.2204,
      "step": 16130
    },
    {
      "epoch": 2.671300893743793,
      "grad_norm": 12.378460884094238,
      "learning_rate": 1.6959607909413555e-05,
      "loss": 0.1638,
      "step": 16140
    },
    {
      "epoch": 2.6729559748427674,
      "grad_norm": 8.242105484008789,
      "learning_rate": 1.6938482339023153e-05,
      "loss": 0.2511,
      "step": 16150
    },
    {
      "epoch": 2.674611055941741,
      "grad_norm": 8.300920486450195,
      "learning_rate": 1.691735676863275e-05,
      "loss": 0.4093,
      "step": 16160
    },
    {
      "epoch": 2.676266137040715,
      "grad_norm": 0.24264715611934662,
      "learning_rate": 1.6896231198242353e-05,
      "loss": 0.1076,
      "step": 16170
    },
    {
      "epoch": 2.677921218139689,
      "grad_norm": 6.404387474060059,
      "learning_rate": 1.6875105627851955e-05,
      "loss": 0.1572,
      "step": 16180
    },
    {
      "epoch": 2.6795762992386627,
      "grad_norm": 20.383085250854492,
      "learning_rate": 1.6853980057461553e-05,
      "loss": 0.2773,
      "step": 16190
    },
    {
      "epoch": 2.6812313803376364,
      "grad_norm": 7.844845771789551,
      "learning_rate": 1.683285448707115e-05,
      "loss": 0.1855,
      "step": 16200
    },
    {
      "epoch": 2.6828864614366106,
      "grad_norm": 17.767353057861328,
      "learning_rate": 1.6811728916680753e-05,
      "loss": 0.2941,
      "step": 16210
    },
    {
      "epoch": 2.6845415425355843,
      "grad_norm": 28.21045684814453,
      "learning_rate": 1.6790603346290352e-05,
      "loss": 0.2122,
      "step": 16220
    },
    {
      "epoch": 2.686196623634558,
      "grad_norm": 96.73346710205078,
      "learning_rate": 1.676947777589995e-05,
      "loss": 0.182,
      "step": 16230
    },
    {
      "epoch": 2.687851704733532,
      "grad_norm": 49.73832702636719,
      "learning_rate": 1.674835220550955e-05,
      "loss": 0.356,
      "step": 16240
    },
    {
      "epoch": 2.689506785832506,
      "grad_norm": 5.647763729095459,
      "learning_rate": 1.672722663511915e-05,
      "loss": 0.4114,
      "step": 16250
    },
    {
      "epoch": 2.6911618669314796,
      "grad_norm": 18.340238571166992,
      "learning_rate": 1.670610106472875e-05,
      "loss": 0.2767,
      "step": 16260
    },
    {
      "epoch": 2.6928169480304534,
      "grad_norm": 21.995361328125,
      "learning_rate": 1.6684975494338347e-05,
      "loss": 0.5088,
      "step": 16270
    },
    {
      "epoch": 2.694472029129427,
      "grad_norm": 73.76434326171875,
      "learning_rate": 1.6663849923947945e-05,
      "loss": 0.3393,
      "step": 16280
    },
    {
      "epoch": 2.6961271102284012,
      "grad_norm": 44.30565643310547,
      "learning_rate": 1.6642724353557547e-05,
      "loss": 0.2733,
      "step": 16290
    },
    {
      "epoch": 2.697782191327375,
      "grad_norm": 1.8831909894943237,
      "learning_rate": 1.6621598783167146e-05,
      "loss": 0.2804,
      "step": 16300
    },
    {
      "epoch": 2.6994372724263487,
      "grad_norm": 24.208890914916992,
      "learning_rate": 1.6600473212776747e-05,
      "loss": 0.176,
      "step": 16310
    },
    {
      "epoch": 2.701092353525323,
      "grad_norm": 5.290039539337158,
      "learning_rate": 1.6579347642386346e-05,
      "loss": 0.5074,
      "step": 16320
    },
    {
      "epoch": 2.7027474346242966,
      "grad_norm": 0.21022431552410126,
      "learning_rate": 1.6558222071995944e-05,
      "loss": 0.3705,
      "step": 16330
    },
    {
      "epoch": 2.7044025157232703,
      "grad_norm": 9.230001449584961,
      "learning_rate": 1.6537096501605546e-05,
      "loss": 0.1963,
      "step": 16340
    },
    {
      "epoch": 2.7060575968222444,
      "grad_norm": 0.6596125960350037,
      "learning_rate": 1.6515970931215144e-05,
      "loss": 0.3144,
      "step": 16350
    },
    {
      "epoch": 2.707712677921218,
      "grad_norm": 0.22895769774913788,
      "learning_rate": 1.6494845360824743e-05,
      "loss": 0.1446,
      "step": 16360
    },
    {
      "epoch": 2.709367759020192,
      "grad_norm": 0.09108495712280273,
      "learning_rate": 1.647371979043434e-05,
      "loss": 0.2657,
      "step": 16370
    },
    {
      "epoch": 2.711022840119166,
      "grad_norm": 40.331787109375,
      "learning_rate": 1.6452594220043943e-05,
      "loss": 0.353,
      "step": 16380
    },
    {
      "epoch": 2.7126779212181398,
      "grad_norm": 21.101587295532227,
      "learning_rate": 1.643146864965354e-05,
      "loss": 0.3289,
      "step": 16390
    },
    {
      "epoch": 2.7143330023171135,
      "grad_norm": 30.532089233398438,
      "learning_rate": 1.641034307926314e-05,
      "loss": 0.7531,
      "step": 16400
    },
    {
      "epoch": 2.7159880834160877,
      "grad_norm": 48.211639404296875,
      "learning_rate": 1.6389217508872738e-05,
      "loss": 0.2527,
      "step": 16410
    },
    {
      "epoch": 2.7176431645150614,
      "grad_norm": 4.512129783630371,
      "learning_rate": 1.636809193848234e-05,
      "loss": 0.2212,
      "step": 16420
    },
    {
      "epoch": 2.719298245614035,
      "grad_norm": 15.837615013122559,
      "learning_rate": 1.634696636809194e-05,
      "loss": 0.2057,
      "step": 16430
    },
    {
      "epoch": 2.720953326713009,
      "grad_norm": 20.00505828857422,
      "learning_rate": 1.632584079770154e-05,
      "loss": 0.5527,
      "step": 16440
    },
    {
      "epoch": 2.7226084078119825,
      "grad_norm": 23.4459285736084,
      "learning_rate": 1.6304715227311138e-05,
      "loss": 0.2957,
      "step": 16450
    },
    {
      "epoch": 2.7242634889109567,
      "grad_norm": 0.18597665429115295,
      "learning_rate": 1.6283589656920737e-05,
      "loss": 0.3178,
      "step": 16460
    },
    {
      "epoch": 2.7259185700099304,
      "grad_norm": 3.1762478351593018,
      "learning_rate": 1.626246408653034e-05,
      "loss": 0.2087,
      "step": 16470
    },
    {
      "epoch": 2.727573651108904,
      "grad_norm": 47.11872863769531,
      "learning_rate": 1.6241338516139937e-05,
      "loss": 0.5578,
      "step": 16480
    },
    {
      "epoch": 2.7292287322078783,
      "grad_norm": 22.079082489013672,
      "learning_rate": 1.6220212945749535e-05,
      "loss": 0.3357,
      "step": 16490
    },
    {
      "epoch": 2.730883813306852,
      "grad_norm": 0.7122053503990173,
      "learning_rate": 1.6199087375359134e-05,
      "loss": 0.3484,
      "step": 16500
    },
    {
      "epoch": 2.7325388944058258,
      "grad_norm": 20.169498443603516,
      "learning_rate": 1.6177961804968735e-05,
      "loss": 0.3618,
      "step": 16510
    },
    {
      "epoch": 2.7341939755048,
      "grad_norm": 32.35073471069336,
      "learning_rate": 1.6156836234578334e-05,
      "loss": 0.2071,
      "step": 16520
    },
    {
      "epoch": 2.7358490566037736,
      "grad_norm": 54.96195602416992,
      "learning_rate": 1.6135710664187932e-05,
      "loss": 0.478,
      "step": 16530
    },
    {
      "epoch": 2.7375041377027474,
      "grad_norm": 59.964698791503906,
      "learning_rate": 1.6114585093797534e-05,
      "loss": 0.1778,
      "step": 16540
    },
    {
      "epoch": 2.7391592188017215,
      "grad_norm": 3.272676706314087,
      "learning_rate": 1.6093459523407132e-05,
      "loss": 0.3601,
      "step": 16550
    },
    {
      "epoch": 2.7408142999006953,
      "grad_norm": 3.6754260063171387,
      "learning_rate": 1.6072333953016734e-05,
      "loss": 0.2361,
      "step": 16560
    },
    {
      "epoch": 2.742469380999669,
      "grad_norm": 3.784467935562134,
      "learning_rate": 1.6051208382626332e-05,
      "loss": 0.303,
      "step": 16570
    },
    {
      "epoch": 2.7441244620986427,
      "grad_norm": 47.78719711303711,
      "learning_rate": 1.603008281223593e-05,
      "loss": 0.3398,
      "step": 16580
    },
    {
      "epoch": 2.7457795431976164,
      "grad_norm": 0.0700768232345581,
      "learning_rate": 1.6008957241845533e-05,
      "loss": 0.1527,
      "step": 16590
    },
    {
      "epoch": 2.7474346242965906,
      "grad_norm": 54.216251373291016,
      "learning_rate": 1.598783167145513e-05,
      "loss": 0.3305,
      "step": 16600
    },
    {
      "epoch": 2.7490897053955643,
      "grad_norm": 9.906445503234863,
      "learning_rate": 1.596670610106473e-05,
      "loss": 0.1721,
      "step": 16610
    },
    {
      "epoch": 2.750744786494538,
      "grad_norm": 159.83087158203125,
      "learning_rate": 1.5945580530674328e-05,
      "loss": 0.5247,
      "step": 16620
    },
    {
      "epoch": 2.752399867593512,
      "grad_norm": 23.916234970092773,
      "learning_rate": 1.5924454960283926e-05,
      "loss": 0.1515,
      "step": 16630
    },
    {
      "epoch": 2.754054948692486,
      "grad_norm": 0.2377818077802658,
      "learning_rate": 1.5903329389893528e-05,
      "loss": 0.3463,
      "step": 16640
    },
    {
      "epoch": 2.7557100297914596,
      "grad_norm": 76.64212036132812,
      "learning_rate": 1.5882203819503126e-05,
      "loss": 0.3107,
      "step": 16650
    },
    {
      "epoch": 2.757365110890434,
      "grad_norm": 75.68334197998047,
      "learning_rate": 1.5861078249112725e-05,
      "loss": 0.3719,
      "step": 16660
    },
    {
      "epoch": 2.7590201919894075,
      "grad_norm": 49.520233154296875,
      "learning_rate": 1.5839952678722326e-05,
      "loss": 0.2272,
      "step": 16670
    },
    {
      "epoch": 2.7606752730883812,
      "grad_norm": 89.68793487548828,
      "learning_rate": 1.5818827108331928e-05,
      "loss": 0.3576,
      "step": 16680
    },
    {
      "epoch": 2.7623303541873554,
      "grad_norm": 34.92802429199219,
      "learning_rate": 1.5797701537941527e-05,
      "loss": 0.3446,
      "step": 16690
    },
    {
      "epoch": 2.763985435286329,
      "grad_norm": 28.014341354370117,
      "learning_rate": 1.5776575967551125e-05,
      "loss": 0.2511,
      "step": 16700
    },
    {
      "epoch": 2.765640516385303,
      "grad_norm": 75.85601043701172,
      "learning_rate": 1.5755450397160723e-05,
      "loss": 0.1998,
      "step": 16710
    },
    {
      "epoch": 2.767295597484277,
      "grad_norm": 21.316926956176758,
      "learning_rate": 1.5734324826770325e-05,
      "loss": 0.2301,
      "step": 16720
    },
    {
      "epoch": 2.7689506785832507,
      "grad_norm": 74.48182678222656,
      "learning_rate": 1.5713199256379924e-05,
      "loss": 0.5005,
      "step": 16730
    },
    {
      "epoch": 2.7706057596822244,
      "grad_norm": 56.65630340576172,
      "learning_rate": 1.5692073685989522e-05,
      "loss": 0.1524,
      "step": 16740
    },
    {
      "epoch": 2.772260840781198,
      "grad_norm": 29.1220645904541,
      "learning_rate": 1.567094811559912e-05,
      "loss": 0.2143,
      "step": 16750
    },
    {
      "epoch": 2.773915921880172,
      "grad_norm": 42.428688049316406,
      "learning_rate": 1.5649822545208722e-05,
      "loss": 0.2816,
      "step": 16760
    },
    {
      "epoch": 2.775571002979146,
      "grad_norm": 8.32712459564209,
      "learning_rate": 1.562869697481832e-05,
      "loss": 0.1294,
      "step": 16770
    },
    {
      "epoch": 2.7772260840781198,
      "grad_norm": 27.039138793945312,
      "learning_rate": 1.560757140442792e-05,
      "loss": 0.5947,
      "step": 16780
    },
    {
      "epoch": 2.7788811651770935,
      "grad_norm": 64.68891906738281,
      "learning_rate": 1.558644583403752e-05,
      "loss": 0.3437,
      "step": 16790
    },
    {
      "epoch": 2.7805362462760677,
      "grad_norm": 12.397055625915527,
      "learning_rate": 1.556532026364712e-05,
      "loss": 0.1443,
      "step": 16800
    },
    {
      "epoch": 2.7821913273750414,
      "grad_norm": 9.176739692687988,
      "learning_rate": 1.554419469325672e-05,
      "loss": 0.3503,
      "step": 16810
    },
    {
      "epoch": 2.783846408474015,
      "grad_norm": 4.406589031219482,
      "learning_rate": 1.552306912286632e-05,
      "loss": 0.4699,
      "step": 16820
    },
    {
      "epoch": 2.7855014895729893,
      "grad_norm": 56.75532531738281,
      "learning_rate": 1.5501943552475918e-05,
      "loss": 0.2819,
      "step": 16830
    },
    {
      "epoch": 2.787156570671963,
      "grad_norm": 10.925479888916016,
      "learning_rate": 1.5480817982085516e-05,
      "loss": 0.2825,
      "step": 16840
    },
    {
      "epoch": 2.7888116517709367,
      "grad_norm": 8.241820335388184,
      "learning_rate": 1.5459692411695118e-05,
      "loss": 0.2885,
      "step": 16850
    },
    {
      "epoch": 2.790466732869911,
      "grad_norm": 15.512083053588867,
      "learning_rate": 1.5438566841304716e-05,
      "loss": 0.37,
      "step": 16860
    },
    {
      "epoch": 2.7921218139688846,
      "grad_norm": 10.052474021911621,
      "learning_rate": 1.5417441270914314e-05,
      "loss": 0.3797,
      "step": 16870
    },
    {
      "epoch": 2.7937768950678583,
      "grad_norm": 22.09886360168457,
      "learning_rate": 1.5396315700523913e-05,
      "loss": 0.2198,
      "step": 16880
    },
    {
      "epoch": 2.795431976166832,
      "grad_norm": 27.252534866333008,
      "learning_rate": 1.5375190130133515e-05,
      "loss": 0.3167,
      "step": 16890
    },
    {
      "epoch": 2.7970870572658058,
      "grad_norm": 98.89641571044922,
      "learning_rate": 1.5354064559743113e-05,
      "loss": 0.3549,
      "step": 16900
    },
    {
      "epoch": 2.79874213836478,
      "grad_norm": 35.889766693115234,
      "learning_rate": 1.5332938989352715e-05,
      "loss": 0.091,
      "step": 16910
    },
    {
      "epoch": 2.8003972194637536,
      "grad_norm": 44.25505065917969,
      "learning_rate": 1.5311813418962313e-05,
      "loss": 0.3188,
      "step": 16920
    },
    {
      "epoch": 2.8020523005627274,
      "grad_norm": 60.34345626831055,
      "learning_rate": 1.5290687848571915e-05,
      "loss": 0.3522,
      "step": 16930
    },
    {
      "epoch": 2.8037073816617015,
      "grad_norm": 2.018914222717285,
      "learning_rate": 1.5269562278181513e-05,
      "loss": 0.4628,
      "step": 16940
    },
    {
      "epoch": 2.8053624627606752,
      "grad_norm": 0.1140153557062149,
      "learning_rate": 1.5248436707791112e-05,
      "loss": 0.1688,
      "step": 16950
    },
    {
      "epoch": 2.807017543859649,
      "grad_norm": 2.4985947608947754,
      "learning_rate": 1.522731113740071e-05,
      "loss": 0.2213,
      "step": 16960
    },
    {
      "epoch": 2.808672624958623,
      "grad_norm": 23.224411010742188,
      "learning_rate": 1.5206185567010308e-05,
      "loss": 0.0449,
      "step": 16970
    },
    {
      "epoch": 2.810327706057597,
      "grad_norm": 22.608692169189453,
      "learning_rate": 1.518505999661991e-05,
      "loss": 0.2062,
      "step": 16980
    },
    {
      "epoch": 2.8119827871565706,
      "grad_norm": 41.71538543701172,
      "learning_rate": 1.5163934426229509e-05,
      "loss": 0.4621,
      "step": 16990
    },
    {
      "epoch": 2.8136378682555447,
      "grad_norm": 86.47195434570312,
      "learning_rate": 1.5142808855839109e-05,
      "loss": 0.2051,
      "step": 17000
    },
    {
      "epoch": 2.8152929493545185,
      "grad_norm": 22.913314819335938,
      "learning_rate": 1.5121683285448707e-05,
      "loss": 0.3708,
      "step": 17010
    },
    {
      "epoch": 2.816948030453492,
      "grad_norm": 35.52001953125,
      "learning_rate": 1.5100557715058309e-05,
      "loss": 0.0462,
      "step": 17020
    },
    {
      "epoch": 2.8186031115524663,
      "grad_norm": 39.15039825439453,
      "learning_rate": 1.5079432144667907e-05,
      "loss": 0.1538,
      "step": 17030
    },
    {
      "epoch": 2.82025819265144,
      "grad_norm": 0.14490379393100739,
      "learning_rate": 1.5058306574277506e-05,
      "loss": 0.2607,
      "step": 17040
    },
    {
      "epoch": 2.821913273750414,
      "grad_norm": 9.24474048614502,
      "learning_rate": 1.5037181003887104e-05,
      "loss": 0.4108,
      "step": 17050
    },
    {
      "epoch": 2.8235683548493875,
      "grad_norm": 21.439863204956055,
      "learning_rate": 1.5016055433496706e-05,
      "loss": 0.5119,
      "step": 17060
    },
    {
      "epoch": 2.8252234359483612,
      "grad_norm": 10.581985473632812,
      "learning_rate": 1.4994929863106306e-05,
      "loss": 0.2021,
      "step": 17070
    },
    {
      "epoch": 2.8268785170473354,
      "grad_norm": 29.392536163330078,
      "learning_rate": 1.4973804292715904e-05,
      "loss": 0.3318,
      "step": 17080
    },
    {
      "epoch": 2.828533598146309,
      "grad_norm": 6.308375358581543,
      "learning_rate": 1.4952678722325503e-05,
      "loss": 0.2772,
      "step": 17090
    },
    {
      "epoch": 2.830188679245283,
      "grad_norm": 64.2750473022461,
      "learning_rate": 1.4931553151935104e-05,
      "loss": 0.4412,
      "step": 17100
    },
    {
      "epoch": 2.831843760344257,
      "grad_norm": 4.0398406982421875,
      "learning_rate": 1.4910427581544703e-05,
      "loss": 0.2135,
      "step": 17110
    },
    {
      "epoch": 2.8334988414432307,
      "grad_norm": 95.53339385986328,
      "learning_rate": 1.4889302011154301e-05,
      "loss": 0.1939,
      "step": 17120
    },
    {
      "epoch": 2.8351539225422044,
      "grad_norm": 106.57283782958984,
      "learning_rate": 1.4868176440763901e-05,
      "loss": 0.537,
      "step": 17130
    },
    {
      "epoch": 2.8368090036411786,
      "grad_norm": 25.49407196044922,
      "learning_rate": 1.48470508703735e-05,
      "loss": 0.3585,
      "step": 17140
    },
    {
      "epoch": 2.8384640847401523,
      "grad_norm": 0.7030529975891113,
      "learning_rate": 1.4825925299983101e-05,
      "loss": 0.2052,
      "step": 17150
    },
    {
      "epoch": 2.840119165839126,
      "grad_norm": 11.785024642944336,
      "learning_rate": 1.48047997295927e-05,
      "loss": 0.2462,
      "step": 17160
    },
    {
      "epoch": 2.8417742469381,
      "grad_norm": 42.42939758300781,
      "learning_rate": 1.4783674159202298e-05,
      "loss": 0.05,
      "step": 17170
    },
    {
      "epoch": 2.843429328037074,
      "grad_norm": 65.4044189453125,
      "learning_rate": 1.4762548588811898e-05,
      "loss": 0.2673,
      "step": 17180
    },
    {
      "epoch": 2.8450844091360477,
      "grad_norm": 56.48021697998047,
      "learning_rate": 1.4741423018421498e-05,
      "loss": 0.5283,
      "step": 17190
    },
    {
      "epoch": 2.8467394902350214,
      "grad_norm": 0.2794357240200043,
      "learning_rate": 1.4720297448031098e-05,
      "loss": 0.2362,
      "step": 17200
    },
    {
      "epoch": 2.848394571333995,
      "grad_norm": 55.23975372314453,
      "learning_rate": 1.4699171877640697e-05,
      "loss": 0.398,
      "step": 17210
    },
    {
      "epoch": 2.8500496524329693,
      "grad_norm": 3.359201669692993,
      "learning_rate": 1.4678046307250295e-05,
      "loss": 0.1874,
      "step": 17220
    },
    {
      "epoch": 2.851704733531943,
      "grad_norm": 69.13455200195312,
      "learning_rate": 1.4656920736859897e-05,
      "loss": 0.2681,
      "step": 17230
    },
    {
      "epoch": 2.8533598146309167,
      "grad_norm": 0.7250457406044006,
      "learning_rate": 1.4635795166469495e-05,
      "loss": 0.1441,
      "step": 17240
    },
    {
      "epoch": 2.855014895729891,
      "grad_norm": 49.61235427856445,
      "learning_rate": 1.4614669596079095e-05,
      "loss": 0.1955,
      "step": 17250
    },
    {
      "epoch": 2.8566699768288646,
      "grad_norm": 50.263797760009766,
      "learning_rate": 1.4593544025688694e-05,
      "loss": 0.4184,
      "step": 17260
    },
    {
      "epoch": 2.8583250579278383,
      "grad_norm": 5.2949652671813965,
      "learning_rate": 1.4572418455298296e-05,
      "loss": 0.0832,
      "step": 17270
    },
    {
      "epoch": 2.8599801390268125,
      "grad_norm": 3.244530439376831,
      "learning_rate": 1.4551292884907894e-05,
      "loss": 0.3581,
      "step": 17280
    },
    {
      "epoch": 2.861635220125786,
      "grad_norm": 17.473426818847656,
      "learning_rate": 1.4530167314517492e-05,
      "loss": 0.2252,
      "step": 17290
    },
    {
      "epoch": 2.86329030122476,
      "grad_norm": 133.01258850097656,
      "learning_rate": 1.450904174412709e-05,
      "loss": 0.1364,
      "step": 17300
    },
    {
      "epoch": 2.864945382323734,
      "grad_norm": 2.198101282119751,
      "learning_rate": 1.448791617373669e-05,
      "loss": 0.5123,
      "step": 17310
    },
    {
      "epoch": 2.866600463422708,
      "grad_norm": 0.12852615118026733,
      "learning_rate": 1.4466790603346293e-05,
      "loss": 0.2237,
      "step": 17320
    },
    {
      "epoch": 2.8682555445216815,
      "grad_norm": 0.19612817466259003,
      "learning_rate": 1.4445665032955891e-05,
      "loss": 0.2777,
      "step": 17330
    },
    {
      "epoch": 2.8699106256206557,
      "grad_norm": 58.69192123413086,
      "learning_rate": 1.442453946256549e-05,
      "loss": 0.2334,
      "step": 17340
    },
    {
      "epoch": 2.8715657067196294,
      "grad_norm": 5.45986795425415,
      "learning_rate": 1.4403413892175088e-05,
      "loss": 0.5139,
      "step": 17350
    },
    {
      "epoch": 2.873220787818603,
      "grad_norm": 0.0604727678000927,
      "learning_rate": 1.438228832178469e-05,
      "loss": 0.3156,
      "step": 17360
    },
    {
      "epoch": 2.874875868917577,
      "grad_norm": 6.243042469024658,
      "learning_rate": 1.436116275139429e-05,
      "loss": 0.1335,
      "step": 17370
    },
    {
      "epoch": 2.8765309500165506,
      "grad_norm": 9.73936653137207,
      "learning_rate": 1.4340037181003888e-05,
      "loss": 0.2821,
      "step": 17380
    },
    {
      "epoch": 2.8781860311155247,
      "grad_norm": 82.2399673461914,
      "learning_rate": 1.4318911610613486e-05,
      "loss": 0.5746,
      "step": 17390
    },
    {
      "epoch": 2.8798411122144985,
      "grad_norm": 4.63948917388916,
      "learning_rate": 1.4297786040223088e-05,
      "loss": 0.1898,
      "step": 17400
    },
    {
      "epoch": 2.881496193313472,
      "grad_norm": 130.85891723632812,
      "learning_rate": 1.4276660469832686e-05,
      "loss": 0.5378,
      "step": 17410
    },
    {
      "epoch": 2.8831512744124463,
      "grad_norm": 17.90825843811035,
      "learning_rate": 1.4255534899442285e-05,
      "loss": 0.1576,
      "step": 17420
    },
    {
      "epoch": 2.88480635551142,
      "grad_norm": 9.924593925476074,
      "learning_rate": 1.4234409329051885e-05,
      "loss": 0.3087,
      "step": 17430
    },
    {
      "epoch": 2.886461436610394,
      "grad_norm": 43.81143569946289,
      "learning_rate": 1.4213283758661487e-05,
      "loss": 0.2472,
      "step": 17440
    },
    {
      "epoch": 2.888116517709368,
      "grad_norm": 114.5516128540039,
      "learning_rate": 1.4192158188271085e-05,
      "loss": 0.3065,
      "step": 17450
    },
    {
      "epoch": 2.8897715988083417,
      "grad_norm": 26.109830856323242,
      "learning_rate": 1.4171032617880683e-05,
      "loss": 0.1605,
      "step": 17460
    },
    {
      "epoch": 2.8914266799073154,
      "grad_norm": 0.2806696593761444,
      "learning_rate": 1.4149907047490282e-05,
      "loss": 0.415,
      "step": 17470
    },
    {
      "epoch": 2.8930817610062896,
      "grad_norm": 26.49663543701172,
      "learning_rate": 1.4128781477099882e-05,
      "loss": 0.3432,
      "step": 17480
    },
    {
      "epoch": 2.8947368421052633,
      "grad_norm": 0.24931389093399048,
      "learning_rate": 1.4107655906709482e-05,
      "loss": 0.2836,
      "step": 17490
    },
    {
      "epoch": 2.896391923204237,
      "grad_norm": 12.88537883758545,
      "learning_rate": 1.4086530336319082e-05,
      "loss": 0.2617,
      "step": 17500
    },
    {
      "epoch": 2.8980470043032107,
      "grad_norm": 49.73482131958008,
      "learning_rate": 1.406540476592868e-05,
      "loss": 0.3691,
      "step": 17510
    },
    {
      "epoch": 2.8997020854021844,
      "grad_norm": 38.64708709716797,
      "learning_rate": 1.4044279195538279e-05,
      "loss": 0.2438,
      "step": 17520
    },
    {
      "epoch": 2.9013571665011586,
      "grad_norm": 0.4638746678829193,
      "learning_rate": 1.402315362514788e-05,
      "loss": 0.1954,
      "step": 17530
    },
    {
      "epoch": 2.9030122476001323,
      "grad_norm": 11.551800727844238,
      "learning_rate": 1.4002028054757479e-05,
      "loss": 0.1942,
      "step": 17540
    },
    {
      "epoch": 2.904667328699106,
      "grad_norm": 35.62576675415039,
      "learning_rate": 1.3980902484367079e-05,
      "loss": 0.248,
      "step": 17550
    },
    {
      "epoch": 2.90632240979808,
      "grad_norm": 12.45541763305664,
      "learning_rate": 1.3959776913976677e-05,
      "loss": 0.2861,
      "step": 17560
    },
    {
      "epoch": 2.907977490897054,
      "grad_norm": 17.70575714111328,
      "learning_rate": 1.393865134358628e-05,
      "loss": 0.2673,
      "step": 17570
    },
    {
      "epoch": 2.9096325719960277,
      "grad_norm": 79.52989196777344,
      "learning_rate": 1.3917525773195878e-05,
      "loss": 0.6501,
      "step": 17580
    },
    {
      "epoch": 2.911287653095002,
      "grad_norm": 9.85456371307373,
      "learning_rate": 1.3896400202805476e-05,
      "loss": 0.2884,
      "step": 17590
    },
    {
      "epoch": 2.9129427341939755,
      "grad_norm": 17.085037231445312,
      "learning_rate": 1.3875274632415074e-05,
      "loss": 0.2184,
      "step": 17600
    },
    {
      "epoch": 2.9145978152929493,
      "grad_norm": 38.73060989379883,
      "learning_rate": 1.3854149062024676e-05,
      "loss": 0.5289,
      "step": 17610
    },
    {
      "epoch": 2.9162528963919234,
      "grad_norm": 4.4648308753967285,
      "learning_rate": 1.3833023491634276e-05,
      "loss": 0.2,
      "step": 17620
    },
    {
      "epoch": 2.917907977490897,
      "grad_norm": 8.352235794067383,
      "learning_rate": 1.3811897921243875e-05,
      "loss": 0.2898,
      "step": 17630
    },
    {
      "epoch": 2.919563058589871,
      "grad_norm": 0.4228231608867645,
      "learning_rate": 1.3790772350853473e-05,
      "loss": 0.1368,
      "step": 17640
    },
    {
      "epoch": 2.921218139688845,
      "grad_norm": 10.3987455368042,
      "learning_rate": 1.3769646780463071e-05,
      "loss": 0.3443,
      "step": 17650
    },
    {
      "epoch": 2.9228732207878187,
      "grad_norm": 16.593616485595703,
      "learning_rate": 1.3748521210072673e-05,
      "loss": 0.3896,
      "step": 17660
    },
    {
      "epoch": 2.9245283018867925,
      "grad_norm": 0.2876780331134796,
      "learning_rate": 1.3727395639682272e-05,
      "loss": 0.2901,
      "step": 17670
    },
    {
      "epoch": 2.926183382985766,
      "grad_norm": 107.10941314697266,
      "learning_rate": 1.3706270069291872e-05,
      "loss": 0.2093,
      "step": 17680
    },
    {
      "epoch": 2.92783846408474,
      "grad_norm": 4.454375743865967,
      "learning_rate": 1.368514449890147e-05,
      "loss": 0.4174,
      "step": 17690
    },
    {
      "epoch": 2.929493545183714,
      "grad_norm": 8.532901763916016,
      "learning_rate": 1.3664018928511072e-05,
      "loss": 0.1415,
      "step": 17700
    },
    {
      "epoch": 2.931148626282688,
      "grad_norm": 35.93784713745117,
      "learning_rate": 1.364289335812067e-05,
      "loss": 0.3575,
      "step": 17710
    },
    {
      "epoch": 2.9328037073816615,
      "grad_norm": 2.7423324584960938,
      "learning_rate": 1.3621767787730269e-05,
      "loss": 0.3944,
      "step": 17720
    },
    {
      "epoch": 2.9344587884806357,
      "grad_norm": 17.000999450683594,
      "learning_rate": 1.3600642217339869e-05,
      "loss": 0.1806,
      "step": 17730
    },
    {
      "epoch": 2.9361138695796094,
      "grad_norm": 42.69471740722656,
      "learning_rate": 1.3579516646949469e-05,
      "loss": 0.2311,
      "step": 17740
    },
    {
      "epoch": 2.937768950678583,
      "grad_norm": 20.44193458557129,
      "learning_rate": 1.3558391076559069e-05,
      "loss": 0.1889,
      "step": 17750
    },
    {
      "epoch": 2.9394240317775573,
      "grad_norm": 11.56645393371582,
      "learning_rate": 1.3537265506168667e-05,
      "loss": 0.1804,
      "step": 17760
    },
    {
      "epoch": 2.941079112876531,
      "grad_norm": 25.394973754882812,
      "learning_rate": 1.3516139935778266e-05,
      "loss": 0.3113,
      "step": 17770
    },
    {
      "epoch": 2.9427341939755047,
      "grad_norm": 19.414363861083984,
      "learning_rate": 1.3495014365387867e-05,
      "loss": 0.1733,
      "step": 17780
    },
    {
      "epoch": 2.944389275074479,
      "grad_norm": 67.35945129394531,
      "learning_rate": 1.3473888794997466e-05,
      "loss": 0.4232,
      "step": 17790
    },
    {
      "epoch": 2.9460443561734526,
      "grad_norm": 11.071313858032227,
      "learning_rate": 1.3452763224607066e-05,
      "loss": 0.1924,
      "step": 17800
    },
    {
      "epoch": 2.9476994372724263,
      "grad_norm": 16.656126022338867,
      "learning_rate": 1.3431637654216664e-05,
      "loss": 0.2477,
      "step": 17810
    },
    {
      "epoch": 2.9493545183714,
      "grad_norm": 12.141694068908691,
      "learning_rate": 1.3410512083826263e-05,
      "loss": 0.0992,
      "step": 17820
    },
    {
      "epoch": 2.951009599470374,
      "grad_norm": 18.01133918762207,
      "learning_rate": 1.3389386513435864e-05,
      "loss": 0.1236,
      "step": 17830
    },
    {
      "epoch": 2.952664680569348,
      "grad_norm": 1.198720097541809,
      "learning_rate": 1.3368260943045463e-05,
      "loss": 0.185,
      "step": 17840
    },
    {
      "epoch": 2.9543197616683217,
      "grad_norm": 30.108610153198242,
      "learning_rate": 1.3347135372655061e-05,
      "loss": 0.4248,
      "step": 17850
    },
    {
      "epoch": 2.9559748427672954,
      "grad_norm": 0.35653916001319885,
      "learning_rate": 1.3326009802264661e-05,
      "loss": 0.1816,
      "step": 17860
    },
    {
      "epoch": 2.9576299238662696,
      "grad_norm": 14.661544799804688,
      "learning_rate": 1.3304884231874263e-05,
      "loss": 0.4321,
      "step": 17870
    },
    {
      "epoch": 2.9592850049652433,
      "grad_norm": 9.48971176147461,
      "learning_rate": 1.3283758661483861e-05,
      "loss": 0.4603,
      "step": 17880
    },
    {
      "epoch": 2.960940086064217,
      "grad_norm": 0.8832417726516724,
      "learning_rate": 1.326263309109346e-05,
      "loss": 0.3078,
      "step": 17890
    },
    {
      "epoch": 2.962595167163191,
      "grad_norm": 9.656832695007324,
      "learning_rate": 1.3241507520703058e-05,
      "loss": 0.1585,
      "step": 17900
    },
    {
      "epoch": 2.964250248262165,
      "grad_norm": 0.6978235840797424,
      "learning_rate": 1.322038195031266e-05,
      "loss": 0.0785,
      "step": 17910
    },
    {
      "epoch": 2.9659053293611386,
      "grad_norm": 33.85856628417969,
      "learning_rate": 1.319925637992226e-05,
      "loss": 0.1291,
      "step": 17920
    },
    {
      "epoch": 2.9675604104601128,
      "grad_norm": 37.93413162231445,
      "learning_rate": 1.3178130809531858e-05,
      "loss": 0.2811,
      "step": 17930
    },
    {
      "epoch": 2.9692154915590865,
      "grad_norm": 78.66813659667969,
      "learning_rate": 1.3157005239141457e-05,
      "loss": 0.5062,
      "step": 17940
    },
    {
      "epoch": 2.97087057265806,
      "grad_norm": 1.06436026096344,
      "learning_rate": 1.3135879668751058e-05,
      "loss": 0.2098,
      "step": 17950
    },
    {
      "epoch": 2.9725256537570344,
      "grad_norm": 73.98670196533203,
      "learning_rate": 1.3114754098360657e-05,
      "loss": 0.3929,
      "step": 17960
    },
    {
      "epoch": 2.974180734856008,
      "grad_norm": 15.063279151916504,
      "learning_rate": 1.3093628527970255e-05,
      "loss": 0.271,
      "step": 17970
    },
    {
      "epoch": 2.975835815954982,
      "grad_norm": 4.106472969055176,
      "learning_rate": 1.3072502957579855e-05,
      "loss": 0.1757,
      "step": 17980
    },
    {
      "epoch": 2.9774908970539555,
      "grad_norm": 38.15757369995117,
      "learning_rate": 1.3051377387189454e-05,
      "loss": 0.4838,
      "step": 17990
    },
    {
      "epoch": 2.9791459781529293,
      "grad_norm": 25.51747703552246,
      "learning_rate": 1.3030251816799055e-05,
      "loss": 0.5556,
      "step": 18000
    },
    {
      "epoch": 2.9808010592519034,
      "grad_norm": 1.9547443389892578,
      "learning_rate": 1.3009126246408654e-05,
      "loss": 0.131,
      "step": 18010
    },
    {
      "epoch": 2.982456140350877,
      "grad_norm": 57.995826721191406,
      "learning_rate": 1.2988000676018252e-05,
      "loss": 0.3142,
      "step": 18020
    },
    {
      "epoch": 2.984111221449851,
      "grad_norm": 16.183502197265625,
      "learning_rate": 1.2966875105627852e-05,
      "loss": 0.2128,
      "step": 18030
    },
    {
      "epoch": 2.985766302548825,
      "grad_norm": 2.707094430923462,
      "learning_rate": 1.2945749535237452e-05,
      "loss": 0.2087,
      "step": 18040
    },
    {
      "epoch": 2.9874213836477987,
      "grad_norm": 37.71603775024414,
      "learning_rate": 1.2924623964847052e-05,
      "loss": 0.568,
      "step": 18050
    },
    {
      "epoch": 2.9890764647467725,
      "grad_norm": 0.8392290472984314,
      "learning_rate": 1.290349839445665e-05,
      "loss": 0.1479,
      "step": 18060
    },
    {
      "epoch": 2.9907315458457466,
      "grad_norm": 3.512394666671753,
      "learning_rate": 1.288237282406625e-05,
      "loss": 0.1778,
      "step": 18070
    },
    {
      "epoch": 2.9923866269447204,
      "grad_norm": 20.318931579589844,
      "learning_rate": 1.2861247253675851e-05,
      "loss": 0.3337,
      "step": 18080
    },
    {
      "epoch": 2.994041708043694,
      "grad_norm": 58.77509307861328,
      "learning_rate": 1.284012168328545e-05,
      "loss": 0.5443,
      "step": 18090
    },
    {
      "epoch": 2.9956967891426682,
      "grad_norm": 0.7058047652244568,
      "learning_rate": 1.281899611289505e-05,
      "loss": 0.4638,
      "step": 18100
    },
    {
      "epoch": 2.997351870241642,
      "grad_norm": 0.4680212438106537,
      "learning_rate": 1.2797870542504648e-05,
      "loss": 0.2497,
      "step": 18110
    },
    {
      "epoch": 2.9990069513406157,
      "grad_norm": 46.56175231933594,
      "learning_rate": 1.277674497211425e-05,
      "loss": 0.2358,
      "step": 18120
    },
    {
      "epoch": 3.0006620324395894,
      "grad_norm": 47.586727142333984,
      "learning_rate": 1.2755619401723848e-05,
      "loss": 0.2225,
      "step": 18130
    },
    {
      "epoch": 3.0023171135385636,
      "grad_norm": 14.872221946716309,
      "learning_rate": 1.2734493831333446e-05,
      "loss": 0.1912,
      "step": 18140
    },
    {
      "epoch": 3.0039721946375373,
      "grad_norm": 0.5755805969238281,
      "learning_rate": 1.2713368260943045e-05,
      "loss": 0.0526,
      "step": 18150
    },
    {
      "epoch": 3.005627275736511,
      "grad_norm": 4.075991630554199,
      "learning_rate": 1.2692242690552645e-05,
      "loss": 0.0872,
      "step": 18160
    },
    {
      "epoch": 3.0072823568354847,
      "grad_norm": 3.581071138381958,
      "learning_rate": 1.2671117120162247e-05,
      "loss": 0.1917,
      "step": 18170
    },
    {
      "epoch": 3.008937437934459,
      "grad_norm": 76.84573364257812,
      "learning_rate": 1.2649991549771845e-05,
      "loss": 0.1408,
      "step": 18180
    },
    {
      "epoch": 3.0105925190334326,
      "grad_norm": 0.20498082041740417,
      "learning_rate": 1.2628865979381443e-05,
      "loss": 0.2666,
      "step": 18190
    },
    {
      "epoch": 3.0122476001324063,
      "grad_norm": 0.26685917377471924,
      "learning_rate": 1.2607740408991042e-05,
      "loss": 0.0264,
      "step": 18200
    },
    {
      "epoch": 3.0139026812313805,
      "grad_norm": 0.05580154433846474,
      "learning_rate": 1.2586614838600644e-05,
      "loss": 0.1599,
      "step": 18210
    },
    {
      "epoch": 3.015557762330354,
      "grad_norm": 0.025961685925722122,
      "learning_rate": 1.2565489268210242e-05,
      "loss": 0.2673,
      "step": 18220
    },
    {
      "epoch": 3.017212843429328,
      "grad_norm": 84.4200668334961,
      "learning_rate": 1.2544363697819842e-05,
      "loss": 0.1356,
      "step": 18230
    },
    {
      "epoch": 3.018867924528302,
      "grad_norm": 0.044063862413167953,
      "learning_rate": 1.252323812742944e-05,
      "loss": 0.0809,
      "step": 18240
    },
    {
      "epoch": 3.020523005627276,
      "grad_norm": 23.406265258789062,
      "learning_rate": 1.2502112557039042e-05,
      "loss": 0.1304,
      "step": 18250
    },
    {
      "epoch": 3.0221780867262495,
      "grad_norm": 9.197020530700684,
      "learning_rate": 1.248098698664864e-05,
      "loss": 0.1806,
      "step": 18260
    },
    {
      "epoch": 3.0238331678252233,
      "grad_norm": 0.8721186518669128,
      "learning_rate": 1.2459861416258239e-05,
      "loss": 0.1153,
      "step": 18270
    },
    {
      "epoch": 3.0254882489241974,
      "grad_norm": 37.89600372314453,
      "learning_rate": 1.2438735845867839e-05,
      "loss": 0.1072,
      "step": 18280
    },
    {
      "epoch": 3.027143330023171,
      "grad_norm": 6.605855464935303,
      "learning_rate": 1.2417610275477439e-05,
      "loss": 0.0659,
      "step": 18290
    },
    {
      "epoch": 3.028798411122145,
      "grad_norm": 29.622692108154297,
      "learning_rate": 1.2396484705087039e-05,
      "loss": 0.1897,
      "step": 18300
    },
    {
      "epoch": 3.030453492221119,
      "grad_norm": 98.6551742553711,
      "learning_rate": 1.2375359134696637e-05,
      "loss": 0.2633,
      "step": 18310
    },
    {
      "epoch": 3.0321085733200928,
      "grad_norm": 0.026351403445005417,
      "learning_rate": 1.2354233564306236e-05,
      "loss": 0.1318,
      "step": 18320
    },
    {
      "epoch": 3.0337636544190665,
      "grad_norm": 0.03674609214067459,
      "learning_rate": 1.2333107993915836e-05,
      "loss": 0.0649,
      "step": 18330
    },
    {
      "epoch": 3.03541873551804,
      "grad_norm": 0.9895926713943481,
      "learning_rate": 1.2311982423525436e-05,
      "loss": 0.0491,
      "step": 18340
    },
    {
      "epoch": 3.0370738166170144,
      "grad_norm": 52.81081008911133,
      "learning_rate": 1.2290856853135036e-05,
      "loss": 0.2553,
      "step": 18350
    },
    {
      "epoch": 3.038728897715988,
      "grad_norm": 0.06585075706243515,
      "learning_rate": 1.2269731282744634e-05,
      "loss": 0.0726,
      "step": 18360
    },
    {
      "epoch": 3.040383978814962,
      "grad_norm": 3.0985403060913086,
      "learning_rate": 1.2248605712354235e-05,
      "loss": 0.1262,
      "step": 18370
    },
    {
      "epoch": 3.042039059913936,
      "grad_norm": 0.2267821878194809,
      "learning_rate": 1.2227480141963833e-05,
      "loss": 0.0524,
      "step": 18380
    },
    {
      "epoch": 3.0436941410129097,
      "grad_norm": 14.018054008483887,
      "learning_rate": 1.2206354571573433e-05,
      "loss": 0.0966,
      "step": 18390
    },
    {
      "epoch": 3.0453492221118834,
      "grad_norm": 31.199548721313477,
      "learning_rate": 1.2185229001183031e-05,
      "loss": 0.2027,
      "step": 18400
    },
    {
      "epoch": 3.047004303210857,
      "grad_norm": 0.053959157317876816,
      "learning_rate": 1.2164103430792633e-05,
      "loss": 0.2657,
      "step": 18410
    },
    {
      "epoch": 3.0486593843098313,
      "grad_norm": 0.3410421907901764,
      "learning_rate": 1.2142977860402232e-05,
      "loss": 0.1144,
      "step": 18420
    },
    {
      "epoch": 3.050314465408805,
      "grad_norm": 13.991857528686523,
      "learning_rate": 1.2121852290011832e-05,
      "loss": 0.0465,
      "step": 18430
    },
    {
      "epoch": 3.0519695465077787,
      "grad_norm": 30.88372802734375,
      "learning_rate": 1.210072671962143e-05,
      "loss": 0.4705,
      "step": 18440
    },
    {
      "epoch": 3.053624627606753,
      "grad_norm": 58.83572006225586,
      "learning_rate": 1.207960114923103e-05,
      "loss": 0.3416,
      "step": 18450
    },
    {
      "epoch": 3.0552797087057266,
      "grad_norm": 29.123523712158203,
      "learning_rate": 1.2058475578840628e-05,
      "loss": 0.2397,
      "step": 18460
    },
    {
      "epoch": 3.0569347898047003,
      "grad_norm": 21.195098876953125,
      "learning_rate": 1.203735000845023e-05,
      "loss": 0.1546,
      "step": 18470
    },
    {
      "epoch": 3.058589870903674,
      "grad_norm": 0.49449053406715393,
      "learning_rate": 1.2016224438059829e-05,
      "loss": 0.1405,
      "step": 18480
    },
    {
      "epoch": 3.0602449520026482,
      "grad_norm": 9.575920104980469,
      "learning_rate": 1.1995098867669427e-05,
      "loss": 0.1078,
      "step": 18490
    },
    {
      "epoch": 3.061900033101622,
      "grad_norm": 4.163110733032227,
      "learning_rate": 1.1973973297279027e-05,
      "loss": 0.0794,
      "step": 18500
    },
    {
      "epoch": 3.0635551142005957,
      "grad_norm": 20.29051399230957,
      "learning_rate": 1.1952847726888625e-05,
      "loss": 0.1386,
      "step": 18510
    },
    {
      "epoch": 3.06521019529957,
      "grad_norm": 7.774297714233398,
      "learning_rate": 1.1931722156498226e-05,
      "loss": 0.1084,
      "step": 18520
    },
    {
      "epoch": 3.0668652763985436,
      "grad_norm": 2.4030706882476807,
      "learning_rate": 1.1910596586107826e-05,
      "loss": 0.103,
      "step": 18530
    },
    {
      "epoch": 3.0685203574975173,
      "grad_norm": 0.12169880419969559,
      "learning_rate": 1.1889471015717426e-05,
      "loss": 0.183,
      "step": 18540
    },
    {
      "epoch": 3.0701754385964914,
      "grad_norm": 0.027215957641601562,
      "learning_rate": 1.1868345445327024e-05,
      "loss": 0.081,
      "step": 18550
    },
    {
      "epoch": 3.071830519695465,
      "grad_norm": 0.010704882442951202,
      "learning_rate": 1.1847219874936624e-05,
      "loss": 0.0441,
      "step": 18560
    },
    {
      "epoch": 3.073485600794439,
      "grad_norm": 0.01014015544205904,
      "learning_rate": 1.1826094304546223e-05,
      "loss": 0.2572,
      "step": 18570
    },
    {
      "epoch": 3.0751406818934126,
      "grad_norm": 0.07707106322050095,
      "learning_rate": 1.1804968734155823e-05,
      "loss": 0.0442,
      "step": 18580
    },
    {
      "epoch": 3.0767957629923868,
      "grad_norm": 27.951425552368164,
      "learning_rate": 1.1783843163765423e-05,
      "loss": 0.3299,
      "step": 18590
    },
    {
      "epoch": 3.0784508440913605,
      "grad_norm": 0.06459348648786545,
      "learning_rate": 1.1762717593375023e-05,
      "loss": 0.0234,
      "step": 18600
    },
    {
      "epoch": 3.080105925190334,
      "grad_norm": 0.4255131781101227,
      "learning_rate": 1.1741592022984621e-05,
      "loss": 0.089,
      "step": 18610
    },
    {
      "epoch": 3.0817610062893084,
      "grad_norm": 0.048052623867988586,
      "learning_rate": 1.1720466452594221e-05,
      "loss": 0.1099,
      "step": 18620
    },
    {
      "epoch": 3.083416087388282,
      "grad_norm": 1.0267412662506104,
      "learning_rate": 1.169934088220382e-05,
      "loss": 0.0491,
      "step": 18630
    },
    {
      "epoch": 3.085071168487256,
      "grad_norm": 2.7648515701293945,
      "learning_rate": 1.167821531181342e-05,
      "loss": 0.133,
      "step": 18640
    },
    {
      "epoch": 3.0867262495862295,
      "grad_norm": 59.1877326965332,
      "learning_rate": 1.165708974142302e-05,
      "loss": 0.1065,
      "step": 18650
    },
    {
      "epoch": 3.0883813306852037,
      "grad_norm": 0.04282218590378761,
      "learning_rate": 1.1635964171032618e-05,
      "loss": 0.2766,
      "step": 18660
    },
    {
      "epoch": 3.0900364117841774,
      "grad_norm": 0.007687096018344164,
      "learning_rate": 1.1614838600642218e-05,
      "loss": 0.1866,
      "step": 18670
    },
    {
      "epoch": 3.091691492883151,
      "grad_norm": 7.7537455558776855,
      "learning_rate": 1.1593713030251817e-05,
      "loss": 0.4471,
      "step": 18680
    },
    {
      "epoch": 3.0933465739821253,
      "grad_norm": 7.244738578796387,
      "learning_rate": 1.1572587459861417e-05,
      "loss": 0.1571,
      "step": 18690
    },
    {
      "epoch": 3.095001655081099,
      "grad_norm": 1.8839435577392578,
      "learning_rate": 1.1551461889471015e-05,
      "loss": 0.0561,
      "step": 18700
    },
    {
      "epoch": 3.0966567361800728,
      "grad_norm": 9.061254501342773,
      "learning_rate": 1.1530336319080615e-05,
      "loss": 0.04,
      "step": 18710
    },
    {
      "epoch": 3.0983118172790465,
      "grad_norm": 66.33647155761719,
      "learning_rate": 1.1509210748690215e-05,
      "loss": 0.1846,
      "step": 18720
    },
    {
      "epoch": 3.0999668983780206,
      "grad_norm": 0.06350373476743698,
      "learning_rate": 1.1488085178299815e-05,
      "loss": 0.1959,
      "step": 18730
    },
    {
      "epoch": 3.1016219794769944,
      "grad_norm": 0.01830371469259262,
      "learning_rate": 1.1466959607909414e-05,
      "loss": 0.1439,
      "step": 18740
    },
    {
      "epoch": 3.103277060575968,
      "grad_norm": 0.4652053117752075,
      "learning_rate": 1.1445834037519014e-05,
      "loss": 0.0046,
      "step": 18750
    },
    {
      "epoch": 3.1049321416749422,
      "grad_norm": 0.0035644688177853823,
      "learning_rate": 1.1424708467128612e-05,
      "loss": 0.1698,
      "step": 18760
    },
    {
      "epoch": 3.106587222773916,
      "grad_norm": 54.154666900634766,
      "learning_rate": 1.1403582896738212e-05,
      "loss": 0.0593,
      "step": 18770
    },
    {
      "epoch": 3.1082423038728897,
      "grad_norm": 0.10847660899162292,
      "learning_rate": 1.1382457326347812e-05,
      "loss": 0.2928,
      "step": 18780
    },
    {
      "epoch": 3.1098973849718634,
      "grad_norm": 0.9358625411987305,
      "learning_rate": 1.1361331755957412e-05,
      "loss": 0.0035,
      "step": 18790
    },
    {
      "epoch": 3.1115524660708376,
      "grad_norm": 0.0486750528216362,
      "learning_rate": 1.134020618556701e-05,
      "loss": 0.3674,
      "step": 18800
    },
    {
      "epoch": 3.1132075471698113,
      "grad_norm": 51.39458465576172,
      "learning_rate": 1.1319080615176611e-05,
      "loss": 0.1835,
      "step": 18810
    },
    {
      "epoch": 3.114862628268785,
      "grad_norm": 0.9120293855667114,
      "learning_rate": 1.129795504478621e-05,
      "loss": 0.1351,
      "step": 18820
    },
    {
      "epoch": 3.116517709367759,
      "grad_norm": 0.020679455250501633,
      "learning_rate": 1.127682947439581e-05,
      "loss": 0.1189,
      "step": 18830
    },
    {
      "epoch": 3.118172790466733,
      "grad_norm": 23.84516716003418,
      "learning_rate": 1.125570390400541e-05,
      "loss": 0.096,
      "step": 18840
    },
    {
      "epoch": 3.1198278715657066,
      "grad_norm": 32.345703125,
      "learning_rate": 1.1234578333615008e-05,
      "loss": 0.1571,
      "step": 18850
    },
    {
      "epoch": 3.121482952664681,
      "grad_norm": 0.01558601763099432,
      "learning_rate": 1.1213452763224608e-05,
      "loss": 0.1195,
      "step": 18860
    },
    {
      "epoch": 3.1231380337636545,
      "grad_norm": 0.027324164286255836,
      "learning_rate": 1.1192327192834206e-05,
      "loss": 0.0891,
      "step": 18870
    },
    {
      "epoch": 3.1247931148626282,
      "grad_norm": 8.261068344116211,
      "learning_rate": 1.1171201622443806e-05,
      "loss": 0.0345,
      "step": 18880
    },
    {
      "epoch": 3.126448195961602,
      "grad_norm": 45.49711227416992,
      "learning_rate": 1.1150076052053406e-05,
      "loss": 0.0346,
      "step": 18890
    },
    {
      "epoch": 3.128103277060576,
      "grad_norm": 20.783653259277344,
      "learning_rate": 1.1128950481663006e-05,
      "loss": 0.1122,
      "step": 18900
    },
    {
      "epoch": 3.12975835815955,
      "grad_norm": 0.09989921003580093,
      "learning_rate": 1.1107824911272605e-05,
      "loss": 0.1456,
      "step": 18910
    },
    {
      "epoch": 3.1314134392585236,
      "grad_norm": 63.80707550048828,
      "learning_rate": 1.1086699340882205e-05,
      "loss": 0.4936,
      "step": 18920
    },
    {
      "epoch": 3.1330685203574977,
      "grad_norm": 0.03323449194431305,
      "learning_rate": 1.1065573770491803e-05,
      "loss": 0.0772,
      "step": 18930
    },
    {
      "epoch": 3.1347236014564714,
      "grad_norm": 22.940433502197266,
      "learning_rate": 1.1044448200101403e-05,
      "loss": 0.1991,
      "step": 18940
    },
    {
      "epoch": 3.136378682555445,
      "grad_norm": 102.00789642333984,
      "learning_rate": 1.1023322629711002e-05,
      "loss": 0.0902,
      "step": 18950
    },
    {
      "epoch": 3.138033763654419,
      "grad_norm": 20.620601654052734,
      "learning_rate": 1.1002197059320604e-05,
      "loss": 0.0521,
      "step": 18960
    },
    {
      "epoch": 3.139688844753393,
      "grad_norm": 7.735713958740234,
      "learning_rate": 1.0981071488930202e-05,
      "loss": 0.1334,
      "step": 18970
    },
    {
      "epoch": 3.1413439258523668,
      "grad_norm": 0.00438883388414979,
      "learning_rate": 1.0959945918539802e-05,
      "loss": 0.1277,
      "step": 18980
    },
    {
      "epoch": 3.1429990069513405,
      "grad_norm": 12.494316101074219,
      "learning_rate": 1.09388203481494e-05,
      "loss": 0.1574,
      "step": 18990
    },
    {
      "epoch": 3.1446540880503147,
      "grad_norm": 10.423482894897461,
      "learning_rate": 1.0917694777758999e-05,
      "loss": 0.0925,
      "step": 19000
    },
    {
      "epoch": 3.1463091691492884,
      "grad_norm": 64.38519287109375,
      "learning_rate": 1.0896569207368599e-05,
      "loss": 0.2015,
      "step": 19010
    },
    {
      "epoch": 3.147964250248262,
      "grad_norm": 28.837512969970703,
      "learning_rate": 1.0875443636978199e-05,
      "loss": 0.1709,
      "step": 19020
    },
    {
      "epoch": 3.149619331347236,
      "grad_norm": 14.7359037399292,
      "learning_rate": 1.0854318066587799e-05,
      "loss": 0.1401,
      "step": 19030
    },
    {
      "epoch": 3.15127441244621,
      "grad_norm": 0.4235212802886963,
      "learning_rate": 1.0833192496197397e-05,
      "loss": 0.1011,
      "step": 19040
    },
    {
      "epoch": 3.1529294935451837,
      "grad_norm": 170.03968811035156,
      "learning_rate": 1.0812066925806997e-05,
      "loss": 0.2407,
      "step": 19050
    },
    {
      "epoch": 3.1545845746441574,
      "grad_norm": 1.1326273679733276,
      "learning_rate": 1.0790941355416596e-05,
      "loss": 0.1127,
      "step": 19060
    },
    {
      "epoch": 3.1562396557431316,
      "grad_norm": 39.6057243347168,
      "learning_rate": 1.0769815785026196e-05,
      "loss": 0.0426,
      "step": 19070
    },
    {
      "epoch": 3.1578947368421053,
      "grad_norm": 49.72392654418945,
      "learning_rate": 1.0748690214635796e-05,
      "loss": 0.1003,
      "step": 19080
    },
    {
      "epoch": 3.159549817941079,
      "grad_norm": 0.010619272477924824,
      "learning_rate": 1.0727564644245396e-05,
      "loss": 0.0186,
      "step": 19090
    },
    {
      "epoch": 3.1612048990400528,
      "grad_norm": 22.387104034423828,
      "learning_rate": 1.0706439073854994e-05,
      "loss": 0.1964,
      "step": 19100
    },
    {
      "epoch": 3.162859980139027,
      "grad_norm": 0.06389638781547546,
      "learning_rate": 1.0685313503464595e-05,
      "loss": 0.1245,
      "step": 19110
    },
    {
      "epoch": 3.1645150612380006,
      "grad_norm": 0.014398196712136269,
      "learning_rate": 1.0664187933074193e-05,
      "loss": 0.1157,
      "step": 19120
    },
    {
      "epoch": 3.1661701423369744,
      "grad_norm": 0.061517566442489624,
      "learning_rate": 1.0643062362683793e-05,
      "loss": 0.101,
      "step": 19130
    },
    {
      "epoch": 3.1678252234359485,
      "grad_norm": 21.30259895324707,
      "learning_rate": 1.0621936792293393e-05,
      "loss": 0.4017,
      "step": 19140
    },
    {
      "epoch": 3.1694803045349222,
      "grad_norm": 0.18995164334774017,
      "learning_rate": 1.0600811221902991e-05,
      "loss": 0.1468,
      "step": 19150
    },
    {
      "epoch": 3.171135385633896,
      "grad_norm": 52.53928756713867,
      "learning_rate": 1.0579685651512592e-05,
      "loss": 0.4049,
      "step": 19160
    },
    {
      "epoch": 3.17279046673287,
      "grad_norm": 62.10258483886719,
      "learning_rate": 1.055856008112219e-05,
      "loss": 0.159,
      "step": 19170
    },
    {
      "epoch": 3.174445547831844,
      "grad_norm": 0.15679378807544708,
      "learning_rate": 1.053743451073179e-05,
      "loss": 0.0421,
      "step": 19180
    },
    {
      "epoch": 3.1761006289308176,
      "grad_norm": 0.07309223711490631,
      "learning_rate": 1.0516308940341388e-05,
      "loss": 0.2474,
      "step": 19190
    },
    {
      "epoch": 3.1777557100297913,
      "grad_norm": 0.047824349254369736,
      "learning_rate": 1.049518336995099e-05,
      "loss": 0.2515,
      "step": 19200
    },
    {
      "epoch": 3.1794107911287655,
      "grad_norm": 0.006876666098833084,
      "learning_rate": 1.0474057799560589e-05,
      "loss": 0.0277,
      "step": 19210
    },
    {
      "epoch": 3.181065872227739,
      "grad_norm": 1.9499484300613403,
      "learning_rate": 1.0452932229170189e-05,
      "loss": 0.1202,
      "step": 19220
    },
    {
      "epoch": 3.182720953326713,
      "grad_norm": 0.03012283705174923,
      "learning_rate": 1.0431806658779787e-05,
      "loss": 0.1038,
      "step": 19230
    },
    {
      "epoch": 3.184376034425687,
      "grad_norm": 0.040975917130708694,
      "learning_rate": 1.0410681088389387e-05,
      "loss": 0.1829,
      "step": 19240
    },
    {
      "epoch": 3.186031115524661,
      "grad_norm": 23.838533401489258,
      "learning_rate": 1.0389555517998985e-05,
      "loss": 0.3642,
      "step": 19250
    },
    {
      "epoch": 3.1876861966236345,
      "grad_norm": 19.65545654296875,
      "learning_rate": 1.0368429947608586e-05,
      "loss": 0.1049,
      "step": 19260
    },
    {
      "epoch": 3.1893412777226082,
      "grad_norm": 7.9892168045043945,
      "learning_rate": 1.0347304377218186e-05,
      "loss": 0.0083,
      "step": 19270
    },
    {
      "epoch": 3.1909963588215824,
      "grad_norm": 169.55645751953125,
      "learning_rate": 1.0326178806827786e-05,
      "loss": 0.155,
      "step": 19280
    },
    {
      "epoch": 3.192651439920556,
      "grad_norm": 2.6486268043518066,
      "learning_rate": 1.0305053236437384e-05,
      "loss": 0.1461,
      "step": 19290
    },
    {
      "epoch": 3.19430652101953,
      "grad_norm": 0.03413470461964607,
      "learning_rate": 1.0283927666046984e-05,
      "loss": 0.1483,
      "step": 19300
    },
    {
      "epoch": 3.195961602118504,
      "grad_norm": 41.78847122192383,
      "learning_rate": 1.0262802095656583e-05,
      "loss": 0.0986,
      "step": 19310
    },
    {
      "epoch": 3.1976166832174777,
      "grad_norm": 0.3175998032093048,
      "learning_rate": 1.0241676525266183e-05,
      "loss": 0.1782,
      "step": 19320
    },
    {
      "epoch": 3.1992717643164514,
      "grad_norm": 3.376892328262329,
      "learning_rate": 1.0220550954875783e-05,
      "loss": 0.2117,
      "step": 19330
    },
    {
      "epoch": 3.200926845415425,
      "grad_norm": 3.619628667831421,
      "learning_rate": 1.0199425384485381e-05,
      "loss": 0.1998,
      "step": 19340
    },
    {
      "epoch": 3.2025819265143993,
      "grad_norm": 11.627298355102539,
      "learning_rate": 1.0178299814094981e-05,
      "loss": 0.1811,
      "step": 19350
    },
    {
      "epoch": 3.204237007613373,
      "grad_norm": 64.71401977539062,
      "learning_rate": 1.015717424370458e-05,
      "loss": 0.0551,
      "step": 19360
    },
    {
      "epoch": 3.2058920887123468,
      "grad_norm": 142.3553466796875,
      "learning_rate": 1.013604867331418e-05,
      "loss": 0.1271,
      "step": 19370
    },
    {
      "epoch": 3.207547169811321,
      "grad_norm": 0.012928701005876064,
      "learning_rate": 1.011492310292378e-05,
      "loss": 0.07,
      "step": 19380
    },
    {
      "epoch": 3.2092022509102947,
      "grad_norm": 129.6248016357422,
      "learning_rate": 1.009379753253338e-05,
      "loss": 0.2325,
      "step": 19390
    },
    {
      "epoch": 3.2108573320092684,
      "grad_norm": 0.07203659415245056,
      "learning_rate": 1.0072671962142978e-05,
      "loss": 0.2962,
      "step": 19400
    },
    {
      "epoch": 3.212512413108242,
      "grad_norm": 0.25001955032348633,
      "learning_rate": 1.0051546391752578e-05,
      "loss": 0.1385,
      "step": 19410
    },
    {
      "epoch": 3.2141674942072163,
      "grad_norm": 0.014414187520742416,
      "learning_rate": 1.0030420821362177e-05,
      "loss": 0.116,
      "step": 19420
    },
    {
      "epoch": 3.21582257530619,
      "grad_norm": 20.700918197631836,
      "learning_rate": 1.0009295250971777e-05,
      "loss": 0.0979,
      "step": 19430
    },
    {
      "epoch": 3.2174776564051637,
      "grad_norm": 0.08648065477609634,
      "learning_rate": 9.988169680581377e-06,
      "loss": 0.2822,
      "step": 19440
    },
    {
      "epoch": 3.219132737504138,
      "grad_norm": 0.009397421963512897,
      "learning_rate": 9.967044110190977e-06,
      "loss": 0.0968,
      "step": 19450
    },
    {
      "epoch": 3.2207878186031116,
      "grad_norm": 0.60981285572052,
      "learning_rate": 9.945918539800575e-06,
      "loss": 0.3396,
      "step": 19460
    },
    {
      "epoch": 3.2224428997020853,
      "grad_norm": 0.0442255400121212,
      "learning_rate": 9.924792969410175e-06,
      "loss": 0.0922,
      "step": 19470
    },
    {
      "epoch": 3.2240979808010595,
      "grad_norm": 18.145505905151367,
      "learning_rate": 9.903667399019774e-06,
      "loss": 0.1097,
      "step": 19480
    },
    {
      "epoch": 3.225753061900033,
      "grad_norm": 0.09621188044548035,
      "learning_rate": 9.882541828629372e-06,
      "loss": 0.0878,
      "step": 19490
    },
    {
      "epoch": 3.227408142999007,
      "grad_norm": 10.446554183959961,
      "learning_rate": 9.861416258238972e-06,
      "loss": 0.1112,
      "step": 19500
    },
    {
      "epoch": 3.2290632240979806,
      "grad_norm": 3.6974503993988037,
      "learning_rate": 9.840290687848572e-06,
      "loss": 0.0093,
      "step": 19510
    },
    {
      "epoch": 3.230718305196955,
      "grad_norm": 1.0079326629638672,
      "learning_rate": 9.819165117458172e-06,
      "loss": 0.1772,
      "step": 19520
    },
    {
      "epoch": 3.2323733862959285,
      "grad_norm": 10.432433128356934,
      "learning_rate": 9.79803954706777e-06,
      "loss": 0.3301,
      "step": 19530
    },
    {
      "epoch": 3.2340284673949022,
      "grad_norm": 4.607972145080566,
      "learning_rate": 9.77691397667737e-06,
      "loss": 0.244,
      "step": 19540
    },
    {
      "epoch": 3.2356835484938764,
      "grad_norm": 0.13509419560432434,
      "learning_rate": 9.75578840628697e-06,
      "loss": 0.0608,
      "step": 19550
    },
    {
      "epoch": 3.23733862959285,
      "grad_norm": 0.03796200826764107,
      "learning_rate": 9.73466283589657e-06,
      "loss": 0.0839,
      "step": 19560
    },
    {
      "epoch": 3.238993710691824,
      "grad_norm": 95.64999389648438,
      "learning_rate": 9.71353726550617e-06,
      "loss": 0.105,
      "step": 19570
    },
    {
      "epoch": 3.2406487917907976,
      "grad_norm": 0.4525887370109558,
      "learning_rate": 9.69241169511577e-06,
      "loss": 0.0526,
      "step": 19580
    },
    {
      "epoch": 3.2423038728897717,
      "grad_norm": 36.069549560546875,
      "learning_rate": 9.671286124725368e-06,
      "loss": 0.0988,
      "step": 19590
    },
    {
      "epoch": 3.2439589539887455,
      "grad_norm": 1.8427926301956177,
      "learning_rate": 9.650160554334968e-06,
      "loss": 0.1206,
      "step": 19600
    },
    {
      "epoch": 3.245614035087719,
      "grad_norm": 53.8623046875,
      "learning_rate": 9.629034983944566e-06,
      "loss": 0.1779,
      "step": 19610
    },
    {
      "epoch": 3.2472691161866933,
      "grad_norm": 98.5615234375,
      "learning_rate": 9.607909413554166e-06,
      "loss": 0.3288,
      "step": 19620
    },
    {
      "epoch": 3.248924197285667,
      "grad_norm": 0.24618537724018097,
      "learning_rate": 9.586783843163766e-06,
      "loss": 0.0388,
      "step": 19630
    },
    {
      "epoch": 3.250579278384641,
      "grad_norm": 16.695377349853516,
      "learning_rate": 9.565658272773366e-06,
      "loss": 0.1622,
      "step": 19640
    },
    {
      "epoch": 3.252234359483615,
      "grad_norm": 1.0175695419311523,
      "learning_rate": 9.544532702382965e-06,
      "loss": 0.1181,
      "step": 19650
    },
    {
      "epoch": 3.2538894405825887,
      "grad_norm": 1.495479941368103,
      "learning_rate": 9.523407131992563e-06,
      "loss": 0.0367,
      "step": 19660
    },
    {
      "epoch": 3.2555445216815624,
      "grad_norm": 0.8056451678276062,
      "learning_rate": 9.502281561602163e-06,
      "loss": 0.0359,
      "step": 19670
    },
    {
      "epoch": 3.257199602780536,
      "grad_norm": 0.03671388700604439,
      "learning_rate": 9.481155991211763e-06,
      "loss": 0.1632,
      "step": 19680
    },
    {
      "epoch": 3.2588546838795103,
      "grad_norm": 12.57313346862793,
      "learning_rate": 9.460030420821363e-06,
      "loss": 0.2322,
      "step": 19690
    },
    {
      "epoch": 3.260509764978484,
      "grad_norm": 0.0455591082572937,
      "learning_rate": 9.438904850430962e-06,
      "loss": 0.2215,
      "step": 19700
    },
    {
      "epoch": 3.2621648460774577,
      "grad_norm": 0.03018953464925289,
      "learning_rate": 9.417779280040562e-06,
      "loss": 0.2592,
      "step": 19710
    },
    {
      "epoch": 3.2638199271764314,
      "grad_norm": 0.20371773838996887,
      "learning_rate": 9.39665370965016e-06,
      "loss": 0.0581,
      "step": 19720
    },
    {
      "epoch": 3.2654750082754056,
      "grad_norm": 7.745580673217773,
      "learning_rate": 9.37552813925976e-06,
      "loss": 0.3637,
      "step": 19730
    },
    {
      "epoch": 3.2671300893743793,
      "grad_norm": 0.050828006118535995,
      "learning_rate": 9.354402568869359e-06,
      "loss": 0.1688,
      "step": 19740
    },
    {
      "epoch": 3.268785170473353,
      "grad_norm": 0.002061649691313505,
      "learning_rate": 9.33327699847896e-06,
      "loss": 0.1173,
      "step": 19750
    },
    {
      "epoch": 3.270440251572327,
      "grad_norm": 65.39591217041016,
      "learning_rate": 9.312151428088559e-06,
      "loss": 0.2753,
      "step": 19760
    },
    {
      "epoch": 3.272095332671301,
      "grad_norm": 0.10573406517505646,
      "learning_rate": 9.291025857698159e-06,
      "loss": 0.1251,
      "step": 19770
    },
    {
      "epoch": 3.2737504137702746,
      "grad_norm": 89.99291229248047,
      "learning_rate": 9.269900287307757e-06,
      "loss": 0.1571,
      "step": 19780
    },
    {
      "epoch": 3.275405494869249,
      "grad_norm": 11.749985694885254,
      "learning_rate": 9.248774716917357e-06,
      "loss": 0.1753,
      "step": 19790
    },
    {
      "epoch": 3.2770605759682225,
      "grad_norm": 0.41973158717155457,
      "learning_rate": 9.227649146526956e-06,
      "loss": 0.0396,
      "step": 19800
    },
    {
      "epoch": 3.2787156570671963,
      "grad_norm": 1.1594444513320923,
      "learning_rate": 9.206523576136556e-06,
      "loss": 0.1928,
      "step": 19810
    },
    {
      "epoch": 3.28037073816617,
      "grad_norm": 15.600133895874023,
      "learning_rate": 9.185398005746156e-06,
      "loss": 0.4568,
      "step": 19820
    },
    {
      "epoch": 3.282025819265144,
      "grad_norm": 34.008827209472656,
      "learning_rate": 9.164272435355754e-06,
      "loss": 0.2693,
      "step": 19830
    },
    {
      "epoch": 3.283680900364118,
      "grad_norm": 62.038028717041016,
      "learning_rate": 9.143146864965354e-06,
      "loss": 0.0982,
      "step": 19840
    },
    {
      "epoch": 3.2853359814630916,
      "grad_norm": 1.6117361783981323,
      "learning_rate": 9.122021294574953e-06,
      "loss": 0.1039,
      "step": 19850
    },
    {
      "epoch": 3.2869910625620653,
      "grad_norm": 16.937002182006836,
      "learning_rate": 9.100895724184553e-06,
      "loss": 0.1622,
      "step": 19860
    },
    {
      "epoch": 3.2886461436610395,
      "grad_norm": 0.03889768198132515,
      "learning_rate": 9.079770153794153e-06,
      "loss": 0.0046,
      "step": 19870
    },
    {
      "epoch": 3.290301224760013,
      "grad_norm": 0.23715832829475403,
      "learning_rate": 9.058644583403753e-06,
      "loss": 0.1592,
      "step": 19880
    },
    {
      "epoch": 3.291956305858987,
      "grad_norm": 0.05844606086611748,
      "learning_rate": 9.037519013013351e-06,
      "loss": 0.0801,
      "step": 19890
    },
    {
      "epoch": 3.293611386957961,
      "grad_norm": 60.139225006103516,
      "learning_rate": 9.016393442622952e-06,
      "loss": 0.1409,
      "step": 19900
    },
    {
      "epoch": 3.295266468056935,
      "grad_norm": 0.12532077729701996,
      "learning_rate": 8.99526787223255e-06,
      "loss": 0.0334,
      "step": 19910
    },
    {
      "epoch": 3.2969215491559085,
      "grad_norm": 4.564908981323242,
      "learning_rate": 8.97414230184215e-06,
      "loss": 0.0935,
      "step": 19920
    },
    {
      "epoch": 3.2985766302548827,
      "grad_norm": 2.369495153427124,
      "learning_rate": 8.95301673145175e-06,
      "loss": 0.1658,
      "step": 19930
    },
    {
      "epoch": 3.3002317113538564,
      "grad_norm": 0.006478725466877222,
      "learning_rate": 8.93189116106135e-06,
      "loss": 0.0836,
      "step": 19940
    },
    {
      "epoch": 3.30188679245283,
      "grad_norm": 0.24253854155540466,
      "learning_rate": 8.910765590670949e-06,
      "loss": 0.1983,
      "step": 19950
    },
    {
      "epoch": 3.3035418735518043,
      "grad_norm": 0.012180988676846027,
      "learning_rate": 8.889640020280549e-06,
      "loss": 0.1315,
      "step": 19960
    },
    {
      "epoch": 3.305196954650778,
      "grad_norm": 0.008576538413763046,
      "learning_rate": 8.868514449890147e-06,
      "loss": 0.1343,
      "step": 19970
    },
    {
      "epoch": 3.3068520357497517,
      "grad_norm": 4.2368574142456055,
      "learning_rate": 8.847388879499747e-06,
      "loss": 0.1047,
      "step": 19980
    },
    {
      "epoch": 3.3085071168487254,
      "grad_norm": 0.9700894951820374,
      "learning_rate": 8.826263309109347e-06,
      "loss": 0.0412,
      "step": 19990
    },
    {
      "epoch": 3.3101621979476996,
      "grad_norm": 0.20095905661582947,
      "learning_rate": 8.805137738718946e-06,
      "loss": 0.1073,
      "step": 20000
    },
    {
      "epoch": 3.3118172790466733,
      "grad_norm": 3.768195629119873,
      "learning_rate": 8.784012168328546e-06,
      "loss": 0.1199,
      "step": 20010
    },
    {
      "epoch": 3.313472360145647,
      "grad_norm": 0.07517141103744507,
      "learning_rate": 8.762886597938144e-06,
      "loss": 0.164,
      "step": 20020
    },
    {
      "epoch": 3.3151274412446208,
      "grad_norm": 0.07605013251304626,
      "learning_rate": 8.741761027547744e-06,
      "loss": 0.0848,
      "step": 20030
    },
    {
      "epoch": 3.316782522343595,
      "grad_norm": 0.13415931165218353,
      "learning_rate": 8.720635457157342e-06,
      "loss": 0.2259,
      "step": 20040
    },
    {
      "epoch": 3.3184376034425687,
      "grad_norm": 0.1633792221546173,
      "learning_rate": 8.699509886766943e-06,
      "loss": 0.4114,
      "step": 20050
    },
    {
      "epoch": 3.3200926845415424,
      "grad_norm": 0.028068332001566887,
      "learning_rate": 8.678384316376543e-06,
      "loss": 0.1005,
      "step": 20060
    },
    {
      "epoch": 3.3217477656405165,
      "grad_norm": 0.012762272730469704,
      "learning_rate": 8.657258745986143e-06,
      "loss": 0.1347,
      "step": 20070
    },
    {
      "epoch": 3.3234028467394903,
      "grad_norm": 0.10799038410186768,
      "learning_rate": 8.636133175595741e-06,
      "loss": 0.0695,
      "step": 20080
    },
    {
      "epoch": 3.325057927838464,
      "grad_norm": 0.43664684891700745,
      "learning_rate": 8.615007605205341e-06,
      "loss": 0.0975,
      "step": 20090
    },
    {
      "epoch": 3.326713008937438,
      "grad_norm": 0.03827007859945297,
      "learning_rate": 8.59388203481494e-06,
      "loss": 0.2104,
      "step": 20100
    },
    {
      "epoch": 3.328368090036412,
      "grad_norm": 0.39215147495269775,
      "learning_rate": 8.57275646442454e-06,
      "loss": 0.1302,
      "step": 20110
    },
    {
      "epoch": 3.3300231711353856,
      "grad_norm": 193.20838928222656,
      "learning_rate": 8.55163089403414e-06,
      "loss": 0.1245,
      "step": 20120
    },
    {
      "epoch": 3.3316782522343593,
      "grad_norm": 19.589519500732422,
      "learning_rate": 8.53050532364374e-06,
      "loss": 0.1146,
      "step": 20130
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 1.017979383468628,
      "learning_rate": 8.509379753253338e-06,
      "loss": 0.0737,
      "step": 20140
    },
    {
      "epoch": 3.334988414432307,
      "grad_norm": 0.08751567453145981,
      "learning_rate": 8.488254182862938e-06,
      "loss": 0.2276,
      "step": 20150
    },
    {
      "epoch": 3.336643495531281,
      "grad_norm": 1.1266798973083496,
      "learning_rate": 8.467128612472537e-06,
      "loss": 0.1098,
      "step": 20160
    },
    {
      "epoch": 3.3382985766302546,
      "grad_norm": 0.5126157402992249,
      "learning_rate": 8.446003042082137e-06,
      "loss": 0.2628,
      "step": 20170
    },
    {
      "epoch": 3.339953657729229,
      "grad_norm": 0.031090853735804558,
      "learning_rate": 8.424877471691737e-06,
      "loss": 0.0765,
      "step": 20180
    },
    {
      "epoch": 3.3416087388282025,
      "grad_norm": 33.753211975097656,
      "learning_rate": 8.403751901301335e-06,
      "loss": 0.0821,
      "step": 20190
    },
    {
      "epoch": 3.3432638199271763,
      "grad_norm": 60.042964935302734,
      "learning_rate": 8.382626330910935e-06,
      "loss": 0.2254,
      "step": 20200
    },
    {
      "epoch": 3.3449189010261504,
      "grad_norm": 129.52540588378906,
      "learning_rate": 8.361500760520534e-06,
      "loss": 0.0839,
      "step": 20210
    },
    {
      "epoch": 3.346573982125124,
      "grad_norm": 55.63346862792969,
      "learning_rate": 8.340375190130134e-06,
      "loss": 0.1336,
      "step": 20220
    },
    {
      "epoch": 3.348229063224098,
      "grad_norm": 1.11031973361969,
      "learning_rate": 8.319249619739732e-06,
      "loss": 0.2336,
      "step": 20230
    },
    {
      "epoch": 3.349884144323072,
      "grad_norm": 0.3510897755622864,
      "learning_rate": 8.298124049349334e-06,
      "loss": 0.1164,
      "step": 20240
    },
    {
      "epoch": 3.3515392254220457,
      "grad_norm": 0.012220739386975765,
      "learning_rate": 8.276998478958932e-06,
      "loss": 0.0417,
      "step": 20250
    },
    {
      "epoch": 3.3531943065210195,
      "grad_norm": 0.004650651477277279,
      "learning_rate": 8.255872908568532e-06,
      "loss": 0.1978,
      "step": 20260
    },
    {
      "epoch": 3.3548493876199936,
      "grad_norm": 1.1486291885375977,
      "learning_rate": 8.23474733817813e-06,
      "loss": 0.1364,
      "step": 20270
    },
    {
      "epoch": 3.3565044687189673,
      "grad_norm": 8.06767463684082,
      "learning_rate": 8.21362176778773e-06,
      "loss": 0.2533,
      "step": 20280
    },
    {
      "epoch": 3.358159549817941,
      "grad_norm": 0.1580887734889984,
      "learning_rate": 8.19249619739733e-06,
      "loss": 0.1363,
      "step": 20290
    },
    {
      "epoch": 3.359814630916915,
      "grad_norm": 0.06423509865999222,
      "learning_rate": 8.171370627006931e-06,
      "loss": 0.0753,
      "step": 20300
    },
    {
      "epoch": 3.361469712015889,
      "grad_norm": 7.0777506828308105,
      "learning_rate": 8.15024505661653e-06,
      "loss": 0.0715,
      "step": 20310
    },
    {
      "epoch": 3.3631247931148627,
      "grad_norm": 1.613925576210022,
      "learning_rate": 8.12911948622613e-06,
      "loss": 0.1438,
      "step": 20320
    },
    {
      "epoch": 3.3647798742138364,
      "grad_norm": 4.907856464385986,
      "learning_rate": 8.107993915835728e-06,
      "loss": 0.113,
      "step": 20330
    },
    {
      "epoch": 3.36643495531281,
      "grad_norm": 9.68255615234375,
      "learning_rate": 8.086868345445326e-06,
      "loss": 0.0498,
      "step": 20340
    },
    {
      "epoch": 3.3680900364117843,
      "grad_norm": 0.45960676670074463,
      "learning_rate": 8.065742775054926e-06,
      "loss": 0.0104,
      "step": 20350
    },
    {
      "epoch": 3.369745117510758,
      "grad_norm": 0.3723115921020508,
      "learning_rate": 8.044617204664526e-06,
      "loss": 0.0107,
      "step": 20360
    },
    {
      "epoch": 3.3714001986097317,
      "grad_norm": 0.004724568221718073,
      "learning_rate": 8.023491634274126e-06,
      "loss": 0.1038,
      "step": 20370
    },
    {
      "epoch": 3.373055279708706,
      "grad_norm": 5.013513565063477,
      "learning_rate": 8.002366063883725e-06,
      "loss": 0.0816,
      "step": 20380
    },
    {
      "epoch": 3.3747103608076796,
      "grad_norm": 63.42707061767578,
      "learning_rate": 7.981240493493325e-06,
      "loss": 0.0523,
      "step": 20390
    },
    {
      "epoch": 3.3763654419066533,
      "grad_norm": 0.008153751492500305,
      "learning_rate": 7.960114923102923e-06,
      "loss": 0.077,
      "step": 20400
    },
    {
      "epoch": 3.3780205230056275,
      "grad_norm": 50.696205139160156,
      "learning_rate": 7.938989352712523e-06,
      "loss": 0.1679,
      "step": 20410
    },
    {
      "epoch": 3.379675604104601,
      "grad_norm": 17.482881546020508,
      "learning_rate": 7.917863782322123e-06,
      "loss": 0.1532,
      "step": 20420
    },
    {
      "epoch": 3.381330685203575,
      "grad_norm": 0.6109359264373779,
      "learning_rate": 7.896738211931723e-06,
      "loss": 0.2898,
      "step": 20430
    },
    {
      "epoch": 3.3829857663025487,
      "grad_norm": 15.06084156036377,
      "learning_rate": 7.875612641541322e-06,
      "loss": 0.1121,
      "step": 20440
    },
    {
      "epoch": 3.384640847401523,
      "grad_norm": 46.99411392211914,
      "learning_rate": 7.854487071150922e-06,
      "loss": 0.091,
      "step": 20450
    },
    {
      "epoch": 3.3862959285004965,
      "grad_norm": 0.0376795195043087,
      "learning_rate": 7.83336150076052e-06,
      "loss": 0.0013,
      "step": 20460
    },
    {
      "epoch": 3.3879510095994703,
      "grad_norm": 0.5744833946228027,
      "learning_rate": 7.81223593037012e-06,
      "loss": 0.0537,
      "step": 20470
    },
    {
      "epoch": 3.389606090698444,
      "grad_norm": 0.01208694837987423,
      "learning_rate": 7.79111035997972e-06,
      "loss": 0.2413,
      "step": 20480
    },
    {
      "epoch": 3.391261171797418,
      "grad_norm": 12.51729679107666,
      "learning_rate": 7.76998478958932e-06,
      "loss": 0.2151,
      "step": 20490
    },
    {
      "epoch": 3.392916252896392,
      "grad_norm": 0.01045236736536026,
      "learning_rate": 7.748859219198919e-06,
      "loss": 0.0267,
      "step": 20500
    },
    {
      "epoch": 3.3945713339953656,
      "grad_norm": 1.4833855628967285,
      "learning_rate": 7.727733648808517e-06,
      "loss": 0.1551,
      "step": 20510
    },
    {
      "epoch": 3.3962264150943398,
      "grad_norm": 0.1674279123544693,
      "learning_rate": 7.706608078418117e-06,
      "loss": 0.1385,
      "step": 20520
    },
    {
      "epoch": 3.3978814961933135,
      "grad_norm": 0.027817221358418465,
      "learning_rate": 7.685482508027716e-06,
      "loss": 0.0459,
      "step": 20530
    },
    {
      "epoch": 3.399536577292287,
      "grad_norm": 0.14837150275707245,
      "learning_rate": 7.664356937637318e-06,
      "loss": 0.1572,
      "step": 20540
    },
    {
      "epoch": 3.4011916583912614,
      "grad_norm": 0.08498934656381607,
      "learning_rate": 7.643231367246916e-06,
      "loss": 0.0635,
      "step": 20550
    },
    {
      "epoch": 3.402846739490235,
      "grad_norm": 0.015626568347215652,
      "learning_rate": 7.622105796856516e-06,
      "loss": 0.1195,
      "step": 20560
    },
    {
      "epoch": 3.404501820589209,
      "grad_norm": 85.75617218017578,
      "learning_rate": 7.600980226466114e-06,
      "loss": 0.2271,
      "step": 20570
    },
    {
      "epoch": 3.406156901688183,
      "grad_norm": 29.47321128845215,
      "learning_rate": 7.5798546560757145e-06,
      "loss": 0.172,
      "step": 20580
    },
    {
      "epoch": 3.4078119827871567,
      "grad_norm": 16.463272094726562,
      "learning_rate": 7.558729085685314e-06,
      "loss": 0.1325,
      "step": 20590
    },
    {
      "epoch": 3.4094670638861304,
      "grad_norm": 29.60060691833496,
      "learning_rate": 7.537603515294914e-06,
      "loss": 0.0163,
      "step": 20600
    },
    {
      "epoch": 3.411122144985104,
      "grad_norm": 0.010700155980885029,
      "learning_rate": 7.516477944904512e-06,
      "loss": 0.3817,
      "step": 20610
    },
    {
      "epoch": 3.4127772260840783,
      "grad_norm": 0.20367132127285004,
      "learning_rate": 7.495352374514113e-06,
      "loss": 0.2377,
      "step": 20620
    },
    {
      "epoch": 3.414432307183052,
      "grad_norm": 27.075599670410156,
      "learning_rate": 7.4742268041237115e-06,
      "loss": 0.089,
      "step": 20630
    },
    {
      "epoch": 3.4160873882820257,
      "grad_norm": 0.19552366435527802,
      "learning_rate": 7.4531012337333115e-06,
      "loss": 0.1558,
      "step": 20640
    },
    {
      "epoch": 3.4177424693809995,
      "grad_norm": 61.508522033691406,
      "learning_rate": 7.431975663342911e-06,
      "loss": 0.1697,
      "step": 20650
    },
    {
      "epoch": 3.4193975504799736,
      "grad_norm": 0.23646019399166107,
      "learning_rate": 7.410850092952511e-06,
      "loss": 0.2708,
      "step": 20660
    },
    {
      "epoch": 3.4210526315789473,
      "grad_norm": 18.887187957763672,
      "learning_rate": 7.389724522562109e-06,
      "loss": 0.0478,
      "step": 20670
    },
    {
      "epoch": 3.422707712677921,
      "grad_norm": 0.18319128453731537,
      "learning_rate": 7.3685989521717085e-06,
      "loss": 0.0421,
      "step": 20680
    },
    {
      "epoch": 3.4243627937768952,
      "grad_norm": 1.3140867948532104,
      "learning_rate": 7.3474733817813085e-06,
      "loss": 0.2407,
      "step": 20690
    },
    {
      "epoch": 3.426017874875869,
      "grad_norm": 24.013225555419922,
      "learning_rate": 7.326347811390907e-06,
      "loss": 0.2118,
      "step": 20700
    },
    {
      "epoch": 3.4276729559748427,
      "grad_norm": 27.75457191467285,
      "learning_rate": 7.305222241000508e-06,
      "loss": 0.145,
      "step": 20710
    },
    {
      "epoch": 3.429328037073817,
      "grad_norm": 0.049790605902671814,
      "learning_rate": 7.284096670610106e-06,
      "loss": 0.1637,
      "step": 20720
    },
    {
      "epoch": 3.4309831181727906,
      "grad_norm": 0.02620565891265869,
      "learning_rate": 7.262971100219706e-06,
      "loss": 0.0022,
      "step": 20730
    },
    {
      "epoch": 3.4326381992717643,
      "grad_norm": 0.05602918565273285,
      "learning_rate": 7.2418455298293055e-06,
      "loss": 0.0671,
      "step": 20740
    },
    {
      "epoch": 3.434293280370738,
      "grad_norm": 0.45903047919273376,
      "learning_rate": 7.220719959438906e-06,
      "loss": 0.1914,
      "step": 20750
    },
    {
      "epoch": 3.435948361469712,
      "grad_norm": 1.5446535348892212,
      "learning_rate": 7.199594389048504e-06,
      "loss": 0.0446,
      "step": 20760
    },
    {
      "epoch": 3.437603442568686,
      "grad_norm": 0.15750819444656372,
      "learning_rate": 7.178468818658104e-06,
      "loss": 0.0926,
      "step": 20770
    },
    {
      "epoch": 3.4392585236676596,
      "grad_norm": 12.211820602416992,
      "learning_rate": 7.157343248267703e-06,
      "loss": 0.0566,
      "step": 20780
    },
    {
      "epoch": 3.4409136047666333,
      "grad_norm": 0.09512022137641907,
      "learning_rate": 7.136217677877303e-06,
      "loss": 0.2601,
      "step": 20790
    },
    {
      "epoch": 3.4425686858656075,
      "grad_norm": 0.04619762673974037,
      "learning_rate": 7.115092107486903e-06,
      "loss": 0.012,
      "step": 20800
    },
    {
      "epoch": 3.444223766964581,
      "grad_norm": 2.8488636016845703,
      "learning_rate": 7.093966537096503e-06,
      "loss": 0.2511,
      "step": 20810
    },
    {
      "epoch": 3.445878848063555,
      "grad_norm": 30.934986114501953,
      "learning_rate": 7.072840966706101e-06,
      "loss": 0.2081,
      "step": 20820
    },
    {
      "epoch": 3.447533929162529,
      "grad_norm": 94.15129852294922,
      "learning_rate": 7.051715396315701e-06,
      "loss": 0.135,
      "step": 20830
    },
    {
      "epoch": 3.449189010261503,
      "grad_norm": 0.01599845662713051,
      "learning_rate": 7.0305898259253e-06,
      "loss": 0.0202,
      "step": 20840
    },
    {
      "epoch": 3.4508440913604765,
      "grad_norm": 2.271690845489502,
      "learning_rate": 7.009464255534899e-06,
      "loss": 0.2182,
      "step": 20850
    },
    {
      "epoch": 3.4524991724594507,
      "grad_norm": 0.8390814661979675,
      "learning_rate": 6.9883386851445e-06,
      "loss": 0.1053,
      "step": 20860
    },
    {
      "epoch": 3.4541542535584244,
      "grad_norm": 0.040197163820266724,
      "learning_rate": 6.967213114754098e-06,
      "loss": 0.0297,
      "step": 20870
    },
    {
      "epoch": 3.455809334657398,
      "grad_norm": 23.729360580444336,
      "learning_rate": 6.946087544363698e-06,
      "loss": 0.1414,
      "step": 20880
    },
    {
      "epoch": 3.4574644157563723,
      "grad_norm": 0.023346295580267906,
      "learning_rate": 6.924961973973297e-06,
      "loss": 0.0541,
      "step": 20890
    },
    {
      "epoch": 3.459119496855346,
      "grad_norm": 0.0033838781528174877,
      "learning_rate": 6.9038364035828975e-06,
      "loss": 0.1048,
      "step": 20900
    },
    {
      "epoch": 3.4607745779543198,
      "grad_norm": 0.0031289239414036274,
      "learning_rate": 6.882710833192496e-06,
      "loss": 0.0847,
      "step": 20910
    },
    {
      "epoch": 3.4624296590532935,
      "grad_norm": 0.43260443210601807,
      "learning_rate": 6.861585262802096e-06,
      "loss": 0.1285,
      "step": 20920
    },
    {
      "epoch": 3.4640847401522676,
      "grad_norm": 1.3018397092819214,
      "learning_rate": 6.840459692411695e-06,
      "loss": 0.1785,
      "step": 20930
    },
    {
      "epoch": 3.4657398212512414,
      "grad_norm": 0.1951046884059906,
      "learning_rate": 6.819334122021295e-06,
      "loss": 0.1406,
      "step": 20940
    },
    {
      "epoch": 3.467394902350215,
      "grad_norm": 10.891997337341309,
      "learning_rate": 6.7982085516308945e-06,
      "loss": 0.1629,
      "step": 20950
    },
    {
      "epoch": 3.469049983449189,
      "grad_norm": 3.191754102706909,
      "learning_rate": 6.7770829812404945e-06,
      "loss": 0.2859,
      "step": 20960
    },
    {
      "epoch": 3.470705064548163,
      "grad_norm": 45.675777435302734,
      "learning_rate": 6.755957410850093e-06,
      "loss": 0.2382,
      "step": 20970
    },
    {
      "epoch": 3.4723601456471367,
      "grad_norm": 4.02777099609375,
      "learning_rate": 6.734831840459693e-06,
      "loss": 0.1165,
      "step": 20980
    },
    {
      "epoch": 3.4740152267461104,
      "grad_norm": 82.80461120605469,
      "learning_rate": 6.713706270069292e-06,
      "loss": 0.0661,
      "step": 20990
    },
    {
      "epoch": 3.4756703078450846,
      "grad_norm": 0.02575400099158287,
      "learning_rate": 6.692580699678892e-06,
      "loss": 0.0181,
      "step": 21000
    },
    {
      "epoch": 3.4773253889440583,
      "grad_norm": 0.15578360855579376,
      "learning_rate": 6.671455129288491e-06,
      "loss": 0.0808,
      "step": 21010
    },
    {
      "epoch": 3.478980470043032,
      "grad_norm": 0.07147613912820816,
      "learning_rate": 6.65032955889809e-06,
      "loss": 0.0133,
      "step": 21020
    },
    {
      "epoch": 3.480635551142006,
      "grad_norm": 4.827787399291992,
      "learning_rate": 6.62920398850769e-06,
      "loss": 0.0516,
      "step": 21030
    },
    {
      "epoch": 3.48229063224098,
      "grad_norm": 0.037380702793598175,
      "learning_rate": 6.608078418117289e-06,
      "loss": 0.1641,
      "step": 21040
    },
    {
      "epoch": 3.4839457133399536,
      "grad_norm": 13.783099174499512,
      "learning_rate": 6.586952847726889e-06,
      "loss": 0.2318,
      "step": 21050
    },
    {
      "epoch": 3.4856007944389273,
      "grad_norm": 16.321090698242188,
      "learning_rate": 6.565827277336488e-06,
      "loss": 0.4352,
      "step": 21060
    },
    {
      "epoch": 3.4872558755379015,
      "grad_norm": 0.04780185967683792,
      "learning_rate": 6.544701706946088e-06,
      "loss": 0.2415,
      "step": 21070
    },
    {
      "epoch": 3.4889109566368752,
      "grad_norm": 5.840874671936035,
      "learning_rate": 6.523576136555687e-06,
      "loss": 0.0117,
      "step": 21080
    },
    {
      "epoch": 3.490566037735849,
      "grad_norm": 0.027617197483778,
      "learning_rate": 6.502450566165287e-06,
      "loss": 0.111,
      "step": 21090
    },
    {
      "epoch": 3.4922211188348227,
      "grad_norm": 0.012061893939971924,
      "learning_rate": 6.4813249957748855e-06,
      "loss": 0.1844,
      "step": 21100
    },
    {
      "epoch": 3.493876199933797,
      "grad_norm": 12.945438385009766,
      "learning_rate": 6.460199425384486e-06,
      "loss": 0.0739,
      "step": 21110
    },
    {
      "epoch": 3.4955312810327706,
      "grad_norm": 0.030434701591730118,
      "learning_rate": 6.439073854994085e-06,
      "loss": 0.12,
      "step": 21120
    },
    {
      "epoch": 3.4971863621317443,
      "grad_norm": 0.023690249770879745,
      "learning_rate": 6.417948284603685e-06,
      "loss": 0.0737,
      "step": 21130
    },
    {
      "epoch": 3.4988414432307184,
      "grad_norm": 0.34019285440444946,
      "learning_rate": 6.396822714213284e-06,
      "loss": 0.0052,
      "step": 21140
    },
    {
      "epoch": 3.500496524329692,
      "grad_norm": 8.572063446044922,
      "learning_rate": 6.375697143822884e-06,
      "loss": 0.1964,
      "step": 21150
    },
    {
      "epoch": 3.502151605428666,
      "grad_norm": 0.01932748220860958,
      "learning_rate": 6.3545715734324825e-06,
      "loss": 0.1381,
      "step": 21160
    },
    {
      "epoch": 3.50380668652764,
      "grad_norm": 0.9871838688850403,
      "learning_rate": 6.3334460030420835e-06,
      "loss": 0.2278,
      "step": 21170
    },
    {
      "epoch": 3.5054617676266138,
      "grad_norm": 0.01553903054445982,
      "learning_rate": 6.312320432651682e-06,
      "loss": 0.2184,
      "step": 21180
    },
    {
      "epoch": 3.5071168487255875,
      "grad_norm": 2.3856008052825928,
      "learning_rate": 6.291194862261281e-06,
      "loss": 0.1806,
      "step": 21190
    },
    {
      "epoch": 3.5087719298245617,
      "grad_norm": 0.1776244342327118,
      "learning_rate": 6.270069291870881e-06,
      "loss": 0.0184,
      "step": 21200
    },
    {
      "epoch": 3.5104270109235354,
      "grad_norm": 0.19182515144348145,
      "learning_rate": 6.24894372148048e-06,
      "loss": 0.0995,
      "step": 21210
    },
    {
      "epoch": 3.512082092022509,
      "grad_norm": 0.023411838337779045,
      "learning_rate": 6.22781815109008e-06,
      "loss": 0.2057,
      "step": 21220
    },
    {
      "epoch": 3.513737173121483,
      "grad_norm": 40.295310974121094,
      "learning_rate": 6.20669258069968e-06,
      "loss": 0.0304,
      "step": 21230
    },
    {
      "epoch": 3.5153922542204565,
      "grad_norm": 0.18805526196956635,
      "learning_rate": 6.185567010309279e-06,
      "loss": 0.3548,
      "step": 21240
    },
    {
      "epoch": 3.5170473353194307,
      "grad_norm": 0.1434120535850525,
      "learning_rate": 6.164441439918878e-06,
      "loss": 0.0149,
      "step": 21250
    },
    {
      "epoch": 3.5187024164184044,
      "grad_norm": 45.624027252197266,
      "learning_rate": 6.143315869528478e-06,
      "loss": 0.3545,
      "step": 21260
    },
    {
      "epoch": 3.520357497517378,
      "grad_norm": 0.300959974527359,
      "learning_rate": 6.122190299138077e-06,
      "loss": 0.0755,
      "step": 21270
    },
    {
      "epoch": 3.5220125786163523,
      "grad_norm": 0.16520895063877106,
      "learning_rate": 6.101064728747676e-06,
      "loss": 0.17,
      "step": 21280
    },
    {
      "epoch": 3.523667659715326,
      "grad_norm": 3.3127553462982178,
      "learning_rate": 6.079939158357276e-06,
      "loss": 0.1764,
      "step": 21290
    },
    {
      "epoch": 3.5253227408142997,
      "grad_norm": 71.96284484863281,
      "learning_rate": 6.058813587966875e-06,
      "loss": 0.1474,
      "step": 21300
    },
    {
      "epoch": 3.526977821913274,
      "grad_norm": 4.08814001083374,
      "learning_rate": 6.037688017576474e-06,
      "loss": 0.06,
      "step": 21310
    },
    {
      "epoch": 3.5286329030122476,
      "grad_norm": 24.554500579833984,
      "learning_rate": 6.0165624471860745e-06,
      "loss": 0.3295,
      "step": 21320
    },
    {
      "epoch": 3.5302879841112214,
      "grad_norm": 0.12155169993638992,
      "learning_rate": 5.995436876795674e-06,
      "loss": 0.1468,
      "step": 21330
    },
    {
      "epoch": 3.5319430652101955,
      "grad_norm": 0.18928368389606476,
      "learning_rate": 5.974311306405273e-06,
      "loss": 0.119,
      "step": 21340
    },
    {
      "epoch": 3.5335981463091692,
      "grad_norm": 111.4303207397461,
      "learning_rate": 5.953185736014873e-06,
      "loss": 0.1482,
      "step": 21350
    },
    {
      "epoch": 3.535253227408143,
      "grad_norm": 76.68241882324219,
      "learning_rate": 5.932060165624472e-06,
      "loss": 0.0979,
      "step": 21360
    },
    {
      "epoch": 3.5369083085071167,
      "grad_norm": 0.021518299356102943,
      "learning_rate": 5.9109345952340715e-06,
      "loss": 0.1403,
      "step": 21370
    },
    {
      "epoch": 3.538563389606091,
      "grad_norm": 0.030538015067577362,
      "learning_rate": 5.8898090248436715e-06,
      "loss": 0.0345,
      "step": 21380
    },
    {
      "epoch": 3.5402184707050646,
      "grad_norm": 9.611040115356445,
      "learning_rate": 5.868683454453271e-06,
      "loss": 0.3322,
      "step": 21390
    },
    {
      "epoch": 3.5418735518040383,
      "grad_norm": 144.5501708984375,
      "learning_rate": 5.84755788406287e-06,
      "loss": 0.2343,
      "step": 21400
    },
    {
      "epoch": 3.543528632903012,
      "grad_norm": 8.551639556884766,
      "learning_rate": 5.82643231367247e-06,
      "loss": 0.2778,
      "step": 21410
    },
    {
      "epoch": 3.545183714001986,
      "grad_norm": 52.935054779052734,
      "learning_rate": 5.805306743282069e-06,
      "loss": 0.1422,
      "step": 21420
    },
    {
      "epoch": 3.54683879510096,
      "grad_norm": 28.30804443359375,
      "learning_rate": 5.7841811728916685e-06,
      "loss": 0.2426,
      "step": 21430
    },
    {
      "epoch": 3.5484938761999336,
      "grad_norm": 0.17182916402816772,
      "learning_rate": 5.763055602501268e-06,
      "loss": 0.0028,
      "step": 21440
    },
    {
      "epoch": 3.550148957298908,
      "grad_norm": 85.15969848632812,
      "learning_rate": 5.741930032110867e-06,
      "loss": 0.2769,
      "step": 21450
    },
    {
      "epoch": 3.5518040383978815,
      "grad_norm": 3.483741283416748,
      "learning_rate": 5.720804461720466e-06,
      "loss": 0.1011,
      "step": 21460
    },
    {
      "epoch": 3.5534591194968552,
      "grad_norm": 9.9072904586792,
      "learning_rate": 5.699678891330066e-06,
      "loss": 0.2834,
      "step": 21470
    },
    {
      "epoch": 3.5551142005958294,
      "grad_norm": 0.1402999609708786,
      "learning_rate": 5.6785533209396655e-06,
      "loss": 0.108,
      "step": 21480
    },
    {
      "epoch": 3.556769281694803,
      "grad_norm": 0.028680823743343353,
      "learning_rate": 5.657427750549265e-06,
      "loss": 0.0398,
      "step": 21490
    },
    {
      "epoch": 3.558424362793777,
      "grad_norm": 30.660091400146484,
      "learning_rate": 5.636302180158865e-06,
      "loss": 0.1651,
      "step": 21500
    },
    {
      "epoch": 3.560079443892751,
      "grad_norm": 0.017705729231238365,
      "learning_rate": 5.615176609768464e-06,
      "loss": 0.1507,
      "step": 21510
    },
    {
      "epoch": 3.5617345249917247,
      "grad_norm": 64.42254638671875,
      "learning_rate": 5.594051039378063e-06,
      "loss": 0.2137,
      "step": 21520
    },
    {
      "epoch": 3.5633896060906984,
      "grad_norm": 15.048855781555176,
      "learning_rate": 5.572925468987663e-06,
      "loss": 0.1049,
      "step": 21530
    },
    {
      "epoch": 3.565044687189672,
      "grad_norm": 70.76481628417969,
      "learning_rate": 5.551799898597263e-06,
      "loss": 0.3132,
      "step": 21540
    },
    {
      "epoch": 3.566699768288646,
      "grad_norm": 4.597249984741211,
      "learning_rate": 5.530674328206862e-06,
      "loss": 0.0609,
      "step": 21550
    },
    {
      "epoch": 3.56835484938762,
      "grad_norm": 0.18628208339214325,
      "learning_rate": 5.509548757816461e-06,
      "loss": 0.1455,
      "step": 21560
    },
    {
      "epoch": 3.5700099304865938,
      "grad_norm": 12.707444190979004,
      "learning_rate": 5.488423187426061e-06,
      "loss": 0.0289,
      "step": 21570
    },
    {
      "epoch": 3.5716650115855675,
      "grad_norm": 0.5682754516601562,
      "learning_rate": 5.46729761703566e-06,
      "loss": 0.002,
      "step": 21580
    },
    {
      "epoch": 3.5733200926845416,
      "grad_norm": 0.6094778180122375,
      "learning_rate": 5.44617204664526e-06,
      "loss": 0.058,
      "step": 21590
    },
    {
      "epoch": 3.5749751737835154,
      "grad_norm": 0.03675151988863945,
      "learning_rate": 5.42504647625486e-06,
      "loss": 0.0491,
      "step": 21600
    },
    {
      "epoch": 3.576630254882489,
      "grad_norm": 0.31020817160606384,
      "learning_rate": 5.403920905864458e-06,
      "loss": 0.105,
      "step": 21610
    },
    {
      "epoch": 3.5782853359814633,
      "grad_norm": 0.0872570127248764,
      "learning_rate": 5.382795335474058e-06,
      "loss": 0.2737,
      "step": 21620
    },
    {
      "epoch": 3.579940417080437,
      "grad_norm": 0.9542826414108276,
      "learning_rate": 5.361669765083657e-06,
      "loss": 0.023,
      "step": 21630
    },
    {
      "epoch": 3.5815954981794107,
      "grad_norm": 2.1802597045898438,
      "learning_rate": 5.340544194693257e-06,
      "loss": 0.1576,
      "step": 21640
    },
    {
      "epoch": 3.583250579278385,
      "grad_norm": 0.4694560170173645,
      "learning_rate": 5.319418624302856e-06,
      "loss": 0.093,
      "step": 21650
    },
    {
      "epoch": 3.5849056603773586,
      "grad_norm": 68.8100814819336,
      "learning_rate": 5.298293053912456e-06,
      "loss": 0.1406,
      "step": 21660
    },
    {
      "epoch": 3.5865607414763323,
      "grad_norm": 14.791501998901367,
      "learning_rate": 5.277167483522055e-06,
      "loss": 0.2065,
      "step": 21670
    },
    {
      "epoch": 3.588215822575306,
      "grad_norm": 0.39877650141716003,
      "learning_rate": 5.256041913131654e-06,
      "loss": 0.0898,
      "step": 21680
    },
    {
      "epoch": 3.58987090367428,
      "grad_norm": 0.19272306561470032,
      "learning_rate": 5.2349163427412545e-06,
      "loss": 0.0655,
      "step": 21690
    },
    {
      "epoch": 3.591525984773254,
      "grad_norm": 0.05347701162099838,
      "learning_rate": 5.213790772350854e-06,
      "loss": 0.0282,
      "step": 21700
    },
    {
      "epoch": 3.5931810658722276,
      "grad_norm": 0.007385142147541046,
      "learning_rate": 5.192665201960453e-06,
      "loss": 0.1015,
      "step": 21710
    },
    {
      "epoch": 3.5948361469712014,
      "grad_norm": 109.6412353515625,
      "learning_rate": 5.171539631570053e-06,
      "loss": 0.1895,
      "step": 21720
    },
    {
      "epoch": 3.5964912280701755,
      "grad_norm": 78.62346649169922,
      "learning_rate": 5.150414061179652e-06,
      "loss": 0.2015,
      "step": 21730
    },
    {
      "epoch": 3.5981463091691492,
      "grad_norm": 0.005153576843440533,
      "learning_rate": 5.1292884907892515e-06,
      "loss": 0.1125,
      "step": 21740
    },
    {
      "epoch": 3.599801390268123,
      "grad_norm": 0.004254956729710102,
      "learning_rate": 5.1081629203988515e-06,
      "loss": 0.0286,
      "step": 21750
    }
  ],
  "logging_steps": 10,
  "max_steps": 24168,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 2417,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6160894723515392e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
