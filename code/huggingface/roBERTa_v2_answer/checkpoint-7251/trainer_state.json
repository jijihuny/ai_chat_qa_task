{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.2000993048659385,
  "eval_steps": 500,
  "global_step": 7251,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016550810989738498,
      "grad_norm": 97.4415512084961,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 3.2653,
      "step": 10
    },
    {
      "epoch": 0.0033101621979476996,
      "grad_norm": 121.34355926513672,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 3.5017,
      "step": 20
    },
    {
      "epoch": 0.004965243296921549,
      "grad_norm": 48.178489685058594,
      "learning_rate": 3e-06,
      "loss": 2.8151,
      "step": 30
    },
    {
      "epoch": 0.006620324395895399,
      "grad_norm": 50.83437728881836,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.6973,
      "step": 40
    },
    {
      "epoch": 0.00827540549486925,
      "grad_norm": 39.91733169555664,
      "learning_rate": 5e-06,
      "loss": 1.5072,
      "step": 50
    },
    {
      "epoch": 0.009930486593843098,
      "grad_norm": 33.2450065612793,
      "learning_rate": 6e-06,
      "loss": 1.8742,
      "step": 60
    },
    {
      "epoch": 0.011585567692816948,
      "grad_norm": 32.60800552368164,
      "learning_rate": 7.000000000000001e-06,
      "loss": 1.6284,
      "step": 70
    },
    {
      "epoch": 0.013240648791790799,
      "grad_norm": 34.24906539916992,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.1873,
      "step": 80
    },
    {
      "epoch": 0.014895729890764648,
      "grad_norm": 50.064056396484375,
      "learning_rate": 9e-06,
      "loss": 1.1691,
      "step": 90
    },
    {
      "epoch": 0.0165508109897385,
      "grad_norm": 47.91412353515625,
      "learning_rate": 1e-05,
      "loss": 1.0641,
      "step": 100
    },
    {
      "epoch": 0.018205892088712348,
      "grad_norm": 43.95795822143555,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.0573,
      "step": 110
    },
    {
      "epoch": 0.019860973187686197,
      "grad_norm": 42.00776672363281,
      "learning_rate": 1.2e-05,
      "loss": 1.0241,
      "step": 120
    },
    {
      "epoch": 0.021516054286660046,
      "grad_norm": 28.95277214050293,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.17,
      "step": 130
    },
    {
      "epoch": 0.023171135385633895,
      "grad_norm": 17.930635452270508,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.0029,
      "step": 140
    },
    {
      "epoch": 0.024826216484607744,
      "grad_norm": 10.3660249710083,
      "learning_rate": 1.5e-05,
      "loss": 1.1087,
      "step": 150
    },
    {
      "epoch": 0.026481297583581597,
      "grad_norm": 40.463050842285156,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.9597,
      "step": 160
    },
    {
      "epoch": 0.028136378682555446,
      "grad_norm": 23.88897132873535,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 1.0799,
      "step": 170
    },
    {
      "epoch": 0.029791459781529295,
      "grad_norm": 23.142667770385742,
      "learning_rate": 1.8e-05,
      "loss": 1.0207,
      "step": 180
    },
    {
      "epoch": 0.031446540880503145,
      "grad_norm": 41.67078399658203,
      "learning_rate": 1.9e-05,
      "loss": 1.2081,
      "step": 190
    },
    {
      "epoch": 0.033101621979477,
      "grad_norm": 28.011249542236328,
      "learning_rate": 2e-05,
      "loss": 1.0128,
      "step": 200
    },
    {
      "epoch": 0.03475670307845084,
      "grad_norm": 22.017784118652344,
      "learning_rate": 2.1e-05,
      "loss": 0.9923,
      "step": 210
    },
    {
      "epoch": 0.036411784177424696,
      "grad_norm": 34.73788070678711,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.8604,
      "step": 220
    },
    {
      "epoch": 0.03806686527639854,
      "grad_norm": 27.730981826782227,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.8893,
      "step": 230
    },
    {
      "epoch": 0.039721946375372394,
      "grad_norm": 16.18294906616211,
      "learning_rate": 2.4e-05,
      "loss": 1.2433,
      "step": 240
    },
    {
      "epoch": 0.04137702747434624,
      "grad_norm": 36.50089645385742,
      "learning_rate": 2.5e-05,
      "loss": 0.9493,
      "step": 250
    },
    {
      "epoch": 0.04303210857332009,
      "grad_norm": 34.64004898071289,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.3145,
      "step": 260
    },
    {
      "epoch": 0.044687189672293945,
      "grad_norm": 35.67694854736328,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.12,
      "step": 270
    },
    {
      "epoch": 0.04634227077126779,
      "grad_norm": 14.698933601379395,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.9535,
      "step": 280
    },
    {
      "epoch": 0.04799735187024164,
      "grad_norm": 81.7050552368164,
      "learning_rate": 2.9e-05,
      "loss": 0.8433,
      "step": 290
    },
    {
      "epoch": 0.04965243296921549,
      "grad_norm": 29.671131134033203,
      "learning_rate": 3e-05,
      "loss": 1.5111,
      "step": 300
    },
    {
      "epoch": 0.05130751406818934,
      "grad_norm": 54.395599365234375,
      "learning_rate": 3.1e-05,
      "loss": 1.3558,
      "step": 310
    },
    {
      "epoch": 0.052962595167163194,
      "grad_norm": 24.62464141845703,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.4381,
      "step": 320
    },
    {
      "epoch": 0.05461767626613704,
      "grad_norm": 38.02064514160156,
      "learning_rate": 3.3e-05,
      "loss": 0.9403,
      "step": 330
    },
    {
      "epoch": 0.05627275736511089,
      "grad_norm": 44.62522506713867,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.9171,
      "step": 340
    },
    {
      "epoch": 0.05792783846408474,
      "grad_norm": 14.072800636291504,
      "learning_rate": 3.5e-05,
      "loss": 1.0232,
      "step": 350
    },
    {
      "epoch": 0.05958291956305859,
      "grad_norm": 14.406139373779297,
      "learning_rate": 3.6e-05,
      "loss": 0.7947,
      "step": 360
    },
    {
      "epoch": 0.06123800066203244,
      "grad_norm": 24.86500358581543,
      "learning_rate": 3.7e-05,
      "loss": 1.4377,
      "step": 370
    },
    {
      "epoch": 0.06289308176100629,
      "grad_norm": 18.5371036529541,
      "learning_rate": 3.8e-05,
      "loss": 0.9852,
      "step": 380
    },
    {
      "epoch": 0.06454816285998013,
      "grad_norm": 35.14235305786133,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.4349,
      "step": 390
    },
    {
      "epoch": 0.066203243958954,
      "grad_norm": 20.02771759033203,
      "learning_rate": 4e-05,
      "loss": 1.3746,
      "step": 400
    },
    {
      "epoch": 0.06785832505792784,
      "grad_norm": 28.747251510620117,
      "learning_rate": 4.1e-05,
      "loss": 1.3989,
      "step": 410
    },
    {
      "epoch": 0.06951340615690169,
      "grad_norm": 23.673721313476562,
      "learning_rate": 4.2e-05,
      "loss": 1.2095,
      "step": 420
    },
    {
      "epoch": 0.07116848725587553,
      "grad_norm": 3.3561432361602783,
      "learning_rate": 4.3e-05,
      "loss": 1.0675,
      "step": 430
    },
    {
      "epoch": 0.07282356835484939,
      "grad_norm": 51.90268325805664,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.0865,
      "step": 440
    },
    {
      "epoch": 0.07447864945382324,
      "grad_norm": 34.15875244140625,
      "learning_rate": 4.5e-05,
      "loss": 1.5736,
      "step": 450
    },
    {
      "epoch": 0.07613373055279708,
      "grad_norm": 26.975828170776367,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.2819,
      "step": 460
    },
    {
      "epoch": 0.07778881165177094,
      "grad_norm": 27.665348052978516,
      "learning_rate": 4.7e-05,
      "loss": 0.6944,
      "step": 470
    },
    {
      "epoch": 0.07944389275074479,
      "grad_norm": 29.756916046142578,
      "learning_rate": 4.8e-05,
      "loss": 1.8494,
      "step": 480
    },
    {
      "epoch": 0.08109897384971863,
      "grad_norm": 55.00706481933594,
      "learning_rate": 4.9e-05,
      "loss": 1.2432,
      "step": 490
    },
    {
      "epoch": 0.08275405494869248,
      "grad_norm": 45.52133560180664,
      "learning_rate": 5e-05,
      "loss": 1.1219,
      "step": 500
    },
    {
      "epoch": 0.08440913604766634,
      "grad_norm": 27.663555145263672,
      "learning_rate": 4.99788744296096e-05,
      "loss": 1.0477,
      "step": 510
    },
    {
      "epoch": 0.08606421714664018,
      "grad_norm": 32.310359954833984,
      "learning_rate": 4.99577488592192e-05,
      "loss": 1.1151,
      "step": 520
    },
    {
      "epoch": 0.08771929824561403,
      "grad_norm": 37.946659088134766,
      "learning_rate": 4.99366232888288e-05,
      "loss": 1.2722,
      "step": 530
    },
    {
      "epoch": 0.08937437934458789,
      "grad_norm": 27.8206729888916,
      "learning_rate": 4.9915497718438396e-05,
      "loss": 1.3977,
      "step": 540
    },
    {
      "epoch": 0.09102946044356174,
      "grad_norm": 22.341928482055664,
      "learning_rate": 4.9894372148048e-05,
      "loss": 1.1873,
      "step": 550
    },
    {
      "epoch": 0.09268454154253558,
      "grad_norm": 20.493959426879883,
      "learning_rate": 4.98732465776576e-05,
      "loss": 0.9783,
      "step": 560
    },
    {
      "epoch": 0.09433962264150944,
      "grad_norm": 36.496646881103516,
      "learning_rate": 4.98521210072672e-05,
      "loss": 1.2776,
      "step": 570
    },
    {
      "epoch": 0.09599470374048329,
      "grad_norm": 44.31249237060547,
      "learning_rate": 4.9830995436876796e-05,
      "loss": 1.2635,
      "step": 580
    },
    {
      "epoch": 0.09764978483945713,
      "grad_norm": 35.21307373046875,
      "learning_rate": 4.9809869866486395e-05,
      "loss": 1.4967,
      "step": 590
    },
    {
      "epoch": 0.09930486593843098,
      "grad_norm": 36.36589431762695,
      "learning_rate": 4.978874429609599e-05,
      "loss": 1.3594,
      "step": 600
    },
    {
      "epoch": 0.10095994703740484,
      "grad_norm": 24.680246353149414,
      "learning_rate": 4.97676187257056e-05,
      "loss": 1.499,
      "step": 610
    },
    {
      "epoch": 0.10261502813637868,
      "grad_norm": 30.076444625854492,
      "learning_rate": 4.9746493155315197e-05,
      "loss": 1.0537,
      "step": 620
    },
    {
      "epoch": 0.10427010923535253,
      "grad_norm": 18.924583435058594,
      "learning_rate": 4.9725367584924795e-05,
      "loss": 1.0623,
      "step": 630
    },
    {
      "epoch": 0.10592519033432639,
      "grad_norm": 35.29458999633789,
      "learning_rate": 4.97042420145344e-05,
      "loss": 1.3919,
      "step": 640
    },
    {
      "epoch": 0.10758027143330023,
      "grad_norm": 26.997940063476562,
      "learning_rate": 4.9683116444144e-05,
      "loss": 1.2173,
      "step": 650
    },
    {
      "epoch": 0.10923535253227408,
      "grad_norm": 23.493989944458008,
      "learning_rate": 4.96619908737536e-05,
      "loss": 1.2526,
      "step": 660
    },
    {
      "epoch": 0.11089043363124793,
      "grad_norm": 21.540573120117188,
      "learning_rate": 4.9640865303363195e-05,
      "loss": 1.2347,
      "step": 670
    },
    {
      "epoch": 0.11254551473022179,
      "grad_norm": 54.90599060058594,
      "learning_rate": 4.9619739732972794e-05,
      "loss": 1.2998,
      "step": 680
    },
    {
      "epoch": 0.11420059582919563,
      "grad_norm": 17.971019744873047,
      "learning_rate": 4.959861416258239e-05,
      "loss": 1.225,
      "step": 690
    },
    {
      "epoch": 0.11585567692816948,
      "grad_norm": 23.759201049804688,
      "learning_rate": 4.957748859219199e-05,
      "loss": 1.1704,
      "step": 700
    },
    {
      "epoch": 0.11751075802714334,
      "grad_norm": 18.30558967590332,
      "learning_rate": 4.955636302180159e-05,
      "loss": 1.1507,
      "step": 710
    },
    {
      "epoch": 0.11916583912611718,
      "grad_norm": 41.52193832397461,
      "learning_rate": 4.9535237451411194e-05,
      "loss": 1.0947,
      "step": 720
    },
    {
      "epoch": 0.12082092022509103,
      "grad_norm": 18.925830841064453,
      "learning_rate": 4.951411188102079e-05,
      "loss": 1.5544,
      "step": 730
    },
    {
      "epoch": 0.12247600132406487,
      "grad_norm": 58.37614059448242,
      "learning_rate": 4.949298631063039e-05,
      "loss": 1.3196,
      "step": 740
    },
    {
      "epoch": 0.12413108242303873,
      "grad_norm": 20.034914016723633,
      "learning_rate": 4.947186074023999e-05,
      "loss": 1.1119,
      "step": 750
    },
    {
      "epoch": 0.12578616352201258,
      "grad_norm": 42.30554962158203,
      "learning_rate": 4.945073516984959e-05,
      "loss": 0.9168,
      "step": 760
    },
    {
      "epoch": 0.12744124462098644,
      "grad_norm": 50.14617156982422,
      "learning_rate": 4.9429609599459186e-05,
      "loss": 1.0796,
      "step": 770
    },
    {
      "epoch": 0.12909632571996027,
      "grad_norm": 69.02271270751953,
      "learning_rate": 4.9408484029068784e-05,
      "loss": 1.2964,
      "step": 780
    },
    {
      "epoch": 0.13075140681893413,
      "grad_norm": 33.62277603149414,
      "learning_rate": 4.938735845867838e-05,
      "loss": 1.3134,
      "step": 790
    },
    {
      "epoch": 0.132406487917908,
      "grad_norm": 83.8985595703125,
      "learning_rate": 4.936623288828799e-05,
      "loss": 1.36,
      "step": 800
    },
    {
      "epoch": 0.13406156901688182,
      "grad_norm": 28.49913787841797,
      "learning_rate": 4.9345107317897586e-05,
      "loss": 1.2215,
      "step": 810
    },
    {
      "epoch": 0.13571665011585568,
      "grad_norm": 44.60554122924805,
      "learning_rate": 4.9323981747507185e-05,
      "loss": 1.2679,
      "step": 820
    },
    {
      "epoch": 0.13737173121482954,
      "grad_norm": 57.60272216796875,
      "learning_rate": 4.930285617711678e-05,
      "loss": 1.0022,
      "step": 830
    },
    {
      "epoch": 0.13902681231380337,
      "grad_norm": 13.666054725646973,
      "learning_rate": 4.928173060672638e-05,
      "loss": 0.9676,
      "step": 840
    },
    {
      "epoch": 0.14068189341277723,
      "grad_norm": 17.73228645324707,
      "learning_rate": 4.926060503633598e-05,
      "loss": 1.0089,
      "step": 850
    },
    {
      "epoch": 0.14233697451175106,
      "grad_norm": 38.72350311279297,
      "learning_rate": 4.9239479465945585e-05,
      "loss": 1.1762,
      "step": 860
    },
    {
      "epoch": 0.14399205561072492,
      "grad_norm": 48.70363235473633,
      "learning_rate": 4.921835389555518e-05,
      "loss": 1.4009,
      "step": 870
    },
    {
      "epoch": 0.14564713670969878,
      "grad_norm": 50.8752555847168,
      "learning_rate": 4.919722832516478e-05,
      "loss": 1.0995,
      "step": 880
    },
    {
      "epoch": 0.14730221780867261,
      "grad_norm": 44.483604431152344,
      "learning_rate": 4.917610275477439e-05,
      "loss": 1.1234,
      "step": 890
    },
    {
      "epoch": 0.14895729890764647,
      "grad_norm": 29.918901443481445,
      "learning_rate": 4.9154977184383985e-05,
      "loss": 1.0949,
      "step": 900
    },
    {
      "epoch": 0.15061238000662033,
      "grad_norm": 18.013593673706055,
      "learning_rate": 4.9133851613993584e-05,
      "loss": 1.0986,
      "step": 910
    },
    {
      "epoch": 0.15226746110559417,
      "grad_norm": 17.03639030456543,
      "learning_rate": 4.911272604360318e-05,
      "loss": 1.0965,
      "step": 920
    },
    {
      "epoch": 0.15392254220456802,
      "grad_norm": 32.880088806152344,
      "learning_rate": 4.909160047321278e-05,
      "loss": 1.1738,
      "step": 930
    },
    {
      "epoch": 0.15557762330354188,
      "grad_norm": 17.56507110595703,
      "learning_rate": 4.907047490282238e-05,
      "loss": 1.2153,
      "step": 940
    },
    {
      "epoch": 0.15723270440251572,
      "grad_norm": 26.05997085571289,
      "learning_rate": 4.904934933243198e-05,
      "loss": 0.9936,
      "step": 950
    },
    {
      "epoch": 0.15888778550148958,
      "grad_norm": 38.58293914794922,
      "learning_rate": 4.9028223762041575e-05,
      "loss": 1.39,
      "step": 960
    },
    {
      "epoch": 0.16054286660046344,
      "grad_norm": 12.222036361694336,
      "learning_rate": 4.900709819165118e-05,
      "loss": 1.0561,
      "step": 970
    },
    {
      "epoch": 0.16219794769943727,
      "grad_norm": 24.692991256713867,
      "learning_rate": 4.898597262126078e-05,
      "loss": 1.3424,
      "step": 980
    },
    {
      "epoch": 0.16385302879841113,
      "grad_norm": 67.37031555175781,
      "learning_rate": 4.896484705087038e-05,
      "loss": 1.1594,
      "step": 990
    },
    {
      "epoch": 0.16550810989738496,
      "grad_norm": 23.87232780456543,
      "learning_rate": 4.8943721480479976e-05,
      "loss": 1.191,
      "step": 1000
    },
    {
      "epoch": 0.16716319099635882,
      "grad_norm": 21.083709716796875,
      "learning_rate": 4.8922595910089574e-05,
      "loss": 1.1213,
      "step": 1010
    },
    {
      "epoch": 0.16881827209533268,
      "grad_norm": 21.680368423461914,
      "learning_rate": 4.890147033969917e-05,
      "loss": 1.0167,
      "step": 1020
    },
    {
      "epoch": 0.1704733531943065,
      "grad_norm": 18.98822593688965,
      "learning_rate": 4.888034476930877e-05,
      "loss": 1.2131,
      "step": 1030
    },
    {
      "epoch": 0.17212843429328037,
      "grad_norm": 40.81112289428711,
      "learning_rate": 4.885921919891837e-05,
      "loss": 1.0608,
      "step": 1040
    },
    {
      "epoch": 0.17378351539225423,
      "grad_norm": 27.607023239135742,
      "learning_rate": 4.883809362852797e-05,
      "loss": 1.4412,
      "step": 1050
    },
    {
      "epoch": 0.17543859649122806,
      "grad_norm": 42.07017517089844,
      "learning_rate": 4.881696805813757e-05,
      "loss": 1.1374,
      "step": 1060
    },
    {
      "epoch": 0.17709367759020192,
      "grad_norm": 35.417564392089844,
      "learning_rate": 4.879584248774717e-05,
      "loss": 1.3788,
      "step": 1070
    },
    {
      "epoch": 0.17874875868917578,
      "grad_norm": 33.0713005065918,
      "learning_rate": 4.877471691735677e-05,
      "loss": 1.1659,
      "step": 1080
    },
    {
      "epoch": 0.1804038397881496,
      "grad_norm": 32.4471549987793,
      "learning_rate": 4.875359134696637e-05,
      "loss": 1.0727,
      "step": 1090
    },
    {
      "epoch": 0.18205892088712347,
      "grad_norm": 38.011470794677734,
      "learning_rate": 4.8732465776575966e-05,
      "loss": 0.9315,
      "step": 1100
    },
    {
      "epoch": 0.18371400198609733,
      "grad_norm": 19.309675216674805,
      "learning_rate": 4.871134020618557e-05,
      "loss": 1.5556,
      "step": 1110
    },
    {
      "epoch": 0.18536908308507116,
      "grad_norm": 60.10500717163086,
      "learning_rate": 4.869021463579517e-05,
      "loss": 0.6319,
      "step": 1120
    },
    {
      "epoch": 0.18702416418404502,
      "grad_norm": 13.00697135925293,
      "learning_rate": 4.866908906540477e-05,
      "loss": 1.4658,
      "step": 1130
    },
    {
      "epoch": 0.18867924528301888,
      "grad_norm": 25.067235946655273,
      "learning_rate": 4.8647963495014373e-05,
      "loss": 1.3293,
      "step": 1140
    },
    {
      "epoch": 0.1903343263819927,
      "grad_norm": 14.16656494140625,
      "learning_rate": 4.862683792462397e-05,
      "loss": 0.7232,
      "step": 1150
    },
    {
      "epoch": 0.19198940748096657,
      "grad_norm": 31.454057693481445,
      "learning_rate": 4.860571235423357e-05,
      "loss": 1.3936,
      "step": 1160
    },
    {
      "epoch": 0.1936444885799404,
      "grad_norm": 45.0728874206543,
      "learning_rate": 4.858458678384317e-05,
      "loss": 1.3844,
      "step": 1170
    },
    {
      "epoch": 0.19529956967891426,
      "grad_norm": 31.794769287109375,
      "learning_rate": 4.856346121345277e-05,
      "loss": 1.0689,
      "step": 1180
    },
    {
      "epoch": 0.19695465077788812,
      "grad_norm": 21.14841651916504,
      "learning_rate": 4.8542335643062365e-05,
      "loss": 1.4044,
      "step": 1190
    },
    {
      "epoch": 0.19860973187686196,
      "grad_norm": 45.91816329956055,
      "learning_rate": 4.8521210072671964e-05,
      "loss": 1.1661,
      "step": 1200
    },
    {
      "epoch": 0.20026481297583582,
      "grad_norm": 18.18951988220215,
      "learning_rate": 4.850008450228156e-05,
      "loss": 1.0786,
      "step": 1210
    },
    {
      "epoch": 0.20191989407480967,
      "grad_norm": 36.60145568847656,
      "learning_rate": 4.847895893189116e-05,
      "loss": 1.443,
      "step": 1220
    },
    {
      "epoch": 0.2035749751737835,
      "grad_norm": 36.871280670166016,
      "learning_rate": 4.8457833361500766e-05,
      "loss": 1.3718,
      "step": 1230
    },
    {
      "epoch": 0.20523005627275737,
      "grad_norm": 18.16054916381836,
      "learning_rate": 4.8436707791110364e-05,
      "loss": 0.9796,
      "step": 1240
    },
    {
      "epoch": 0.20688513737173123,
      "grad_norm": 44.73019790649414,
      "learning_rate": 4.841558222071996e-05,
      "loss": 1.0275,
      "step": 1250
    },
    {
      "epoch": 0.20854021847070506,
      "grad_norm": 17.73753547668457,
      "learning_rate": 4.839445665032956e-05,
      "loss": 0.7922,
      "step": 1260
    },
    {
      "epoch": 0.21019529956967892,
      "grad_norm": 42.6441650390625,
      "learning_rate": 4.837333107993916e-05,
      "loss": 1.4442,
      "step": 1270
    },
    {
      "epoch": 0.21185038066865278,
      "grad_norm": 24.02442741394043,
      "learning_rate": 4.835220550954876e-05,
      "loss": 1.0102,
      "step": 1280
    },
    {
      "epoch": 0.2135054617676266,
      "grad_norm": 48.59553146362305,
      "learning_rate": 4.8331079939158356e-05,
      "loss": 1.2905,
      "step": 1290
    },
    {
      "epoch": 0.21516054286660047,
      "grad_norm": 17.703311920166016,
      "learning_rate": 4.8309954368767954e-05,
      "loss": 0.8522,
      "step": 1300
    },
    {
      "epoch": 0.2168156239655743,
      "grad_norm": 64.39567565917969,
      "learning_rate": 4.828882879837756e-05,
      "loss": 1.1265,
      "step": 1310
    },
    {
      "epoch": 0.21847070506454816,
      "grad_norm": 23.48064613342285,
      "learning_rate": 4.826770322798716e-05,
      "loss": 1.3915,
      "step": 1320
    },
    {
      "epoch": 0.22012578616352202,
      "grad_norm": 41.70143127441406,
      "learning_rate": 4.8246577657596756e-05,
      "loss": 1.3118,
      "step": 1330
    },
    {
      "epoch": 0.22178086726249585,
      "grad_norm": 69.24468994140625,
      "learning_rate": 4.8225452087206355e-05,
      "loss": 0.8991,
      "step": 1340
    },
    {
      "epoch": 0.2234359483614697,
      "grad_norm": 43.01826477050781,
      "learning_rate": 4.820432651681595e-05,
      "loss": 0.8805,
      "step": 1350
    },
    {
      "epoch": 0.22509102946044357,
      "grad_norm": 47.81785202026367,
      "learning_rate": 4.818320094642556e-05,
      "loss": 0.8368,
      "step": 1360
    },
    {
      "epoch": 0.2267461105594174,
      "grad_norm": 30.941341400146484,
      "learning_rate": 4.816207537603516e-05,
      "loss": 1.5406,
      "step": 1370
    },
    {
      "epoch": 0.22840119165839126,
      "grad_norm": 39.646812438964844,
      "learning_rate": 4.8140949805644755e-05,
      "loss": 0.9361,
      "step": 1380
    },
    {
      "epoch": 0.23005627275736512,
      "grad_norm": 13.443039894104004,
      "learning_rate": 4.811982423525435e-05,
      "loss": 1.2759,
      "step": 1390
    },
    {
      "epoch": 0.23171135385633895,
      "grad_norm": 26.6287899017334,
      "learning_rate": 4.809869866486396e-05,
      "loss": 1.072,
      "step": 1400
    },
    {
      "epoch": 0.2333664349553128,
      "grad_norm": 63.24628448486328,
      "learning_rate": 4.807757309447356e-05,
      "loss": 1.4455,
      "step": 1410
    },
    {
      "epoch": 0.23502151605428667,
      "grad_norm": 24.016357421875,
      "learning_rate": 4.8056447524083155e-05,
      "loss": 1.3908,
      "step": 1420
    },
    {
      "epoch": 0.2366765971532605,
      "grad_norm": 12.46517276763916,
      "learning_rate": 4.8035321953692754e-05,
      "loss": 0.7688,
      "step": 1430
    },
    {
      "epoch": 0.23833167825223436,
      "grad_norm": 10.827034950256348,
      "learning_rate": 4.801419638330235e-05,
      "loss": 1.0949,
      "step": 1440
    },
    {
      "epoch": 0.2399867593512082,
      "grad_norm": 136.52789306640625,
      "learning_rate": 4.799307081291195e-05,
      "loss": 0.7759,
      "step": 1450
    },
    {
      "epoch": 0.24164184045018205,
      "grad_norm": 35.291507720947266,
      "learning_rate": 4.797194524252155e-05,
      "loss": 1.319,
      "step": 1460
    },
    {
      "epoch": 0.24329692154915591,
      "grad_norm": 25.31490135192871,
      "learning_rate": 4.795081967213115e-05,
      "loss": 1.2102,
      "step": 1470
    },
    {
      "epoch": 0.24495200264812975,
      "grad_norm": 5.311086177825928,
      "learning_rate": 4.792969410174075e-05,
      "loss": 0.8988,
      "step": 1480
    },
    {
      "epoch": 0.2466070837471036,
      "grad_norm": 28.51626968383789,
      "learning_rate": 4.790856853135035e-05,
      "loss": 1.0968,
      "step": 1490
    },
    {
      "epoch": 0.24826216484607747,
      "grad_norm": 54.17319869995117,
      "learning_rate": 4.788744296095995e-05,
      "loss": 1.0518,
      "step": 1500
    },
    {
      "epoch": 0.2499172459450513,
      "grad_norm": 26.058807373046875,
      "learning_rate": 4.786631739056955e-05,
      "loss": 1.3213,
      "step": 1510
    },
    {
      "epoch": 0.25157232704402516,
      "grad_norm": 23.86149787902832,
      "learning_rate": 4.7845191820179146e-05,
      "loss": 1.3633,
      "step": 1520
    },
    {
      "epoch": 0.253227408142999,
      "grad_norm": 36.164154052734375,
      "learning_rate": 4.7824066249788744e-05,
      "loss": 1.0884,
      "step": 1530
    },
    {
      "epoch": 0.2548824892419729,
      "grad_norm": 55.85585403442383,
      "learning_rate": 4.780294067939834e-05,
      "loss": 0.8493,
      "step": 1540
    },
    {
      "epoch": 0.2565375703409467,
      "grad_norm": 27.191936492919922,
      "learning_rate": 4.778181510900794e-05,
      "loss": 0.9305,
      "step": 1550
    },
    {
      "epoch": 0.25819265143992054,
      "grad_norm": 40.60062026977539,
      "learning_rate": 4.776068953861754e-05,
      "loss": 1.1499,
      "step": 1560
    },
    {
      "epoch": 0.2598477325388944,
      "grad_norm": 30.577640533447266,
      "learning_rate": 4.7739563968227145e-05,
      "loss": 1.0569,
      "step": 1570
    },
    {
      "epoch": 0.26150281363786826,
      "grad_norm": 18.682828903198242,
      "learning_rate": 4.771843839783674e-05,
      "loss": 0.8512,
      "step": 1580
    },
    {
      "epoch": 0.2631578947368421,
      "grad_norm": 17.841514587402344,
      "learning_rate": 4.769731282744634e-05,
      "loss": 1.2974,
      "step": 1590
    },
    {
      "epoch": 0.264812975835816,
      "grad_norm": 28.501638412475586,
      "learning_rate": 4.767618725705594e-05,
      "loss": 1.141,
      "step": 1600
    },
    {
      "epoch": 0.2664680569347898,
      "grad_norm": 37.53023910522461,
      "learning_rate": 4.7655061686665545e-05,
      "loss": 0.7923,
      "step": 1610
    },
    {
      "epoch": 0.26812313803376364,
      "grad_norm": 22.709646224975586,
      "learning_rate": 4.763393611627514e-05,
      "loss": 1.319,
      "step": 1620
    },
    {
      "epoch": 0.26977821913273753,
      "grad_norm": 27.045103073120117,
      "learning_rate": 4.761281054588474e-05,
      "loss": 0.7483,
      "step": 1630
    },
    {
      "epoch": 0.27143330023171136,
      "grad_norm": 55.98433303833008,
      "learning_rate": 4.759168497549434e-05,
      "loss": 1.3818,
      "step": 1640
    },
    {
      "epoch": 0.2730883813306852,
      "grad_norm": 28.643022537231445,
      "learning_rate": 4.7570559405103945e-05,
      "loss": 1.173,
      "step": 1650
    },
    {
      "epoch": 0.2747434624296591,
      "grad_norm": 26.441133499145508,
      "learning_rate": 4.7549433834713544e-05,
      "loss": 1.2022,
      "step": 1660
    },
    {
      "epoch": 0.2763985435286329,
      "grad_norm": 46.60511016845703,
      "learning_rate": 4.752830826432314e-05,
      "loss": 1.0439,
      "step": 1670
    },
    {
      "epoch": 0.27805362462760674,
      "grad_norm": 20.532909393310547,
      "learning_rate": 4.750718269393274e-05,
      "loss": 0.997,
      "step": 1680
    },
    {
      "epoch": 0.2797087057265806,
      "grad_norm": 37.974918365478516,
      "learning_rate": 4.748605712354234e-05,
      "loss": 0.9507,
      "step": 1690
    },
    {
      "epoch": 0.28136378682555446,
      "grad_norm": 58.13558578491211,
      "learning_rate": 4.746493155315194e-05,
      "loss": 1.2208,
      "step": 1700
    },
    {
      "epoch": 0.2830188679245283,
      "grad_norm": 43.653099060058594,
      "learning_rate": 4.7443805982761536e-05,
      "loss": 1.4586,
      "step": 1710
    },
    {
      "epoch": 0.2846739490235021,
      "grad_norm": 11.706361770629883,
      "learning_rate": 4.7422680412371134e-05,
      "loss": 1.1155,
      "step": 1720
    },
    {
      "epoch": 0.286329030122476,
      "grad_norm": 24.717708587646484,
      "learning_rate": 4.740155484198073e-05,
      "loss": 1.1153,
      "step": 1730
    },
    {
      "epoch": 0.28798411122144985,
      "grad_norm": 28.47773551940918,
      "learning_rate": 4.738042927159034e-05,
      "loss": 1.3297,
      "step": 1740
    },
    {
      "epoch": 0.2896391923204237,
      "grad_norm": 55.19663619995117,
      "learning_rate": 4.7359303701199936e-05,
      "loss": 1.4315,
      "step": 1750
    },
    {
      "epoch": 0.29129427341939756,
      "grad_norm": 20.91109848022461,
      "learning_rate": 4.7338178130809534e-05,
      "loss": 1.0999,
      "step": 1760
    },
    {
      "epoch": 0.2929493545183714,
      "grad_norm": 26.083189010620117,
      "learning_rate": 4.731705256041913e-05,
      "loss": 1.0293,
      "step": 1770
    },
    {
      "epoch": 0.29460443561734523,
      "grad_norm": 42.47421646118164,
      "learning_rate": 4.729592699002873e-05,
      "loss": 1.2907,
      "step": 1780
    },
    {
      "epoch": 0.2962595167163191,
      "grad_norm": 24.587677001953125,
      "learning_rate": 4.727480141963833e-05,
      "loss": 1.0462,
      "step": 1790
    },
    {
      "epoch": 0.29791459781529295,
      "grad_norm": 54.55634307861328,
      "learning_rate": 4.725367584924793e-05,
      "loss": 0.9033,
      "step": 1800
    },
    {
      "epoch": 0.2995696789142668,
      "grad_norm": 24.767593383789062,
      "learning_rate": 4.7232550278857526e-05,
      "loss": 1.2028,
      "step": 1810
    },
    {
      "epoch": 0.30122476001324067,
      "grad_norm": 44.51886749267578,
      "learning_rate": 4.721142470846713e-05,
      "loss": 1.2678,
      "step": 1820
    },
    {
      "epoch": 0.3028798411122145,
      "grad_norm": 51.98725509643555,
      "learning_rate": 4.719029913807673e-05,
      "loss": 1.0257,
      "step": 1830
    },
    {
      "epoch": 0.30453492221118833,
      "grad_norm": 33.271419525146484,
      "learning_rate": 4.716917356768633e-05,
      "loss": 1.2286,
      "step": 1840
    },
    {
      "epoch": 0.3061900033101622,
      "grad_norm": 33.29391098022461,
      "learning_rate": 4.714804799729593e-05,
      "loss": 1.1303,
      "step": 1850
    },
    {
      "epoch": 0.30784508440913605,
      "grad_norm": 33.18928146362305,
      "learning_rate": 4.712692242690553e-05,
      "loss": 1.1277,
      "step": 1860
    },
    {
      "epoch": 0.3095001655081099,
      "grad_norm": 23.810184478759766,
      "learning_rate": 4.710579685651513e-05,
      "loss": 1.1335,
      "step": 1870
    },
    {
      "epoch": 0.31115524660708377,
      "grad_norm": 99.15519714355469,
      "learning_rate": 4.708467128612473e-05,
      "loss": 1.1959,
      "step": 1880
    },
    {
      "epoch": 0.3128103277060576,
      "grad_norm": 29.15110206604004,
      "learning_rate": 4.706354571573433e-05,
      "loss": 1.4197,
      "step": 1890
    },
    {
      "epoch": 0.31446540880503143,
      "grad_norm": 35.4796028137207,
      "learning_rate": 4.7042420145343925e-05,
      "loss": 1.0968,
      "step": 1900
    },
    {
      "epoch": 0.3161204899040053,
      "grad_norm": 33.30064010620117,
      "learning_rate": 4.702129457495353e-05,
      "loss": 1.0341,
      "step": 1910
    },
    {
      "epoch": 0.31777557100297915,
      "grad_norm": 33.59657287597656,
      "learning_rate": 4.700016900456313e-05,
      "loss": 1.1013,
      "step": 1920
    },
    {
      "epoch": 0.319430652101953,
      "grad_norm": 40.53507614135742,
      "learning_rate": 4.697904343417273e-05,
      "loss": 1.3462,
      "step": 1930
    },
    {
      "epoch": 0.32108573320092687,
      "grad_norm": 31.782255172729492,
      "learning_rate": 4.6957917863782325e-05,
      "loss": 1.1407,
      "step": 1940
    },
    {
      "epoch": 0.3227408142999007,
      "grad_norm": 21.611434936523438,
      "learning_rate": 4.6936792293391924e-05,
      "loss": 1.0156,
      "step": 1950
    },
    {
      "epoch": 0.32439589539887453,
      "grad_norm": 104.43136596679688,
      "learning_rate": 4.691566672300152e-05,
      "loss": 1.1035,
      "step": 1960
    },
    {
      "epoch": 0.3260509764978484,
      "grad_norm": 36.75350570678711,
      "learning_rate": 4.689454115261112e-05,
      "loss": 0.9366,
      "step": 1970
    },
    {
      "epoch": 0.32770605759682225,
      "grad_norm": 36.79549789428711,
      "learning_rate": 4.687341558222072e-05,
      "loss": 1.1395,
      "step": 1980
    },
    {
      "epoch": 0.3293611386957961,
      "grad_norm": 24.88180160522461,
      "learning_rate": 4.6852290011830324e-05,
      "loss": 0.856,
      "step": 1990
    },
    {
      "epoch": 0.3310162197947699,
      "grad_norm": 34.89110565185547,
      "learning_rate": 4.683116444143992e-05,
      "loss": 0.9102,
      "step": 2000
    },
    {
      "epoch": 0.3326713008937438,
      "grad_norm": 20.741331100463867,
      "learning_rate": 4.681003887104952e-05,
      "loss": 1.192,
      "step": 2010
    },
    {
      "epoch": 0.33432638199271764,
      "grad_norm": 36.0693244934082,
      "learning_rate": 4.678891330065912e-05,
      "loss": 1.1723,
      "step": 2020
    },
    {
      "epoch": 0.33598146309169147,
      "grad_norm": 69.0107650756836,
      "learning_rate": 4.676778773026872e-05,
      "loss": 0.9863,
      "step": 2030
    },
    {
      "epoch": 0.33763654419066536,
      "grad_norm": 29.610151290893555,
      "learning_rate": 4.6746662159878316e-05,
      "loss": 1.2898,
      "step": 2040
    },
    {
      "epoch": 0.3392916252896392,
      "grad_norm": 15.492105484008789,
      "learning_rate": 4.6725536589487914e-05,
      "loss": 1.0231,
      "step": 2050
    },
    {
      "epoch": 0.340946706388613,
      "grad_norm": 25.773880004882812,
      "learning_rate": 4.670441101909751e-05,
      "loss": 1.5425,
      "step": 2060
    },
    {
      "epoch": 0.3426017874875869,
      "grad_norm": 47.44260025024414,
      "learning_rate": 4.668328544870712e-05,
      "loss": 1.2476,
      "step": 2070
    },
    {
      "epoch": 0.34425686858656074,
      "grad_norm": 77.98101806640625,
      "learning_rate": 4.6662159878316716e-05,
      "loss": 1.378,
      "step": 2080
    },
    {
      "epoch": 0.34591194968553457,
      "grad_norm": 43.10260009765625,
      "learning_rate": 4.6641034307926315e-05,
      "loss": 1.2092,
      "step": 2090
    },
    {
      "epoch": 0.34756703078450846,
      "grad_norm": 38.76762771606445,
      "learning_rate": 4.661990873753592e-05,
      "loss": 1.0358,
      "step": 2100
    },
    {
      "epoch": 0.3492221118834823,
      "grad_norm": 27.373626708984375,
      "learning_rate": 4.659878316714552e-05,
      "loss": 0.8094,
      "step": 2110
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 24.611284255981445,
      "learning_rate": 4.657765759675512e-05,
      "loss": 0.9375,
      "step": 2120
    },
    {
      "epoch": 0.35253227408143,
      "grad_norm": 24.617549896240234,
      "learning_rate": 4.6556532026364715e-05,
      "loss": 1.377,
      "step": 2130
    },
    {
      "epoch": 0.35418735518040384,
      "grad_norm": 25.508323669433594,
      "learning_rate": 4.6535406455974313e-05,
      "loss": 1.0572,
      "step": 2140
    },
    {
      "epoch": 0.35584243627937767,
      "grad_norm": 34.27818298339844,
      "learning_rate": 4.651428088558391e-05,
      "loss": 1.2413,
      "step": 2150
    },
    {
      "epoch": 0.35749751737835156,
      "grad_norm": 86.87638854980469,
      "learning_rate": 4.649315531519352e-05,
      "loss": 1.1934,
      "step": 2160
    },
    {
      "epoch": 0.3591525984773254,
      "grad_norm": 11.199602127075195,
      "learning_rate": 4.6472029744803115e-05,
      "loss": 1.3045,
      "step": 2170
    },
    {
      "epoch": 0.3608076795762992,
      "grad_norm": 13.859945297241211,
      "learning_rate": 4.6450904174412714e-05,
      "loss": 0.9475,
      "step": 2180
    },
    {
      "epoch": 0.3624627606752731,
      "grad_norm": 31.423864364624023,
      "learning_rate": 4.642977860402231e-05,
      "loss": 1.1896,
      "step": 2190
    },
    {
      "epoch": 0.36411784177424694,
      "grad_norm": 29.091333389282227,
      "learning_rate": 4.640865303363191e-05,
      "loss": 1.1421,
      "step": 2200
    },
    {
      "epoch": 0.3657729228732208,
      "grad_norm": 40.34096145629883,
      "learning_rate": 4.638752746324151e-05,
      "loss": 1.0777,
      "step": 2210
    },
    {
      "epoch": 0.36742800397219466,
      "grad_norm": 21.893680572509766,
      "learning_rate": 4.636640189285111e-05,
      "loss": 1.0392,
      "step": 2220
    },
    {
      "epoch": 0.3690830850711685,
      "grad_norm": 29.696298599243164,
      "learning_rate": 4.6345276322460706e-05,
      "loss": 1.0009,
      "step": 2230
    },
    {
      "epoch": 0.3707381661701423,
      "grad_norm": 59.434635162353516,
      "learning_rate": 4.6324150752070304e-05,
      "loss": 1.1633,
      "step": 2240
    },
    {
      "epoch": 0.3723932472691162,
      "grad_norm": 10.650955200195312,
      "learning_rate": 4.630302518167991e-05,
      "loss": 1.1615,
      "step": 2250
    },
    {
      "epoch": 0.37404832836809004,
      "grad_norm": 35.60771179199219,
      "learning_rate": 4.628189961128951e-05,
      "loss": 0.9056,
      "step": 2260
    },
    {
      "epoch": 0.3757034094670639,
      "grad_norm": 32.109474182128906,
      "learning_rate": 4.6260774040899106e-05,
      "loss": 0.9758,
      "step": 2270
    },
    {
      "epoch": 0.37735849056603776,
      "grad_norm": 27.079797744750977,
      "learning_rate": 4.6239648470508704e-05,
      "loss": 0.8164,
      "step": 2280
    },
    {
      "epoch": 0.3790135716650116,
      "grad_norm": 52.39609146118164,
      "learning_rate": 4.62185229001183e-05,
      "loss": 1.3938,
      "step": 2290
    },
    {
      "epoch": 0.3806686527639854,
      "grad_norm": 38.14473342895508,
      "learning_rate": 4.61973973297279e-05,
      "loss": 1.2163,
      "step": 2300
    },
    {
      "epoch": 0.38232373386295926,
      "grad_norm": 28.690258026123047,
      "learning_rate": 4.61762717593375e-05,
      "loss": 1.1468,
      "step": 2310
    },
    {
      "epoch": 0.38397881496193315,
      "grad_norm": 67.65990447998047,
      "learning_rate": 4.6155146188947105e-05,
      "loss": 0.9443,
      "step": 2320
    },
    {
      "epoch": 0.385633896060907,
      "grad_norm": 35.638084411621094,
      "learning_rate": 4.61340206185567e-05,
      "loss": 0.7823,
      "step": 2330
    },
    {
      "epoch": 0.3872889771598808,
      "grad_norm": 262.21441650390625,
      "learning_rate": 4.61128950481663e-05,
      "loss": 1.3207,
      "step": 2340
    },
    {
      "epoch": 0.3889440582588547,
      "grad_norm": 31.428504943847656,
      "learning_rate": 4.6091769477775907e-05,
      "loss": 0.9319,
      "step": 2350
    },
    {
      "epoch": 0.39059913935782853,
      "grad_norm": 31.62726593017578,
      "learning_rate": 4.6070643907385505e-05,
      "loss": 1.1942,
      "step": 2360
    },
    {
      "epoch": 0.39225422045680236,
      "grad_norm": 44.401485443115234,
      "learning_rate": 4.60495183369951e-05,
      "loss": 0.8836,
      "step": 2370
    },
    {
      "epoch": 0.39390930155577625,
      "grad_norm": 58.04596710205078,
      "learning_rate": 4.60283927666047e-05,
      "loss": 1.3717,
      "step": 2380
    },
    {
      "epoch": 0.3955643826547501,
      "grad_norm": 92.31807708740234,
      "learning_rate": 4.60072671962143e-05,
      "loss": 1.1065,
      "step": 2390
    },
    {
      "epoch": 0.3972194637537239,
      "grad_norm": 14.041876792907715,
      "learning_rate": 4.59861416258239e-05,
      "loss": 0.9633,
      "step": 2400
    },
    {
      "epoch": 0.3988745448526978,
      "grad_norm": 18.356443405151367,
      "learning_rate": 4.59650160554335e-05,
      "loss": 0.921,
      "step": 2410
    },
    {
      "epoch": 0.40052962595167163,
      "grad_norm": 31.046247482299805,
      "learning_rate": 4.59438904850431e-05,
      "loss": 0.9325,
      "step": 2420
    },
    {
      "epoch": 0.40218470705064546,
      "grad_norm": 31.712236404418945,
      "learning_rate": 4.59227649146527e-05,
      "loss": 1.3867,
      "step": 2430
    },
    {
      "epoch": 0.40383978814961935,
      "grad_norm": 17.54230499267578,
      "learning_rate": 4.59016393442623e-05,
      "loss": 0.8318,
      "step": 2440
    },
    {
      "epoch": 0.4054948692485932,
      "grad_norm": 36.878849029541016,
      "learning_rate": 4.58805137738719e-05,
      "loss": 0.8369,
      "step": 2450
    },
    {
      "epoch": 0.407149950347567,
      "grad_norm": 35.157989501953125,
      "learning_rate": 4.5859388203481496e-05,
      "loss": 1.2673,
      "step": 2460
    },
    {
      "epoch": 0.4088050314465409,
      "grad_norm": 6.213328838348389,
      "learning_rate": 4.5838262633091094e-05,
      "loss": 0.6922,
      "step": 2470
    },
    {
      "epoch": 0.41046011254551473,
      "grad_norm": 18.879789352416992,
      "learning_rate": 4.581713706270069e-05,
      "loss": 1.0078,
      "step": 2480
    },
    {
      "epoch": 0.41211519364448856,
      "grad_norm": 31.578876495361328,
      "learning_rate": 4.579601149231029e-05,
      "loss": 1.0876,
      "step": 2490
    },
    {
      "epoch": 0.41377027474346245,
      "grad_norm": 21.7144718170166,
      "learning_rate": 4.5774885921919896e-05,
      "loss": 1.0685,
      "step": 2500
    },
    {
      "epoch": 0.4154253558424363,
      "grad_norm": 30.247915267944336,
      "learning_rate": 4.5753760351529494e-05,
      "loss": 1.0728,
      "step": 2510
    },
    {
      "epoch": 0.4170804369414101,
      "grad_norm": 30.390466690063477,
      "learning_rate": 4.573263478113909e-05,
      "loss": 1.0073,
      "step": 2520
    },
    {
      "epoch": 0.418735518040384,
      "grad_norm": 28.40842628479004,
      "learning_rate": 4.571150921074869e-05,
      "loss": 1.0378,
      "step": 2530
    },
    {
      "epoch": 0.42039059913935783,
      "grad_norm": 42.324851989746094,
      "learning_rate": 4.569038364035829e-05,
      "loss": 0.8659,
      "step": 2540
    },
    {
      "epoch": 0.42204568023833167,
      "grad_norm": 33.81584548950195,
      "learning_rate": 4.566925806996789e-05,
      "loss": 0.9631,
      "step": 2550
    },
    {
      "epoch": 0.42370076133730555,
      "grad_norm": 65.20732116699219,
      "learning_rate": 4.5648132499577486e-05,
      "loss": 1.0285,
      "step": 2560
    },
    {
      "epoch": 0.4253558424362794,
      "grad_norm": 32.36189270019531,
      "learning_rate": 4.562700692918709e-05,
      "loss": 0.9369,
      "step": 2570
    },
    {
      "epoch": 0.4270109235352532,
      "grad_norm": 34.64073944091797,
      "learning_rate": 4.560588135879669e-05,
      "loss": 1.0328,
      "step": 2580
    },
    {
      "epoch": 0.4286660046342271,
      "grad_norm": 47.48529052734375,
      "learning_rate": 4.558475578840629e-05,
      "loss": 1.1025,
      "step": 2590
    },
    {
      "epoch": 0.43032108573320094,
      "grad_norm": 19.65098762512207,
      "learning_rate": 4.556363021801589e-05,
      "loss": 0.9529,
      "step": 2600
    },
    {
      "epoch": 0.43197616683217477,
      "grad_norm": 32.361305236816406,
      "learning_rate": 4.554250464762549e-05,
      "loss": 1.0927,
      "step": 2610
    },
    {
      "epoch": 0.4336312479311486,
      "grad_norm": 18.194734573364258,
      "learning_rate": 4.552137907723509e-05,
      "loss": 1.1426,
      "step": 2620
    },
    {
      "epoch": 0.4352863290301225,
      "grad_norm": 37.406986236572266,
      "learning_rate": 4.550025350684469e-05,
      "loss": 1.2334,
      "step": 2630
    },
    {
      "epoch": 0.4369414101290963,
      "grad_norm": 66.44937133789062,
      "learning_rate": 4.547912793645429e-05,
      "loss": 1.1419,
      "step": 2640
    },
    {
      "epoch": 0.43859649122807015,
      "grad_norm": 163.84278869628906,
      "learning_rate": 4.5458002366063885e-05,
      "loss": 1.1752,
      "step": 2650
    },
    {
      "epoch": 0.44025157232704404,
      "grad_norm": 40.34383010864258,
      "learning_rate": 4.5436876795673484e-05,
      "loss": 1.0691,
      "step": 2660
    },
    {
      "epoch": 0.44190665342601787,
      "grad_norm": 59.469512939453125,
      "learning_rate": 4.541575122528309e-05,
      "loss": 1.2363,
      "step": 2670
    },
    {
      "epoch": 0.4435617345249917,
      "grad_norm": 29.32086753845215,
      "learning_rate": 4.539462565489269e-05,
      "loss": 1.2027,
      "step": 2680
    },
    {
      "epoch": 0.4452168156239656,
      "grad_norm": 30.00234603881836,
      "learning_rate": 4.5373500084502286e-05,
      "loss": 0.9681,
      "step": 2690
    },
    {
      "epoch": 0.4468718967229394,
      "grad_norm": 39.90174865722656,
      "learning_rate": 4.5352374514111884e-05,
      "loss": 1.0401,
      "step": 2700
    },
    {
      "epoch": 0.44852697782191325,
      "grad_norm": 48.75502395629883,
      "learning_rate": 4.533124894372148e-05,
      "loss": 1.2143,
      "step": 2710
    },
    {
      "epoch": 0.45018205892088714,
      "grad_norm": 70.19911193847656,
      "learning_rate": 4.531012337333108e-05,
      "loss": 0.7516,
      "step": 2720
    },
    {
      "epoch": 0.45183714001986097,
      "grad_norm": 64.30555725097656,
      "learning_rate": 4.528899780294068e-05,
      "loss": 0.8186,
      "step": 2730
    },
    {
      "epoch": 0.4534922211188348,
      "grad_norm": 43.67957305908203,
      "learning_rate": 4.526787223255028e-05,
      "loss": 0.8879,
      "step": 2740
    },
    {
      "epoch": 0.4551473022178087,
      "grad_norm": 23.11676597595215,
      "learning_rate": 4.5246746662159876e-05,
      "loss": 1.4472,
      "step": 2750
    },
    {
      "epoch": 0.4568023833167825,
      "grad_norm": 31.11016845703125,
      "learning_rate": 4.522562109176948e-05,
      "loss": 1.0396,
      "step": 2760
    },
    {
      "epoch": 0.45845746441575635,
      "grad_norm": 12.208727836608887,
      "learning_rate": 4.520449552137908e-05,
      "loss": 0.7298,
      "step": 2770
    },
    {
      "epoch": 0.46011254551473024,
      "grad_norm": 43.10711669921875,
      "learning_rate": 4.518336995098868e-05,
      "loss": 1.3463,
      "step": 2780
    },
    {
      "epoch": 0.4617676266137041,
      "grad_norm": 16.272186279296875,
      "learning_rate": 4.5162244380598276e-05,
      "loss": 1.42,
      "step": 2790
    },
    {
      "epoch": 0.4634227077126779,
      "grad_norm": 37.473304748535156,
      "learning_rate": 4.5141118810207875e-05,
      "loss": 0.9635,
      "step": 2800
    },
    {
      "epoch": 0.4650777888116518,
      "grad_norm": 24.73524284362793,
      "learning_rate": 4.511999323981747e-05,
      "loss": 0.9983,
      "step": 2810
    },
    {
      "epoch": 0.4667328699106256,
      "grad_norm": 58.638614654541016,
      "learning_rate": 4.509886766942708e-05,
      "loss": 0.8488,
      "step": 2820
    },
    {
      "epoch": 0.46838795100959946,
      "grad_norm": 76.90815734863281,
      "learning_rate": 4.5077742099036676e-05,
      "loss": 0.7436,
      "step": 2830
    },
    {
      "epoch": 0.47004303210857334,
      "grad_norm": 58.285491943359375,
      "learning_rate": 4.505661652864628e-05,
      "loss": 0.6724,
      "step": 2840
    },
    {
      "epoch": 0.4716981132075472,
      "grad_norm": 55.37185287475586,
      "learning_rate": 4.503549095825588e-05,
      "loss": 1.0724,
      "step": 2850
    },
    {
      "epoch": 0.473353194306521,
      "grad_norm": 28.99238395690918,
      "learning_rate": 4.501436538786548e-05,
      "loss": 1.1103,
      "step": 2860
    },
    {
      "epoch": 0.4750082754054949,
      "grad_norm": 24.56254768371582,
      "learning_rate": 4.499323981747508e-05,
      "loss": 1.0626,
      "step": 2870
    },
    {
      "epoch": 0.4766633565044687,
      "grad_norm": 27.78445816040039,
      "learning_rate": 4.4972114247084675e-05,
      "loss": 0.9471,
      "step": 2880
    },
    {
      "epoch": 0.47831843760344256,
      "grad_norm": 30.564992904663086,
      "learning_rate": 4.4950988676694274e-05,
      "loss": 1.0842,
      "step": 2890
    },
    {
      "epoch": 0.4799735187024164,
      "grad_norm": 38.38351821899414,
      "learning_rate": 4.492986310630387e-05,
      "loss": 1.1071,
      "step": 2900
    },
    {
      "epoch": 0.4816285998013903,
      "grad_norm": 11.51467514038086,
      "learning_rate": 4.490873753591347e-05,
      "loss": 0.8578,
      "step": 2910
    },
    {
      "epoch": 0.4832836809003641,
      "grad_norm": 48.67621612548828,
      "learning_rate": 4.488761196552307e-05,
      "loss": 0.9966,
      "step": 2920
    },
    {
      "epoch": 0.48493876199933794,
      "grad_norm": 62.043243408203125,
      "learning_rate": 4.4866486395132674e-05,
      "loss": 1.3475,
      "step": 2930
    },
    {
      "epoch": 0.48659384309831183,
      "grad_norm": 18.47925567626953,
      "learning_rate": 4.484536082474227e-05,
      "loss": 0.986,
      "step": 2940
    },
    {
      "epoch": 0.48824892419728566,
      "grad_norm": 23.822511672973633,
      "learning_rate": 4.482423525435187e-05,
      "loss": 0.9373,
      "step": 2950
    },
    {
      "epoch": 0.4899040052962595,
      "grad_norm": 32.23102569580078,
      "learning_rate": 4.480310968396147e-05,
      "loss": 1.0518,
      "step": 2960
    },
    {
      "epoch": 0.4915590863952334,
      "grad_norm": 12.534701347351074,
      "learning_rate": 4.478198411357107e-05,
      "loss": 0.7396,
      "step": 2970
    },
    {
      "epoch": 0.4932141674942072,
      "grad_norm": 35.98786544799805,
      "learning_rate": 4.4760858543180666e-05,
      "loss": 1.0152,
      "step": 2980
    },
    {
      "epoch": 0.49486924859318104,
      "grad_norm": 11.84727668762207,
      "learning_rate": 4.4739732972790264e-05,
      "loss": 1.1239,
      "step": 2990
    },
    {
      "epoch": 0.49652432969215493,
      "grad_norm": 71.0889892578125,
      "learning_rate": 4.471860740239986e-05,
      "loss": 1.108,
      "step": 3000
    },
    {
      "epoch": 0.49817941079112876,
      "grad_norm": 28.665138244628906,
      "learning_rate": 4.469748183200947e-05,
      "loss": 1.1014,
      "step": 3010
    },
    {
      "epoch": 0.4998344918901026,
      "grad_norm": 24.225772857666016,
      "learning_rate": 4.4676356261619066e-05,
      "loss": 1.0068,
      "step": 3020
    },
    {
      "epoch": 0.5014895729890765,
      "grad_norm": 33.060577392578125,
      "learning_rate": 4.4655230691228664e-05,
      "loss": 1.1641,
      "step": 3030
    },
    {
      "epoch": 0.5031446540880503,
      "grad_norm": 22.529266357421875,
      "learning_rate": 4.463410512083826e-05,
      "loss": 0.8886,
      "step": 3040
    },
    {
      "epoch": 0.5047997351870241,
      "grad_norm": 36.67279052734375,
      "learning_rate": 4.461297955044786e-05,
      "loss": 1.0461,
      "step": 3050
    },
    {
      "epoch": 0.506454816285998,
      "grad_norm": 58.06285858154297,
      "learning_rate": 4.4591853980057466e-05,
      "loss": 1.0847,
      "step": 3060
    },
    {
      "epoch": 0.5081098973849718,
      "grad_norm": 44.26525115966797,
      "learning_rate": 4.4570728409667065e-05,
      "loss": 1.1326,
      "step": 3070
    },
    {
      "epoch": 0.5097649784839458,
      "grad_norm": 39.28578186035156,
      "learning_rate": 4.454960283927666e-05,
      "loss": 1.1133,
      "step": 3080
    },
    {
      "epoch": 0.5114200595829196,
      "grad_norm": 50.24728775024414,
      "learning_rate": 4.452847726888626e-05,
      "loss": 0.8941,
      "step": 3090
    },
    {
      "epoch": 0.5130751406818934,
      "grad_norm": 11.468934059143066,
      "learning_rate": 4.450735169849587e-05,
      "loss": 0.8624,
      "step": 3100
    },
    {
      "epoch": 0.5147302217808672,
      "grad_norm": 38.07728958129883,
      "learning_rate": 4.4486226128105465e-05,
      "loss": 1.0228,
      "step": 3110
    },
    {
      "epoch": 0.5163853028798411,
      "grad_norm": 46.164031982421875,
      "learning_rate": 4.4465100557715063e-05,
      "loss": 0.976,
      "step": 3120
    },
    {
      "epoch": 0.5180403839788149,
      "grad_norm": 53.20150375366211,
      "learning_rate": 4.444397498732466e-05,
      "loss": 1.1195,
      "step": 3130
    },
    {
      "epoch": 0.5196954650777889,
      "grad_norm": 21.280811309814453,
      "learning_rate": 4.442284941693426e-05,
      "loss": 0.9734,
      "step": 3140
    },
    {
      "epoch": 0.5213505461767627,
      "grad_norm": 3.9784908294677734,
      "learning_rate": 4.440172384654386e-05,
      "loss": 0.8636,
      "step": 3150
    },
    {
      "epoch": 0.5230056272757365,
      "grad_norm": 33.209293365478516,
      "learning_rate": 4.438059827615346e-05,
      "loss": 1.2984,
      "step": 3160
    },
    {
      "epoch": 0.5246607083747103,
      "grad_norm": 42.088531494140625,
      "learning_rate": 4.4359472705763055e-05,
      "loss": 1.206,
      "step": 3170
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 24.37703514099121,
      "learning_rate": 4.433834713537266e-05,
      "loss": 1.2237,
      "step": 3180
    },
    {
      "epoch": 0.527970870572658,
      "grad_norm": 122.7417984008789,
      "learning_rate": 4.431722156498226e-05,
      "loss": 0.8702,
      "step": 3190
    },
    {
      "epoch": 0.529625951671632,
      "grad_norm": 54.10929870605469,
      "learning_rate": 4.429609599459186e-05,
      "loss": 1.3202,
      "step": 3200
    },
    {
      "epoch": 0.5312810327706058,
      "grad_norm": 65.12776184082031,
      "learning_rate": 4.4274970424201456e-05,
      "loss": 0.9525,
      "step": 3210
    },
    {
      "epoch": 0.5329361138695796,
      "grad_norm": 47.61336898803711,
      "learning_rate": 4.4253844853811054e-05,
      "loss": 1.0271,
      "step": 3220
    },
    {
      "epoch": 0.5345911949685535,
      "grad_norm": 27.298389434814453,
      "learning_rate": 4.423271928342065e-05,
      "loss": 1.219,
      "step": 3230
    },
    {
      "epoch": 0.5362462760675273,
      "grad_norm": 43.79317092895508,
      "learning_rate": 4.421159371303025e-05,
      "loss": 1.0317,
      "step": 3240
    },
    {
      "epoch": 0.5379013571665011,
      "grad_norm": 27.77593994140625,
      "learning_rate": 4.419046814263985e-05,
      "loss": 1.1975,
      "step": 3250
    },
    {
      "epoch": 0.5395564382654751,
      "grad_norm": 3.324611186981201,
      "learning_rate": 4.416934257224945e-05,
      "loss": 0.7069,
      "step": 3260
    },
    {
      "epoch": 0.5412115193644489,
      "grad_norm": 24.413789749145508,
      "learning_rate": 4.414821700185905e-05,
      "loss": 1.3402,
      "step": 3270
    },
    {
      "epoch": 0.5428666004634227,
      "grad_norm": 33.32161331176758,
      "learning_rate": 4.412709143146865e-05,
      "loss": 1.2173,
      "step": 3280
    },
    {
      "epoch": 0.5445216815623966,
      "grad_norm": 51.53544998168945,
      "learning_rate": 4.410596586107825e-05,
      "loss": 1.2179,
      "step": 3290
    },
    {
      "epoch": 0.5461767626613704,
      "grad_norm": 18.494131088256836,
      "learning_rate": 4.408484029068785e-05,
      "loss": 0.6608,
      "step": 3300
    },
    {
      "epoch": 0.5478318437603442,
      "grad_norm": 17.877620697021484,
      "learning_rate": 4.406371472029745e-05,
      "loss": 0.8259,
      "step": 3310
    },
    {
      "epoch": 0.5494869248593182,
      "grad_norm": 33.14328384399414,
      "learning_rate": 4.404258914990705e-05,
      "loss": 0.989,
      "step": 3320
    },
    {
      "epoch": 0.551142005958292,
      "grad_norm": 46.79896926879883,
      "learning_rate": 4.402146357951665e-05,
      "loss": 1.2171,
      "step": 3330
    },
    {
      "epoch": 0.5527970870572658,
      "grad_norm": 14.918326377868652,
      "learning_rate": 4.400033800912625e-05,
      "loss": 0.9357,
      "step": 3340
    },
    {
      "epoch": 0.5544521681562397,
      "grad_norm": 11.87060260772705,
      "learning_rate": 4.397921243873585e-05,
      "loss": 0.656,
      "step": 3350
    },
    {
      "epoch": 0.5561072492552135,
      "grad_norm": 39.53138732910156,
      "learning_rate": 4.395808686834545e-05,
      "loss": 1.0588,
      "step": 3360
    },
    {
      "epoch": 0.5577623303541873,
      "grad_norm": 77.6053695678711,
      "learning_rate": 4.393696129795505e-05,
      "loss": 0.856,
      "step": 3370
    },
    {
      "epoch": 0.5594174114531612,
      "grad_norm": 15.92467212677002,
      "learning_rate": 4.391583572756465e-05,
      "loss": 0.985,
      "step": 3380
    },
    {
      "epoch": 0.5610724925521351,
      "grad_norm": 26.090070724487305,
      "learning_rate": 4.389471015717425e-05,
      "loss": 0.9932,
      "step": 3390
    },
    {
      "epoch": 0.5627275736511089,
      "grad_norm": 52.27733612060547,
      "learning_rate": 4.3873584586783845e-05,
      "loss": 0.7681,
      "step": 3400
    },
    {
      "epoch": 0.5643826547500828,
      "grad_norm": 62.16057586669922,
      "learning_rate": 4.3852459016393444e-05,
      "loss": 1.2947,
      "step": 3410
    },
    {
      "epoch": 0.5660377358490566,
      "grad_norm": 38.06724166870117,
      "learning_rate": 4.383133344600304e-05,
      "loss": 1.0936,
      "step": 3420
    },
    {
      "epoch": 0.5676928169480304,
      "grad_norm": 17.531719207763672,
      "learning_rate": 4.381020787561264e-05,
      "loss": 0.8624,
      "step": 3430
    },
    {
      "epoch": 0.5693478980470043,
      "grad_norm": 24.88350486755371,
      "learning_rate": 4.3789082305222246e-05,
      "loss": 1.0214,
      "step": 3440
    },
    {
      "epoch": 0.5710029791459782,
      "grad_norm": 55.01778793334961,
      "learning_rate": 4.3767956734831844e-05,
      "loss": 1.106,
      "step": 3450
    },
    {
      "epoch": 0.572658060244952,
      "grad_norm": 27.850460052490234,
      "learning_rate": 4.374683116444144e-05,
      "loss": 1.0337,
      "step": 3460
    },
    {
      "epoch": 0.5743131413439259,
      "grad_norm": 46.280765533447266,
      "learning_rate": 4.372570559405104e-05,
      "loss": 1.2113,
      "step": 3470
    },
    {
      "epoch": 0.5759682224428997,
      "grad_norm": 29.35649299621582,
      "learning_rate": 4.370458002366064e-05,
      "loss": 1.4945,
      "step": 3480
    },
    {
      "epoch": 0.5776233035418735,
      "grad_norm": 27.5532283782959,
      "learning_rate": 4.368345445327024e-05,
      "loss": 0.8778,
      "step": 3490
    },
    {
      "epoch": 0.5792783846408474,
      "grad_norm": 30.413143157958984,
      "learning_rate": 4.3662328882879836e-05,
      "loss": 0.7766,
      "step": 3500
    },
    {
      "epoch": 0.5809334657398213,
      "grad_norm": 48.63931655883789,
      "learning_rate": 4.3641203312489434e-05,
      "loss": 0.7546,
      "step": 3510
    },
    {
      "epoch": 0.5825885468387951,
      "grad_norm": 26.924457550048828,
      "learning_rate": 4.362007774209904e-05,
      "loss": 0.9927,
      "step": 3520
    },
    {
      "epoch": 0.584243627937769,
      "grad_norm": 23.608797073364258,
      "learning_rate": 4.359895217170864e-05,
      "loss": 1.1612,
      "step": 3530
    },
    {
      "epoch": 0.5858987090367428,
      "grad_norm": 30.908000946044922,
      "learning_rate": 4.3577826601318236e-05,
      "loss": 1.1725,
      "step": 3540
    },
    {
      "epoch": 0.5875537901357166,
      "grad_norm": 24.111806869506836,
      "learning_rate": 4.3556701030927835e-05,
      "loss": 1.139,
      "step": 3550
    },
    {
      "epoch": 0.5892088712346905,
      "grad_norm": 24.285385131835938,
      "learning_rate": 4.353557546053744e-05,
      "loss": 1.1201,
      "step": 3560
    },
    {
      "epoch": 0.5908639523336644,
      "grad_norm": 35.23525619506836,
      "learning_rate": 4.351444989014704e-05,
      "loss": 0.8126,
      "step": 3570
    },
    {
      "epoch": 0.5925190334326382,
      "grad_norm": 25.917625427246094,
      "learning_rate": 4.3493324319756637e-05,
      "loss": 0.6303,
      "step": 3580
    },
    {
      "epoch": 0.5941741145316121,
      "grad_norm": 20.659053802490234,
      "learning_rate": 4.3472198749366235e-05,
      "loss": 1.1397,
      "step": 3590
    },
    {
      "epoch": 0.5958291956305859,
      "grad_norm": 21.246225357055664,
      "learning_rate": 4.345107317897583e-05,
      "loss": 1.0096,
      "step": 3600
    },
    {
      "epoch": 0.5974842767295597,
      "grad_norm": 53.92167663574219,
      "learning_rate": 4.342994760858544e-05,
      "loss": 0.6409,
      "step": 3610
    },
    {
      "epoch": 0.5991393578285336,
      "grad_norm": 58.40272903442383,
      "learning_rate": 4.340882203819504e-05,
      "loss": 1.1804,
      "step": 3620
    },
    {
      "epoch": 0.6007944389275075,
      "grad_norm": 49.088958740234375,
      "learning_rate": 4.3387696467804635e-05,
      "loss": 1.1878,
      "step": 3630
    },
    {
      "epoch": 0.6024495200264813,
      "grad_norm": 61.187171936035156,
      "learning_rate": 4.3366570897414234e-05,
      "loss": 1.1599,
      "step": 3640
    },
    {
      "epoch": 0.6041046011254552,
      "grad_norm": 32.11727523803711,
      "learning_rate": 4.334544532702383e-05,
      "loss": 0.9688,
      "step": 3650
    },
    {
      "epoch": 0.605759682224429,
      "grad_norm": 40.61669158935547,
      "learning_rate": 4.332431975663343e-05,
      "loss": 0.9756,
      "step": 3660
    },
    {
      "epoch": 0.6074147633234028,
      "grad_norm": 30.08391761779785,
      "learning_rate": 4.330319418624303e-05,
      "loss": 1.0081,
      "step": 3670
    },
    {
      "epoch": 0.6090698444223767,
      "grad_norm": 30.464418411254883,
      "learning_rate": 4.328206861585263e-05,
      "loss": 0.8904,
      "step": 3680
    },
    {
      "epoch": 0.6107249255213505,
      "grad_norm": 61.88846969604492,
      "learning_rate": 4.326094304546223e-05,
      "loss": 0.9793,
      "step": 3690
    },
    {
      "epoch": 0.6123800066203244,
      "grad_norm": 40.16982650756836,
      "learning_rate": 4.323981747507183e-05,
      "loss": 0.8866,
      "step": 3700
    },
    {
      "epoch": 0.6140350877192983,
      "grad_norm": 35.0702018737793,
      "learning_rate": 4.321869190468143e-05,
      "loss": 0.9236,
      "step": 3710
    },
    {
      "epoch": 0.6156901688182721,
      "grad_norm": 108.08277893066406,
      "learning_rate": 4.319756633429103e-05,
      "loss": 0.9811,
      "step": 3720
    },
    {
      "epoch": 0.6173452499172459,
      "grad_norm": 14.359704971313477,
      "learning_rate": 4.3176440763900626e-05,
      "loss": 1.0931,
      "step": 3730
    },
    {
      "epoch": 0.6190003310162198,
      "grad_norm": 44.02459716796875,
      "learning_rate": 4.3155315193510224e-05,
      "loss": 0.9219,
      "step": 3740
    },
    {
      "epoch": 0.6206554121151936,
      "grad_norm": 33.16947937011719,
      "learning_rate": 4.313418962311982e-05,
      "loss": 1.0206,
      "step": 3750
    },
    {
      "epoch": 0.6223104932141675,
      "grad_norm": 33.4658203125,
      "learning_rate": 4.311306405272942e-05,
      "loss": 1.187,
      "step": 3760
    },
    {
      "epoch": 0.6239655743131414,
      "grad_norm": 21.02299690246582,
      "learning_rate": 4.309193848233902e-05,
      "loss": 0.8634,
      "step": 3770
    },
    {
      "epoch": 0.6256206554121152,
      "grad_norm": 14.111098289489746,
      "learning_rate": 4.3070812911948625e-05,
      "loss": 0.9826,
      "step": 3780
    },
    {
      "epoch": 0.627275736511089,
      "grad_norm": 36.8897819519043,
      "learning_rate": 4.304968734155822e-05,
      "loss": 1.0663,
      "step": 3790
    },
    {
      "epoch": 0.6289308176100629,
      "grad_norm": 28.036787033081055,
      "learning_rate": 4.302856177116782e-05,
      "loss": 0.9095,
      "step": 3800
    },
    {
      "epoch": 0.6305858987090367,
      "grad_norm": 40.62337875366211,
      "learning_rate": 4.3007436200777426e-05,
      "loss": 1.0071,
      "step": 3810
    },
    {
      "epoch": 0.6322409798080106,
      "grad_norm": 18.000431060791016,
      "learning_rate": 4.2986310630387025e-05,
      "loss": 0.8433,
      "step": 3820
    },
    {
      "epoch": 0.6338960609069845,
      "grad_norm": 29.96490478515625,
      "learning_rate": 4.296518505999662e-05,
      "loss": 1.0397,
      "step": 3830
    },
    {
      "epoch": 0.6355511420059583,
      "grad_norm": 48.65170669555664,
      "learning_rate": 4.294405948960622e-05,
      "loss": 0.8542,
      "step": 3840
    },
    {
      "epoch": 0.6372062231049321,
      "grad_norm": 28.370372772216797,
      "learning_rate": 4.292293391921582e-05,
      "loss": 1.4465,
      "step": 3850
    },
    {
      "epoch": 0.638861304203906,
      "grad_norm": 23.024667739868164,
      "learning_rate": 4.2901808348825425e-05,
      "loss": 0.8225,
      "step": 3860
    },
    {
      "epoch": 0.6405163853028798,
      "grad_norm": 22.802209854125977,
      "learning_rate": 4.2880682778435024e-05,
      "loss": 0.9894,
      "step": 3870
    },
    {
      "epoch": 0.6421714664018537,
      "grad_norm": 43.88615798950195,
      "learning_rate": 4.285955720804462e-05,
      "loss": 0.9835,
      "step": 3880
    },
    {
      "epoch": 0.6438265475008276,
      "grad_norm": 27.256874084472656,
      "learning_rate": 4.283843163765422e-05,
      "loss": 0.6628,
      "step": 3890
    },
    {
      "epoch": 0.6454816285998014,
      "grad_norm": 30.44654083251953,
      "learning_rate": 4.281730606726382e-05,
      "loss": 0.9703,
      "step": 3900
    },
    {
      "epoch": 0.6471367096987752,
      "grad_norm": 28.35403060913086,
      "learning_rate": 4.279618049687342e-05,
      "loss": 1.0148,
      "step": 3910
    },
    {
      "epoch": 0.6487917907977491,
      "grad_norm": 55.94342803955078,
      "learning_rate": 4.2775054926483015e-05,
      "loss": 0.9131,
      "step": 3920
    },
    {
      "epoch": 0.6504468718967229,
      "grad_norm": 30.77705192565918,
      "learning_rate": 4.2753929356092614e-05,
      "loss": 1.1561,
      "step": 3930
    },
    {
      "epoch": 0.6521019529956968,
      "grad_norm": 21.770986557006836,
      "learning_rate": 4.273280378570221e-05,
      "loss": 1.1906,
      "step": 3940
    },
    {
      "epoch": 0.6537570340946707,
      "grad_norm": 33.03350830078125,
      "learning_rate": 4.271167821531182e-05,
      "loss": 1.3382,
      "step": 3950
    },
    {
      "epoch": 0.6554121151936445,
      "grad_norm": 34.91267013549805,
      "learning_rate": 4.2690552644921416e-05,
      "loss": 0.9367,
      "step": 3960
    },
    {
      "epoch": 0.6570671962926183,
      "grad_norm": 40.27107238769531,
      "learning_rate": 4.2669427074531014e-05,
      "loss": 1.0694,
      "step": 3970
    },
    {
      "epoch": 0.6587222773915922,
      "grad_norm": 23.485074996948242,
      "learning_rate": 4.264830150414061e-05,
      "loss": 1.2919,
      "step": 3980
    },
    {
      "epoch": 0.660377358490566,
      "grad_norm": 73.80374908447266,
      "learning_rate": 4.262717593375021e-05,
      "loss": 0.9429,
      "step": 3990
    },
    {
      "epoch": 0.6620324395895398,
      "grad_norm": 29.777626037597656,
      "learning_rate": 4.260605036335981e-05,
      "loss": 1.5669,
      "step": 4000
    },
    {
      "epoch": 0.6636875206885138,
      "grad_norm": 35.779815673828125,
      "learning_rate": 4.258492479296941e-05,
      "loss": 0.9587,
      "step": 4010
    },
    {
      "epoch": 0.6653426017874876,
      "grad_norm": 30.935335159301758,
      "learning_rate": 4.2563799222579006e-05,
      "loss": 0.8064,
      "step": 4020
    },
    {
      "epoch": 0.6669976828864614,
      "grad_norm": 19.49384117126465,
      "learning_rate": 4.254267365218861e-05,
      "loss": 0.94,
      "step": 4030
    },
    {
      "epoch": 0.6686527639854353,
      "grad_norm": 37.684349060058594,
      "learning_rate": 4.252154808179821e-05,
      "loss": 1.1691,
      "step": 4040
    },
    {
      "epoch": 0.6703078450844091,
      "grad_norm": 47.306121826171875,
      "learning_rate": 4.2500422511407815e-05,
      "loss": 0.8677,
      "step": 4050
    },
    {
      "epoch": 0.6719629261833829,
      "grad_norm": 25.551767349243164,
      "learning_rate": 4.247929694101741e-05,
      "loss": 1.3394,
      "step": 4060
    },
    {
      "epoch": 0.6736180072823569,
      "grad_norm": 65.29220581054688,
      "learning_rate": 4.245817137062701e-05,
      "loss": 1.1042,
      "step": 4070
    },
    {
      "epoch": 0.6752730883813307,
      "grad_norm": 32.81867599487305,
      "learning_rate": 4.243704580023661e-05,
      "loss": 0.9489,
      "step": 4080
    },
    {
      "epoch": 0.6769281694803045,
      "grad_norm": 50.70890808105469,
      "learning_rate": 4.241592022984621e-05,
      "loss": 0.729,
      "step": 4090
    },
    {
      "epoch": 0.6785832505792784,
      "grad_norm": 35.11899948120117,
      "learning_rate": 4.239479465945581e-05,
      "loss": 1.2056,
      "step": 4100
    },
    {
      "epoch": 0.6802383316782522,
      "grad_norm": 16.49527931213379,
      "learning_rate": 4.2373669089065405e-05,
      "loss": 1.2025,
      "step": 4110
    },
    {
      "epoch": 0.681893412777226,
      "grad_norm": 24.486730575561523,
      "learning_rate": 4.235254351867501e-05,
      "loss": 0.9311,
      "step": 4120
    },
    {
      "epoch": 0.6835484938762,
      "grad_norm": 24.59572410583496,
      "learning_rate": 4.233141794828461e-05,
      "loss": 1.1472,
      "step": 4130
    },
    {
      "epoch": 0.6852035749751738,
      "grad_norm": 15.923365592956543,
      "learning_rate": 4.231029237789421e-05,
      "loss": 0.9433,
      "step": 4140
    },
    {
      "epoch": 0.6868586560741476,
      "grad_norm": 45.41712951660156,
      "learning_rate": 4.2289166807503805e-05,
      "loss": 1.0075,
      "step": 4150
    },
    {
      "epoch": 0.6885137371731215,
      "grad_norm": 29.924531936645508,
      "learning_rate": 4.2268041237113404e-05,
      "loss": 0.9492,
      "step": 4160
    },
    {
      "epoch": 0.6901688182720953,
      "grad_norm": 55.1723518371582,
      "learning_rate": 4.2246915666723e-05,
      "loss": 0.9912,
      "step": 4170
    },
    {
      "epoch": 0.6918238993710691,
      "grad_norm": 22.32375717163086,
      "learning_rate": 4.22257900963326e-05,
      "loss": 0.9665,
      "step": 4180
    },
    {
      "epoch": 0.6934789804700431,
      "grad_norm": 8.212151527404785,
      "learning_rate": 4.22046645259422e-05,
      "loss": 0.5996,
      "step": 4190
    },
    {
      "epoch": 0.6951340615690169,
      "grad_norm": 17.61923599243164,
      "learning_rate": 4.2183538955551804e-05,
      "loss": 0.9646,
      "step": 4200
    },
    {
      "epoch": 0.6967891426679907,
      "grad_norm": 24.540983200073242,
      "learning_rate": 4.21624133851614e-05,
      "loss": 1.1525,
      "step": 4210
    },
    {
      "epoch": 0.6984442237669646,
      "grad_norm": 20.798940658569336,
      "learning_rate": 4.2141287814771e-05,
      "loss": 0.9483,
      "step": 4220
    },
    {
      "epoch": 0.7000993048659384,
      "grad_norm": 21.86621856689453,
      "learning_rate": 4.21201622443806e-05,
      "loss": 0.9096,
      "step": 4230
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 49.925994873046875,
      "learning_rate": 4.20990366739902e-05,
      "loss": 0.7554,
      "step": 4240
    },
    {
      "epoch": 0.7034094670638862,
      "grad_norm": 16.87919044494629,
      "learning_rate": 4.2077911103599796e-05,
      "loss": 1.0287,
      "step": 4250
    },
    {
      "epoch": 0.70506454816286,
      "grad_norm": 41.30405807495117,
      "learning_rate": 4.2056785533209394e-05,
      "loss": 1.0356,
      "step": 4260
    },
    {
      "epoch": 0.7067196292618338,
      "grad_norm": 21.572566986083984,
      "learning_rate": 4.2035659962819e-05,
      "loss": 0.7868,
      "step": 4270
    },
    {
      "epoch": 0.7083747103608077,
      "grad_norm": 115.95557403564453,
      "learning_rate": 4.20145343924286e-05,
      "loss": 1.1376,
      "step": 4280
    },
    {
      "epoch": 0.7100297914597815,
      "grad_norm": 18.10597801208496,
      "learning_rate": 4.1993408822038196e-05,
      "loss": 0.84,
      "step": 4290
    },
    {
      "epoch": 0.7116848725587553,
      "grad_norm": 23.530176162719727,
      "learning_rate": 4.19722832516478e-05,
      "loss": 1.0425,
      "step": 4300
    },
    {
      "epoch": 0.7133399536577292,
      "grad_norm": 18.510290145874023,
      "learning_rate": 4.19511576812574e-05,
      "loss": 0.8814,
      "step": 4310
    },
    {
      "epoch": 0.7149950347567031,
      "grad_norm": 29.641555786132812,
      "learning_rate": 4.1930032110867e-05,
      "loss": 0.9694,
      "step": 4320
    },
    {
      "epoch": 0.716650115855677,
      "grad_norm": 25.45354461669922,
      "learning_rate": 4.1908906540476597e-05,
      "loss": 0.9587,
      "step": 4330
    },
    {
      "epoch": 0.7183051969546508,
      "grad_norm": 54.723060607910156,
      "learning_rate": 4.1887780970086195e-05,
      "loss": 0.7461,
      "step": 4340
    },
    {
      "epoch": 0.7199602780536246,
      "grad_norm": 19.017486572265625,
      "learning_rate": 4.186665539969579e-05,
      "loss": 0.5914,
      "step": 4350
    },
    {
      "epoch": 0.7216153591525984,
      "grad_norm": 33.76288604736328,
      "learning_rate": 4.184552982930539e-05,
      "loss": 0.9077,
      "step": 4360
    },
    {
      "epoch": 0.7232704402515723,
      "grad_norm": 10.511170387268066,
      "learning_rate": 4.1824404258915e-05,
      "loss": 1.1006,
      "step": 4370
    },
    {
      "epoch": 0.7249255213505462,
      "grad_norm": 27.957937240600586,
      "learning_rate": 4.1803278688524595e-05,
      "loss": 0.6678,
      "step": 4380
    },
    {
      "epoch": 0.72658060244952,
      "grad_norm": 28.102434158325195,
      "learning_rate": 4.1782153118134194e-05,
      "loss": 0.8939,
      "step": 4390
    },
    {
      "epoch": 0.7282356835484939,
      "grad_norm": 33.85995101928711,
      "learning_rate": 4.176102754774379e-05,
      "loss": 1.0002,
      "step": 4400
    },
    {
      "epoch": 0.7298907646474677,
      "grad_norm": 23.318620681762695,
      "learning_rate": 4.173990197735339e-05,
      "loss": 0.8716,
      "step": 4410
    },
    {
      "epoch": 0.7315458457464415,
      "grad_norm": 39.76333236694336,
      "learning_rate": 4.171877640696299e-05,
      "loss": 0.773,
      "step": 4420
    },
    {
      "epoch": 0.7332009268454154,
      "grad_norm": 32.521297454833984,
      "learning_rate": 4.169765083657259e-05,
      "loss": 0.8318,
      "step": 4430
    },
    {
      "epoch": 0.7348560079443893,
      "grad_norm": 42.59047317504883,
      "learning_rate": 4.1676525266182186e-05,
      "loss": 1.2121,
      "step": 4440
    },
    {
      "epoch": 0.7365110890433632,
      "grad_norm": 20.450075149536133,
      "learning_rate": 4.1655399695791784e-05,
      "loss": 1.0118,
      "step": 4450
    },
    {
      "epoch": 0.738166170142337,
      "grad_norm": 41.27296447753906,
      "learning_rate": 4.163427412540139e-05,
      "loss": 1.0816,
      "step": 4460
    },
    {
      "epoch": 0.7398212512413108,
      "grad_norm": 81.6081314086914,
      "learning_rate": 4.161314855501099e-05,
      "loss": 0.9787,
      "step": 4470
    },
    {
      "epoch": 0.7414763323402846,
      "grad_norm": 8.307270050048828,
      "learning_rate": 4.1592022984620586e-05,
      "loss": 0.9335,
      "step": 4480
    },
    {
      "epoch": 0.7431314134392585,
      "grad_norm": 37.8497314453125,
      "learning_rate": 4.1570897414230184e-05,
      "loss": 0.9319,
      "step": 4490
    },
    {
      "epoch": 0.7447864945382324,
      "grad_norm": 16.711477279663086,
      "learning_rate": 4.154977184383978e-05,
      "loss": 0.8074,
      "step": 4500
    },
    {
      "epoch": 0.7464415756372063,
      "grad_norm": 26.266727447509766,
      "learning_rate": 4.152864627344938e-05,
      "loss": 1.0552,
      "step": 4510
    },
    {
      "epoch": 0.7480966567361801,
      "grad_norm": 23.072359085083008,
      "learning_rate": 4.1507520703058986e-05,
      "loss": 0.8194,
      "step": 4520
    },
    {
      "epoch": 0.7497517378351539,
      "grad_norm": 16.523712158203125,
      "learning_rate": 4.1486395132668585e-05,
      "loss": 1.0683,
      "step": 4530
    },
    {
      "epoch": 0.7514068189341278,
      "grad_norm": 65.1489486694336,
      "learning_rate": 4.146526956227818e-05,
      "loss": 0.5869,
      "step": 4540
    },
    {
      "epoch": 0.7530619000331016,
      "grad_norm": 30.347267150878906,
      "learning_rate": 4.144414399188779e-05,
      "loss": 1.1597,
      "step": 4550
    },
    {
      "epoch": 0.7547169811320755,
      "grad_norm": 36.275550842285156,
      "learning_rate": 4.1423018421497387e-05,
      "loss": 0.9384,
      "step": 4560
    },
    {
      "epoch": 0.7563720622310494,
      "grad_norm": 47.993350982666016,
      "learning_rate": 4.1401892851106985e-05,
      "loss": 0.7494,
      "step": 4570
    },
    {
      "epoch": 0.7580271433300232,
      "grad_norm": 58.59830856323242,
      "learning_rate": 4.138076728071658e-05,
      "loss": 0.9838,
      "step": 4580
    },
    {
      "epoch": 0.759682224428997,
      "grad_norm": 41.1681022644043,
      "learning_rate": 4.135964171032618e-05,
      "loss": 0.8667,
      "step": 4590
    },
    {
      "epoch": 0.7613373055279709,
      "grad_norm": 42.74210739135742,
      "learning_rate": 4.133851613993578e-05,
      "loss": 1.3909,
      "step": 4600
    },
    {
      "epoch": 0.7629923866269447,
      "grad_norm": 17.520587921142578,
      "learning_rate": 4.131739056954538e-05,
      "loss": 1.1192,
      "step": 4610
    },
    {
      "epoch": 0.7646474677259185,
      "grad_norm": 25.94455909729004,
      "learning_rate": 4.129626499915498e-05,
      "loss": 0.6019,
      "step": 4620
    },
    {
      "epoch": 0.7663025488248925,
      "grad_norm": 23.717060089111328,
      "learning_rate": 4.127513942876458e-05,
      "loss": 1.2677,
      "step": 4630
    },
    {
      "epoch": 0.7679576299238663,
      "grad_norm": 35.15800476074219,
      "learning_rate": 4.125401385837418e-05,
      "loss": 1.3152,
      "step": 4640
    },
    {
      "epoch": 0.7696127110228401,
      "grad_norm": 24.99532127380371,
      "learning_rate": 4.123288828798378e-05,
      "loss": 0.6985,
      "step": 4650
    },
    {
      "epoch": 0.771267792121814,
      "grad_norm": 65.27812194824219,
      "learning_rate": 4.121176271759338e-05,
      "loss": 0.845,
      "step": 4660
    },
    {
      "epoch": 0.7729228732207878,
      "grad_norm": 18.420948028564453,
      "learning_rate": 4.1190637147202976e-05,
      "loss": 0.9871,
      "step": 4670
    },
    {
      "epoch": 0.7745779543197616,
      "grad_norm": 12.860443115234375,
      "learning_rate": 4.1169511576812574e-05,
      "loss": 0.9623,
      "step": 4680
    },
    {
      "epoch": 0.7762330354187356,
      "grad_norm": 5.086736679077148,
      "learning_rate": 4.114838600642217e-05,
      "loss": 0.4524,
      "step": 4690
    },
    {
      "epoch": 0.7778881165177094,
      "grad_norm": 16.871694564819336,
      "learning_rate": 4.112726043603177e-05,
      "loss": 0.9623,
      "step": 4700
    },
    {
      "epoch": 0.7795431976166832,
      "grad_norm": 45.56202697753906,
      "learning_rate": 4.1106134865641376e-05,
      "loss": 0.9394,
      "step": 4710
    },
    {
      "epoch": 0.7811982787156571,
      "grad_norm": 51.37690734863281,
      "learning_rate": 4.1085009295250974e-05,
      "loss": 1.1341,
      "step": 4720
    },
    {
      "epoch": 0.7828533598146309,
      "grad_norm": 29.63262367248535,
      "learning_rate": 4.106388372486057e-05,
      "loss": 0.742,
      "step": 4730
    },
    {
      "epoch": 0.7845084409136047,
      "grad_norm": 39.99495315551758,
      "learning_rate": 4.104275815447017e-05,
      "loss": 1.1854,
      "step": 4740
    },
    {
      "epoch": 0.7861635220125787,
      "grad_norm": 27.609437942504883,
      "learning_rate": 4.102163258407977e-05,
      "loss": 1.0299,
      "step": 4750
    },
    {
      "epoch": 0.7878186031115525,
      "grad_norm": 67.61388397216797,
      "learning_rate": 4.100050701368937e-05,
      "loss": 1.0392,
      "step": 4760
    },
    {
      "epoch": 0.7894736842105263,
      "grad_norm": 25.84587860107422,
      "learning_rate": 4.097938144329897e-05,
      "loss": 1.1141,
      "step": 4770
    },
    {
      "epoch": 0.7911287653095002,
      "grad_norm": 41.50617599487305,
      "learning_rate": 4.095825587290857e-05,
      "loss": 1.3059,
      "step": 4780
    },
    {
      "epoch": 0.792783846408474,
      "grad_norm": 23.353759765625,
      "learning_rate": 4.093713030251817e-05,
      "loss": 0.9132,
      "step": 4790
    },
    {
      "epoch": 0.7944389275074478,
      "grad_norm": 16.491046905517578,
      "learning_rate": 4.0916004732127775e-05,
      "loss": 0.7818,
      "step": 4800
    },
    {
      "epoch": 0.7960940086064218,
      "grad_norm": 24.868755340576172,
      "learning_rate": 4.089487916173737e-05,
      "loss": 0.8768,
      "step": 4810
    },
    {
      "epoch": 0.7977490897053956,
      "grad_norm": 37.912105560302734,
      "learning_rate": 4.087375359134697e-05,
      "loss": 1.1469,
      "step": 4820
    },
    {
      "epoch": 0.7994041708043694,
      "grad_norm": 47.86464309692383,
      "learning_rate": 4.085262802095657e-05,
      "loss": 1.1507,
      "step": 4830
    },
    {
      "epoch": 0.8010592519033433,
      "grad_norm": 30.63715362548828,
      "learning_rate": 4.083150245056617e-05,
      "loss": 0.7953,
      "step": 4840
    },
    {
      "epoch": 0.8027143330023171,
      "grad_norm": 37.060062408447266,
      "learning_rate": 4.081037688017577e-05,
      "loss": 1.2063,
      "step": 4850
    },
    {
      "epoch": 0.8043694141012909,
      "grad_norm": 24.586227416992188,
      "learning_rate": 4.0789251309785365e-05,
      "loss": 0.6582,
      "step": 4860
    },
    {
      "epoch": 0.8060244952002649,
      "grad_norm": 17.097469329833984,
      "learning_rate": 4.0768125739394963e-05,
      "loss": 0.8553,
      "step": 4870
    },
    {
      "epoch": 0.8076795762992387,
      "grad_norm": 26.19289207458496,
      "learning_rate": 4.074700016900457e-05,
      "loss": 0.7912,
      "step": 4880
    },
    {
      "epoch": 0.8093346573982125,
      "grad_norm": 33.3283805847168,
      "learning_rate": 4.072587459861417e-05,
      "loss": 0.8896,
      "step": 4890
    },
    {
      "epoch": 0.8109897384971864,
      "grad_norm": 25.70931053161621,
      "learning_rate": 4.0704749028223765e-05,
      "loss": 1.046,
      "step": 4900
    },
    {
      "epoch": 0.8126448195961602,
      "grad_norm": 31.056930541992188,
      "learning_rate": 4.0683623457833364e-05,
      "loss": 0.8603,
      "step": 4910
    },
    {
      "epoch": 0.814299900695134,
      "grad_norm": 34.41746139526367,
      "learning_rate": 4.066249788744296e-05,
      "loss": 1.0313,
      "step": 4920
    },
    {
      "epoch": 0.8159549817941079,
      "grad_norm": 22.61565399169922,
      "learning_rate": 4.064137231705256e-05,
      "loss": 0.6345,
      "step": 4930
    },
    {
      "epoch": 0.8176100628930818,
      "grad_norm": 57.303367614746094,
      "learning_rate": 4.062024674666216e-05,
      "loss": 0.8763,
      "step": 4940
    },
    {
      "epoch": 0.8192651439920556,
      "grad_norm": 14.989606857299805,
      "learning_rate": 4.059912117627176e-05,
      "loss": 1.01,
      "step": 4950
    },
    {
      "epoch": 0.8209202250910295,
      "grad_norm": 41.18373107910156,
      "learning_rate": 4.0577995605881356e-05,
      "loss": 1.171,
      "step": 4960
    },
    {
      "epoch": 0.8225753061900033,
      "grad_norm": 8.303975105285645,
      "learning_rate": 4.055687003549096e-05,
      "loss": 0.6651,
      "step": 4970
    },
    {
      "epoch": 0.8242303872889771,
      "grad_norm": 60.517173767089844,
      "learning_rate": 4.053574446510056e-05,
      "loss": 0.7118,
      "step": 4980
    },
    {
      "epoch": 0.825885468387951,
      "grad_norm": 37.4803352355957,
      "learning_rate": 4.051461889471016e-05,
      "loss": 1.1228,
      "step": 4990
    },
    {
      "epoch": 0.8275405494869249,
      "grad_norm": 12.022649765014648,
      "learning_rate": 4.0493493324319756e-05,
      "loss": 0.812,
      "step": 5000
    },
    {
      "epoch": 0.8291956305858987,
      "grad_norm": 18.875782012939453,
      "learning_rate": 4.0472367753929354e-05,
      "loss": 1.2018,
      "step": 5010
    },
    {
      "epoch": 0.8308507116848726,
      "grad_norm": 45.75765609741211,
      "learning_rate": 4.045124218353896e-05,
      "loss": 0.7898,
      "step": 5020
    },
    {
      "epoch": 0.8325057927838464,
      "grad_norm": 33.374141693115234,
      "learning_rate": 4.043011661314856e-05,
      "loss": 1.0045,
      "step": 5030
    },
    {
      "epoch": 0.8341608738828202,
      "grad_norm": 26.64209747314453,
      "learning_rate": 4.0408991042758156e-05,
      "loss": 1.0374,
      "step": 5040
    },
    {
      "epoch": 0.8358159549817941,
      "grad_norm": 26.986963272094727,
      "learning_rate": 4.038786547236776e-05,
      "loss": 0.7405,
      "step": 5050
    },
    {
      "epoch": 0.837471036080768,
      "grad_norm": 29.854278564453125,
      "learning_rate": 4.036673990197736e-05,
      "loss": 1.088,
      "step": 5060
    },
    {
      "epoch": 0.8391261171797418,
      "grad_norm": 31.561147689819336,
      "learning_rate": 4.034561433158696e-05,
      "loss": 0.9318,
      "step": 5070
    },
    {
      "epoch": 0.8407811982787157,
      "grad_norm": 55.01606750488281,
      "learning_rate": 4.032448876119656e-05,
      "loss": 1.0969,
      "step": 5080
    },
    {
      "epoch": 0.8424362793776895,
      "grad_norm": 22.20467758178711,
      "learning_rate": 4.0303363190806155e-05,
      "loss": 0.9399,
      "step": 5090
    },
    {
      "epoch": 0.8440913604766633,
      "grad_norm": 16.404600143432617,
      "learning_rate": 4.0282237620415753e-05,
      "loss": 1.0694,
      "step": 5100
    },
    {
      "epoch": 0.8457464415756372,
      "grad_norm": 49.124752044677734,
      "learning_rate": 4.026111205002535e-05,
      "loss": 0.9494,
      "step": 5110
    },
    {
      "epoch": 0.8474015226746111,
      "grad_norm": 23.652305603027344,
      "learning_rate": 4.023998647963495e-05,
      "loss": 1.4924,
      "step": 5120
    },
    {
      "epoch": 0.8490566037735849,
      "grad_norm": 35.03337860107422,
      "learning_rate": 4.021886090924455e-05,
      "loss": 0.8561,
      "step": 5130
    },
    {
      "epoch": 0.8507116848725588,
      "grad_norm": 10.916654586791992,
      "learning_rate": 4.0197735338854154e-05,
      "loss": 0.6841,
      "step": 5140
    },
    {
      "epoch": 0.8523667659715326,
      "grad_norm": 29.743663787841797,
      "learning_rate": 4.017660976846375e-05,
      "loss": 0.8381,
      "step": 5150
    },
    {
      "epoch": 0.8540218470705064,
      "grad_norm": 48.18903732299805,
      "learning_rate": 4.015548419807335e-05,
      "loss": 1.1105,
      "step": 5160
    },
    {
      "epoch": 0.8556769281694803,
      "grad_norm": 15.480923652648926,
      "learning_rate": 4.013435862768295e-05,
      "loss": 0.5612,
      "step": 5170
    },
    {
      "epoch": 0.8573320092684542,
      "grad_norm": 54.044368743896484,
      "learning_rate": 4.011323305729255e-05,
      "loss": 1.2448,
      "step": 5180
    },
    {
      "epoch": 0.858987090367428,
      "grad_norm": 39.72895050048828,
      "learning_rate": 4.0092107486902146e-05,
      "loss": 0.7588,
      "step": 5190
    },
    {
      "epoch": 0.8606421714664019,
      "grad_norm": 44.82255172729492,
      "learning_rate": 4.0070981916511744e-05,
      "loss": 0.68,
      "step": 5200
    },
    {
      "epoch": 0.8622972525653757,
      "grad_norm": 42.09828186035156,
      "learning_rate": 4.004985634612134e-05,
      "loss": 1.152,
      "step": 5210
    },
    {
      "epoch": 0.8639523336643495,
      "grad_norm": 12.558135032653809,
      "learning_rate": 4.002873077573095e-05,
      "loss": 0.974,
      "step": 5220
    },
    {
      "epoch": 0.8656074147633234,
      "grad_norm": 68.55506134033203,
      "learning_rate": 4.0007605205340546e-05,
      "loss": 0.998,
      "step": 5230
    },
    {
      "epoch": 0.8672624958622972,
      "grad_norm": 31.091636657714844,
      "learning_rate": 3.9986479634950144e-05,
      "loss": 0.7319,
      "step": 5240
    },
    {
      "epoch": 0.8689175769612711,
      "grad_norm": 66.26123809814453,
      "learning_rate": 3.996535406455974e-05,
      "loss": 1.0208,
      "step": 5250
    },
    {
      "epoch": 0.870572658060245,
      "grad_norm": 24.81817626953125,
      "learning_rate": 3.994422849416935e-05,
      "loss": 0.9526,
      "step": 5260
    },
    {
      "epoch": 0.8722277391592188,
      "grad_norm": 23.893579483032227,
      "learning_rate": 3.9923102923778946e-05,
      "loss": 0.7943,
      "step": 5270
    },
    {
      "epoch": 0.8738828202581926,
      "grad_norm": 22.225414276123047,
      "learning_rate": 3.9901977353388545e-05,
      "loss": 0.7798,
      "step": 5280
    },
    {
      "epoch": 0.8755379013571665,
      "grad_norm": 124.0031967163086,
      "learning_rate": 3.988085178299814e-05,
      "loss": 0.7872,
      "step": 5290
    },
    {
      "epoch": 0.8771929824561403,
      "grad_norm": 32.75639724731445,
      "learning_rate": 3.985972621260774e-05,
      "loss": 1.0514,
      "step": 5300
    },
    {
      "epoch": 0.8788480635551142,
      "grad_norm": 34.68301773071289,
      "learning_rate": 3.9838600642217347e-05,
      "loss": 1.1533,
      "step": 5310
    },
    {
      "epoch": 0.8805031446540881,
      "grad_norm": 41.72272491455078,
      "learning_rate": 3.9817475071826945e-05,
      "loss": 0.8542,
      "step": 5320
    },
    {
      "epoch": 0.8821582257530619,
      "grad_norm": 36.96218490600586,
      "learning_rate": 3.979634950143654e-05,
      "loss": 0.9289,
      "step": 5330
    },
    {
      "epoch": 0.8838133068520357,
      "grad_norm": 25.352962493896484,
      "learning_rate": 3.977522393104614e-05,
      "loss": 0.7114,
      "step": 5340
    },
    {
      "epoch": 0.8854683879510096,
      "grad_norm": 29.200422286987305,
      "learning_rate": 3.975409836065574e-05,
      "loss": 0.8159,
      "step": 5350
    },
    {
      "epoch": 0.8871234690499834,
      "grad_norm": 37.4744987487793,
      "learning_rate": 3.973297279026534e-05,
      "loss": 1.1216,
      "step": 5360
    },
    {
      "epoch": 0.8887785501489573,
      "grad_norm": 107.9002685546875,
      "learning_rate": 3.971184721987494e-05,
      "loss": 1.135,
      "step": 5370
    },
    {
      "epoch": 0.8904336312479312,
      "grad_norm": 10.678826332092285,
      "learning_rate": 3.9690721649484535e-05,
      "loss": 0.7345,
      "step": 5380
    },
    {
      "epoch": 0.892088712346905,
      "grad_norm": 20.760501861572266,
      "learning_rate": 3.966959607909414e-05,
      "loss": 0.9014,
      "step": 5390
    },
    {
      "epoch": 0.8937437934458788,
      "grad_norm": 45.45418930053711,
      "learning_rate": 3.964847050870374e-05,
      "loss": 0.8495,
      "step": 5400
    },
    {
      "epoch": 0.8953988745448527,
      "grad_norm": 10.497682571411133,
      "learning_rate": 3.962734493831334e-05,
      "loss": 0.6992,
      "step": 5410
    },
    {
      "epoch": 0.8970539556438265,
      "grad_norm": 48.815982818603516,
      "learning_rate": 3.9606219367922936e-05,
      "loss": 0.9699,
      "step": 5420
    },
    {
      "epoch": 0.8987090367428004,
      "grad_norm": 26.05363655090332,
      "learning_rate": 3.9585093797532534e-05,
      "loss": 0.778,
      "step": 5430
    },
    {
      "epoch": 0.9003641178417743,
      "grad_norm": 11.353460311889648,
      "learning_rate": 3.956396822714213e-05,
      "loss": 0.7267,
      "step": 5440
    },
    {
      "epoch": 0.9020191989407481,
      "grad_norm": 25.102746963500977,
      "learning_rate": 3.954284265675173e-05,
      "loss": 0.6099,
      "step": 5450
    },
    {
      "epoch": 0.9036742800397219,
      "grad_norm": 25.125328063964844,
      "learning_rate": 3.952171708636133e-05,
      "loss": 0.6322,
      "step": 5460
    },
    {
      "epoch": 0.9053293611386958,
      "grad_norm": 54.90968704223633,
      "learning_rate": 3.950059151597093e-05,
      "loss": 0.9016,
      "step": 5470
    },
    {
      "epoch": 0.9069844422376696,
      "grad_norm": 33.970218658447266,
      "learning_rate": 3.947946594558053e-05,
      "loss": 1.0876,
      "step": 5480
    },
    {
      "epoch": 0.9086395233366436,
      "grad_norm": 15.54452133178711,
      "learning_rate": 3.945834037519013e-05,
      "loss": 0.6732,
      "step": 5490
    },
    {
      "epoch": 0.9102946044356174,
      "grad_norm": 27.127317428588867,
      "learning_rate": 3.943721480479973e-05,
      "loss": 1.1233,
      "step": 5500
    },
    {
      "epoch": 0.9119496855345912,
      "grad_norm": 37.726463317871094,
      "learning_rate": 3.9416089234409335e-05,
      "loss": 1.0778,
      "step": 5510
    },
    {
      "epoch": 0.913604766633565,
      "grad_norm": 17.310577392578125,
      "learning_rate": 3.939496366401893e-05,
      "loss": 0.7675,
      "step": 5520
    },
    {
      "epoch": 0.9152598477325389,
      "grad_norm": 17.762847900390625,
      "learning_rate": 3.937383809362853e-05,
      "loss": 0.9618,
      "step": 5530
    },
    {
      "epoch": 0.9169149288315127,
      "grad_norm": 38.27465057373047,
      "learning_rate": 3.935271252323813e-05,
      "loss": 0.9575,
      "step": 5540
    },
    {
      "epoch": 0.9185700099304865,
      "grad_norm": 16.514741897583008,
      "learning_rate": 3.933158695284773e-05,
      "loss": 0.9325,
      "step": 5550
    },
    {
      "epoch": 0.9202250910294605,
      "grad_norm": 21.655864715576172,
      "learning_rate": 3.931046138245733e-05,
      "loss": 0.926,
      "step": 5560
    },
    {
      "epoch": 0.9218801721284343,
      "grad_norm": 46.66102600097656,
      "learning_rate": 3.928933581206693e-05,
      "loss": 0.7173,
      "step": 5570
    },
    {
      "epoch": 0.9235352532274081,
      "grad_norm": 7.062456130981445,
      "learning_rate": 3.926821024167653e-05,
      "loss": 0.6103,
      "step": 5580
    },
    {
      "epoch": 0.925190334326382,
      "grad_norm": 39.740440368652344,
      "learning_rate": 3.924708467128613e-05,
      "loss": 0.9664,
      "step": 5590
    },
    {
      "epoch": 0.9268454154253558,
      "grad_norm": 14.52615737915039,
      "learning_rate": 3.922595910089573e-05,
      "loss": 1.0266,
      "step": 5600
    },
    {
      "epoch": 0.9285004965243296,
      "grad_norm": 15.054244995117188,
      "learning_rate": 3.9204833530505325e-05,
      "loss": 0.842,
      "step": 5610
    },
    {
      "epoch": 0.9301555776233036,
      "grad_norm": 17.36724853515625,
      "learning_rate": 3.9183707960114924e-05,
      "loss": 0.5741,
      "step": 5620
    },
    {
      "epoch": 0.9318106587222774,
      "grad_norm": 40.45450973510742,
      "learning_rate": 3.916258238972452e-05,
      "loss": 1.0123,
      "step": 5630
    },
    {
      "epoch": 0.9334657398212513,
      "grad_norm": 23.949005126953125,
      "learning_rate": 3.914145681933412e-05,
      "loss": 0.7034,
      "step": 5640
    },
    {
      "epoch": 0.9351208209202251,
      "grad_norm": 34.041748046875,
      "learning_rate": 3.9120331248943725e-05,
      "loss": 0.8579,
      "step": 5650
    },
    {
      "epoch": 0.9367759020191989,
      "grad_norm": 30.048744201660156,
      "learning_rate": 3.9099205678553324e-05,
      "loss": 1.0491,
      "step": 5660
    },
    {
      "epoch": 0.9384309831181727,
      "grad_norm": 39.627445220947266,
      "learning_rate": 3.907808010816292e-05,
      "loss": 0.7466,
      "step": 5670
    },
    {
      "epoch": 0.9400860642171467,
      "grad_norm": 18.481027603149414,
      "learning_rate": 3.905695453777252e-05,
      "loss": 0.7082,
      "step": 5680
    },
    {
      "epoch": 0.9417411453161205,
      "grad_norm": 26.240121841430664,
      "learning_rate": 3.903582896738212e-05,
      "loss": 0.7483,
      "step": 5690
    },
    {
      "epoch": 0.9433962264150944,
      "grad_norm": 38.89731216430664,
      "learning_rate": 3.901470339699172e-05,
      "loss": 1.0431,
      "step": 5700
    },
    {
      "epoch": 0.9450513075140682,
      "grad_norm": 40.050323486328125,
      "learning_rate": 3.8993577826601316e-05,
      "loss": 1.0079,
      "step": 5710
    },
    {
      "epoch": 0.946706388613042,
      "grad_norm": 16.640865325927734,
      "learning_rate": 3.8972452256210914e-05,
      "loss": 0.833,
      "step": 5720
    },
    {
      "epoch": 0.9483614697120158,
      "grad_norm": 32.167396545410156,
      "learning_rate": 3.895132668582052e-05,
      "loss": 0.9518,
      "step": 5730
    },
    {
      "epoch": 0.9500165508109898,
      "grad_norm": 18.119890213012695,
      "learning_rate": 3.893020111543012e-05,
      "loss": 0.9064,
      "step": 5740
    },
    {
      "epoch": 0.9516716319099636,
      "grad_norm": 26.22325325012207,
      "learning_rate": 3.8909075545039716e-05,
      "loss": 0.807,
      "step": 5750
    },
    {
      "epoch": 0.9533267130089375,
      "grad_norm": 49.05755615234375,
      "learning_rate": 3.888794997464932e-05,
      "loss": 1.0806,
      "step": 5760
    },
    {
      "epoch": 0.9549817941079113,
      "grad_norm": 20.40446662902832,
      "learning_rate": 3.886682440425892e-05,
      "loss": 1.1333,
      "step": 5770
    },
    {
      "epoch": 0.9566368752068851,
      "grad_norm": 29.72589874267578,
      "learning_rate": 3.884569883386852e-05,
      "loss": 0.6462,
      "step": 5780
    },
    {
      "epoch": 0.958291956305859,
      "grad_norm": 21.21282386779785,
      "learning_rate": 3.8824573263478116e-05,
      "loss": 1.1139,
      "step": 5790
    },
    {
      "epoch": 0.9599470374048328,
      "grad_norm": 37.28199005126953,
      "learning_rate": 3.8803447693087715e-05,
      "loss": 0.6569,
      "step": 5800
    },
    {
      "epoch": 0.9616021185038067,
      "grad_norm": 41.15958786010742,
      "learning_rate": 3.878232212269731e-05,
      "loss": 0.8854,
      "step": 5810
    },
    {
      "epoch": 0.9632571996027806,
      "grad_norm": 27.973587036132812,
      "learning_rate": 3.876119655230692e-05,
      "loss": 1.0271,
      "step": 5820
    },
    {
      "epoch": 0.9649122807017544,
      "grad_norm": 16.447437286376953,
      "learning_rate": 3.874007098191652e-05,
      "loss": 0.9797,
      "step": 5830
    },
    {
      "epoch": 0.9665673618007282,
      "grad_norm": 20.319849014282227,
      "learning_rate": 3.8718945411526115e-05,
      "loss": 1.0463,
      "step": 5840
    },
    {
      "epoch": 0.968222442899702,
      "grad_norm": 54.23350524902344,
      "learning_rate": 3.8697819841135713e-05,
      "loss": 1.0145,
      "step": 5850
    },
    {
      "epoch": 0.9698775239986759,
      "grad_norm": 13.759140968322754,
      "learning_rate": 3.867669427074531e-05,
      "loss": 0.8332,
      "step": 5860
    },
    {
      "epoch": 0.9715326050976498,
      "grad_norm": 10.593096733093262,
      "learning_rate": 3.865556870035491e-05,
      "loss": 0.679,
      "step": 5870
    },
    {
      "epoch": 0.9731876861966237,
      "grad_norm": 60.50212097167969,
      "learning_rate": 3.863444312996451e-05,
      "loss": 0.8477,
      "step": 5880
    },
    {
      "epoch": 0.9748427672955975,
      "grad_norm": 37.749107360839844,
      "learning_rate": 3.861331755957411e-05,
      "loss": 0.8214,
      "step": 5890
    },
    {
      "epoch": 0.9764978483945713,
      "grad_norm": 19.80242919921875,
      "learning_rate": 3.859219198918371e-05,
      "loss": 0.7335,
      "step": 5900
    },
    {
      "epoch": 0.9781529294935452,
      "grad_norm": 30.348848342895508,
      "learning_rate": 3.857106641879331e-05,
      "loss": 1.0282,
      "step": 5910
    },
    {
      "epoch": 0.979808010592519,
      "grad_norm": 53.1911506652832,
      "learning_rate": 3.854994084840291e-05,
      "loss": 0.5311,
      "step": 5920
    },
    {
      "epoch": 0.9814630916914929,
      "grad_norm": 62.07038497924805,
      "learning_rate": 3.852881527801251e-05,
      "loss": 1.258,
      "step": 5930
    },
    {
      "epoch": 0.9831181727904668,
      "grad_norm": 29.08894157409668,
      "learning_rate": 3.8507689707622106e-05,
      "loss": 0.7495,
      "step": 5940
    },
    {
      "epoch": 0.9847732538894406,
      "grad_norm": 16.66305160522461,
      "learning_rate": 3.8486564137231704e-05,
      "loss": 1.0708,
      "step": 5950
    },
    {
      "epoch": 0.9864283349884144,
      "grad_norm": 17.30808448791504,
      "learning_rate": 3.84654385668413e-05,
      "loss": 0.8558,
      "step": 5960
    },
    {
      "epoch": 0.9880834160873883,
      "grad_norm": 111.9420166015625,
      "learning_rate": 3.84443129964509e-05,
      "loss": 0.7558,
      "step": 5970
    },
    {
      "epoch": 0.9897384971863621,
      "grad_norm": 18.218891143798828,
      "learning_rate": 3.8423187426060506e-05,
      "loss": 0.8794,
      "step": 5980
    },
    {
      "epoch": 0.991393578285336,
      "grad_norm": 59.237060546875,
      "learning_rate": 3.8402061855670104e-05,
      "loss": 1.0428,
      "step": 5990
    },
    {
      "epoch": 0.9930486593843099,
      "grad_norm": 90.12430572509766,
      "learning_rate": 3.83809362852797e-05,
      "loss": 0.7552,
      "step": 6000
    },
    {
      "epoch": 0.9947037404832837,
      "grad_norm": 75.5633544921875,
      "learning_rate": 3.835981071488931e-05,
      "loss": 0.9478,
      "step": 6010
    },
    {
      "epoch": 0.9963588215822575,
      "grad_norm": 20.95497703552246,
      "learning_rate": 3.8338685144498906e-05,
      "loss": 0.7591,
      "step": 6020
    },
    {
      "epoch": 0.9980139026812314,
      "grad_norm": 16.415205001831055,
      "learning_rate": 3.8317559574108505e-05,
      "loss": 0.8699,
      "step": 6030
    },
    {
      "epoch": 0.9996689837802052,
      "grad_norm": 48.7807731628418,
      "learning_rate": 3.82964340037181e-05,
      "loss": 0.7141,
      "step": 6040
    },
    {
      "epoch": 1.001324064879179,
      "grad_norm": 23.99674415588379,
      "learning_rate": 3.82753084333277e-05,
      "loss": 0.7393,
      "step": 6050
    },
    {
      "epoch": 1.002979145978153,
      "grad_norm": 2.6606550216674805,
      "learning_rate": 3.82541828629373e-05,
      "loss": 0.4528,
      "step": 6060
    },
    {
      "epoch": 1.0046342270771267,
      "grad_norm": 22.87383270263672,
      "learning_rate": 3.8233057292546905e-05,
      "loss": 0.8134,
      "step": 6070
    },
    {
      "epoch": 1.0062893081761006,
      "grad_norm": 35.86394500732422,
      "learning_rate": 3.8211931722156503e-05,
      "loss": 0.7342,
      "step": 6080
    },
    {
      "epoch": 1.0079443892750746,
      "grad_norm": 27.55295181274414,
      "learning_rate": 3.81908061517661e-05,
      "loss": 0.4937,
      "step": 6090
    },
    {
      "epoch": 1.0095994703740483,
      "grad_norm": 11.351792335510254,
      "learning_rate": 3.81696805813757e-05,
      "loss": 0.5858,
      "step": 6100
    },
    {
      "epoch": 1.0112545514730222,
      "grad_norm": 28.147884368896484,
      "learning_rate": 3.81485550109853e-05,
      "loss": 0.5892,
      "step": 6110
    },
    {
      "epoch": 1.012909632571996,
      "grad_norm": 25.35367202758789,
      "learning_rate": 3.81274294405949e-05,
      "loss": 0.6338,
      "step": 6120
    },
    {
      "epoch": 1.01456471367097,
      "grad_norm": 2.7460508346557617,
      "learning_rate": 3.8106303870204495e-05,
      "loss": 0.4591,
      "step": 6130
    },
    {
      "epoch": 1.0162197947699436,
      "grad_norm": 30.059175491333008,
      "learning_rate": 3.8085178299814094e-05,
      "loss": 0.5005,
      "step": 6140
    },
    {
      "epoch": 1.0178748758689176,
      "grad_norm": 5.3687639236450195,
      "learning_rate": 3.806405272942369e-05,
      "loss": 0.5319,
      "step": 6150
    },
    {
      "epoch": 1.0195299569678915,
      "grad_norm": 33.88230895996094,
      "learning_rate": 3.80429271590333e-05,
      "loss": 0.9129,
      "step": 6160
    },
    {
      "epoch": 1.0211850380668652,
      "grad_norm": 12.727429389953613,
      "learning_rate": 3.8021801588642896e-05,
      "loss": 1.0625,
      "step": 6170
    },
    {
      "epoch": 1.0228401191658392,
      "grad_norm": 54.05946350097656,
      "learning_rate": 3.8000676018252494e-05,
      "loss": 0.7673,
      "step": 6180
    },
    {
      "epoch": 1.0244952002648129,
      "grad_norm": 20.86033058166504,
      "learning_rate": 3.797955044786209e-05,
      "loss": 0.7079,
      "step": 6190
    },
    {
      "epoch": 1.0261502813637868,
      "grad_norm": 21.361989974975586,
      "learning_rate": 3.795842487747169e-05,
      "loss": 0.8103,
      "step": 6200
    },
    {
      "epoch": 1.0278053624627608,
      "grad_norm": 27.68878746032715,
      "learning_rate": 3.793729930708129e-05,
      "loss": 0.6271,
      "step": 6210
    },
    {
      "epoch": 1.0294604435617345,
      "grad_norm": 40.39561080932617,
      "learning_rate": 3.791617373669089e-05,
      "loss": 0.5853,
      "step": 6220
    },
    {
      "epoch": 1.0311155246607084,
      "grad_norm": 24.63524627685547,
      "learning_rate": 3.789504816630049e-05,
      "loss": 0.4345,
      "step": 6230
    },
    {
      "epoch": 1.0327706057596822,
      "grad_norm": 61.5844612121582,
      "learning_rate": 3.787392259591009e-05,
      "loss": 0.8169,
      "step": 6240
    },
    {
      "epoch": 1.034425686858656,
      "grad_norm": 29.03615951538086,
      "learning_rate": 3.785279702551969e-05,
      "loss": 0.8474,
      "step": 6250
    },
    {
      "epoch": 1.0360807679576298,
      "grad_norm": 24.305652618408203,
      "learning_rate": 3.7831671455129295e-05,
      "loss": 0.5968,
      "step": 6260
    },
    {
      "epoch": 1.0377358490566038,
      "grad_norm": 24.659137725830078,
      "learning_rate": 3.781054588473889e-05,
      "loss": 0.6915,
      "step": 6270
    },
    {
      "epoch": 1.0393909301555777,
      "grad_norm": 40.681617736816406,
      "learning_rate": 3.778942031434849e-05,
      "loss": 0.4404,
      "step": 6280
    },
    {
      "epoch": 1.0410460112545514,
      "grad_norm": 11.239706993103027,
      "learning_rate": 3.776829474395809e-05,
      "loss": 0.6377,
      "step": 6290
    },
    {
      "epoch": 1.0427010923535254,
      "grad_norm": 28.902429580688477,
      "learning_rate": 3.774716917356769e-05,
      "loss": 0.3951,
      "step": 6300
    },
    {
      "epoch": 1.044356173452499,
      "grad_norm": 32.730438232421875,
      "learning_rate": 3.7726043603177287e-05,
      "loss": 0.6843,
      "step": 6310
    },
    {
      "epoch": 1.046011254551473,
      "grad_norm": 45.89207077026367,
      "learning_rate": 3.7704918032786885e-05,
      "loss": 0.5394,
      "step": 6320
    },
    {
      "epoch": 1.047666335650447,
      "grad_norm": 19.825157165527344,
      "learning_rate": 3.768379246239649e-05,
      "loss": 0.6372,
      "step": 6330
    },
    {
      "epoch": 1.0493214167494207,
      "grad_norm": 10.737863540649414,
      "learning_rate": 3.766266689200609e-05,
      "loss": 0.6095,
      "step": 6340
    },
    {
      "epoch": 1.0509764978483946,
      "grad_norm": 35.61298370361328,
      "learning_rate": 3.764154132161569e-05,
      "loss": 0.4843,
      "step": 6350
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 41.52722930908203,
      "learning_rate": 3.7620415751225285e-05,
      "loss": 0.5839,
      "step": 6360
    },
    {
      "epoch": 1.0542866600463423,
      "grad_norm": 38.534664154052734,
      "learning_rate": 3.7599290180834884e-05,
      "loss": 0.5889,
      "step": 6370
    },
    {
      "epoch": 1.055941741145316,
      "grad_norm": 6.589816570281982,
      "learning_rate": 3.757816461044448e-05,
      "loss": 0.4791,
      "step": 6380
    },
    {
      "epoch": 1.05759682224429,
      "grad_norm": 10.527118682861328,
      "learning_rate": 3.755703904005408e-05,
      "loss": 0.6583,
      "step": 6390
    },
    {
      "epoch": 1.059251903343264,
      "grad_norm": 17.307870864868164,
      "learning_rate": 3.753591346966368e-05,
      "loss": 0.6156,
      "step": 6400
    },
    {
      "epoch": 1.0609069844422376,
      "grad_norm": 15.441558837890625,
      "learning_rate": 3.7514787899273284e-05,
      "loss": 0.9004,
      "step": 6410
    },
    {
      "epoch": 1.0625620655412116,
      "grad_norm": 27.434940338134766,
      "learning_rate": 3.749366232888288e-05,
      "loss": 0.5577,
      "step": 6420
    },
    {
      "epoch": 1.0642171466401853,
      "grad_norm": 69.26449584960938,
      "learning_rate": 3.747253675849248e-05,
      "loss": 0.5812,
      "step": 6430
    },
    {
      "epoch": 1.0658722277391592,
      "grad_norm": 38.147804260253906,
      "learning_rate": 3.745141118810208e-05,
      "loss": 0.8755,
      "step": 6440
    },
    {
      "epoch": 1.067527308838133,
      "grad_norm": 9.738122940063477,
      "learning_rate": 3.743028561771168e-05,
      "loss": 0.6124,
      "step": 6450
    },
    {
      "epoch": 1.069182389937107,
      "grad_norm": 21.58034324645996,
      "learning_rate": 3.7409160047321276e-05,
      "loss": 0.7137,
      "step": 6460
    },
    {
      "epoch": 1.0708374710360808,
      "grad_norm": 24.178417205810547,
      "learning_rate": 3.738803447693088e-05,
      "loss": 0.7232,
      "step": 6470
    },
    {
      "epoch": 1.0724925521350546,
      "grad_norm": 37.47402572631836,
      "learning_rate": 3.736690890654048e-05,
      "loss": 0.7478,
      "step": 6480
    },
    {
      "epoch": 1.0741476332340285,
      "grad_norm": 12.621219635009766,
      "learning_rate": 3.734578333615008e-05,
      "loss": 0.5389,
      "step": 6490
    },
    {
      "epoch": 1.0758027143330022,
      "grad_norm": 19.407360076904297,
      "learning_rate": 3.732465776575968e-05,
      "loss": 0.61,
      "step": 6500
    },
    {
      "epoch": 1.0774577954319762,
      "grad_norm": 90.82967376708984,
      "learning_rate": 3.730353219536928e-05,
      "loss": 1.0033,
      "step": 6510
    },
    {
      "epoch": 1.0791128765309501,
      "grad_norm": 21.24578285217285,
      "learning_rate": 3.728240662497888e-05,
      "loss": 0.8119,
      "step": 6520
    },
    {
      "epoch": 1.0807679576299238,
      "grad_norm": 29.143354415893555,
      "learning_rate": 3.726128105458848e-05,
      "loss": 0.5768,
      "step": 6530
    },
    {
      "epoch": 1.0824230387288978,
      "grad_norm": 37.799713134765625,
      "learning_rate": 3.7240155484198076e-05,
      "loss": 0.6227,
      "step": 6540
    },
    {
      "epoch": 1.0840781198278715,
      "grad_norm": 32.8206672668457,
      "learning_rate": 3.7219029913807675e-05,
      "loss": 0.8603,
      "step": 6550
    },
    {
      "epoch": 1.0857332009268454,
      "grad_norm": 35.560325622558594,
      "learning_rate": 3.719790434341727e-05,
      "loss": 0.7908,
      "step": 6560
    },
    {
      "epoch": 1.0873882820258192,
      "grad_norm": 13.880935668945312,
      "learning_rate": 3.717677877302687e-05,
      "loss": 0.4804,
      "step": 6570
    },
    {
      "epoch": 1.089043363124793,
      "grad_norm": 5.424874305725098,
      "learning_rate": 3.715565320263648e-05,
      "loss": 0.4581,
      "step": 6580
    },
    {
      "epoch": 1.090698444223767,
      "grad_norm": 47.82488250732422,
      "learning_rate": 3.7134527632246075e-05,
      "loss": 0.6467,
      "step": 6590
    },
    {
      "epoch": 1.0923535253227408,
      "grad_norm": 47.30984878540039,
      "learning_rate": 3.7113402061855674e-05,
      "loss": 0.6121,
      "step": 6600
    },
    {
      "epoch": 1.0940086064217147,
      "grad_norm": 29.692768096923828,
      "learning_rate": 3.709227649146527e-05,
      "loss": 0.6096,
      "step": 6610
    },
    {
      "epoch": 1.0956636875206884,
      "grad_norm": 14.344732284545898,
      "learning_rate": 3.707115092107487e-05,
      "loss": 0.4658,
      "step": 6620
    },
    {
      "epoch": 1.0973187686196624,
      "grad_norm": 17.299856185913086,
      "learning_rate": 3.705002535068447e-05,
      "loss": 0.6885,
      "step": 6630
    },
    {
      "epoch": 1.0989738497186363,
      "grad_norm": 31.473751068115234,
      "learning_rate": 3.702889978029407e-05,
      "loss": 0.9624,
      "step": 6640
    },
    {
      "epoch": 1.10062893081761,
      "grad_norm": 6.045357704162598,
      "learning_rate": 3.7007774209903665e-05,
      "loss": 0.577,
      "step": 6650
    },
    {
      "epoch": 1.102284011916584,
      "grad_norm": 24.454242706298828,
      "learning_rate": 3.6986648639513264e-05,
      "loss": 0.6653,
      "step": 6660
    },
    {
      "epoch": 1.1039390930155577,
      "grad_norm": 16.46118927001953,
      "learning_rate": 3.696552306912287e-05,
      "loss": 0.7013,
      "step": 6670
    },
    {
      "epoch": 1.1055941741145316,
      "grad_norm": 41.0910530090332,
      "learning_rate": 3.694439749873247e-05,
      "loss": 0.7117,
      "step": 6680
    },
    {
      "epoch": 1.1072492552135054,
      "grad_norm": 108.03185272216797,
      "learning_rate": 3.6923271928342066e-05,
      "loss": 0.5915,
      "step": 6690
    },
    {
      "epoch": 1.1089043363124793,
      "grad_norm": 26.44959831237793,
      "learning_rate": 3.6902146357951664e-05,
      "loss": 0.4162,
      "step": 6700
    },
    {
      "epoch": 1.1105594174114533,
      "grad_norm": 46.18669128417969,
      "learning_rate": 3.688102078756126e-05,
      "loss": 0.772,
      "step": 6710
    },
    {
      "epoch": 1.112214498510427,
      "grad_norm": 11.763246536254883,
      "learning_rate": 3.685989521717087e-05,
      "loss": 0.7397,
      "step": 6720
    },
    {
      "epoch": 1.113869579609401,
      "grad_norm": 26.21137809753418,
      "learning_rate": 3.6838769646780466e-05,
      "loss": 0.5436,
      "step": 6730
    },
    {
      "epoch": 1.1155246607083746,
      "grad_norm": 35.607383728027344,
      "learning_rate": 3.6817644076390064e-05,
      "loss": 0.6109,
      "step": 6740
    },
    {
      "epoch": 1.1171797418073486,
      "grad_norm": 33.45698165893555,
      "learning_rate": 3.679651850599967e-05,
      "loss": 0.7481,
      "step": 6750
    },
    {
      "epoch": 1.1188348229063223,
      "grad_norm": 22.97040557861328,
      "learning_rate": 3.677539293560927e-05,
      "loss": 0.9309,
      "step": 6760
    },
    {
      "epoch": 1.1204899040052962,
      "grad_norm": 23.57598876953125,
      "learning_rate": 3.6754267365218866e-05,
      "loss": 0.5564,
      "step": 6770
    },
    {
      "epoch": 1.1221449851042702,
      "grad_norm": 40.754241943359375,
      "learning_rate": 3.6733141794828465e-05,
      "loss": 0.6838,
      "step": 6780
    },
    {
      "epoch": 1.123800066203244,
      "grad_norm": 194.5100555419922,
      "learning_rate": 3.671201622443806e-05,
      "loss": 0.4433,
      "step": 6790
    },
    {
      "epoch": 1.1254551473022179,
      "grad_norm": 70.14537048339844,
      "learning_rate": 3.669089065404766e-05,
      "loss": 0.5632,
      "step": 6800
    },
    {
      "epoch": 1.1271102284011916,
      "grad_norm": 45.88896179199219,
      "learning_rate": 3.666976508365726e-05,
      "loss": 0.4323,
      "step": 6810
    },
    {
      "epoch": 1.1287653095001655,
      "grad_norm": 30.459667205810547,
      "learning_rate": 3.664863951326686e-05,
      "loss": 0.8318,
      "step": 6820
    },
    {
      "epoch": 1.1304203905991392,
      "grad_norm": 30.067577362060547,
      "learning_rate": 3.662751394287646e-05,
      "loss": 0.4547,
      "step": 6830
    },
    {
      "epoch": 1.1320754716981132,
      "grad_norm": 25.983854293823242,
      "learning_rate": 3.660638837248606e-05,
      "loss": 0.5291,
      "step": 6840
    },
    {
      "epoch": 1.1337305527970871,
      "grad_norm": 37.30104446411133,
      "learning_rate": 3.658526280209566e-05,
      "loss": 0.8222,
      "step": 6850
    },
    {
      "epoch": 1.1353856338960608,
      "grad_norm": 13.5311918258667,
      "learning_rate": 3.656413723170526e-05,
      "loss": 0.6255,
      "step": 6860
    },
    {
      "epoch": 1.1370407149950348,
      "grad_norm": 14.898466110229492,
      "learning_rate": 3.654301166131486e-05,
      "loss": 0.6047,
      "step": 6870
    },
    {
      "epoch": 1.1386957960940087,
      "grad_norm": 89.04261779785156,
      "learning_rate": 3.6521886090924455e-05,
      "loss": 0.7569,
      "step": 6880
    },
    {
      "epoch": 1.1403508771929824,
      "grad_norm": 25.644994735717773,
      "learning_rate": 3.6500760520534054e-05,
      "loss": 0.7446,
      "step": 6890
    },
    {
      "epoch": 1.1420059582919564,
      "grad_norm": 18.464025497436523,
      "learning_rate": 3.647963495014365e-05,
      "loss": 0.5555,
      "step": 6900
    },
    {
      "epoch": 1.1436610393909301,
      "grad_norm": 27.158897399902344,
      "learning_rate": 3.645850937975325e-05,
      "loss": 0.6505,
      "step": 6910
    },
    {
      "epoch": 1.145316120489904,
      "grad_norm": 24.003767013549805,
      "learning_rate": 3.6437383809362856e-05,
      "loss": 0.5802,
      "step": 6920
    },
    {
      "epoch": 1.1469712015888778,
      "grad_norm": 29.795583724975586,
      "learning_rate": 3.6416258238972454e-05,
      "loss": 0.8738,
      "step": 6930
    },
    {
      "epoch": 1.1486262826878517,
      "grad_norm": 13.572277069091797,
      "learning_rate": 3.639513266858205e-05,
      "loss": 0.5452,
      "step": 6940
    },
    {
      "epoch": 1.1502813637868257,
      "grad_norm": 15.532824516296387,
      "learning_rate": 3.637400709819165e-05,
      "loss": 0.5303,
      "step": 6950
    },
    {
      "epoch": 1.1519364448857994,
      "grad_norm": 27.94902229309082,
      "learning_rate": 3.635288152780125e-05,
      "loss": 0.9961,
      "step": 6960
    },
    {
      "epoch": 1.1535915259847733,
      "grad_norm": 27.398401260375977,
      "learning_rate": 3.6331755957410854e-05,
      "loss": 0.3987,
      "step": 6970
    },
    {
      "epoch": 1.155246607083747,
      "grad_norm": 19.338653564453125,
      "learning_rate": 3.631063038702045e-05,
      "loss": 0.7439,
      "step": 6980
    },
    {
      "epoch": 1.156901688182721,
      "grad_norm": 36.364017486572266,
      "learning_rate": 3.628950481663005e-05,
      "loss": 0.6934,
      "step": 6990
    },
    {
      "epoch": 1.1585567692816947,
      "grad_norm": 45.0477409362793,
      "learning_rate": 3.626837924623965e-05,
      "loss": 0.6027,
      "step": 7000
    },
    {
      "epoch": 1.1602118503806687,
      "grad_norm": 32.046730041503906,
      "learning_rate": 3.6247253675849255e-05,
      "loss": 0.6637,
      "step": 7010
    },
    {
      "epoch": 1.1618669314796426,
      "grad_norm": 30.81036949157715,
      "learning_rate": 3.622612810545885e-05,
      "loss": 0.8079,
      "step": 7020
    },
    {
      "epoch": 1.1635220125786163,
      "grad_norm": 38.34925079345703,
      "learning_rate": 3.620500253506845e-05,
      "loss": 0.4865,
      "step": 7030
    },
    {
      "epoch": 1.1651770936775903,
      "grad_norm": 17.540435791015625,
      "learning_rate": 3.618387696467805e-05,
      "loss": 0.5234,
      "step": 7040
    },
    {
      "epoch": 1.166832174776564,
      "grad_norm": 41.68998336791992,
      "learning_rate": 3.616275139428765e-05,
      "loss": 0.5734,
      "step": 7050
    },
    {
      "epoch": 1.168487255875538,
      "grad_norm": 28.82371711730957,
      "learning_rate": 3.614162582389725e-05,
      "loss": 0.6707,
      "step": 7060
    },
    {
      "epoch": 1.1701423369745116,
      "grad_norm": 58.00960159301758,
      "learning_rate": 3.6120500253506845e-05,
      "loss": 0.7818,
      "step": 7070
    },
    {
      "epoch": 1.1717974180734856,
      "grad_norm": 32.1250114440918,
      "learning_rate": 3.6099374683116443e-05,
      "loss": 0.8632,
      "step": 7080
    },
    {
      "epoch": 1.1734524991724595,
      "grad_norm": 20.238388061523438,
      "learning_rate": 3.607824911272605e-05,
      "loss": 0.3957,
      "step": 7090
    },
    {
      "epoch": 1.1751075802714332,
      "grad_norm": 41.980316162109375,
      "learning_rate": 3.605712354233565e-05,
      "loss": 0.7519,
      "step": 7100
    },
    {
      "epoch": 1.1767626613704072,
      "grad_norm": 80.91207122802734,
      "learning_rate": 3.6035997971945245e-05,
      "loss": 0.5851,
      "step": 7110
    },
    {
      "epoch": 1.178417742469381,
      "grad_norm": 34.86690139770508,
      "learning_rate": 3.6014872401554844e-05,
      "loss": 0.7178,
      "step": 7120
    },
    {
      "epoch": 1.1800728235683549,
      "grad_norm": 49.62925720214844,
      "learning_rate": 3.599374683116444e-05,
      "loss": 0.4759,
      "step": 7130
    },
    {
      "epoch": 1.1817279046673286,
      "grad_norm": 18.404447555541992,
      "learning_rate": 3.597262126077404e-05,
      "loss": 0.4374,
      "step": 7140
    },
    {
      "epoch": 1.1833829857663025,
      "grad_norm": 19.61612892150879,
      "learning_rate": 3.595149569038364e-05,
      "loss": 0.7304,
      "step": 7150
    },
    {
      "epoch": 1.1850380668652765,
      "grad_norm": 7.475507736206055,
      "learning_rate": 3.593037011999324e-05,
      "loss": 0.5899,
      "step": 7160
    },
    {
      "epoch": 1.1866931479642502,
      "grad_norm": 42.93541717529297,
      "learning_rate": 3.5909244549602836e-05,
      "loss": 0.663,
      "step": 7170
    },
    {
      "epoch": 1.1883482290632241,
      "grad_norm": 37.30464553833008,
      "learning_rate": 3.588811897921244e-05,
      "loss": 0.678,
      "step": 7180
    },
    {
      "epoch": 1.190003310162198,
      "grad_norm": 33.16069793701172,
      "learning_rate": 3.586699340882204e-05,
      "loss": 0.4303,
      "step": 7190
    },
    {
      "epoch": 1.1916583912611718,
      "grad_norm": 13.771459579467773,
      "learning_rate": 3.584586783843164e-05,
      "loss": 0.4089,
      "step": 7200
    },
    {
      "epoch": 1.1933134723601457,
      "grad_norm": 15.139047622680664,
      "learning_rate": 3.5824742268041236e-05,
      "loss": 0.6308,
      "step": 7210
    },
    {
      "epoch": 1.1949685534591195,
      "grad_norm": 8.202455520629883,
      "learning_rate": 3.580361669765084e-05,
      "loss": 0.471,
      "step": 7220
    },
    {
      "epoch": 1.1966236345580934,
      "grad_norm": 18.460041046142578,
      "learning_rate": 3.578249112726044e-05,
      "loss": 0.7535,
      "step": 7230
    },
    {
      "epoch": 1.1982787156570671,
      "grad_norm": 39.69636535644531,
      "learning_rate": 3.576136555687004e-05,
      "loss": 0.7314,
      "step": 7240
    },
    {
      "epoch": 1.199933796756041,
      "grad_norm": 32.82419204711914,
      "learning_rate": 3.5740239986479636e-05,
      "loss": 0.6163,
      "step": 7250
    }
  ],
  "logging_steps": 10,
  "max_steps": 24168,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 2417,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.386964907838464e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
