{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.400198609731877,
  "eval_steps": 500,
  "global_step": 14502,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016550810989738498,
      "grad_norm": 10.491287231445312,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 3.6997,
      "step": 10
    },
    {
      "epoch": 0.0033101621979476996,
      "grad_norm": 8.1528959274292,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 4.0751,
      "step": 20
    },
    {
      "epoch": 0.004965243296921549,
      "grad_norm": 11.886052131652832,
      "learning_rate": 3e-06,
      "loss": 3.9032,
      "step": 30
    },
    {
      "epoch": 0.006620324395895399,
      "grad_norm": 5.415994167327881,
      "learning_rate": 4.000000000000001e-06,
      "loss": 3.4249,
      "step": 40
    },
    {
      "epoch": 0.00827540549486925,
      "grad_norm": 12.988325119018555,
      "learning_rate": 5e-06,
      "loss": 3.5436,
      "step": 50
    },
    {
      "epoch": 0.009930486593843098,
      "grad_norm": 11.426213264465332,
      "learning_rate": 6e-06,
      "loss": 4.8924,
      "step": 60
    },
    {
      "epoch": 0.011585567692816948,
      "grad_norm": 9.009767532348633,
      "learning_rate": 7.000000000000001e-06,
      "loss": 4.0708,
      "step": 70
    },
    {
      "epoch": 0.013240648791790799,
      "grad_norm": 5.40336275100708,
      "learning_rate": 8.000000000000001e-06,
      "loss": 3.6271,
      "step": 80
    },
    {
      "epoch": 0.014895729890764648,
      "grad_norm": 14.442776679992676,
      "learning_rate": 9e-06,
      "loss": 3.5951,
      "step": 90
    },
    {
      "epoch": 0.0165508109897385,
      "grad_norm": 7.636654853820801,
      "learning_rate": 1e-05,
      "loss": 3.6199,
      "step": 100
    },
    {
      "epoch": 0.018205892088712348,
      "grad_norm": 7.5129075050354,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 3.2084,
      "step": 110
    },
    {
      "epoch": 0.019860973187686197,
      "grad_norm": 10.015198707580566,
      "learning_rate": 1.2e-05,
      "loss": 3.9251,
      "step": 120
    },
    {
      "epoch": 0.021516054286660046,
      "grad_norm": 13.5173978805542,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 4.1535,
      "step": 130
    },
    {
      "epoch": 0.023171135385633895,
      "grad_norm": 8.180093765258789,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 3.5428,
      "step": 140
    },
    {
      "epoch": 0.024826216484607744,
      "grad_norm": 2.851651906967163,
      "learning_rate": 1.5e-05,
      "loss": 2.7841,
      "step": 150
    },
    {
      "epoch": 0.026481297583581597,
      "grad_norm": 11.961166381835938,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 2.3978,
      "step": 160
    },
    {
      "epoch": 0.028136378682555446,
      "grad_norm": 8.412672996520996,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 2.4668,
      "step": 170
    },
    {
      "epoch": 0.029791459781529295,
      "grad_norm": 14.415621757507324,
      "learning_rate": 1.8e-05,
      "loss": 2.8025,
      "step": 180
    },
    {
      "epoch": 0.031446540880503145,
      "grad_norm": 12.371260643005371,
      "learning_rate": 1.9e-05,
      "loss": 2.2191,
      "step": 190
    },
    {
      "epoch": 0.033101621979477,
      "grad_norm": 15.451358795166016,
      "learning_rate": 2e-05,
      "loss": 2.3608,
      "step": 200
    },
    {
      "epoch": 0.03475670307845084,
      "grad_norm": 7.22649621963501,
      "learning_rate": 2.1e-05,
      "loss": 1.8181,
      "step": 210
    },
    {
      "epoch": 0.036411784177424696,
      "grad_norm": 4.971599578857422,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.7533,
      "step": 220
    },
    {
      "epoch": 0.03806686527639854,
      "grad_norm": 4.988760948181152,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 1.8505,
      "step": 230
    },
    {
      "epoch": 0.039721946375372394,
      "grad_norm": 3.332304000854492,
      "learning_rate": 2.4e-05,
      "loss": 1.8816,
      "step": 240
    },
    {
      "epoch": 0.04137702747434624,
      "grad_norm": 7.057027339935303,
      "learning_rate": 2.5e-05,
      "loss": 1.6548,
      "step": 250
    },
    {
      "epoch": 0.04303210857332009,
      "grad_norm": 7.131735324859619,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.9387,
      "step": 260
    },
    {
      "epoch": 0.044687189672293945,
      "grad_norm": 8.290486335754395,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.8058,
      "step": 270
    },
    {
      "epoch": 0.04634227077126779,
      "grad_norm": 5.917972087860107,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.8249,
      "step": 280
    },
    {
      "epoch": 0.04799735187024164,
      "grad_norm": 6.318717002868652,
      "learning_rate": 2.9e-05,
      "loss": 1.2006,
      "step": 290
    },
    {
      "epoch": 0.04965243296921549,
      "grad_norm": 5.791667938232422,
      "learning_rate": 3e-05,
      "loss": 2.1717,
      "step": 300
    },
    {
      "epoch": 0.05130751406818934,
      "grad_norm": 5.035686492919922,
      "learning_rate": 3.1e-05,
      "loss": 1.9212,
      "step": 310
    },
    {
      "epoch": 0.052962595167163194,
      "grad_norm": 7.047187805175781,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.7182,
      "step": 320
    },
    {
      "epoch": 0.05461767626613704,
      "grad_norm": 5.455808639526367,
      "learning_rate": 3.3e-05,
      "loss": 1.352,
      "step": 330
    },
    {
      "epoch": 0.05627275736511089,
      "grad_norm": 7.1284308433532715,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.2526,
      "step": 340
    },
    {
      "epoch": 0.05792783846408474,
      "grad_norm": 2.6936628818511963,
      "learning_rate": 3.5e-05,
      "loss": 1.4086,
      "step": 350
    },
    {
      "epoch": 0.05958291956305859,
      "grad_norm": 6.265352249145508,
      "learning_rate": 3.6e-05,
      "loss": 1.4256,
      "step": 360
    },
    {
      "epoch": 0.06123800066203244,
      "grad_norm": 6.356632709503174,
      "learning_rate": 3.7e-05,
      "loss": 1.6221,
      "step": 370
    },
    {
      "epoch": 0.06289308176100629,
      "grad_norm": 4.151512145996094,
      "learning_rate": 3.8e-05,
      "loss": 1.2793,
      "step": 380
    },
    {
      "epoch": 0.06454816285998013,
      "grad_norm": 5.9260993003845215,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.6107,
      "step": 390
    },
    {
      "epoch": 0.066203243958954,
      "grad_norm": 4.526922225952148,
      "learning_rate": 4e-05,
      "loss": 1.5396,
      "step": 400
    },
    {
      "epoch": 0.06785832505792784,
      "grad_norm": 5.142249584197998,
      "learning_rate": 4.1e-05,
      "loss": 1.5912,
      "step": 410
    },
    {
      "epoch": 0.06951340615690169,
      "grad_norm": 5.746337413787842,
      "learning_rate": 4.2e-05,
      "loss": 1.4797,
      "step": 420
    },
    {
      "epoch": 0.07116848725587553,
      "grad_norm": 2.304018497467041,
      "learning_rate": 4.3e-05,
      "loss": 0.9609,
      "step": 430
    },
    {
      "epoch": 0.07282356835484939,
      "grad_norm": 6.13914155960083,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.1815,
      "step": 440
    },
    {
      "epoch": 0.07447864945382324,
      "grad_norm": 6.120832443237305,
      "learning_rate": 4.5e-05,
      "loss": 1.6981,
      "step": 450
    },
    {
      "epoch": 0.07613373055279708,
      "grad_norm": 3.058312177658081,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.3897,
      "step": 460
    },
    {
      "epoch": 0.07778881165177094,
      "grad_norm": 7.008513927459717,
      "learning_rate": 4.7e-05,
      "loss": 0.8884,
      "step": 470
    },
    {
      "epoch": 0.07944389275074479,
      "grad_norm": 8.14610481262207,
      "learning_rate": 4.8e-05,
      "loss": 1.8417,
      "step": 480
    },
    {
      "epoch": 0.08109897384971863,
      "grad_norm": 9.467963218688965,
      "learning_rate": 4.9e-05,
      "loss": 1.4363,
      "step": 490
    },
    {
      "epoch": 0.08275405494869248,
      "grad_norm": 6.290310859680176,
      "learning_rate": 5e-05,
      "loss": 1.2156,
      "step": 500
    },
    {
      "epoch": 0.08440913604766634,
      "grad_norm": 10.386557579040527,
      "learning_rate": 4.99788744296096e-05,
      "loss": 1.0915,
      "step": 510
    },
    {
      "epoch": 0.08606421714664018,
      "grad_norm": 4.732298851013184,
      "learning_rate": 4.99577488592192e-05,
      "loss": 1.0337,
      "step": 520
    },
    {
      "epoch": 0.08771929824561403,
      "grad_norm": 7.166783809661865,
      "learning_rate": 4.99366232888288e-05,
      "loss": 1.3134,
      "step": 530
    },
    {
      "epoch": 0.08937437934458789,
      "grad_norm": 8.70981502532959,
      "learning_rate": 4.9915497718438396e-05,
      "loss": 1.4855,
      "step": 540
    },
    {
      "epoch": 0.09102946044356174,
      "grad_norm": 9.945058822631836,
      "learning_rate": 4.9894372148048e-05,
      "loss": 1.2601,
      "step": 550
    },
    {
      "epoch": 0.09268454154253558,
      "grad_norm": 3.8860156536102295,
      "learning_rate": 4.98732465776576e-05,
      "loss": 0.9946,
      "step": 560
    },
    {
      "epoch": 0.09433962264150944,
      "grad_norm": 5.446142673492432,
      "learning_rate": 4.98521210072672e-05,
      "loss": 1.2979,
      "step": 570
    },
    {
      "epoch": 0.09599470374048329,
      "grad_norm": 13.52455997467041,
      "learning_rate": 4.9830995436876796e-05,
      "loss": 1.1829,
      "step": 580
    },
    {
      "epoch": 0.09764978483945713,
      "grad_norm": 6.01290225982666,
      "learning_rate": 4.9809869866486395e-05,
      "loss": 1.4549,
      "step": 590
    },
    {
      "epoch": 0.09930486593843098,
      "grad_norm": 6.11596155166626,
      "learning_rate": 4.978874429609599e-05,
      "loss": 1.1775,
      "step": 600
    },
    {
      "epoch": 0.10095994703740484,
      "grad_norm": 7.768533706665039,
      "learning_rate": 4.97676187257056e-05,
      "loss": 1.5284,
      "step": 610
    },
    {
      "epoch": 0.10261502813637868,
      "grad_norm": 4.7729973793029785,
      "learning_rate": 4.9746493155315197e-05,
      "loss": 0.9855,
      "step": 620
    },
    {
      "epoch": 0.10427010923535253,
      "grad_norm": 6.364742755889893,
      "learning_rate": 4.9725367584924795e-05,
      "loss": 1.0399,
      "step": 630
    },
    {
      "epoch": 0.10592519033432639,
      "grad_norm": 9.551055908203125,
      "learning_rate": 4.97042420145344e-05,
      "loss": 1.2766,
      "step": 640
    },
    {
      "epoch": 0.10758027143330023,
      "grad_norm": 5.833217144012451,
      "learning_rate": 4.9683116444144e-05,
      "loss": 1.3197,
      "step": 650
    },
    {
      "epoch": 0.10923535253227408,
      "grad_norm": 9.327737808227539,
      "learning_rate": 4.96619908737536e-05,
      "loss": 1.2004,
      "step": 660
    },
    {
      "epoch": 0.11089043363124793,
      "grad_norm": 6.029761791229248,
      "learning_rate": 4.9640865303363195e-05,
      "loss": 1.1285,
      "step": 670
    },
    {
      "epoch": 0.11254551473022179,
      "grad_norm": 7.959981918334961,
      "learning_rate": 4.9619739732972794e-05,
      "loss": 1.3077,
      "step": 680
    },
    {
      "epoch": 0.11420059582919563,
      "grad_norm": 7.7021260261535645,
      "learning_rate": 4.959861416258239e-05,
      "loss": 1.2503,
      "step": 690
    },
    {
      "epoch": 0.11585567692816948,
      "grad_norm": 4.773941516876221,
      "learning_rate": 4.957748859219199e-05,
      "loss": 1.2603,
      "step": 700
    },
    {
      "epoch": 0.11751075802714334,
      "grad_norm": 11.288993835449219,
      "learning_rate": 4.955636302180159e-05,
      "loss": 0.8977,
      "step": 710
    },
    {
      "epoch": 0.11916583912611718,
      "grad_norm": 9.984114646911621,
      "learning_rate": 4.9535237451411194e-05,
      "loss": 0.9651,
      "step": 720
    },
    {
      "epoch": 0.12082092022509103,
      "grad_norm": 4.741395473480225,
      "learning_rate": 4.951411188102079e-05,
      "loss": 1.508,
      "step": 730
    },
    {
      "epoch": 0.12247600132406487,
      "grad_norm": 6.255035400390625,
      "learning_rate": 4.949298631063039e-05,
      "loss": 0.9833,
      "step": 740
    },
    {
      "epoch": 0.12413108242303873,
      "grad_norm": 4.81515645980835,
      "learning_rate": 4.947186074023999e-05,
      "loss": 1.0659,
      "step": 750
    },
    {
      "epoch": 0.12578616352201258,
      "grad_norm": 8.93841552734375,
      "learning_rate": 4.945073516984959e-05,
      "loss": 1.0845,
      "step": 760
    },
    {
      "epoch": 0.12744124462098644,
      "grad_norm": 11.186384201049805,
      "learning_rate": 4.9429609599459186e-05,
      "loss": 1.0993,
      "step": 770
    },
    {
      "epoch": 0.12909632571996027,
      "grad_norm": 4.88622522354126,
      "learning_rate": 4.9408484029068784e-05,
      "loss": 1.2958,
      "step": 780
    },
    {
      "epoch": 0.13075140681893413,
      "grad_norm": 8.505717277526855,
      "learning_rate": 4.938735845867838e-05,
      "loss": 1.3057,
      "step": 790
    },
    {
      "epoch": 0.132406487917908,
      "grad_norm": 7.886797904968262,
      "learning_rate": 4.936623288828799e-05,
      "loss": 1.2592,
      "step": 800
    },
    {
      "epoch": 0.13406156901688182,
      "grad_norm": 5.561014652252197,
      "learning_rate": 4.9345107317897586e-05,
      "loss": 1.1112,
      "step": 810
    },
    {
      "epoch": 0.13571665011585568,
      "grad_norm": 7.795823574066162,
      "learning_rate": 4.9323981747507185e-05,
      "loss": 1.1049,
      "step": 820
    },
    {
      "epoch": 0.13737173121482954,
      "grad_norm": 4.678593158721924,
      "learning_rate": 4.930285617711678e-05,
      "loss": 0.9318,
      "step": 830
    },
    {
      "epoch": 0.13902681231380337,
      "grad_norm": 6.312258720397949,
      "learning_rate": 4.928173060672638e-05,
      "loss": 0.8961,
      "step": 840
    },
    {
      "epoch": 0.14068189341277723,
      "grad_norm": 5.5256123542785645,
      "learning_rate": 4.926060503633598e-05,
      "loss": 1.058,
      "step": 850
    },
    {
      "epoch": 0.14233697451175106,
      "grad_norm": 9.251810073852539,
      "learning_rate": 4.9239479465945585e-05,
      "loss": 1.0689,
      "step": 860
    },
    {
      "epoch": 0.14399205561072492,
      "grad_norm": 10.534529685974121,
      "learning_rate": 4.921835389555518e-05,
      "loss": 1.2571,
      "step": 870
    },
    {
      "epoch": 0.14564713670969878,
      "grad_norm": 8.464441299438477,
      "learning_rate": 4.919722832516478e-05,
      "loss": 0.7042,
      "step": 880
    },
    {
      "epoch": 0.14730221780867261,
      "grad_norm": 6.656115531921387,
      "learning_rate": 4.917610275477439e-05,
      "loss": 1.1147,
      "step": 890
    },
    {
      "epoch": 0.14895729890764647,
      "grad_norm": 5.315909385681152,
      "learning_rate": 4.9154977184383985e-05,
      "loss": 1.085,
      "step": 900
    },
    {
      "epoch": 0.15061238000662033,
      "grad_norm": 6.3923444747924805,
      "learning_rate": 4.9133851613993584e-05,
      "loss": 0.8607,
      "step": 910
    },
    {
      "epoch": 0.15226746110559417,
      "grad_norm": 1.0524730682373047,
      "learning_rate": 4.911272604360318e-05,
      "loss": 1.4105,
      "step": 920
    },
    {
      "epoch": 0.15392254220456802,
      "grad_norm": 8.541412353515625,
      "learning_rate": 4.909160047321278e-05,
      "loss": 1.2395,
      "step": 930
    },
    {
      "epoch": 0.15557762330354188,
      "grad_norm": 8.274399757385254,
      "learning_rate": 4.907047490282238e-05,
      "loss": 1.2217,
      "step": 940
    },
    {
      "epoch": 0.15723270440251572,
      "grad_norm": 12.976698875427246,
      "learning_rate": 4.904934933243198e-05,
      "loss": 0.9044,
      "step": 950
    },
    {
      "epoch": 0.15888778550148958,
      "grad_norm": 9.225919723510742,
      "learning_rate": 4.9028223762041575e-05,
      "loss": 1.6052,
      "step": 960
    },
    {
      "epoch": 0.16054286660046344,
      "grad_norm": 10.446148872375488,
      "learning_rate": 4.900709819165118e-05,
      "loss": 1.1844,
      "step": 970
    },
    {
      "epoch": 0.16219794769943727,
      "grad_norm": 7.481561660766602,
      "learning_rate": 4.898597262126078e-05,
      "loss": 0.9881,
      "step": 980
    },
    {
      "epoch": 0.16385302879841113,
      "grad_norm": 6.5786919593811035,
      "learning_rate": 4.896484705087038e-05,
      "loss": 1.2164,
      "step": 990
    },
    {
      "epoch": 0.16550810989738496,
      "grad_norm": 7.9727582931518555,
      "learning_rate": 4.8943721480479976e-05,
      "loss": 0.9195,
      "step": 1000
    },
    {
      "epoch": 0.16716319099635882,
      "grad_norm": 5.710793972015381,
      "learning_rate": 4.8922595910089574e-05,
      "loss": 1.2506,
      "step": 1010
    },
    {
      "epoch": 0.16881827209533268,
      "grad_norm": 5.972925662994385,
      "learning_rate": 4.890147033969917e-05,
      "loss": 0.9573,
      "step": 1020
    },
    {
      "epoch": 0.1704733531943065,
      "grad_norm": 4.672503471374512,
      "learning_rate": 4.888034476930877e-05,
      "loss": 1.076,
      "step": 1030
    },
    {
      "epoch": 0.17212843429328037,
      "grad_norm": 8.415233612060547,
      "learning_rate": 4.885921919891837e-05,
      "loss": 0.8532,
      "step": 1040
    },
    {
      "epoch": 0.17378351539225423,
      "grad_norm": 5.87423038482666,
      "learning_rate": 4.883809362852797e-05,
      "loss": 1.0938,
      "step": 1050
    },
    {
      "epoch": 0.17543859649122806,
      "grad_norm": 7.390071868896484,
      "learning_rate": 4.881696805813757e-05,
      "loss": 0.9239,
      "step": 1060
    },
    {
      "epoch": 0.17709367759020192,
      "grad_norm": 7.279072284698486,
      "learning_rate": 4.879584248774717e-05,
      "loss": 1.4614,
      "step": 1070
    },
    {
      "epoch": 0.17874875868917578,
      "grad_norm": 7.712642669677734,
      "learning_rate": 4.877471691735677e-05,
      "loss": 1.2552,
      "step": 1080
    },
    {
      "epoch": 0.1804038397881496,
      "grad_norm": 5.452831745147705,
      "learning_rate": 4.875359134696637e-05,
      "loss": 1.0082,
      "step": 1090
    },
    {
      "epoch": 0.18205892088712347,
      "grad_norm": 3.5975940227508545,
      "learning_rate": 4.8732465776575966e-05,
      "loss": 1.0337,
      "step": 1100
    },
    {
      "epoch": 0.18371400198609733,
      "grad_norm": 5.293908596038818,
      "learning_rate": 4.871134020618557e-05,
      "loss": 1.1785,
      "step": 1110
    },
    {
      "epoch": 0.18536908308507116,
      "grad_norm": 6.499624252319336,
      "learning_rate": 4.869021463579517e-05,
      "loss": 0.9932,
      "step": 1120
    },
    {
      "epoch": 0.18702416418404502,
      "grad_norm": 6.528020858764648,
      "learning_rate": 4.866908906540477e-05,
      "loss": 1.0532,
      "step": 1130
    },
    {
      "epoch": 0.18867924528301888,
      "grad_norm": 6.886061668395996,
      "learning_rate": 4.8647963495014373e-05,
      "loss": 1.2608,
      "step": 1140
    },
    {
      "epoch": 0.1903343263819927,
      "grad_norm": 1.0990080833435059,
      "learning_rate": 4.862683792462397e-05,
      "loss": 0.6045,
      "step": 1150
    },
    {
      "epoch": 0.19198940748096657,
      "grad_norm": 9.182982444763184,
      "learning_rate": 4.860571235423357e-05,
      "loss": 1.1212,
      "step": 1160
    },
    {
      "epoch": 0.1936444885799404,
      "grad_norm": 12.97887134552002,
      "learning_rate": 4.858458678384317e-05,
      "loss": 1.2021,
      "step": 1170
    },
    {
      "epoch": 0.19529956967891426,
      "grad_norm": 10.343672752380371,
      "learning_rate": 4.856346121345277e-05,
      "loss": 0.9037,
      "step": 1180
    },
    {
      "epoch": 0.19695465077788812,
      "grad_norm": 5.573896884918213,
      "learning_rate": 4.8542335643062365e-05,
      "loss": 1.181,
      "step": 1190
    },
    {
      "epoch": 0.19860973187686196,
      "grad_norm": 4.694684982299805,
      "learning_rate": 4.8521210072671964e-05,
      "loss": 1.2865,
      "step": 1200
    },
    {
      "epoch": 0.20026481297583582,
      "grad_norm": 6.90242862701416,
      "learning_rate": 4.850008450228156e-05,
      "loss": 1.0798,
      "step": 1210
    },
    {
      "epoch": 0.20191989407480967,
      "grad_norm": 6.095139980316162,
      "learning_rate": 4.847895893189116e-05,
      "loss": 1.0511,
      "step": 1220
    },
    {
      "epoch": 0.2035749751737835,
      "grad_norm": 7.662085056304932,
      "learning_rate": 4.8457833361500766e-05,
      "loss": 1.1618,
      "step": 1230
    },
    {
      "epoch": 0.20523005627275737,
      "grad_norm": 4.982565879821777,
      "learning_rate": 4.8436707791110364e-05,
      "loss": 0.8293,
      "step": 1240
    },
    {
      "epoch": 0.20688513737173123,
      "grad_norm": 11.322824478149414,
      "learning_rate": 4.841558222071996e-05,
      "loss": 0.8927,
      "step": 1250
    },
    {
      "epoch": 0.20854021847070506,
      "grad_norm": 4.266307353973389,
      "learning_rate": 4.839445665032956e-05,
      "loss": 0.7087,
      "step": 1260
    },
    {
      "epoch": 0.21019529956967892,
      "grad_norm": 9.22838306427002,
      "learning_rate": 4.837333107993916e-05,
      "loss": 1.0479,
      "step": 1270
    },
    {
      "epoch": 0.21185038066865278,
      "grad_norm": 14.849045753479004,
      "learning_rate": 4.835220550954876e-05,
      "loss": 0.9063,
      "step": 1280
    },
    {
      "epoch": 0.2135054617676266,
      "grad_norm": 12.852745056152344,
      "learning_rate": 4.8331079939158356e-05,
      "loss": 0.9111,
      "step": 1290
    },
    {
      "epoch": 0.21516054286660047,
      "grad_norm": 10.388618469238281,
      "learning_rate": 4.8309954368767954e-05,
      "loss": 0.8381,
      "step": 1300
    },
    {
      "epoch": 0.2168156239655743,
      "grad_norm": 8.935135841369629,
      "learning_rate": 4.828882879837756e-05,
      "loss": 0.793,
      "step": 1310
    },
    {
      "epoch": 0.21847070506454816,
      "grad_norm": 9.99417495727539,
      "learning_rate": 4.826770322798716e-05,
      "loss": 1.4309,
      "step": 1320
    },
    {
      "epoch": 0.22012578616352202,
      "grad_norm": 9.451557159423828,
      "learning_rate": 4.8246577657596756e-05,
      "loss": 1.148,
      "step": 1330
    },
    {
      "epoch": 0.22178086726249585,
      "grad_norm": 5.033353328704834,
      "learning_rate": 4.8225452087206355e-05,
      "loss": 0.7394,
      "step": 1340
    },
    {
      "epoch": 0.2234359483614697,
      "grad_norm": 7.628906726837158,
      "learning_rate": 4.820432651681595e-05,
      "loss": 0.7979,
      "step": 1350
    },
    {
      "epoch": 0.22509102946044357,
      "grad_norm": 4.09216833114624,
      "learning_rate": 4.818320094642556e-05,
      "loss": 0.9801,
      "step": 1360
    },
    {
      "epoch": 0.2267461105594174,
      "grad_norm": 9.092968940734863,
      "learning_rate": 4.816207537603516e-05,
      "loss": 1.5788,
      "step": 1370
    },
    {
      "epoch": 0.22840119165839126,
      "grad_norm": 8.166152000427246,
      "learning_rate": 4.8140949805644755e-05,
      "loss": 0.7949,
      "step": 1380
    },
    {
      "epoch": 0.23005627275736512,
      "grad_norm": 4.641820907592773,
      "learning_rate": 4.811982423525435e-05,
      "loss": 1.2087,
      "step": 1390
    },
    {
      "epoch": 0.23171135385633895,
      "grad_norm": 7.541827201843262,
      "learning_rate": 4.809869866486396e-05,
      "loss": 1.0402,
      "step": 1400
    },
    {
      "epoch": 0.2333664349553128,
      "grad_norm": 4.966800689697266,
      "learning_rate": 4.807757309447356e-05,
      "loss": 1.2821,
      "step": 1410
    },
    {
      "epoch": 0.23502151605428667,
      "grad_norm": 4.65952730178833,
      "learning_rate": 4.8056447524083155e-05,
      "loss": 1.1426,
      "step": 1420
    },
    {
      "epoch": 0.2366765971532605,
      "grad_norm": 7.216237545013428,
      "learning_rate": 4.8035321953692754e-05,
      "loss": 0.898,
      "step": 1430
    },
    {
      "epoch": 0.23833167825223436,
      "grad_norm": 9.235370635986328,
      "learning_rate": 4.801419638330235e-05,
      "loss": 0.9671,
      "step": 1440
    },
    {
      "epoch": 0.2399867593512082,
      "grad_norm": 4.024963855743408,
      "learning_rate": 4.799307081291195e-05,
      "loss": 0.6373,
      "step": 1450
    },
    {
      "epoch": 0.24164184045018205,
      "grad_norm": 6.676187038421631,
      "learning_rate": 4.797194524252155e-05,
      "loss": 1.0034,
      "step": 1460
    },
    {
      "epoch": 0.24329692154915591,
      "grad_norm": 6.9953413009643555,
      "learning_rate": 4.795081967213115e-05,
      "loss": 1.2088,
      "step": 1470
    },
    {
      "epoch": 0.24495200264812975,
      "grad_norm": 2.9078562259674072,
      "learning_rate": 4.792969410174075e-05,
      "loss": 0.8398,
      "step": 1480
    },
    {
      "epoch": 0.2466070837471036,
      "grad_norm": 6.16707706451416,
      "learning_rate": 4.790856853135035e-05,
      "loss": 1.1536,
      "step": 1490
    },
    {
      "epoch": 0.24826216484607747,
      "grad_norm": 4.324840545654297,
      "learning_rate": 4.788744296095995e-05,
      "loss": 1.0408,
      "step": 1500
    },
    {
      "epoch": 0.2499172459450513,
      "grad_norm": 15.712027549743652,
      "learning_rate": 4.786631739056955e-05,
      "loss": 0.9953,
      "step": 1510
    },
    {
      "epoch": 0.25157232704402516,
      "grad_norm": 9.069661140441895,
      "learning_rate": 4.7845191820179146e-05,
      "loss": 1.2565,
      "step": 1520
    },
    {
      "epoch": 0.253227408142999,
      "grad_norm": 12.16580581665039,
      "learning_rate": 4.7824066249788744e-05,
      "loss": 0.9488,
      "step": 1530
    },
    {
      "epoch": 0.2548824892419729,
      "grad_norm": 13.393353462219238,
      "learning_rate": 4.780294067939834e-05,
      "loss": 0.733,
      "step": 1540
    },
    {
      "epoch": 0.2565375703409467,
      "grad_norm": 3.2282142639160156,
      "learning_rate": 4.778181510900794e-05,
      "loss": 0.852,
      "step": 1550
    },
    {
      "epoch": 0.25819265143992054,
      "grad_norm": 6.335540294647217,
      "learning_rate": 4.776068953861754e-05,
      "loss": 1.0715,
      "step": 1560
    },
    {
      "epoch": 0.2598477325388944,
      "grad_norm": 9.485562324523926,
      "learning_rate": 4.7739563968227145e-05,
      "loss": 0.7638,
      "step": 1570
    },
    {
      "epoch": 0.26150281363786826,
      "grad_norm": 5.468998908996582,
      "learning_rate": 4.771843839783674e-05,
      "loss": 0.796,
      "step": 1580
    },
    {
      "epoch": 0.2631578947368421,
      "grad_norm": 4.316401481628418,
      "learning_rate": 4.769731282744634e-05,
      "loss": 0.9601,
      "step": 1590
    },
    {
      "epoch": 0.264812975835816,
      "grad_norm": 8.861205101013184,
      "learning_rate": 4.767618725705594e-05,
      "loss": 0.824,
      "step": 1600
    },
    {
      "epoch": 0.2664680569347898,
      "grad_norm": 5.394019603729248,
      "learning_rate": 4.7655061686665545e-05,
      "loss": 0.8,
      "step": 1610
    },
    {
      "epoch": 0.26812313803376364,
      "grad_norm": 6.36236047744751,
      "learning_rate": 4.763393611627514e-05,
      "loss": 1.3162,
      "step": 1620
    },
    {
      "epoch": 0.26977821913273753,
      "grad_norm": 5.691561222076416,
      "learning_rate": 4.761281054588474e-05,
      "loss": 0.7874,
      "step": 1630
    },
    {
      "epoch": 0.27143330023171136,
      "grad_norm": 11.217209815979004,
      "learning_rate": 4.759168497549434e-05,
      "loss": 1.2582,
      "step": 1640
    },
    {
      "epoch": 0.2730883813306852,
      "grad_norm": 11.502766609191895,
      "learning_rate": 4.7570559405103945e-05,
      "loss": 0.9681,
      "step": 1650
    },
    {
      "epoch": 0.2747434624296591,
      "grad_norm": 5.960086822509766,
      "learning_rate": 4.7549433834713544e-05,
      "loss": 0.9101,
      "step": 1660
    },
    {
      "epoch": 0.2763985435286329,
      "grad_norm": 4.755646228790283,
      "learning_rate": 4.752830826432314e-05,
      "loss": 1.1065,
      "step": 1670
    },
    {
      "epoch": 0.27805362462760674,
      "grad_norm": 5.744128227233887,
      "learning_rate": 4.750718269393274e-05,
      "loss": 0.7898,
      "step": 1680
    },
    {
      "epoch": 0.2797087057265806,
      "grad_norm": 6.398251056671143,
      "learning_rate": 4.748605712354234e-05,
      "loss": 0.881,
      "step": 1690
    },
    {
      "epoch": 0.28136378682555446,
      "grad_norm": 11.929609298706055,
      "learning_rate": 4.746493155315194e-05,
      "loss": 1.1886,
      "step": 1700
    },
    {
      "epoch": 0.2830188679245283,
      "grad_norm": 11.165188789367676,
      "learning_rate": 4.7443805982761536e-05,
      "loss": 1.3147,
      "step": 1710
    },
    {
      "epoch": 0.2846739490235021,
      "grad_norm": 7.1000471115112305,
      "learning_rate": 4.7422680412371134e-05,
      "loss": 1.0023,
      "step": 1720
    },
    {
      "epoch": 0.286329030122476,
      "grad_norm": 8.47039794921875,
      "learning_rate": 4.740155484198073e-05,
      "loss": 0.9653,
      "step": 1730
    },
    {
      "epoch": 0.28798411122144985,
      "grad_norm": 6.560060024261475,
      "learning_rate": 4.738042927159034e-05,
      "loss": 1.1021,
      "step": 1740
    },
    {
      "epoch": 0.2896391923204237,
      "grad_norm": 13.052764892578125,
      "learning_rate": 4.7359303701199936e-05,
      "loss": 1.2055,
      "step": 1750
    },
    {
      "epoch": 0.29129427341939756,
      "grad_norm": 3.893538475036621,
      "learning_rate": 4.7338178130809534e-05,
      "loss": 1.0219,
      "step": 1760
    },
    {
      "epoch": 0.2929493545183714,
      "grad_norm": 4.123079776763916,
      "learning_rate": 4.731705256041913e-05,
      "loss": 1.0483,
      "step": 1770
    },
    {
      "epoch": 0.29460443561734523,
      "grad_norm": 8.050045013427734,
      "learning_rate": 4.729592699002873e-05,
      "loss": 1.1029,
      "step": 1780
    },
    {
      "epoch": 0.2962595167163191,
      "grad_norm": 5.392127513885498,
      "learning_rate": 4.727480141963833e-05,
      "loss": 0.8663,
      "step": 1790
    },
    {
      "epoch": 0.29791459781529295,
      "grad_norm": 7.382465362548828,
      "learning_rate": 4.725367584924793e-05,
      "loss": 0.6713,
      "step": 1800
    },
    {
      "epoch": 0.2995696789142668,
      "grad_norm": 8.146446228027344,
      "learning_rate": 4.7232550278857526e-05,
      "loss": 1.055,
      "step": 1810
    },
    {
      "epoch": 0.30122476001324067,
      "grad_norm": 8.0614013671875,
      "learning_rate": 4.721142470846713e-05,
      "loss": 1.318,
      "step": 1820
    },
    {
      "epoch": 0.3028798411122145,
      "grad_norm": 3.2113471031188965,
      "learning_rate": 4.719029913807673e-05,
      "loss": 0.8232,
      "step": 1830
    },
    {
      "epoch": 0.30453492221118833,
      "grad_norm": 4.718456268310547,
      "learning_rate": 4.716917356768633e-05,
      "loss": 0.8375,
      "step": 1840
    },
    {
      "epoch": 0.3061900033101622,
      "grad_norm": 5.668194770812988,
      "learning_rate": 4.714804799729593e-05,
      "loss": 0.8218,
      "step": 1850
    },
    {
      "epoch": 0.30784508440913605,
      "grad_norm": 8.703760147094727,
      "learning_rate": 4.712692242690553e-05,
      "loss": 1.3189,
      "step": 1860
    },
    {
      "epoch": 0.3095001655081099,
      "grad_norm": 6.846735954284668,
      "learning_rate": 4.710579685651513e-05,
      "loss": 1.0485,
      "step": 1870
    },
    {
      "epoch": 0.31115524660708377,
      "grad_norm": 9.82274055480957,
      "learning_rate": 4.708467128612473e-05,
      "loss": 1.0404,
      "step": 1880
    },
    {
      "epoch": 0.3128103277060576,
      "grad_norm": 5.211826801300049,
      "learning_rate": 4.706354571573433e-05,
      "loss": 1.2764,
      "step": 1890
    },
    {
      "epoch": 0.31446540880503143,
      "grad_norm": 6.238792896270752,
      "learning_rate": 4.7042420145343925e-05,
      "loss": 0.9796,
      "step": 1900
    },
    {
      "epoch": 0.3161204899040053,
      "grad_norm": 14.759909629821777,
      "learning_rate": 4.702129457495353e-05,
      "loss": 1.1424,
      "step": 1910
    },
    {
      "epoch": 0.31777557100297915,
      "grad_norm": 8.369919776916504,
      "learning_rate": 4.700016900456313e-05,
      "loss": 1.0176,
      "step": 1920
    },
    {
      "epoch": 0.319430652101953,
      "grad_norm": 11.669900894165039,
      "learning_rate": 4.697904343417273e-05,
      "loss": 0.9761,
      "step": 1930
    },
    {
      "epoch": 0.32108573320092687,
      "grad_norm": 12.827033996582031,
      "learning_rate": 4.6957917863782325e-05,
      "loss": 0.9359,
      "step": 1940
    },
    {
      "epoch": 0.3227408142999007,
      "grad_norm": 7.350112438201904,
      "learning_rate": 4.6936792293391924e-05,
      "loss": 0.8662,
      "step": 1950
    },
    {
      "epoch": 0.32439589539887453,
      "grad_norm": 9.767650604248047,
      "learning_rate": 4.691566672300152e-05,
      "loss": 0.9339,
      "step": 1960
    },
    {
      "epoch": 0.3260509764978484,
      "grad_norm": 6.722286701202393,
      "learning_rate": 4.689454115261112e-05,
      "loss": 0.8763,
      "step": 1970
    },
    {
      "epoch": 0.32770605759682225,
      "grad_norm": 5.984364986419678,
      "learning_rate": 4.687341558222072e-05,
      "loss": 0.6197,
      "step": 1980
    },
    {
      "epoch": 0.3293611386957961,
      "grad_norm": 8.830606460571289,
      "learning_rate": 4.6852290011830324e-05,
      "loss": 0.7596,
      "step": 1990
    },
    {
      "epoch": 0.3310162197947699,
      "grad_norm": 10.46382999420166,
      "learning_rate": 4.683116444143992e-05,
      "loss": 0.9028,
      "step": 2000
    },
    {
      "epoch": 0.3326713008937438,
      "grad_norm": 4.29202127456665,
      "learning_rate": 4.681003887104952e-05,
      "loss": 0.9622,
      "step": 2010
    },
    {
      "epoch": 0.33432638199271764,
      "grad_norm": 5.198490619659424,
      "learning_rate": 4.678891330065912e-05,
      "loss": 1.1256,
      "step": 2020
    },
    {
      "epoch": 0.33598146309169147,
      "grad_norm": 7.314266204833984,
      "learning_rate": 4.676778773026872e-05,
      "loss": 0.7641,
      "step": 2030
    },
    {
      "epoch": 0.33763654419066536,
      "grad_norm": 7.8869853019714355,
      "learning_rate": 4.6746662159878316e-05,
      "loss": 1.215,
      "step": 2040
    },
    {
      "epoch": 0.3392916252896392,
      "grad_norm": 4.416752338409424,
      "learning_rate": 4.6725536589487914e-05,
      "loss": 0.9831,
      "step": 2050
    },
    {
      "epoch": 0.340946706388613,
      "grad_norm": 7.837658405303955,
      "learning_rate": 4.670441101909751e-05,
      "loss": 1.278,
      "step": 2060
    },
    {
      "epoch": 0.3426017874875869,
      "grad_norm": 11.287732124328613,
      "learning_rate": 4.668328544870712e-05,
      "loss": 1.1857,
      "step": 2070
    },
    {
      "epoch": 0.34425686858656074,
      "grad_norm": 6.965438365936279,
      "learning_rate": 4.6662159878316716e-05,
      "loss": 1.3615,
      "step": 2080
    },
    {
      "epoch": 0.34591194968553457,
      "grad_norm": 12.797938346862793,
      "learning_rate": 4.6641034307926315e-05,
      "loss": 1.0073,
      "step": 2090
    },
    {
      "epoch": 0.34756703078450846,
      "grad_norm": 7.017601013183594,
      "learning_rate": 4.661990873753592e-05,
      "loss": 0.9076,
      "step": 2100
    },
    {
      "epoch": 0.3492221118834823,
      "grad_norm": 15.2490234375,
      "learning_rate": 4.659878316714552e-05,
      "loss": 0.9688,
      "step": 2110
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 6.6677656173706055,
      "learning_rate": 4.657765759675512e-05,
      "loss": 0.8588,
      "step": 2120
    },
    {
      "epoch": 0.35253227408143,
      "grad_norm": 2.888070821762085,
      "learning_rate": 4.6556532026364715e-05,
      "loss": 1.1205,
      "step": 2130
    },
    {
      "epoch": 0.35418735518040384,
      "grad_norm": 4.525887489318848,
      "learning_rate": 4.6535406455974313e-05,
      "loss": 0.7531,
      "step": 2140
    },
    {
      "epoch": 0.35584243627937767,
      "grad_norm": 13.461175918579102,
      "learning_rate": 4.651428088558391e-05,
      "loss": 1.1408,
      "step": 2150
    },
    {
      "epoch": 0.35749751737835156,
      "grad_norm": 4.689554214477539,
      "learning_rate": 4.649315531519352e-05,
      "loss": 1.0282,
      "step": 2160
    },
    {
      "epoch": 0.3591525984773254,
      "grad_norm": 4.277727127075195,
      "learning_rate": 4.6472029744803115e-05,
      "loss": 1.1317,
      "step": 2170
    },
    {
      "epoch": 0.3608076795762992,
      "grad_norm": 5.21662712097168,
      "learning_rate": 4.6450904174412714e-05,
      "loss": 0.9825,
      "step": 2180
    },
    {
      "epoch": 0.3624627606752731,
      "grad_norm": 13.37752914428711,
      "learning_rate": 4.642977860402231e-05,
      "loss": 1.1966,
      "step": 2190
    },
    {
      "epoch": 0.36411784177424694,
      "grad_norm": 5.78953218460083,
      "learning_rate": 4.640865303363191e-05,
      "loss": 0.9366,
      "step": 2200
    },
    {
      "epoch": 0.3657729228732208,
      "grad_norm": 14.460609436035156,
      "learning_rate": 4.638752746324151e-05,
      "loss": 1.1944,
      "step": 2210
    },
    {
      "epoch": 0.36742800397219466,
      "grad_norm": 6.542474269866943,
      "learning_rate": 4.636640189285111e-05,
      "loss": 0.9912,
      "step": 2220
    },
    {
      "epoch": 0.3690830850711685,
      "grad_norm": 6.891678333282471,
      "learning_rate": 4.6345276322460706e-05,
      "loss": 0.761,
      "step": 2230
    },
    {
      "epoch": 0.3707381661701423,
      "grad_norm": 14.132966041564941,
      "learning_rate": 4.6324150752070304e-05,
      "loss": 1.1693,
      "step": 2240
    },
    {
      "epoch": 0.3723932472691162,
      "grad_norm": 3.1992459297180176,
      "learning_rate": 4.630302518167991e-05,
      "loss": 1.1172,
      "step": 2250
    },
    {
      "epoch": 0.37404832836809004,
      "grad_norm": 4.0023322105407715,
      "learning_rate": 4.628189961128951e-05,
      "loss": 0.7275,
      "step": 2260
    },
    {
      "epoch": 0.3757034094670639,
      "grad_norm": 10.657421112060547,
      "learning_rate": 4.6260774040899106e-05,
      "loss": 0.6986,
      "step": 2270
    },
    {
      "epoch": 0.37735849056603776,
      "grad_norm": 14.291274070739746,
      "learning_rate": 4.6239648470508704e-05,
      "loss": 0.7852,
      "step": 2280
    },
    {
      "epoch": 0.3790135716650116,
      "grad_norm": 5.616158962249756,
      "learning_rate": 4.62185229001183e-05,
      "loss": 1.0561,
      "step": 2290
    },
    {
      "epoch": 0.3806686527639854,
      "grad_norm": 9.056979179382324,
      "learning_rate": 4.61973973297279e-05,
      "loss": 0.9167,
      "step": 2300
    },
    {
      "epoch": 0.38232373386295926,
      "grad_norm": 10.916108131408691,
      "learning_rate": 4.61762717593375e-05,
      "loss": 1.1045,
      "step": 2310
    },
    {
      "epoch": 0.38397881496193315,
      "grad_norm": 7.0765581130981445,
      "learning_rate": 4.6155146188947105e-05,
      "loss": 0.7871,
      "step": 2320
    },
    {
      "epoch": 0.385633896060907,
      "grad_norm": 13.04115104675293,
      "learning_rate": 4.61340206185567e-05,
      "loss": 0.6958,
      "step": 2330
    },
    {
      "epoch": 0.3872889771598808,
      "grad_norm": 6.533575534820557,
      "learning_rate": 4.61128950481663e-05,
      "loss": 0.9052,
      "step": 2340
    },
    {
      "epoch": 0.3889440582588547,
      "grad_norm": 6.860217571258545,
      "learning_rate": 4.6091769477775907e-05,
      "loss": 0.8782,
      "step": 2350
    },
    {
      "epoch": 0.39059913935782853,
      "grad_norm": 13.65745735168457,
      "learning_rate": 4.6070643907385505e-05,
      "loss": 0.9703,
      "step": 2360
    },
    {
      "epoch": 0.39225422045680236,
      "grad_norm": 3.47294545173645,
      "learning_rate": 4.60495183369951e-05,
      "loss": 0.664,
      "step": 2370
    },
    {
      "epoch": 0.39390930155577625,
      "grad_norm": 11.261844635009766,
      "learning_rate": 4.60283927666047e-05,
      "loss": 1.1676,
      "step": 2380
    },
    {
      "epoch": 0.3955643826547501,
      "grad_norm": 11.997441291809082,
      "learning_rate": 4.60072671962143e-05,
      "loss": 1.0542,
      "step": 2390
    },
    {
      "epoch": 0.3972194637537239,
      "grad_norm": 6.871723651885986,
      "learning_rate": 4.59861416258239e-05,
      "loss": 0.9611,
      "step": 2400
    },
    {
      "epoch": 0.3988745448526978,
      "grad_norm": 5.230261325836182,
      "learning_rate": 4.59650160554335e-05,
      "loss": 0.7119,
      "step": 2410
    },
    {
      "epoch": 0.40052962595167163,
      "grad_norm": 4.8493499755859375,
      "learning_rate": 4.59438904850431e-05,
      "loss": 1.0232,
      "step": 2420
    },
    {
      "epoch": 0.40218470705064546,
      "grad_norm": 6.881191730499268,
      "learning_rate": 4.59227649146527e-05,
      "loss": 0.9283,
      "step": 2430
    },
    {
      "epoch": 0.40383978814961935,
      "grad_norm": 6.700901031494141,
      "learning_rate": 4.59016393442623e-05,
      "loss": 0.8026,
      "step": 2440
    },
    {
      "epoch": 0.4054948692485932,
      "grad_norm": 6.372035503387451,
      "learning_rate": 4.58805137738719e-05,
      "loss": 0.7256,
      "step": 2450
    },
    {
      "epoch": 0.407149950347567,
      "grad_norm": 4.700034141540527,
      "learning_rate": 4.5859388203481496e-05,
      "loss": 1.1296,
      "step": 2460
    },
    {
      "epoch": 0.4088050314465409,
      "grad_norm": 1.616067886352539,
      "learning_rate": 4.5838262633091094e-05,
      "loss": 0.6541,
      "step": 2470
    },
    {
      "epoch": 0.41046011254551473,
      "grad_norm": 9.335246086120605,
      "learning_rate": 4.581713706270069e-05,
      "loss": 0.688,
      "step": 2480
    },
    {
      "epoch": 0.41211519364448856,
      "grad_norm": 8.422526359558105,
      "learning_rate": 4.579601149231029e-05,
      "loss": 0.7278,
      "step": 2490
    },
    {
      "epoch": 0.41377027474346245,
      "grad_norm": 9.080416679382324,
      "learning_rate": 4.5774885921919896e-05,
      "loss": 0.9465,
      "step": 2500
    },
    {
      "epoch": 0.4154253558424363,
      "grad_norm": 6.632617950439453,
      "learning_rate": 4.5753760351529494e-05,
      "loss": 0.9072,
      "step": 2510
    },
    {
      "epoch": 0.4170804369414101,
      "grad_norm": 3.826263904571533,
      "learning_rate": 4.573263478113909e-05,
      "loss": 0.9312,
      "step": 2520
    },
    {
      "epoch": 0.418735518040384,
      "grad_norm": 2.985973596572876,
      "learning_rate": 4.571150921074869e-05,
      "loss": 0.7634,
      "step": 2530
    },
    {
      "epoch": 0.42039059913935783,
      "grad_norm": 9.88821029663086,
      "learning_rate": 4.569038364035829e-05,
      "loss": 0.8229,
      "step": 2540
    },
    {
      "epoch": 0.42204568023833167,
      "grad_norm": 6.639096260070801,
      "learning_rate": 4.566925806996789e-05,
      "loss": 0.7482,
      "step": 2550
    },
    {
      "epoch": 0.42370076133730555,
      "grad_norm": 8.494214057922363,
      "learning_rate": 4.5648132499577486e-05,
      "loss": 0.7668,
      "step": 2560
    },
    {
      "epoch": 0.4253558424362794,
      "grad_norm": 2.4266209602355957,
      "learning_rate": 4.562700692918709e-05,
      "loss": 0.7687,
      "step": 2570
    },
    {
      "epoch": 0.4270109235352532,
      "grad_norm": 12.757013320922852,
      "learning_rate": 4.560588135879669e-05,
      "loss": 0.8125,
      "step": 2580
    },
    {
      "epoch": 0.4286660046342271,
      "grad_norm": 8.268671989440918,
      "learning_rate": 4.558475578840629e-05,
      "loss": 0.9711,
      "step": 2590
    },
    {
      "epoch": 0.43032108573320094,
      "grad_norm": 6.398922920227051,
      "learning_rate": 4.556363021801589e-05,
      "loss": 0.7699,
      "step": 2600
    },
    {
      "epoch": 0.43197616683217477,
      "grad_norm": 6.036654949188232,
      "learning_rate": 4.554250464762549e-05,
      "loss": 0.9779,
      "step": 2610
    },
    {
      "epoch": 0.4336312479311486,
      "grad_norm": 2.2752764225006104,
      "learning_rate": 4.552137907723509e-05,
      "loss": 0.6848,
      "step": 2620
    },
    {
      "epoch": 0.4352863290301225,
      "grad_norm": 6.142755031585693,
      "learning_rate": 4.550025350684469e-05,
      "loss": 0.832,
      "step": 2630
    },
    {
      "epoch": 0.4369414101290963,
      "grad_norm": 3.3886430263519287,
      "learning_rate": 4.547912793645429e-05,
      "loss": 0.9572,
      "step": 2640
    },
    {
      "epoch": 0.43859649122807015,
      "grad_norm": 2.131927013397217,
      "learning_rate": 4.5458002366063885e-05,
      "loss": 1.1079,
      "step": 2650
    },
    {
      "epoch": 0.44025157232704404,
      "grad_norm": 10.101700782775879,
      "learning_rate": 4.5436876795673484e-05,
      "loss": 0.9348,
      "step": 2660
    },
    {
      "epoch": 0.44190665342601787,
      "grad_norm": 7.716516494750977,
      "learning_rate": 4.541575122528309e-05,
      "loss": 0.9035,
      "step": 2670
    },
    {
      "epoch": 0.4435617345249917,
      "grad_norm": 8.790989875793457,
      "learning_rate": 4.539462565489269e-05,
      "loss": 0.9221,
      "step": 2680
    },
    {
      "epoch": 0.4452168156239656,
      "grad_norm": 7.028026580810547,
      "learning_rate": 4.5373500084502286e-05,
      "loss": 0.7573,
      "step": 2690
    },
    {
      "epoch": 0.4468718967229394,
      "grad_norm": 6.49647331237793,
      "learning_rate": 4.5352374514111884e-05,
      "loss": 0.913,
      "step": 2700
    },
    {
      "epoch": 0.44852697782191325,
      "grad_norm": 8.924870491027832,
      "learning_rate": 4.533124894372148e-05,
      "loss": 1.1755,
      "step": 2710
    },
    {
      "epoch": 0.45018205892088714,
      "grad_norm": 6.205325603485107,
      "learning_rate": 4.531012337333108e-05,
      "loss": 0.7005,
      "step": 2720
    },
    {
      "epoch": 0.45183714001986097,
      "grad_norm": 12.115555763244629,
      "learning_rate": 4.528899780294068e-05,
      "loss": 0.6898,
      "step": 2730
    },
    {
      "epoch": 0.4534922211188348,
      "grad_norm": 5.578763008117676,
      "learning_rate": 4.526787223255028e-05,
      "loss": 0.8865,
      "step": 2740
    },
    {
      "epoch": 0.4551473022178087,
      "grad_norm": 9.250011444091797,
      "learning_rate": 4.5246746662159876e-05,
      "loss": 0.9077,
      "step": 2750
    },
    {
      "epoch": 0.4568023833167825,
      "grad_norm": 6.993340492248535,
      "learning_rate": 4.522562109176948e-05,
      "loss": 1.1887,
      "step": 2760
    },
    {
      "epoch": 0.45845746441575635,
      "grad_norm": 5.107805252075195,
      "learning_rate": 4.520449552137908e-05,
      "loss": 0.8029,
      "step": 2770
    },
    {
      "epoch": 0.46011254551473024,
      "grad_norm": 4.955021858215332,
      "learning_rate": 4.518336995098868e-05,
      "loss": 0.9129,
      "step": 2780
    },
    {
      "epoch": 0.4617676266137041,
      "grad_norm": 5.654029369354248,
      "learning_rate": 4.5162244380598276e-05,
      "loss": 1.0356,
      "step": 2790
    },
    {
      "epoch": 0.4634227077126779,
      "grad_norm": 7.8578104972839355,
      "learning_rate": 4.5141118810207875e-05,
      "loss": 0.9541,
      "step": 2800
    },
    {
      "epoch": 0.4650777888116518,
      "grad_norm": 2.639310836791992,
      "learning_rate": 4.511999323981747e-05,
      "loss": 0.8483,
      "step": 2810
    },
    {
      "epoch": 0.4667328699106256,
      "grad_norm": 11.16014289855957,
      "learning_rate": 4.509886766942708e-05,
      "loss": 0.603,
      "step": 2820
    },
    {
      "epoch": 0.46838795100959946,
      "grad_norm": 9.685157775878906,
      "learning_rate": 4.5077742099036676e-05,
      "loss": 0.7042,
      "step": 2830
    },
    {
      "epoch": 0.47004303210857334,
      "grad_norm": 6.205262184143066,
      "learning_rate": 4.505661652864628e-05,
      "loss": 0.8134,
      "step": 2840
    },
    {
      "epoch": 0.4716981132075472,
      "grad_norm": 6.234001159667969,
      "learning_rate": 4.503549095825588e-05,
      "loss": 0.8736,
      "step": 2850
    },
    {
      "epoch": 0.473353194306521,
      "grad_norm": 6.411564350128174,
      "learning_rate": 4.501436538786548e-05,
      "loss": 0.9473,
      "step": 2860
    },
    {
      "epoch": 0.4750082754054949,
      "grad_norm": 9.144033432006836,
      "learning_rate": 4.499323981747508e-05,
      "loss": 0.8906,
      "step": 2870
    },
    {
      "epoch": 0.4766633565044687,
      "grad_norm": 8.112288475036621,
      "learning_rate": 4.4972114247084675e-05,
      "loss": 1.0941,
      "step": 2880
    },
    {
      "epoch": 0.47831843760344256,
      "grad_norm": 3.6994872093200684,
      "learning_rate": 4.4950988676694274e-05,
      "loss": 0.9745,
      "step": 2890
    },
    {
      "epoch": 0.4799735187024164,
      "grad_norm": 6.30675745010376,
      "learning_rate": 4.492986310630387e-05,
      "loss": 0.8931,
      "step": 2900
    },
    {
      "epoch": 0.4816285998013903,
      "grad_norm": 8.197927474975586,
      "learning_rate": 4.490873753591347e-05,
      "loss": 0.6748,
      "step": 2910
    },
    {
      "epoch": 0.4832836809003641,
      "grad_norm": 8.401453018188477,
      "learning_rate": 4.488761196552307e-05,
      "loss": 1.115,
      "step": 2920
    },
    {
      "epoch": 0.48493876199933794,
      "grad_norm": 6.450730323791504,
      "learning_rate": 4.4866486395132674e-05,
      "loss": 0.9554,
      "step": 2930
    },
    {
      "epoch": 0.48659384309831183,
      "grad_norm": 4.708397388458252,
      "learning_rate": 4.484536082474227e-05,
      "loss": 0.7489,
      "step": 2940
    },
    {
      "epoch": 0.48824892419728566,
      "grad_norm": 8.569350242614746,
      "learning_rate": 4.482423525435187e-05,
      "loss": 0.7748,
      "step": 2950
    },
    {
      "epoch": 0.4899040052962595,
      "grad_norm": 4.660645484924316,
      "learning_rate": 4.480310968396147e-05,
      "loss": 0.8927,
      "step": 2960
    },
    {
      "epoch": 0.4915590863952334,
      "grad_norm": 3.760030746459961,
      "learning_rate": 4.478198411357107e-05,
      "loss": 0.9522,
      "step": 2970
    },
    {
      "epoch": 0.4932141674942072,
      "grad_norm": 7.2157769203186035,
      "learning_rate": 4.4760858543180666e-05,
      "loss": 0.7931,
      "step": 2980
    },
    {
      "epoch": 0.49486924859318104,
      "grad_norm": 5.604398250579834,
      "learning_rate": 4.4739732972790264e-05,
      "loss": 1.1067,
      "step": 2990
    },
    {
      "epoch": 0.49652432969215493,
      "grad_norm": 4.288139820098877,
      "learning_rate": 4.471860740239986e-05,
      "loss": 0.8416,
      "step": 3000
    },
    {
      "epoch": 0.49817941079112876,
      "grad_norm": 3.1991305351257324,
      "learning_rate": 4.469748183200947e-05,
      "loss": 0.7996,
      "step": 3010
    },
    {
      "epoch": 0.4998344918901026,
      "grad_norm": 7.078308582305908,
      "learning_rate": 4.4676356261619066e-05,
      "loss": 1.0293,
      "step": 3020
    },
    {
      "epoch": 0.5014895729890765,
      "grad_norm": 6.171448230743408,
      "learning_rate": 4.4655230691228664e-05,
      "loss": 1.0262,
      "step": 3030
    },
    {
      "epoch": 0.5031446540880503,
      "grad_norm": 7.430249214172363,
      "learning_rate": 4.463410512083826e-05,
      "loss": 0.7363,
      "step": 3040
    },
    {
      "epoch": 0.5047997351870241,
      "grad_norm": 6.61668062210083,
      "learning_rate": 4.461297955044786e-05,
      "loss": 0.903,
      "step": 3050
    },
    {
      "epoch": 0.506454816285998,
      "grad_norm": 7.731870174407959,
      "learning_rate": 4.4591853980057466e-05,
      "loss": 0.8504,
      "step": 3060
    },
    {
      "epoch": 0.5081098973849718,
      "grad_norm": 7.338734149932861,
      "learning_rate": 4.4570728409667065e-05,
      "loss": 0.8842,
      "step": 3070
    },
    {
      "epoch": 0.5097649784839458,
      "grad_norm": 4.170329570770264,
      "learning_rate": 4.454960283927666e-05,
      "loss": 1.0367,
      "step": 3080
    },
    {
      "epoch": 0.5114200595829196,
      "grad_norm": 11.470917701721191,
      "learning_rate": 4.452847726888626e-05,
      "loss": 0.9406,
      "step": 3090
    },
    {
      "epoch": 0.5130751406818934,
      "grad_norm": 4.3268632888793945,
      "learning_rate": 4.450735169849587e-05,
      "loss": 0.6569,
      "step": 3100
    },
    {
      "epoch": 0.5147302217808672,
      "grad_norm": 6.205569744110107,
      "learning_rate": 4.4486226128105465e-05,
      "loss": 0.7701,
      "step": 3110
    },
    {
      "epoch": 0.5163853028798411,
      "grad_norm": 9.798870086669922,
      "learning_rate": 4.4465100557715063e-05,
      "loss": 0.7788,
      "step": 3120
    },
    {
      "epoch": 0.5180403839788149,
      "grad_norm": 7.253307819366455,
      "learning_rate": 4.444397498732466e-05,
      "loss": 1.2477,
      "step": 3130
    },
    {
      "epoch": 0.5196954650777889,
      "grad_norm": 7.995439052581787,
      "learning_rate": 4.442284941693426e-05,
      "loss": 1.0995,
      "step": 3140
    },
    {
      "epoch": 0.5213505461767627,
      "grad_norm": 3.534459114074707,
      "learning_rate": 4.440172384654386e-05,
      "loss": 0.8614,
      "step": 3150
    },
    {
      "epoch": 0.5230056272757365,
      "grad_norm": 6.06298303604126,
      "learning_rate": 4.438059827615346e-05,
      "loss": 1.0413,
      "step": 3160
    },
    {
      "epoch": 0.5246607083747103,
      "grad_norm": 10.025978088378906,
      "learning_rate": 4.4359472705763055e-05,
      "loss": 0.7898,
      "step": 3170
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 6.205329418182373,
      "learning_rate": 4.433834713537266e-05,
      "loss": 0.8492,
      "step": 3180
    },
    {
      "epoch": 0.527970870572658,
      "grad_norm": 3.651817798614502,
      "learning_rate": 4.431722156498226e-05,
      "loss": 0.734,
      "step": 3190
    },
    {
      "epoch": 0.529625951671632,
      "grad_norm": 6.25150203704834,
      "learning_rate": 4.429609599459186e-05,
      "loss": 1.1334,
      "step": 3200
    },
    {
      "epoch": 0.5312810327706058,
      "grad_norm": 8.238245964050293,
      "learning_rate": 4.4274970424201456e-05,
      "loss": 0.7096,
      "step": 3210
    },
    {
      "epoch": 0.5329361138695796,
      "grad_norm": 8.685776710510254,
      "learning_rate": 4.4253844853811054e-05,
      "loss": 0.9741,
      "step": 3220
    },
    {
      "epoch": 0.5345911949685535,
      "grad_norm": 8.368990898132324,
      "learning_rate": 4.423271928342065e-05,
      "loss": 1.2481,
      "step": 3230
    },
    {
      "epoch": 0.5362462760675273,
      "grad_norm": 9.404903411865234,
      "learning_rate": 4.421159371303025e-05,
      "loss": 0.8451,
      "step": 3240
    },
    {
      "epoch": 0.5379013571665011,
      "grad_norm": 5.186917781829834,
      "learning_rate": 4.419046814263985e-05,
      "loss": 1.1256,
      "step": 3250
    },
    {
      "epoch": 0.5395564382654751,
      "grad_norm": 1.7970277070999146,
      "learning_rate": 4.416934257224945e-05,
      "loss": 0.8035,
      "step": 3260
    },
    {
      "epoch": 0.5412115193644489,
      "grad_norm": 5.78540563583374,
      "learning_rate": 4.414821700185905e-05,
      "loss": 0.9053,
      "step": 3270
    },
    {
      "epoch": 0.5428666004634227,
      "grad_norm": 4.145542621612549,
      "learning_rate": 4.412709143146865e-05,
      "loss": 0.773,
      "step": 3280
    },
    {
      "epoch": 0.5445216815623966,
      "grad_norm": 6.171741008758545,
      "learning_rate": 4.410596586107825e-05,
      "loss": 1.0764,
      "step": 3290
    },
    {
      "epoch": 0.5461767626613704,
      "grad_norm": 1.7947089672088623,
      "learning_rate": 4.408484029068785e-05,
      "loss": 0.538,
      "step": 3300
    },
    {
      "epoch": 0.5478318437603442,
      "grad_norm": 6.015474796295166,
      "learning_rate": 4.406371472029745e-05,
      "loss": 0.6116,
      "step": 3310
    },
    {
      "epoch": 0.5494869248593182,
      "grad_norm": 5.405176162719727,
      "learning_rate": 4.404258914990705e-05,
      "loss": 0.7102,
      "step": 3320
    },
    {
      "epoch": 0.551142005958292,
      "grad_norm": 8.834525108337402,
      "learning_rate": 4.402146357951665e-05,
      "loss": 0.7622,
      "step": 3330
    },
    {
      "epoch": 0.5527970870572658,
      "grad_norm": 5.83921480178833,
      "learning_rate": 4.400033800912625e-05,
      "loss": 0.938,
      "step": 3340
    },
    {
      "epoch": 0.5544521681562397,
      "grad_norm": 6.873477458953857,
      "learning_rate": 4.397921243873585e-05,
      "loss": 0.703,
      "step": 3350
    },
    {
      "epoch": 0.5561072492552135,
      "grad_norm": 6.438570499420166,
      "learning_rate": 4.395808686834545e-05,
      "loss": 0.8803,
      "step": 3360
    },
    {
      "epoch": 0.5577623303541873,
      "grad_norm": 6.389456748962402,
      "learning_rate": 4.393696129795505e-05,
      "loss": 0.9387,
      "step": 3370
    },
    {
      "epoch": 0.5594174114531612,
      "grad_norm": 8.315299987792969,
      "learning_rate": 4.391583572756465e-05,
      "loss": 0.7676,
      "step": 3380
    },
    {
      "epoch": 0.5610724925521351,
      "grad_norm": 8.758903503417969,
      "learning_rate": 4.389471015717425e-05,
      "loss": 0.7509,
      "step": 3390
    },
    {
      "epoch": 0.5627275736511089,
      "grad_norm": 2.8985164165496826,
      "learning_rate": 4.3873584586783845e-05,
      "loss": 0.5929,
      "step": 3400
    },
    {
      "epoch": 0.5643826547500828,
      "grad_norm": 11.930763244628906,
      "learning_rate": 4.3852459016393444e-05,
      "loss": 0.9522,
      "step": 3410
    },
    {
      "epoch": 0.5660377358490566,
      "grad_norm": 9.046439170837402,
      "learning_rate": 4.383133344600304e-05,
      "loss": 0.8276,
      "step": 3420
    },
    {
      "epoch": 0.5676928169480304,
      "grad_norm": 6.21189546585083,
      "learning_rate": 4.381020787561264e-05,
      "loss": 0.8181,
      "step": 3430
    },
    {
      "epoch": 0.5693478980470043,
      "grad_norm": 8.005921363830566,
      "learning_rate": 4.3789082305222246e-05,
      "loss": 0.8295,
      "step": 3440
    },
    {
      "epoch": 0.5710029791459782,
      "grad_norm": 5.560092449188232,
      "learning_rate": 4.3767956734831844e-05,
      "loss": 0.8907,
      "step": 3450
    },
    {
      "epoch": 0.572658060244952,
      "grad_norm": 5.807984828948975,
      "learning_rate": 4.374683116444144e-05,
      "loss": 0.6233,
      "step": 3460
    },
    {
      "epoch": 0.5743131413439259,
      "grad_norm": 6.971253871917725,
      "learning_rate": 4.372570559405104e-05,
      "loss": 0.9676,
      "step": 3470
    },
    {
      "epoch": 0.5759682224428997,
      "grad_norm": 9.718036651611328,
      "learning_rate": 4.370458002366064e-05,
      "loss": 0.8843,
      "step": 3480
    },
    {
      "epoch": 0.5776233035418735,
      "grad_norm": 4.739887714385986,
      "learning_rate": 4.368345445327024e-05,
      "loss": 0.6793,
      "step": 3490
    },
    {
      "epoch": 0.5792783846408474,
      "grad_norm": 6.5463690757751465,
      "learning_rate": 4.3662328882879836e-05,
      "loss": 0.8926,
      "step": 3500
    },
    {
      "epoch": 0.5809334657398213,
      "grad_norm": 8.068760871887207,
      "learning_rate": 4.3641203312489434e-05,
      "loss": 0.7175,
      "step": 3510
    },
    {
      "epoch": 0.5825885468387951,
      "grad_norm": 4.675145626068115,
      "learning_rate": 4.362007774209904e-05,
      "loss": 1.0869,
      "step": 3520
    },
    {
      "epoch": 0.584243627937769,
      "grad_norm": 7.863040924072266,
      "learning_rate": 4.359895217170864e-05,
      "loss": 0.7445,
      "step": 3530
    },
    {
      "epoch": 0.5858987090367428,
      "grad_norm": 8.852269172668457,
      "learning_rate": 4.3577826601318236e-05,
      "loss": 1.3173,
      "step": 3540
    },
    {
      "epoch": 0.5875537901357166,
      "grad_norm": 5.696547985076904,
      "learning_rate": 4.3556701030927835e-05,
      "loss": 0.9989,
      "step": 3550
    },
    {
      "epoch": 0.5892088712346905,
      "grad_norm": 10.914084434509277,
      "learning_rate": 4.353557546053744e-05,
      "loss": 0.7949,
      "step": 3560
    },
    {
      "epoch": 0.5908639523336644,
      "grad_norm": 5.762917518615723,
      "learning_rate": 4.351444989014704e-05,
      "loss": 0.754,
      "step": 3570
    },
    {
      "epoch": 0.5925190334326382,
      "grad_norm": 7.852947235107422,
      "learning_rate": 4.3493324319756637e-05,
      "loss": 0.7189,
      "step": 3580
    },
    {
      "epoch": 0.5941741145316121,
      "grad_norm": 3.6804656982421875,
      "learning_rate": 4.3472198749366235e-05,
      "loss": 0.9927,
      "step": 3590
    },
    {
      "epoch": 0.5958291956305859,
      "grad_norm": 4.671380043029785,
      "learning_rate": 4.345107317897583e-05,
      "loss": 0.7421,
      "step": 3600
    },
    {
      "epoch": 0.5974842767295597,
      "grad_norm": 4.10764217376709,
      "learning_rate": 4.342994760858544e-05,
      "loss": 0.5662,
      "step": 3610
    },
    {
      "epoch": 0.5991393578285336,
      "grad_norm": 14.370808601379395,
      "learning_rate": 4.340882203819504e-05,
      "loss": 0.8655,
      "step": 3620
    },
    {
      "epoch": 0.6007944389275075,
      "grad_norm": 9.864126205444336,
      "learning_rate": 4.3387696467804635e-05,
      "loss": 0.9969,
      "step": 3630
    },
    {
      "epoch": 0.6024495200264813,
      "grad_norm": 4.301313877105713,
      "learning_rate": 4.3366570897414234e-05,
      "loss": 0.9638,
      "step": 3640
    },
    {
      "epoch": 0.6041046011254552,
      "grad_norm": 4.02168083190918,
      "learning_rate": 4.334544532702383e-05,
      "loss": 0.5628,
      "step": 3650
    },
    {
      "epoch": 0.605759682224429,
      "grad_norm": 9.22834300994873,
      "learning_rate": 4.332431975663343e-05,
      "loss": 0.7957,
      "step": 3660
    },
    {
      "epoch": 0.6074147633234028,
      "grad_norm": 6.008138656616211,
      "learning_rate": 4.330319418624303e-05,
      "loss": 1.0057,
      "step": 3670
    },
    {
      "epoch": 0.6090698444223767,
      "grad_norm": 7.340017795562744,
      "learning_rate": 4.328206861585263e-05,
      "loss": 0.8517,
      "step": 3680
    },
    {
      "epoch": 0.6107249255213505,
      "grad_norm": 7.928074359893799,
      "learning_rate": 4.326094304546223e-05,
      "loss": 0.7449,
      "step": 3690
    },
    {
      "epoch": 0.6123800066203244,
      "grad_norm": 8.329083442687988,
      "learning_rate": 4.323981747507183e-05,
      "loss": 0.7467,
      "step": 3700
    },
    {
      "epoch": 0.6140350877192983,
      "grad_norm": 10.044578552246094,
      "learning_rate": 4.321869190468143e-05,
      "loss": 0.7991,
      "step": 3710
    },
    {
      "epoch": 0.6156901688182721,
      "grad_norm": 1.9473140239715576,
      "learning_rate": 4.319756633429103e-05,
      "loss": 0.6097,
      "step": 3720
    },
    {
      "epoch": 0.6173452499172459,
      "grad_norm": 1.8311842679977417,
      "learning_rate": 4.3176440763900626e-05,
      "loss": 0.7742,
      "step": 3730
    },
    {
      "epoch": 0.6190003310162198,
      "grad_norm": 6.810498237609863,
      "learning_rate": 4.3155315193510224e-05,
      "loss": 0.7616,
      "step": 3740
    },
    {
      "epoch": 0.6206554121151936,
      "grad_norm": 11.013484001159668,
      "learning_rate": 4.313418962311982e-05,
      "loss": 0.7351,
      "step": 3750
    },
    {
      "epoch": 0.6223104932141675,
      "grad_norm": 6.745355606079102,
      "learning_rate": 4.311306405272942e-05,
      "loss": 1.1457,
      "step": 3760
    },
    {
      "epoch": 0.6239655743131414,
      "grad_norm": 6.882542133331299,
      "learning_rate": 4.309193848233902e-05,
      "loss": 0.7859,
      "step": 3770
    },
    {
      "epoch": 0.6256206554121152,
      "grad_norm": 15.17281436920166,
      "learning_rate": 4.3070812911948625e-05,
      "loss": 0.8339,
      "step": 3780
    },
    {
      "epoch": 0.627275736511089,
      "grad_norm": 6.302368640899658,
      "learning_rate": 4.304968734155822e-05,
      "loss": 0.844,
      "step": 3790
    },
    {
      "epoch": 0.6289308176100629,
      "grad_norm": 12.103489875793457,
      "learning_rate": 4.302856177116782e-05,
      "loss": 0.8042,
      "step": 3800
    },
    {
      "epoch": 0.6305858987090367,
      "grad_norm": 9.590091705322266,
      "learning_rate": 4.3007436200777426e-05,
      "loss": 0.8798,
      "step": 3810
    },
    {
      "epoch": 0.6322409798080106,
      "grad_norm": 8.327310562133789,
      "learning_rate": 4.2986310630387025e-05,
      "loss": 0.7716,
      "step": 3820
    },
    {
      "epoch": 0.6338960609069845,
      "grad_norm": 8.551024436950684,
      "learning_rate": 4.296518505999662e-05,
      "loss": 0.8283,
      "step": 3830
    },
    {
      "epoch": 0.6355511420059583,
      "grad_norm": 8.548646926879883,
      "learning_rate": 4.294405948960622e-05,
      "loss": 0.7693,
      "step": 3840
    },
    {
      "epoch": 0.6372062231049321,
      "grad_norm": 7.225947380065918,
      "learning_rate": 4.292293391921582e-05,
      "loss": 1.1526,
      "step": 3850
    },
    {
      "epoch": 0.638861304203906,
      "grad_norm": 7.045748233795166,
      "learning_rate": 4.2901808348825425e-05,
      "loss": 0.6072,
      "step": 3860
    },
    {
      "epoch": 0.6405163853028798,
      "grad_norm": 4.800227165222168,
      "learning_rate": 4.2880682778435024e-05,
      "loss": 0.9431,
      "step": 3870
    },
    {
      "epoch": 0.6421714664018537,
      "grad_norm": 5.048583984375,
      "learning_rate": 4.285955720804462e-05,
      "loss": 0.8312,
      "step": 3880
    },
    {
      "epoch": 0.6438265475008276,
      "grad_norm": 6.927045822143555,
      "learning_rate": 4.283843163765422e-05,
      "loss": 0.685,
      "step": 3890
    },
    {
      "epoch": 0.6454816285998014,
      "grad_norm": 5.825406074523926,
      "learning_rate": 4.281730606726382e-05,
      "loss": 0.761,
      "step": 3900
    },
    {
      "epoch": 0.6471367096987752,
      "grad_norm": 4.584779262542725,
      "learning_rate": 4.279618049687342e-05,
      "loss": 0.7183,
      "step": 3910
    },
    {
      "epoch": 0.6487917907977491,
      "grad_norm": 5.699444770812988,
      "learning_rate": 4.2775054926483015e-05,
      "loss": 0.7682,
      "step": 3920
    },
    {
      "epoch": 0.6504468718967229,
      "grad_norm": 6.175193786621094,
      "learning_rate": 4.2753929356092614e-05,
      "loss": 0.7666,
      "step": 3930
    },
    {
      "epoch": 0.6521019529956968,
      "grad_norm": 6.399385929107666,
      "learning_rate": 4.273280378570221e-05,
      "loss": 1.0835,
      "step": 3940
    },
    {
      "epoch": 0.6537570340946707,
      "grad_norm": 4.850484371185303,
      "learning_rate": 4.271167821531182e-05,
      "loss": 0.9761,
      "step": 3950
    },
    {
      "epoch": 0.6554121151936445,
      "grad_norm": 7.150592803955078,
      "learning_rate": 4.2690552644921416e-05,
      "loss": 1.0394,
      "step": 3960
    },
    {
      "epoch": 0.6570671962926183,
      "grad_norm": 6.800861835479736,
      "learning_rate": 4.2669427074531014e-05,
      "loss": 0.8463,
      "step": 3970
    },
    {
      "epoch": 0.6587222773915922,
      "grad_norm": 8.591506004333496,
      "learning_rate": 4.264830150414061e-05,
      "loss": 1.0708,
      "step": 3980
    },
    {
      "epoch": 0.660377358490566,
      "grad_norm": 10.600196838378906,
      "learning_rate": 4.262717593375021e-05,
      "loss": 0.8306,
      "step": 3990
    },
    {
      "epoch": 0.6620324395895398,
      "grad_norm": 6.348047256469727,
      "learning_rate": 4.260605036335981e-05,
      "loss": 1.1668,
      "step": 4000
    },
    {
      "epoch": 0.6636875206885138,
      "grad_norm": 5.897114276885986,
      "learning_rate": 4.258492479296941e-05,
      "loss": 0.5943,
      "step": 4010
    },
    {
      "epoch": 0.6653426017874876,
      "grad_norm": 12.60709285736084,
      "learning_rate": 4.2563799222579006e-05,
      "loss": 0.7178,
      "step": 4020
    },
    {
      "epoch": 0.6669976828864614,
      "grad_norm": 7.019259452819824,
      "learning_rate": 4.254267365218861e-05,
      "loss": 0.845,
      "step": 4030
    },
    {
      "epoch": 0.6686527639854353,
      "grad_norm": 7.819041728973389,
      "learning_rate": 4.252154808179821e-05,
      "loss": 0.898,
      "step": 4040
    },
    {
      "epoch": 0.6703078450844091,
      "grad_norm": 9.637151718139648,
      "learning_rate": 4.2500422511407815e-05,
      "loss": 0.7292,
      "step": 4050
    },
    {
      "epoch": 0.6719629261833829,
      "grad_norm": 7.209911823272705,
      "learning_rate": 4.247929694101741e-05,
      "loss": 1.0762,
      "step": 4060
    },
    {
      "epoch": 0.6736180072823569,
      "grad_norm": 8.248425483703613,
      "learning_rate": 4.245817137062701e-05,
      "loss": 0.7516,
      "step": 4070
    },
    {
      "epoch": 0.6752730883813307,
      "grad_norm": 9.18157958984375,
      "learning_rate": 4.243704580023661e-05,
      "loss": 1.0679,
      "step": 4080
    },
    {
      "epoch": 0.6769281694803045,
      "grad_norm": 9.46638011932373,
      "learning_rate": 4.241592022984621e-05,
      "loss": 0.9436,
      "step": 4090
    },
    {
      "epoch": 0.6785832505792784,
      "grad_norm": 9.864697456359863,
      "learning_rate": 4.239479465945581e-05,
      "loss": 1.05,
      "step": 4100
    },
    {
      "epoch": 0.6802383316782522,
      "grad_norm": 3.716219186782837,
      "learning_rate": 4.2373669089065405e-05,
      "loss": 0.7355,
      "step": 4110
    },
    {
      "epoch": 0.681893412777226,
      "grad_norm": 5.667409896850586,
      "learning_rate": 4.235254351867501e-05,
      "loss": 0.8628,
      "step": 4120
    },
    {
      "epoch": 0.6835484938762,
      "grad_norm": 9.33585262298584,
      "learning_rate": 4.233141794828461e-05,
      "loss": 0.8877,
      "step": 4130
    },
    {
      "epoch": 0.6852035749751738,
      "grad_norm": 5.699236869812012,
      "learning_rate": 4.231029237789421e-05,
      "loss": 0.9821,
      "step": 4140
    },
    {
      "epoch": 0.6868586560741476,
      "grad_norm": 7.901264190673828,
      "learning_rate": 4.2289166807503805e-05,
      "loss": 1.0466,
      "step": 4150
    },
    {
      "epoch": 0.6885137371731215,
      "grad_norm": 9.64258861541748,
      "learning_rate": 4.2268041237113404e-05,
      "loss": 1.2131,
      "step": 4160
    },
    {
      "epoch": 0.6901688182720953,
      "grad_norm": 2.1907119750976562,
      "learning_rate": 4.2246915666723e-05,
      "loss": 0.5354,
      "step": 4170
    },
    {
      "epoch": 0.6918238993710691,
      "grad_norm": 9.380793571472168,
      "learning_rate": 4.22257900963326e-05,
      "loss": 0.8112,
      "step": 4180
    },
    {
      "epoch": 0.6934789804700431,
      "grad_norm": 7.183408260345459,
      "learning_rate": 4.22046645259422e-05,
      "loss": 0.723,
      "step": 4190
    },
    {
      "epoch": 0.6951340615690169,
      "grad_norm": 3.154494047164917,
      "learning_rate": 4.2183538955551804e-05,
      "loss": 1.0919,
      "step": 4200
    },
    {
      "epoch": 0.6967891426679907,
      "grad_norm": 7.608426570892334,
      "learning_rate": 4.21624133851614e-05,
      "loss": 0.8934,
      "step": 4210
    },
    {
      "epoch": 0.6984442237669646,
      "grad_norm": 4.74393367767334,
      "learning_rate": 4.2141287814771e-05,
      "loss": 0.8568,
      "step": 4220
    },
    {
      "epoch": 0.7000993048659384,
      "grad_norm": 2.195302724838257,
      "learning_rate": 4.21201622443806e-05,
      "loss": 0.8487,
      "step": 4230
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 11.308738708496094,
      "learning_rate": 4.20990366739902e-05,
      "loss": 0.8214,
      "step": 4240
    },
    {
      "epoch": 0.7034094670638862,
      "grad_norm": 4.068066120147705,
      "learning_rate": 4.2077911103599796e-05,
      "loss": 0.6701,
      "step": 4250
    },
    {
      "epoch": 0.70506454816286,
      "grad_norm": 11.175095558166504,
      "learning_rate": 4.2056785533209394e-05,
      "loss": 0.8972,
      "step": 4260
    },
    {
      "epoch": 0.7067196292618338,
      "grad_norm": 7.744206428527832,
      "learning_rate": 4.2035659962819e-05,
      "loss": 0.7442,
      "step": 4270
    },
    {
      "epoch": 0.7083747103608077,
      "grad_norm": 8.913176536560059,
      "learning_rate": 4.20145343924286e-05,
      "loss": 0.7186,
      "step": 4280
    },
    {
      "epoch": 0.7100297914597815,
      "grad_norm": 7.044890403747559,
      "learning_rate": 4.1993408822038196e-05,
      "loss": 0.8237,
      "step": 4290
    },
    {
      "epoch": 0.7116848725587553,
      "grad_norm": 16.207719802856445,
      "learning_rate": 4.19722832516478e-05,
      "loss": 0.9432,
      "step": 4300
    },
    {
      "epoch": 0.7133399536577292,
      "grad_norm": 9.865875244140625,
      "learning_rate": 4.19511576812574e-05,
      "loss": 0.8535,
      "step": 4310
    },
    {
      "epoch": 0.7149950347567031,
      "grad_norm": 7.980419635772705,
      "learning_rate": 4.1930032110867e-05,
      "loss": 0.9213,
      "step": 4320
    },
    {
      "epoch": 0.716650115855677,
      "grad_norm": 4.096696853637695,
      "learning_rate": 4.1908906540476597e-05,
      "loss": 0.6829,
      "step": 4330
    },
    {
      "epoch": 0.7183051969546508,
      "grad_norm": 10.01707935333252,
      "learning_rate": 4.1887780970086195e-05,
      "loss": 0.6344,
      "step": 4340
    },
    {
      "epoch": 0.7199602780536246,
      "grad_norm": 10.038146018981934,
      "learning_rate": 4.186665539969579e-05,
      "loss": 0.5451,
      "step": 4350
    },
    {
      "epoch": 0.7216153591525984,
      "grad_norm": 10.179122924804688,
      "learning_rate": 4.184552982930539e-05,
      "loss": 1.0201,
      "step": 4360
    },
    {
      "epoch": 0.7232704402515723,
      "grad_norm": 5.868716716766357,
      "learning_rate": 4.1824404258915e-05,
      "loss": 0.811,
      "step": 4370
    },
    {
      "epoch": 0.7249255213505462,
      "grad_norm": 4.631462097167969,
      "learning_rate": 4.1803278688524595e-05,
      "loss": 0.4404,
      "step": 4380
    },
    {
      "epoch": 0.72658060244952,
      "grad_norm": 5.620421886444092,
      "learning_rate": 4.1782153118134194e-05,
      "loss": 0.8852,
      "step": 4390
    },
    {
      "epoch": 0.7282356835484939,
      "grad_norm": 10.263690948486328,
      "learning_rate": 4.176102754774379e-05,
      "loss": 1.0185,
      "step": 4400
    },
    {
      "epoch": 0.7298907646474677,
      "grad_norm": 24.96410369873047,
      "learning_rate": 4.173990197735339e-05,
      "loss": 0.8385,
      "step": 4410
    },
    {
      "epoch": 0.7315458457464415,
      "grad_norm": 9.493979454040527,
      "learning_rate": 4.171877640696299e-05,
      "loss": 0.5377,
      "step": 4420
    },
    {
      "epoch": 0.7332009268454154,
      "grad_norm": 7.22935676574707,
      "learning_rate": 4.169765083657259e-05,
      "loss": 0.6193,
      "step": 4430
    },
    {
      "epoch": 0.7348560079443893,
      "grad_norm": 7.223391056060791,
      "learning_rate": 4.1676525266182186e-05,
      "loss": 0.9331,
      "step": 4440
    },
    {
      "epoch": 0.7365110890433632,
      "grad_norm": 6.107539176940918,
      "learning_rate": 4.1655399695791784e-05,
      "loss": 0.8986,
      "step": 4450
    },
    {
      "epoch": 0.738166170142337,
      "grad_norm": 3.726116180419922,
      "learning_rate": 4.163427412540139e-05,
      "loss": 0.6729,
      "step": 4460
    },
    {
      "epoch": 0.7398212512413108,
      "grad_norm": 11.27258586883545,
      "learning_rate": 4.161314855501099e-05,
      "loss": 0.8042,
      "step": 4470
    },
    {
      "epoch": 0.7414763323402846,
      "grad_norm": 6.260288238525391,
      "learning_rate": 4.1592022984620586e-05,
      "loss": 0.8616,
      "step": 4480
    },
    {
      "epoch": 0.7431314134392585,
      "grad_norm": 6.1774821281433105,
      "learning_rate": 4.1570897414230184e-05,
      "loss": 0.8522,
      "step": 4490
    },
    {
      "epoch": 0.7447864945382324,
      "grad_norm": 8.04418659210205,
      "learning_rate": 4.154977184383978e-05,
      "loss": 0.6581,
      "step": 4500
    },
    {
      "epoch": 0.7464415756372063,
      "grad_norm": 8.601615905761719,
      "learning_rate": 4.152864627344938e-05,
      "loss": 0.872,
      "step": 4510
    },
    {
      "epoch": 0.7480966567361801,
      "grad_norm": 3.2736144065856934,
      "learning_rate": 4.1507520703058986e-05,
      "loss": 0.8518,
      "step": 4520
    },
    {
      "epoch": 0.7497517378351539,
      "grad_norm": 6.955022811889648,
      "learning_rate": 4.1486395132668585e-05,
      "loss": 0.8147,
      "step": 4530
    },
    {
      "epoch": 0.7514068189341278,
      "grad_norm": 13.691156387329102,
      "learning_rate": 4.146526956227818e-05,
      "loss": 0.724,
      "step": 4540
    },
    {
      "epoch": 0.7530619000331016,
      "grad_norm": 7.433265686035156,
      "learning_rate": 4.144414399188779e-05,
      "loss": 1.1402,
      "step": 4550
    },
    {
      "epoch": 0.7547169811320755,
      "grad_norm": 3.2315666675567627,
      "learning_rate": 4.1423018421497387e-05,
      "loss": 0.7936,
      "step": 4560
    },
    {
      "epoch": 0.7563720622310494,
      "grad_norm": 7.950416564941406,
      "learning_rate": 4.1401892851106985e-05,
      "loss": 0.765,
      "step": 4570
    },
    {
      "epoch": 0.7580271433300232,
      "grad_norm": 7.682269096374512,
      "learning_rate": 4.138076728071658e-05,
      "loss": 0.791,
      "step": 4580
    },
    {
      "epoch": 0.759682224428997,
      "grad_norm": 6.168712139129639,
      "learning_rate": 4.135964171032618e-05,
      "loss": 0.6551,
      "step": 4590
    },
    {
      "epoch": 0.7613373055279709,
      "grad_norm": 13.752392768859863,
      "learning_rate": 4.133851613993578e-05,
      "loss": 1.1661,
      "step": 4600
    },
    {
      "epoch": 0.7629923866269447,
      "grad_norm": 4.9881720542907715,
      "learning_rate": 4.131739056954538e-05,
      "loss": 1.0483,
      "step": 4610
    },
    {
      "epoch": 0.7646474677259185,
      "grad_norm": 3.2760379314422607,
      "learning_rate": 4.129626499915498e-05,
      "loss": 0.6487,
      "step": 4620
    },
    {
      "epoch": 0.7663025488248925,
      "grad_norm": 5.3312764167785645,
      "learning_rate": 4.127513942876458e-05,
      "loss": 0.8175,
      "step": 4630
    },
    {
      "epoch": 0.7679576299238663,
      "grad_norm": 7.6243977546691895,
      "learning_rate": 4.125401385837418e-05,
      "loss": 0.8402,
      "step": 4640
    },
    {
      "epoch": 0.7696127110228401,
      "grad_norm": 0.8809918165206909,
      "learning_rate": 4.123288828798378e-05,
      "loss": 0.4948,
      "step": 4650
    },
    {
      "epoch": 0.771267792121814,
      "grad_norm": 12.90571117401123,
      "learning_rate": 4.121176271759338e-05,
      "loss": 0.8692,
      "step": 4660
    },
    {
      "epoch": 0.7729228732207878,
      "grad_norm": 4.1047682762146,
      "learning_rate": 4.1190637147202976e-05,
      "loss": 1.0653,
      "step": 4670
    },
    {
      "epoch": 0.7745779543197616,
      "grad_norm": 6.952812671661377,
      "learning_rate": 4.1169511576812574e-05,
      "loss": 0.8876,
      "step": 4680
    },
    {
      "epoch": 0.7762330354187356,
      "grad_norm": 3.454697847366333,
      "learning_rate": 4.114838600642217e-05,
      "loss": 0.3122,
      "step": 4690
    },
    {
      "epoch": 0.7778881165177094,
      "grad_norm": 9.471778869628906,
      "learning_rate": 4.112726043603177e-05,
      "loss": 0.8549,
      "step": 4700
    },
    {
      "epoch": 0.7795431976166832,
      "grad_norm": 3.304105520248413,
      "learning_rate": 4.1106134865641376e-05,
      "loss": 0.7305,
      "step": 4710
    },
    {
      "epoch": 0.7811982787156571,
      "grad_norm": 5.420029163360596,
      "learning_rate": 4.1085009295250974e-05,
      "loss": 1.0034,
      "step": 4720
    },
    {
      "epoch": 0.7828533598146309,
      "grad_norm": 6.436787128448486,
      "learning_rate": 4.106388372486057e-05,
      "loss": 0.7558,
      "step": 4730
    },
    {
      "epoch": 0.7845084409136047,
      "grad_norm": 9.845918655395508,
      "learning_rate": 4.104275815447017e-05,
      "loss": 0.8401,
      "step": 4740
    },
    {
      "epoch": 0.7861635220125787,
      "grad_norm": 13.204890251159668,
      "learning_rate": 4.102163258407977e-05,
      "loss": 1.0185,
      "step": 4750
    },
    {
      "epoch": 0.7878186031115525,
      "grad_norm": 9.306239128112793,
      "learning_rate": 4.100050701368937e-05,
      "loss": 0.8975,
      "step": 4760
    },
    {
      "epoch": 0.7894736842105263,
      "grad_norm": 7.704019546508789,
      "learning_rate": 4.097938144329897e-05,
      "loss": 0.7853,
      "step": 4770
    },
    {
      "epoch": 0.7911287653095002,
      "grad_norm": 6.60499382019043,
      "learning_rate": 4.095825587290857e-05,
      "loss": 1.0487,
      "step": 4780
    },
    {
      "epoch": 0.792783846408474,
      "grad_norm": 4.177285671234131,
      "learning_rate": 4.093713030251817e-05,
      "loss": 0.7792,
      "step": 4790
    },
    {
      "epoch": 0.7944389275074478,
      "grad_norm": 7.4618821144104,
      "learning_rate": 4.0916004732127775e-05,
      "loss": 0.7785,
      "step": 4800
    },
    {
      "epoch": 0.7960940086064218,
      "grad_norm": 6.4297332763671875,
      "learning_rate": 4.089487916173737e-05,
      "loss": 0.7934,
      "step": 4810
    },
    {
      "epoch": 0.7977490897053956,
      "grad_norm": 6.923448085784912,
      "learning_rate": 4.087375359134697e-05,
      "loss": 0.819,
      "step": 4820
    },
    {
      "epoch": 0.7994041708043694,
      "grad_norm": 9.160228729248047,
      "learning_rate": 4.085262802095657e-05,
      "loss": 1.0765,
      "step": 4830
    },
    {
      "epoch": 0.8010592519033433,
      "grad_norm": 7.735018253326416,
      "learning_rate": 4.083150245056617e-05,
      "loss": 0.8465,
      "step": 4840
    },
    {
      "epoch": 0.8027143330023171,
      "grad_norm": 3.1149091720581055,
      "learning_rate": 4.081037688017577e-05,
      "loss": 0.7236,
      "step": 4850
    },
    {
      "epoch": 0.8043694141012909,
      "grad_norm": 4.222911357879639,
      "learning_rate": 4.0789251309785365e-05,
      "loss": 0.5268,
      "step": 4860
    },
    {
      "epoch": 0.8060244952002649,
      "grad_norm": 5.7403059005737305,
      "learning_rate": 4.0768125739394963e-05,
      "loss": 0.955,
      "step": 4870
    },
    {
      "epoch": 0.8076795762992387,
      "grad_norm": 6.928130149841309,
      "learning_rate": 4.074700016900457e-05,
      "loss": 0.7802,
      "step": 4880
    },
    {
      "epoch": 0.8093346573982125,
      "grad_norm": 6.3596696853637695,
      "learning_rate": 4.072587459861417e-05,
      "loss": 0.5101,
      "step": 4890
    },
    {
      "epoch": 0.8109897384971864,
      "grad_norm": 4.623471260070801,
      "learning_rate": 4.0704749028223765e-05,
      "loss": 0.9866,
      "step": 4900
    },
    {
      "epoch": 0.8126448195961602,
      "grad_norm": 5.588627815246582,
      "learning_rate": 4.0683623457833364e-05,
      "loss": 0.6054,
      "step": 4910
    },
    {
      "epoch": 0.814299900695134,
      "grad_norm": 4.1112141609191895,
      "learning_rate": 4.066249788744296e-05,
      "loss": 0.8039,
      "step": 4920
    },
    {
      "epoch": 0.8159549817941079,
      "grad_norm": 5.37169075012207,
      "learning_rate": 4.064137231705256e-05,
      "loss": 0.572,
      "step": 4930
    },
    {
      "epoch": 0.8176100628930818,
      "grad_norm": 9.34904956817627,
      "learning_rate": 4.062024674666216e-05,
      "loss": 0.8807,
      "step": 4940
    },
    {
      "epoch": 0.8192651439920556,
      "grad_norm": 9.200342178344727,
      "learning_rate": 4.059912117627176e-05,
      "loss": 0.8793,
      "step": 4950
    },
    {
      "epoch": 0.8209202250910295,
      "grad_norm": 8.0599365234375,
      "learning_rate": 4.0577995605881356e-05,
      "loss": 0.9513,
      "step": 4960
    },
    {
      "epoch": 0.8225753061900033,
      "grad_norm": 4.9278178215026855,
      "learning_rate": 4.055687003549096e-05,
      "loss": 0.5799,
      "step": 4970
    },
    {
      "epoch": 0.8242303872889771,
      "grad_norm": 2.4664244651794434,
      "learning_rate": 4.053574446510056e-05,
      "loss": 0.7996,
      "step": 4980
    },
    {
      "epoch": 0.825885468387951,
      "grad_norm": 7.465128421783447,
      "learning_rate": 4.051461889471016e-05,
      "loss": 0.9056,
      "step": 4990
    },
    {
      "epoch": 0.8275405494869249,
      "grad_norm": 2.758134365081787,
      "learning_rate": 4.0493493324319756e-05,
      "loss": 0.7909,
      "step": 5000
    },
    {
      "epoch": 0.8291956305858987,
      "grad_norm": 6.137884616851807,
      "learning_rate": 4.0472367753929354e-05,
      "loss": 0.7415,
      "step": 5010
    },
    {
      "epoch": 0.8308507116848726,
      "grad_norm": 4.549850940704346,
      "learning_rate": 4.045124218353896e-05,
      "loss": 0.8621,
      "step": 5020
    },
    {
      "epoch": 0.8325057927838464,
      "grad_norm": 3.003980875015259,
      "learning_rate": 4.043011661314856e-05,
      "loss": 0.7899,
      "step": 5030
    },
    {
      "epoch": 0.8341608738828202,
      "grad_norm": 6.8583197593688965,
      "learning_rate": 4.0408991042758156e-05,
      "loss": 0.6648,
      "step": 5040
    },
    {
      "epoch": 0.8358159549817941,
      "grad_norm": 16.9272403717041,
      "learning_rate": 4.038786547236776e-05,
      "loss": 0.7334,
      "step": 5050
    },
    {
      "epoch": 0.837471036080768,
      "grad_norm": 11.164928436279297,
      "learning_rate": 4.036673990197736e-05,
      "loss": 1.0891,
      "step": 5060
    },
    {
      "epoch": 0.8391261171797418,
      "grad_norm": 2.0962514877319336,
      "learning_rate": 4.034561433158696e-05,
      "loss": 0.8106,
      "step": 5070
    },
    {
      "epoch": 0.8407811982787157,
      "grad_norm": 8.039371490478516,
      "learning_rate": 4.032448876119656e-05,
      "loss": 0.9451,
      "step": 5080
    },
    {
      "epoch": 0.8424362793776895,
      "grad_norm": 6.283297061920166,
      "learning_rate": 4.0303363190806155e-05,
      "loss": 0.9477,
      "step": 5090
    },
    {
      "epoch": 0.8440913604766633,
      "grad_norm": 10.314887046813965,
      "learning_rate": 4.0282237620415753e-05,
      "loss": 0.8392,
      "step": 5100
    },
    {
      "epoch": 0.8457464415756372,
      "grad_norm": 7.8519673347473145,
      "learning_rate": 4.026111205002535e-05,
      "loss": 1.1044,
      "step": 5110
    },
    {
      "epoch": 0.8474015226746111,
      "grad_norm": 11.174814224243164,
      "learning_rate": 4.023998647963495e-05,
      "loss": 1.1346,
      "step": 5120
    },
    {
      "epoch": 0.8490566037735849,
      "grad_norm": 10.319960594177246,
      "learning_rate": 4.021886090924455e-05,
      "loss": 0.8132,
      "step": 5130
    },
    {
      "epoch": 0.8507116848725588,
      "grad_norm": 5.573333263397217,
      "learning_rate": 4.0197735338854154e-05,
      "loss": 0.6748,
      "step": 5140
    },
    {
      "epoch": 0.8523667659715326,
      "grad_norm": 9.64225959777832,
      "learning_rate": 4.017660976846375e-05,
      "loss": 0.839,
      "step": 5150
    },
    {
      "epoch": 0.8540218470705064,
      "grad_norm": 4.789793491363525,
      "learning_rate": 4.015548419807335e-05,
      "loss": 0.8002,
      "step": 5160
    },
    {
      "epoch": 0.8556769281694803,
      "grad_norm": 5.123698711395264,
      "learning_rate": 4.013435862768295e-05,
      "loss": 0.6224,
      "step": 5170
    },
    {
      "epoch": 0.8573320092684542,
      "grad_norm": 7.579616069793701,
      "learning_rate": 4.011323305729255e-05,
      "loss": 0.9422,
      "step": 5180
    },
    {
      "epoch": 0.858987090367428,
      "grad_norm": 10.534358978271484,
      "learning_rate": 4.0092107486902146e-05,
      "loss": 0.4776,
      "step": 5190
    },
    {
      "epoch": 0.8606421714664019,
      "grad_norm": 6.348382472991943,
      "learning_rate": 4.0070981916511744e-05,
      "loss": 0.643,
      "step": 5200
    },
    {
      "epoch": 0.8622972525653757,
      "grad_norm": 12.83508014678955,
      "learning_rate": 4.004985634612134e-05,
      "loss": 1.2666,
      "step": 5210
    },
    {
      "epoch": 0.8639523336643495,
      "grad_norm": 6.381442070007324,
      "learning_rate": 4.002873077573095e-05,
      "loss": 0.8589,
      "step": 5220
    },
    {
      "epoch": 0.8656074147633234,
      "grad_norm": 11.411808013916016,
      "learning_rate": 4.0007605205340546e-05,
      "loss": 0.8776,
      "step": 5230
    },
    {
      "epoch": 0.8672624958622972,
      "grad_norm": 7.441321849822998,
      "learning_rate": 3.9986479634950144e-05,
      "loss": 0.7178,
      "step": 5240
    },
    {
      "epoch": 0.8689175769612711,
      "grad_norm": 8.345239639282227,
      "learning_rate": 3.996535406455974e-05,
      "loss": 0.7912,
      "step": 5250
    },
    {
      "epoch": 0.870572658060245,
      "grad_norm": 7.41257381439209,
      "learning_rate": 3.994422849416935e-05,
      "loss": 0.7871,
      "step": 5260
    },
    {
      "epoch": 0.8722277391592188,
      "grad_norm": 5.282618999481201,
      "learning_rate": 3.9923102923778946e-05,
      "loss": 0.5823,
      "step": 5270
    },
    {
      "epoch": 0.8738828202581926,
      "grad_norm": 9.641045570373535,
      "learning_rate": 3.9901977353388545e-05,
      "loss": 0.788,
      "step": 5280
    },
    {
      "epoch": 0.8755379013571665,
      "grad_norm": 6.7407426834106445,
      "learning_rate": 3.988085178299814e-05,
      "loss": 0.8002,
      "step": 5290
    },
    {
      "epoch": 0.8771929824561403,
      "grad_norm": 6.914491176605225,
      "learning_rate": 3.985972621260774e-05,
      "loss": 0.7538,
      "step": 5300
    },
    {
      "epoch": 0.8788480635551142,
      "grad_norm": 6.258886814117432,
      "learning_rate": 3.9838600642217347e-05,
      "loss": 0.9445,
      "step": 5310
    },
    {
      "epoch": 0.8805031446540881,
      "grad_norm": 3.8856136798858643,
      "learning_rate": 3.9817475071826945e-05,
      "loss": 0.5205,
      "step": 5320
    },
    {
      "epoch": 0.8821582257530619,
      "grad_norm": 3.695891857147217,
      "learning_rate": 3.979634950143654e-05,
      "loss": 0.6468,
      "step": 5330
    },
    {
      "epoch": 0.8838133068520357,
      "grad_norm": 6.838665962219238,
      "learning_rate": 3.977522393104614e-05,
      "loss": 0.8835,
      "step": 5340
    },
    {
      "epoch": 0.8854683879510096,
      "grad_norm": 4.32612419128418,
      "learning_rate": 3.975409836065574e-05,
      "loss": 0.6721,
      "step": 5350
    },
    {
      "epoch": 0.8871234690499834,
      "grad_norm": 6.689059257507324,
      "learning_rate": 3.973297279026534e-05,
      "loss": 0.9238,
      "step": 5360
    },
    {
      "epoch": 0.8887785501489573,
      "grad_norm": 8.288667678833008,
      "learning_rate": 3.971184721987494e-05,
      "loss": 0.9151,
      "step": 5370
    },
    {
      "epoch": 0.8904336312479312,
      "grad_norm": 3.008657932281494,
      "learning_rate": 3.9690721649484535e-05,
      "loss": 0.7425,
      "step": 5380
    },
    {
      "epoch": 0.892088712346905,
      "grad_norm": 6.7204084396362305,
      "learning_rate": 3.966959607909414e-05,
      "loss": 1.02,
      "step": 5390
    },
    {
      "epoch": 0.8937437934458788,
      "grad_norm": 8.979735374450684,
      "learning_rate": 3.964847050870374e-05,
      "loss": 0.645,
      "step": 5400
    },
    {
      "epoch": 0.8953988745448527,
      "grad_norm": 3.6422133445739746,
      "learning_rate": 3.962734493831334e-05,
      "loss": 0.6467,
      "step": 5410
    },
    {
      "epoch": 0.8970539556438265,
      "grad_norm": 11.633221626281738,
      "learning_rate": 3.9606219367922936e-05,
      "loss": 0.8949,
      "step": 5420
    },
    {
      "epoch": 0.8987090367428004,
      "grad_norm": 5.105805397033691,
      "learning_rate": 3.9585093797532534e-05,
      "loss": 0.8518,
      "step": 5430
    },
    {
      "epoch": 0.9003641178417743,
      "grad_norm": 6.702571392059326,
      "learning_rate": 3.956396822714213e-05,
      "loss": 0.6298,
      "step": 5440
    },
    {
      "epoch": 0.9020191989407481,
      "grad_norm": 3.891049385070801,
      "learning_rate": 3.954284265675173e-05,
      "loss": 0.5076,
      "step": 5450
    },
    {
      "epoch": 0.9036742800397219,
      "grad_norm": 7.306512832641602,
      "learning_rate": 3.952171708636133e-05,
      "loss": 0.7208,
      "step": 5460
    },
    {
      "epoch": 0.9053293611386958,
      "grad_norm": 8.937118530273438,
      "learning_rate": 3.950059151597093e-05,
      "loss": 0.7317,
      "step": 5470
    },
    {
      "epoch": 0.9069844422376696,
      "grad_norm": 9.849403381347656,
      "learning_rate": 3.947946594558053e-05,
      "loss": 1.0634,
      "step": 5480
    },
    {
      "epoch": 0.9086395233366436,
      "grad_norm": 4.4751129150390625,
      "learning_rate": 3.945834037519013e-05,
      "loss": 0.7869,
      "step": 5490
    },
    {
      "epoch": 0.9102946044356174,
      "grad_norm": 9.121310234069824,
      "learning_rate": 3.943721480479973e-05,
      "loss": 1.2411,
      "step": 5500
    },
    {
      "epoch": 0.9119496855345912,
      "grad_norm": 4.661601543426514,
      "learning_rate": 3.9416089234409335e-05,
      "loss": 0.797,
      "step": 5510
    },
    {
      "epoch": 0.913604766633565,
      "grad_norm": 9.620807647705078,
      "learning_rate": 3.939496366401893e-05,
      "loss": 0.8303,
      "step": 5520
    },
    {
      "epoch": 0.9152598477325389,
      "grad_norm": 8.210502624511719,
      "learning_rate": 3.937383809362853e-05,
      "loss": 0.6475,
      "step": 5530
    },
    {
      "epoch": 0.9169149288315127,
      "grad_norm": 6.49641752243042,
      "learning_rate": 3.935271252323813e-05,
      "loss": 1.0418,
      "step": 5540
    },
    {
      "epoch": 0.9185700099304865,
      "grad_norm": 5.502089977264404,
      "learning_rate": 3.933158695284773e-05,
      "loss": 0.8705,
      "step": 5550
    },
    {
      "epoch": 0.9202250910294605,
      "grad_norm": 5.70926570892334,
      "learning_rate": 3.931046138245733e-05,
      "loss": 0.6527,
      "step": 5560
    },
    {
      "epoch": 0.9218801721284343,
      "grad_norm": 8.291305541992188,
      "learning_rate": 3.928933581206693e-05,
      "loss": 0.7902,
      "step": 5570
    },
    {
      "epoch": 0.9235352532274081,
      "grad_norm": 3.212561845779419,
      "learning_rate": 3.926821024167653e-05,
      "loss": 0.5282,
      "step": 5580
    },
    {
      "epoch": 0.925190334326382,
      "grad_norm": 13.752019882202148,
      "learning_rate": 3.924708467128613e-05,
      "loss": 1.0058,
      "step": 5590
    },
    {
      "epoch": 0.9268454154253558,
      "grad_norm": 9.438868522644043,
      "learning_rate": 3.922595910089573e-05,
      "loss": 1.0936,
      "step": 5600
    },
    {
      "epoch": 0.9285004965243296,
      "grad_norm": 3.17415189743042,
      "learning_rate": 3.9204833530505325e-05,
      "loss": 0.8907,
      "step": 5610
    },
    {
      "epoch": 0.9301555776233036,
      "grad_norm": 6.175893306732178,
      "learning_rate": 3.9183707960114924e-05,
      "loss": 0.6455,
      "step": 5620
    },
    {
      "epoch": 0.9318106587222774,
      "grad_norm": 8.105611801147461,
      "learning_rate": 3.916258238972452e-05,
      "loss": 0.7816,
      "step": 5630
    },
    {
      "epoch": 0.9334657398212513,
      "grad_norm": 6.192255020141602,
      "learning_rate": 3.914145681933412e-05,
      "loss": 0.8329,
      "step": 5640
    },
    {
      "epoch": 0.9351208209202251,
      "grad_norm": 7.72561502456665,
      "learning_rate": 3.9120331248943725e-05,
      "loss": 0.7949,
      "step": 5650
    },
    {
      "epoch": 0.9367759020191989,
      "grad_norm": 2.6464052200317383,
      "learning_rate": 3.9099205678553324e-05,
      "loss": 0.9183,
      "step": 5660
    },
    {
      "epoch": 0.9384309831181727,
      "grad_norm": 7.433149337768555,
      "learning_rate": 3.907808010816292e-05,
      "loss": 0.5919,
      "step": 5670
    },
    {
      "epoch": 0.9400860642171467,
      "grad_norm": 5.4882283210754395,
      "learning_rate": 3.905695453777252e-05,
      "loss": 0.5833,
      "step": 5680
    },
    {
      "epoch": 0.9417411453161205,
      "grad_norm": 7.088761329650879,
      "learning_rate": 3.903582896738212e-05,
      "loss": 0.669,
      "step": 5690
    },
    {
      "epoch": 0.9433962264150944,
      "grad_norm": 8.576591491699219,
      "learning_rate": 3.901470339699172e-05,
      "loss": 0.7211,
      "step": 5700
    },
    {
      "epoch": 0.9450513075140682,
      "grad_norm": 13.387581825256348,
      "learning_rate": 3.8993577826601316e-05,
      "loss": 0.8661,
      "step": 5710
    },
    {
      "epoch": 0.946706388613042,
      "grad_norm": 5.815371990203857,
      "learning_rate": 3.8972452256210914e-05,
      "loss": 0.5873,
      "step": 5720
    },
    {
      "epoch": 0.9483614697120158,
      "grad_norm": 3.7529213428497314,
      "learning_rate": 3.895132668582052e-05,
      "loss": 0.7352,
      "step": 5730
    },
    {
      "epoch": 0.9500165508109898,
      "grad_norm": 3.91986346244812,
      "learning_rate": 3.893020111543012e-05,
      "loss": 0.8345,
      "step": 5740
    },
    {
      "epoch": 0.9516716319099636,
      "grad_norm": 4.097564220428467,
      "learning_rate": 3.8909075545039716e-05,
      "loss": 0.8311,
      "step": 5750
    },
    {
      "epoch": 0.9533267130089375,
      "grad_norm": 5.831536769866943,
      "learning_rate": 3.888794997464932e-05,
      "loss": 0.651,
      "step": 5760
    },
    {
      "epoch": 0.9549817941079113,
      "grad_norm": 3.1209757328033447,
      "learning_rate": 3.886682440425892e-05,
      "loss": 0.9341,
      "step": 5770
    },
    {
      "epoch": 0.9566368752068851,
      "grad_norm": 8.368700981140137,
      "learning_rate": 3.884569883386852e-05,
      "loss": 0.565,
      "step": 5780
    },
    {
      "epoch": 0.958291956305859,
      "grad_norm": 4.004268169403076,
      "learning_rate": 3.8824573263478116e-05,
      "loss": 0.6752,
      "step": 5790
    },
    {
      "epoch": 0.9599470374048328,
      "grad_norm": 6.96585750579834,
      "learning_rate": 3.8803447693087715e-05,
      "loss": 0.6275,
      "step": 5800
    },
    {
      "epoch": 0.9616021185038067,
      "grad_norm": 11.93501091003418,
      "learning_rate": 3.878232212269731e-05,
      "loss": 0.8464,
      "step": 5810
    },
    {
      "epoch": 0.9632571996027806,
      "grad_norm": 7.300952434539795,
      "learning_rate": 3.876119655230692e-05,
      "loss": 1.0381,
      "step": 5820
    },
    {
      "epoch": 0.9649122807017544,
      "grad_norm": 3.9323360919952393,
      "learning_rate": 3.874007098191652e-05,
      "loss": 0.798,
      "step": 5830
    },
    {
      "epoch": 0.9665673618007282,
      "grad_norm": 8.626023292541504,
      "learning_rate": 3.8718945411526115e-05,
      "loss": 0.8782,
      "step": 5840
    },
    {
      "epoch": 0.968222442899702,
      "grad_norm": 9.177884101867676,
      "learning_rate": 3.8697819841135713e-05,
      "loss": 0.6883,
      "step": 5850
    },
    {
      "epoch": 0.9698775239986759,
      "grad_norm": 5.639456272125244,
      "learning_rate": 3.867669427074531e-05,
      "loss": 0.8211,
      "step": 5860
    },
    {
      "epoch": 0.9715326050976498,
      "grad_norm": 2.318056106567383,
      "learning_rate": 3.865556870035491e-05,
      "loss": 0.5665,
      "step": 5870
    },
    {
      "epoch": 0.9731876861966237,
      "grad_norm": 9.1118803024292,
      "learning_rate": 3.863444312996451e-05,
      "loss": 0.8282,
      "step": 5880
    },
    {
      "epoch": 0.9748427672955975,
      "grad_norm": 4.295466899871826,
      "learning_rate": 3.861331755957411e-05,
      "loss": 0.6521,
      "step": 5890
    },
    {
      "epoch": 0.9764978483945713,
      "grad_norm": 11.82414722442627,
      "learning_rate": 3.859219198918371e-05,
      "loss": 0.7619,
      "step": 5900
    },
    {
      "epoch": 0.9781529294935452,
      "grad_norm": 2.394678831100464,
      "learning_rate": 3.857106641879331e-05,
      "loss": 1.0147,
      "step": 5910
    },
    {
      "epoch": 0.979808010592519,
      "grad_norm": 8.692333221435547,
      "learning_rate": 3.854994084840291e-05,
      "loss": 0.5116,
      "step": 5920
    },
    {
      "epoch": 0.9814630916914929,
      "grad_norm": 7.333255767822266,
      "learning_rate": 3.852881527801251e-05,
      "loss": 0.965,
      "step": 5930
    },
    {
      "epoch": 0.9831181727904668,
      "grad_norm": 7.43820333480835,
      "learning_rate": 3.8507689707622106e-05,
      "loss": 0.7462,
      "step": 5940
    },
    {
      "epoch": 0.9847732538894406,
      "grad_norm": 4.187070369720459,
      "learning_rate": 3.8486564137231704e-05,
      "loss": 0.8058,
      "step": 5950
    },
    {
      "epoch": 0.9864283349884144,
      "grad_norm": 3.2813329696655273,
      "learning_rate": 3.84654385668413e-05,
      "loss": 0.6014,
      "step": 5960
    },
    {
      "epoch": 0.9880834160873883,
      "grad_norm": 4.6697096824646,
      "learning_rate": 3.84443129964509e-05,
      "loss": 0.7311,
      "step": 5970
    },
    {
      "epoch": 0.9897384971863621,
      "grad_norm": 7.3077898025512695,
      "learning_rate": 3.8423187426060506e-05,
      "loss": 0.7493,
      "step": 5980
    },
    {
      "epoch": 0.991393578285336,
      "grad_norm": 9.459609985351562,
      "learning_rate": 3.8402061855670104e-05,
      "loss": 0.7926,
      "step": 5990
    },
    {
      "epoch": 0.9930486593843099,
      "grad_norm": 9.969450950622559,
      "learning_rate": 3.83809362852797e-05,
      "loss": 0.8097,
      "step": 6000
    },
    {
      "epoch": 0.9947037404832837,
      "grad_norm": 12.550638198852539,
      "learning_rate": 3.835981071488931e-05,
      "loss": 0.7239,
      "step": 6010
    },
    {
      "epoch": 0.9963588215822575,
      "grad_norm": 7.678092002868652,
      "learning_rate": 3.8338685144498906e-05,
      "loss": 0.7383,
      "step": 6020
    },
    {
      "epoch": 0.9980139026812314,
      "grad_norm": 4.705316543579102,
      "learning_rate": 3.8317559574108505e-05,
      "loss": 0.7249,
      "step": 6030
    },
    {
      "epoch": 0.9996689837802052,
      "grad_norm": 11.151215553283691,
      "learning_rate": 3.82964340037181e-05,
      "loss": 0.6281,
      "step": 6040
    },
    {
      "epoch": 1.001324064879179,
      "grad_norm": 6.518406867980957,
      "learning_rate": 3.82753084333277e-05,
      "loss": 0.9236,
      "step": 6050
    },
    {
      "epoch": 1.002979145978153,
      "grad_norm": 2.786750555038452,
      "learning_rate": 3.82541828629373e-05,
      "loss": 0.6503,
      "step": 6060
    },
    {
      "epoch": 1.0046342270771267,
      "grad_norm": 4.405419826507568,
      "learning_rate": 3.8233057292546905e-05,
      "loss": 0.6991,
      "step": 6070
    },
    {
      "epoch": 1.0062893081761006,
      "grad_norm": 9.034991264343262,
      "learning_rate": 3.8211931722156503e-05,
      "loss": 0.6314,
      "step": 6080
    },
    {
      "epoch": 1.0079443892750746,
      "grad_norm": 8.976570129394531,
      "learning_rate": 3.81908061517661e-05,
      "loss": 0.7638,
      "step": 6090
    },
    {
      "epoch": 1.0095994703740483,
      "grad_norm": 11.087831497192383,
      "learning_rate": 3.81696805813757e-05,
      "loss": 0.7247,
      "step": 6100
    },
    {
      "epoch": 1.0112545514730222,
      "grad_norm": 7.215321063995361,
      "learning_rate": 3.81485550109853e-05,
      "loss": 0.7285,
      "step": 6110
    },
    {
      "epoch": 1.012909632571996,
      "grad_norm": 5.415118217468262,
      "learning_rate": 3.81274294405949e-05,
      "loss": 0.7109,
      "step": 6120
    },
    {
      "epoch": 1.01456471367097,
      "grad_norm": 2.8213400840759277,
      "learning_rate": 3.8106303870204495e-05,
      "loss": 0.5543,
      "step": 6130
    },
    {
      "epoch": 1.0162197947699436,
      "grad_norm": 7.7865824699401855,
      "learning_rate": 3.8085178299814094e-05,
      "loss": 0.5905,
      "step": 6140
    },
    {
      "epoch": 1.0178748758689176,
      "grad_norm": 8.589634895324707,
      "learning_rate": 3.806405272942369e-05,
      "loss": 0.5962,
      "step": 6150
    },
    {
      "epoch": 1.0195299569678915,
      "grad_norm": 9.689987182617188,
      "learning_rate": 3.80429271590333e-05,
      "loss": 0.6255,
      "step": 6160
    },
    {
      "epoch": 1.0211850380668652,
      "grad_norm": 6.403554916381836,
      "learning_rate": 3.8021801588642896e-05,
      "loss": 0.783,
      "step": 6170
    },
    {
      "epoch": 1.0228401191658392,
      "grad_norm": 10.512937545776367,
      "learning_rate": 3.8000676018252494e-05,
      "loss": 0.808,
      "step": 6180
    },
    {
      "epoch": 1.0244952002648129,
      "grad_norm": 6.386028289794922,
      "learning_rate": 3.797955044786209e-05,
      "loss": 0.9294,
      "step": 6190
    },
    {
      "epoch": 1.0261502813637868,
      "grad_norm": 12.173444747924805,
      "learning_rate": 3.795842487747169e-05,
      "loss": 1.0584,
      "step": 6200
    },
    {
      "epoch": 1.0278053624627608,
      "grad_norm": 10.3660249710083,
      "learning_rate": 3.793729930708129e-05,
      "loss": 0.5132,
      "step": 6210
    },
    {
      "epoch": 1.0294604435617345,
      "grad_norm": 5.885777950286865,
      "learning_rate": 3.791617373669089e-05,
      "loss": 0.8248,
      "step": 6220
    },
    {
      "epoch": 1.0311155246607084,
      "grad_norm": 11.990609169006348,
      "learning_rate": 3.789504816630049e-05,
      "loss": 0.6344,
      "step": 6230
    },
    {
      "epoch": 1.0327706057596822,
      "grad_norm": 8.579591751098633,
      "learning_rate": 3.787392259591009e-05,
      "loss": 1.164,
      "step": 6240
    },
    {
      "epoch": 1.034425686858656,
      "grad_norm": 4.441565990447998,
      "learning_rate": 3.785279702551969e-05,
      "loss": 0.8635,
      "step": 6250
    },
    {
      "epoch": 1.0360807679576298,
      "grad_norm": 5.815997123718262,
      "learning_rate": 3.7831671455129295e-05,
      "loss": 0.8239,
      "step": 6260
    },
    {
      "epoch": 1.0377358490566038,
      "grad_norm": 4.0193376541137695,
      "learning_rate": 3.781054588473889e-05,
      "loss": 0.5734,
      "step": 6270
    },
    {
      "epoch": 1.0393909301555777,
      "grad_norm": 2.8566458225250244,
      "learning_rate": 3.778942031434849e-05,
      "loss": 0.7098,
      "step": 6280
    },
    {
      "epoch": 1.0410460112545514,
      "grad_norm": 4.620614528656006,
      "learning_rate": 3.776829474395809e-05,
      "loss": 0.7029,
      "step": 6290
    },
    {
      "epoch": 1.0427010923535254,
      "grad_norm": 5.069474697113037,
      "learning_rate": 3.774716917356769e-05,
      "loss": 0.5052,
      "step": 6300
    },
    {
      "epoch": 1.044356173452499,
      "grad_norm": 10.774474143981934,
      "learning_rate": 3.7726043603177287e-05,
      "loss": 1.0114,
      "step": 6310
    },
    {
      "epoch": 1.046011254551473,
      "grad_norm": 6.714102745056152,
      "learning_rate": 3.7704918032786885e-05,
      "loss": 0.604,
      "step": 6320
    },
    {
      "epoch": 1.047666335650447,
      "grad_norm": 7.08482551574707,
      "learning_rate": 3.768379246239649e-05,
      "loss": 0.9567,
      "step": 6330
    },
    {
      "epoch": 1.0493214167494207,
      "grad_norm": 3.9563658237457275,
      "learning_rate": 3.766266689200609e-05,
      "loss": 0.4573,
      "step": 6340
    },
    {
      "epoch": 1.0509764978483946,
      "grad_norm": 5.296104431152344,
      "learning_rate": 3.764154132161569e-05,
      "loss": 0.5425,
      "step": 6350
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 11.595404624938965,
      "learning_rate": 3.7620415751225285e-05,
      "loss": 0.6648,
      "step": 6360
    },
    {
      "epoch": 1.0542866600463423,
      "grad_norm": 4.364054203033447,
      "learning_rate": 3.7599290180834884e-05,
      "loss": 0.6395,
      "step": 6370
    },
    {
      "epoch": 1.055941741145316,
      "grad_norm": 2.6158525943756104,
      "learning_rate": 3.757816461044448e-05,
      "loss": 0.6826,
      "step": 6380
    },
    {
      "epoch": 1.05759682224429,
      "grad_norm": 6.0076117515563965,
      "learning_rate": 3.755703904005408e-05,
      "loss": 0.7826,
      "step": 6390
    },
    {
      "epoch": 1.059251903343264,
      "grad_norm": 8.394646644592285,
      "learning_rate": 3.753591346966368e-05,
      "loss": 0.6281,
      "step": 6400
    },
    {
      "epoch": 1.0609069844422376,
      "grad_norm": 8.699710845947266,
      "learning_rate": 3.7514787899273284e-05,
      "loss": 0.7419,
      "step": 6410
    },
    {
      "epoch": 1.0625620655412116,
      "grad_norm": 15.326727867126465,
      "learning_rate": 3.749366232888288e-05,
      "loss": 0.698,
      "step": 6420
    },
    {
      "epoch": 1.0642171466401853,
      "grad_norm": 9.417495727539062,
      "learning_rate": 3.747253675849248e-05,
      "loss": 0.8707,
      "step": 6430
    },
    {
      "epoch": 1.0658722277391592,
      "grad_norm": 15.093852996826172,
      "learning_rate": 3.745141118810208e-05,
      "loss": 0.6771,
      "step": 6440
    },
    {
      "epoch": 1.067527308838133,
      "grad_norm": 5.133484363555908,
      "learning_rate": 3.743028561771168e-05,
      "loss": 0.8722,
      "step": 6450
    },
    {
      "epoch": 1.069182389937107,
      "grad_norm": 3.690852403640747,
      "learning_rate": 3.7409160047321276e-05,
      "loss": 0.9093,
      "step": 6460
    },
    {
      "epoch": 1.0708374710360808,
      "grad_norm": 4.166110992431641,
      "learning_rate": 3.738803447693088e-05,
      "loss": 0.6697,
      "step": 6470
    },
    {
      "epoch": 1.0724925521350546,
      "grad_norm": 10.393726348876953,
      "learning_rate": 3.736690890654048e-05,
      "loss": 0.8661,
      "step": 6480
    },
    {
      "epoch": 1.0741476332340285,
      "grad_norm": 3.2394423484802246,
      "learning_rate": 3.734578333615008e-05,
      "loss": 0.7986,
      "step": 6490
    },
    {
      "epoch": 1.0758027143330022,
      "grad_norm": 5.046095848083496,
      "learning_rate": 3.732465776575968e-05,
      "loss": 0.8333,
      "step": 6500
    },
    {
      "epoch": 1.0774577954319762,
      "grad_norm": 7.35997200012207,
      "learning_rate": 3.730353219536928e-05,
      "loss": 0.8187,
      "step": 6510
    },
    {
      "epoch": 1.0791128765309501,
      "grad_norm": 3.15278959274292,
      "learning_rate": 3.728240662497888e-05,
      "loss": 0.7423,
      "step": 6520
    },
    {
      "epoch": 1.0807679576299238,
      "grad_norm": 5.886096477508545,
      "learning_rate": 3.726128105458848e-05,
      "loss": 0.5739,
      "step": 6530
    },
    {
      "epoch": 1.0824230387288978,
      "grad_norm": 8.116938591003418,
      "learning_rate": 3.7240155484198076e-05,
      "loss": 0.6777,
      "step": 6540
    },
    {
      "epoch": 1.0840781198278715,
      "grad_norm": 8.417621612548828,
      "learning_rate": 3.7219029913807675e-05,
      "loss": 0.8631,
      "step": 6550
    },
    {
      "epoch": 1.0857332009268454,
      "grad_norm": 7.217428684234619,
      "learning_rate": 3.719790434341727e-05,
      "loss": 0.8336,
      "step": 6560
    },
    {
      "epoch": 1.0873882820258192,
      "grad_norm": 1.7699183225631714,
      "learning_rate": 3.717677877302687e-05,
      "loss": 0.6011,
      "step": 6570
    },
    {
      "epoch": 1.089043363124793,
      "grad_norm": 4.5065083503723145,
      "learning_rate": 3.715565320263648e-05,
      "loss": 0.5588,
      "step": 6580
    },
    {
      "epoch": 1.090698444223767,
      "grad_norm": 8.196365356445312,
      "learning_rate": 3.7134527632246075e-05,
      "loss": 0.5082,
      "step": 6590
    },
    {
      "epoch": 1.0923535253227408,
      "grad_norm": 12.140345573425293,
      "learning_rate": 3.7113402061855674e-05,
      "loss": 0.8012,
      "step": 6600
    },
    {
      "epoch": 1.0940086064217147,
      "grad_norm": 6.861212730407715,
      "learning_rate": 3.709227649146527e-05,
      "loss": 0.8159,
      "step": 6610
    },
    {
      "epoch": 1.0956636875206884,
      "grad_norm": 0.9522998332977295,
      "learning_rate": 3.707115092107487e-05,
      "loss": 0.4879,
      "step": 6620
    },
    {
      "epoch": 1.0973187686196624,
      "grad_norm": 4.11716890335083,
      "learning_rate": 3.705002535068447e-05,
      "loss": 0.636,
      "step": 6630
    },
    {
      "epoch": 1.0989738497186363,
      "grad_norm": 4.967156887054443,
      "learning_rate": 3.702889978029407e-05,
      "loss": 0.7844,
      "step": 6640
    },
    {
      "epoch": 1.10062893081761,
      "grad_norm": 9.6195650100708,
      "learning_rate": 3.7007774209903665e-05,
      "loss": 0.5735,
      "step": 6650
    },
    {
      "epoch": 1.102284011916584,
      "grad_norm": 8.064162254333496,
      "learning_rate": 3.6986648639513264e-05,
      "loss": 0.7889,
      "step": 6660
    },
    {
      "epoch": 1.1039390930155577,
      "grad_norm": 2.745148181915283,
      "learning_rate": 3.696552306912287e-05,
      "loss": 0.9982,
      "step": 6670
    },
    {
      "epoch": 1.1055941741145316,
      "grad_norm": 12.541773796081543,
      "learning_rate": 3.694439749873247e-05,
      "loss": 0.8908,
      "step": 6680
    },
    {
      "epoch": 1.1072492552135054,
      "grad_norm": 7.391824722290039,
      "learning_rate": 3.6923271928342066e-05,
      "loss": 0.6141,
      "step": 6690
    },
    {
      "epoch": 1.1089043363124793,
      "grad_norm": 4.546953201293945,
      "learning_rate": 3.6902146357951664e-05,
      "loss": 0.4683,
      "step": 6700
    },
    {
      "epoch": 1.1105594174114533,
      "grad_norm": 10.516761779785156,
      "learning_rate": 3.688102078756126e-05,
      "loss": 0.88,
      "step": 6710
    },
    {
      "epoch": 1.112214498510427,
      "grad_norm": 5.7534284591674805,
      "learning_rate": 3.685989521717087e-05,
      "loss": 0.6763,
      "step": 6720
    },
    {
      "epoch": 1.113869579609401,
      "grad_norm": 8.477033615112305,
      "learning_rate": 3.6838769646780466e-05,
      "loss": 0.6031,
      "step": 6730
    },
    {
      "epoch": 1.1155246607083746,
      "grad_norm": 10.461743354797363,
      "learning_rate": 3.6817644076390064e-05,
      "loss": 0.8644,
      "step": 6740
    },
    {
      "epoch": 1.1171797418073486,
      "grad_norm": 4.804139137268066,
      "learning_rate": 3.679651850599967e-05,
      "loss": 1.0702,
      "step": 6750
    },
    {
      "epoch": 1.1188348229063223,
      "grad_norm": 11.583818435668945,
      "learning_rate": 3.677539293560927e-05,
      "loss": 1.0061,
      "step": 6760
    },
    {
      "epoch": 1.1204899040052962,
      "grad_norm": 4.435029983520508,
      "learning_rate": 3.6754267365218866e-05,
      "loss": 0.9434,
      "step": 6770
    },
    {
      "epoch": 1.1221449851042702,
      "grad_norm": 5.4313788414001465,
      "learning_rate": 3.6733141794828465e-05,
      "loss": 0.5003,
      "step": 6780
    },
    {
      "epoch": 1.123800066203244,
      "grad_norm": 7.3584794998168945,
      "learning_rate": 3.671201622443806e-05,
      "loss": 0.6557,
      "step": 6790
    },
    {
      "epoch": 1.1254551473022179,
      "grad_norm": 3.4365134239196777,
      "learning_rate": 3.669089065404766e-05,
      "loss": 0.6309,
      "step": 6800
    },
    {
      "epoch": 1.1271102284011916,
      "grad_norm": 10.767348289489746,
      "learning_rate": 3.666976508365726e-05,
      "loss": 0.7167,
      "step": 6810
    },
    {
      "epoch": 1.1287653095001655,
      "grad_norm": 12.988149642944336,
      "learning_rate": 3.664863951326686e-05,
      "loss": 0.8825,
      "step": 6820
    },
    {
      "epoch": 1.1304203905991392,
      "grad_norm": 5.682524681091309,
      "learning_rate": 3.662751394287646e-05,
      "loss": 0.4676,
      "step": 6830
    },
    {
      "epoch": 1.1320754716981132,
      "grad_norm": 10.703333854675293,
      "learning_rate": 3.660638837248606e-05,
      "loss": 0.6629,
      "step": 6840
    },
    {
      "epoch": 1.1337305527970871,
      "grad_norm": 7.927700996398926,
      "learning_rate": 3.658526280209566e-05,
      "loss": 0.7073,
      "step": 6850
    },
    {
      "epoch": 1.1353856338960608,
      "grad_norm": 8.109436988830566,
      "learning_rate": 3.656413723170526e-05,
      "loss": 0.6591,
      "step": 6860
    },
    {
      "epoch": 1.1370407149950348,
      "grad_norm": 6.961367607116699,
      "learning_rate": 3.654301166131486e-05,
      "loss": 1.0573,
      "step": 6870
    },
    {
      "epoch": 1.1386957960940087,
      "grad_norm": 7.393187046051025,
      "learning_rate": 3.6521886090924455e-05,
      "loss": 0.5463,
      "step": 6880
    },
    {
      "epoch": 1.1403508771929824,
      "grad_norm": 3.9798266887664795,
      "learning_rate": 3.6500760520534054e-05,
      "loss": 0.7738,
      "step": 6890
    },
    {
      "epoch": 1.1420059582919564,
      "grad_norm": 5.561964988708496,
      "learning_rate": 3.647963495014365e-05,
      "loss": 0.6792,
      "step": 6900
    },
    {
      "epoch": 1.1436610393909301,
      "grad_norm": 8.877525329589844,
      "learning_rate": 3.645850937975325e-05,
      "loss": 0.7545,
      "step": 6910
    },
    {
      "epoch": 1.145316120489904,
      "grad_norm": 8.257837295532227,
      "learning_rate": 3.6437383809362856e-05,
      "loss": 0.655,
      "step": 6920
    },
    {
      "epoch": 1.1469712015888778,
      "grad_norm": 3.726146936416626,
      "learning_rate": 3.6416258238972454e-05,
      "loss": 0.7954,
      "step": 6930
    },
    {
      "epoch": 1.1486262826878517,
      "grad_norm": 8.134098052978516,
      "learning_rate": 3.639513266858205e-05,
      "loss": 0.6725,
      "step": 6940
    },
    {
      "epoch": 1.1502813637868257,
      "grad_norm": 8.043886184692383,
      "learning_rate": 3.637400709819165e-05,
      "loss": 0.8112,
      "step": 6950
    },
    {
      "epoch": 1.1519364448857994,
      "grad_norm": 9.990250587463379,
      "learning_rate": 3.635288152780125e-05,
      "loss": 1.2451,
      "step": 6960
    },
    {
      "epoch": 1.1535915259847733,
      "grad_norm": 5.516153812408447,
      "learning_rate": 3.6331755957410854e-05,
      "loss": 0.5153,
      "step": 6970
    },
    {
      "epoch": 1.155246607083747,
      "grad_norm": 1.6596720218658447,
      "learning_rate": 3.631063038702045e-05,
      "loss": 0.7634,
      "step": 6980
    },
    {
      "epoch": 1.156901688182721,
      "grad_norm": 4.818734645843506,
      "learning_rate": 3.628950481663005e-05,
      "loss": 0.7347,
      "step": 6990
    },
    {
      "epoch": 1.1585567692816947,
      "grad_norm": 14.623958587646484,
      "learning_rate": 3.626837924623965e-05,
      "loss": 0.5582,
      "step": 7000
    },
    {
      "epoch": 1.1602118503806687,
      "grad_norm": 4.6976237297058105,
      "learning_rate": 3.6247253675849255e-05,
      "loss": 0.7431,
      "step": 7010
    },
    {
      "epoch": 1.1618669314796426,
      "grad_norm": 6.487015724182129,
      "learning_rate": 3.622612810545885e-05,
      "loss": 0.5759,
      "step": 7020
    },
    {
      "epoch": 1.1635220125786163,
      "grad_norm": 15.384269714355469,
      "learning_rate": 3.620500253506845e-05,
      "loss": 0.8358,
      "step": 7030
    },
    {
      "epoch": 1.1651770936775903,
      "grad_norm": 10.690239906311035,
      "learning_rate": 3.618387696467805e-05,
      "loss": 0.8372,
      "step": 7040
    },
    {
      "epoch": 1.166832174776564,
      "grad_norm": 9.670157432556152,
      "learning_rate": 3.616275139428765e-05,
      "loss": 0.8187,
      "step": 7050
    },
    {
      "epoch": 1.168487255875538,
      "grad_norm": 10.020424842834473,
      "learning_rate": 3.614162582389725e-05,
      "loss": 0.7706,
      "step": 7060
    },
    {
      "epoch": 1.1701423369745116,
      "grad_norm": 10.113683700561523,
      "learning_rate": 3.6120500253506845e-05,
      "loss": 0.9841,
      "step": 7070
    },
    {
      "epoch": 1.1717974180734856,
      "grad_norm": 11.158884048461914,
      "learning_rate": 3.6099374683116443e-05,
      "loss": 0.8783,
      "step": 7080
    },
    {
      "epoch": 1.1734524991724595,
      "grad_norm": 5.935790538787842,
      "learning_rate": 3.607824911272605e-05,
      "loss": 0.6582,
      "step": 7090
    },
    {
      "epoch": 1.1751075802714332,
      "grad_norm": 10.57275676727295,
      "learning_rate": 3.605712354233565e-05,
      "loss": 0.7496,
      "step": 7100
    },
    {
      "epoch": 1.1767626613704072,
      "grad_norm": 2.951037645339966,
      "learning_rate": 3.6035997971945245e-05,
      "loss": 0.6413,
      "step": 7110
    },
    {
      "epoch": 1.178417742469381,
      "grad_norm": 10.563003540039062,
      "learning_rate": 3.6014872401554844e-05,
      "loss": 0.9732,
      "step": 7120
    },
    {
      "epoch": 1.1800728235683549,
      "grad_norm": 7.732124328613281,
      "learning_rate": 3.599374683116444e-05,
      "loss": 0.9223,
      "step": 7130
    },
    {
      "epoch": 1.1817279046673286,
      "grad_norm": 6.886355876922607,
      "learning_rate": 3.597262126077404e-05,
      "loss": 0.5639,
      "step": 7140
    },
    {
      "epoch": 1.1833829857663025,
      "grad_norm": 7.257668495178223,
      "learning_rate": 3.595149569038364e-05,
      "loss": 0.6674,
      "step": 7150
    },
    {
      "epoch": 1.1850380668652765,
      "grad_norm": 4.526784420013428,
      "learning_rate": 3.593037011999324e-05,
      "loss": 0.8718,
      "step": 7160
    },
    {
      "epoch": 1.1866931479642502,
      "grad_norm": 9.535927772521973,
      "learning_rate": 3.5909244549602836e-05,
      "loss": 0.7178,
      "step": 7170
    },
    {
      "epoch": 1.1883482290632241,
      "grad_norm": 6.364920139312744,
      "learning_rate": 3.588811897921244e-05,
      "loss": 0.6439,
      "step": 7180
    },
    {
      "epoch": 1.190003310162198,
      "grad_norm": 6.341578483581543,
      "learning_rate": 3.586699340882204e-05,
      "loss": 0.5866,
      "step": 7190
    },
    {
      "epoch": 1.1916583912611718,
      "grad_norm": 8.837087631225586,
      "learning_rate": 3.584586783843164e-05,
      "loss": 0.5328,
      "step": 7200
    },
    {
      "epoch": 1.1933134723601457,
      "grad_norm": 3.193737745285034,
      "learning_rate": 3.5824742268041236e-05,
      "loss": 0.6661,
      "step": 7210
    },
    {
      "epoch": 1.1949685534591195,
      "grad_norm": 7.678332328796387,
      "learning_rate": 3.580361669765084e-05,
      "loss": 0.6165,
      "step": 7220
    },
    {
      "epoch": 1.1966236345580934,
      "grad_norm": 5.2279253005981445,
      "learning_rate": 3.578249112726044e-05,
      "loss": 1.0299,
      "step": 7230
    },
    {
      "epoch": 1.1982787156570671,
      "grad_norm": 9.129426956176758,
      "learning_rate": 3.576136555687004e-05,
      "loss": 0.9512,
      "step": 7240
    },
    {
      "epoch": 1.199933796756041,
      "grad_norm": 6.320796012878418,
      "learning_rate": 3.5740239986479636e-05,
      "loss": 1.0282,
      "step": 7250
    },
    {
      "epoch": 1.201588877855015,
      "grad_norm": 7.459604740142822,
      "learning_rate": 3.571911441608924e-05,
      "loss": 0.8481,
      "step": 7260
    },
    {
      "epoch": 1.2032439589539887,
      "grad_norm": 3.3185272216796875,
      "learning_rate": 3.569798884569884e-05,
      "loss": 0.4843,
      "step": 7270
    },
    {
      "epoch": 1.2048990400529627,
      "grad_norm": 11.688035011291504,
      "learning_rate": 3.567686327530844e-05,
      "loss": 0.9265,
      "step": 7280
    },
    {
      "epoch": 1.2065541211519364,
      "grad_norm": 6.99226713180542,
      "learning_rate": 3.5655737704918037e-05,
      "loss": 0.5674,
      "step": 7290
    },
    {
      "epoch": 1.2082092022509103,
      "grad_norm": 6.594771385192871,
      "learning_rate": 3.5634612134527635e-05,
      "loss": 0.6439,
      "step": 7300
    },
    {
      "epoch": 1.209864283349884,
      "grad_norm": 6.251147747039795,
      "learning_rate": 3.561348656413723e-05,
      "loss": 0.5264,
      "step": 7310
    },
    {
      "epoch": 1.211519364448858,
      "grad_norm": 10.47301959991455,
      "learning_rate": 3.559236099374683e-05,
      "loss": 0.7328,
      "step": 7320
    },
    {
      "epoch": 1.213174445547832,
      "grad_norm": 5.256150245666504,
      "learning_rate": 3.557123542335643e-05,
      "loss": 0.6593,
      "step": 7330
    },
    {
      "epoch": 1.2148295266468057,
      "grad_norm": 12.11567211151123,
      "learning_rate": 3.555010985296603e-05,
      "loss": 1.1692,
      "step": 7340
    },
    {
      "epoch": 1.2164846077457796,
      "grad_norm": 9.258344650268555,
      "learning_rate": 3.5528984282575634e-05,
      "loss": 0.6069,
      "step": 7350
    },
    {
      "epoch": 1.2181396888447533,
      "grad_norm": 9.085851669311523,
      "learning_rate": 3.550785871218523e-05,
      "loss": 0.6817,
      "step": 7360
    },
    {
      "epoch": 1.2197947699437273,
      "grad_norm": 8.504349708557129,
      "learning_rate": 3.548673314179483e-05,
      "loss": 0.958,
      "step": 7370
    },
    {
      "epoch": 1.221449851042701,
      "grad_norm": 8.018280029296875,
      "learning_rate": 3.546560757140443e-05,
      "loss": 0.4337,
      "step": 7380
    },
    {
      "epoch": 1.223104932141675,
      "grad_norm": 7.995080947875977,
      "learning_rate": 3.544448200101403e-05,
      "loss": 0.6597,
      "step": 7390
    },
    {
      "epoch": 1.2247600132406489,
      "grad_norm": 10.100285530090332,
      "learning_rate": 3.5423356430623626e-05,
      "loss": 0.7213,
      "step": 7400
    },
    {
      "epoch": 1.2264150943396226,
      "grad_norm": 4.597739219665527,
      "learning_rate": 3.5402230860233224e-05,
      "loss": 0.5943,
      "step": 7410
    },
    {
      "epoch": 1.2280701754385965,
      "grad_norm": 8.804723739624023,
      "learning_rate": 3.538110528984282e-05,
      "loss": 0.7117,
      "step": 7420
    },
    {
      "epoch": 1.2297252565375703,
      "grad_norm": 2.448058843612671,
      "learning_rate": 3.535997971945243e-05,
      "loss": 0.6533,
      "step": 7430
    },
    {
      "epoch": 1.2313803376365442,
      "grad_norm": 8.154659271240234,
      "learning_rate": 3.5338854149062026e-05,
      "loss": 0.739,
      "step": 7440
    },
    {
      "epoch": 1.233035418735518,
      "grad_norm": 3.411768674850464,
      "learning_rate": 3.5317728578671624e-05,
      "loss": 0.7088,
      "step": 7450
    },
    {
      "epoch": 1.2346904998344919,
      "grad_norm": 4.676724433898926,
      "learning_rate": 3.529660300828123e-05,
      "loss": 0.8394,
      "step": 7460
    },
    {
      "epoch": 1.2363455809334658,
      "grad_norm": 10.319320678710938,
      "learning_rate": 3.527547743789083e-05,
      "loss": 0.9726,
      "step": 7470
    },
    {
      "epoch": 1.2380006620324395,
      "grad_norm": 4.5372796058654785,
      "learning_rate": 3.5254351867500426e-05,
      "loss": 0.729,
      "step": 7480
    },
    {
      "epoch": 1.2396557431314135,
      "grad_norm": 7.622683048248291,
      "learning_rate": 3.5233226297110025e-05,
      "loss": 0.8399,
      "step": 7490
    },
    {
      "epoch": 1.2413108242303874,
      "grad_norm": 7.711987018585205,
      "learning_rate": 3.521210072671962e-05,
      "loss": 0.7103,
      "step": 7500
    },
    {
      "epoch": 1.2429659053293611,
      "grad_norm": 6.814263820648193,
      "learning_rate": 3.519097515632922e-05,
      "loss": 0.9604,
      "step": 7510
    },
    {
      "epoch": 1.244620986428335,
      "grad_norm": 7.50216817855835,
      "learning_rate": 3.5169849585938826e-05,
      "loss": 0.5048,
      "step": 7520
    },
    {
      "epoch": 1.2462760675273088,
      "grad_norm": 5.681636333465576,
      "learning_rate": 3.5148724015548425e-05,
      "loss": 0.7858,
      "step": 7530
    },
    {
      "epoch": 1.2479311486262827,
      "grad_norm": 9.040925025939941,
      "learning_rate": 3.512759844515802e-05,
      "loss": 0.8666,
      "step": 7540
    },
    {
      "epoch": 1.2495862297252565,
      "grad_norm": 5.31890869140625,
      "learning_rate": 3.510647287476762e-05,
      "loss": 0.6261,
      "step": 7550
    },
    {
      "epoch": 1.2512413108242304,
      "grad_norm": 9.894919395446777,
      "learning_rate": 3.508534730437722e-05,
      "loss": 0.8547,
      "step": 7560
    },
    {
      "epoch": 1.2528963919232043,
      "grad_norm": 3.8524532318115234,
      "learning_rate": 3.506422173398682e-05,
      "loss": 0.4473,
      "step": 7570
    },
    {
      "epoch": 1.254551473022178,
      "grad_norm": 5.985054969787598,
      "learning_rate": 3.504309616359642e-05,
      "loss": 0.5111,
      "step": 7580
    },
    {
      "epoch": 1.256206554121152,
      "grad_norm": 7.695760726928711,
      "learning_rate": 3.5021970593206015e-05,
      "loss": 0.7445,
      "step": 7590
    },
    {
      "epoch": 1.2578616352201257,
      "grad_norm": 6.626453399658203,
      "learning_rate": 3.500084502281562e-05,
      "loss": 0.616,
      "step": 7600
    },
    {
      "epoch": 1.2595167163190997,
      "grad_norm": 13.192097663879395,
      "learning_rate": 3.497971945242522e-05,
      "loss": 0.6814,
      "step": 7610
    },
    {
      "epoch": 1.2611717974180734,
      "grad_norm": 11.941505432128906,
      "learning_rate": 3.495859388203482e-05,
      "loss": 1.1657,
      "step": 7620
    },
    {
      "epoch": 1.2628268785170473,
      "grad_norm": 10.585745811462402,
      "learning_rate": 3.4937468311644415e-05,
      "loss": 0.6979,
      "step": 7630
    },
    {
      "epoch": 1.2644819596160213,
      "grad_norm": 8.929100036621094,
      "learning_rate": 3.4916342741254014e-05,
      "loss": 0.9371,
      "step": 7640
    },
    {
      "epoch": 1.266137040714995,
      "grad_norm": 7.016721725463867,
      "learning_rate": 3.489521717086361e-05,
      "loss": 0.7197,
      "step": 7650
    },
    {
      "epoch": 1.267792121813969,
      "grad_norm": 8.017862319946289,
      "learning_rate": 3.487409160047321e-05,
      "loss": 0.7189,
      "step": 7660
    },
    {
      "epoch": 1.2694472029129427,
      "grad_norm": 13.833950996398926,
      "learning_rate": 3.485296603008281e-05,
      "loss": 0.9093,
      "step": 7670
    },
    {
      "epoch": 1.2711022840119166,
      "grad_norm": 4.734326362609863,
      "learning_rate": 3.4831840459692414e-05,
      "loss": 0.6067,
      "step": 7680
    },
    {
      "epoch": 1.2727573651108903,
      "grad_norm": 1.7852694988250732,
      "learning_rate": 3.481071488930201e-05,
      "loss": 0.5751,
      "step": 7690
    },
    {
      "epoch": 1.2744124462098643,
      "grad_norm": 3.5768258571624756,
      "learning_rate": 3.478958931891161e-05,
      "loss": 0.781,
      "step": 7700
    },
    {
      "epoch": 1.2760675273088382,
      "grad_norm": 8.414502143859863,
      "learning_rate": 3.4768463748521216e-05,
      "loss": 0.6276,
      "step": 7710
    },
    {
      "epoch": 1.277722608407812,
      "grad_norm": 9.705824851989746,
      "learning_rate": 3.4747338178130814e-05,
      "loss": 0.5048,
      "step": 7720
    },
    {
      "epoch": 1.2793776895067859,
      "grad_norm": 6.068521022796631,
      "learning_rate": 3.472621260774041e-05,
      "loss": 0.7538,
      "step": 7730
    },
    {
      "epoch": 1.2810327706057598,
      "grad_norm": 12.727599143981934,
      "learning_rate": 3.470508703735001e-05,
      "loss": 0.9761,
      "step": 7740
    },
    {
      "epoch": 1.2826878517047335,
      "grad_norm": 9.110688209533691,
      "learning_rate": 3.468396146695961e-05,
      "loss": 0.6823,
      "step": 7750
    },
    {
      "epoch": 1.2843429328037073,
      "grad_norm": 9.145857810974121,
      "learning_rate": 3.466283589656921e-05,
      "loss": 1.009,
      "step": 7760
    },
    {
      "epoch": 1.2859980139026812,
      "grad_norm": 3.2220211029052734,
      "learning_rate": 3.464171032617881e-05,
      "loss": 0.614,
      "step": 7770
    },
    {
      "epoch": 1.2876530950016551,
      "grad_norm": 4.339253902435303,
      "learning_rate": 3.462058475578841e-05,
      "loss": 0.6776,
      "step": 7780
    },
    {
      "epoch": 1.2893081761006289,
      "grad_norm": 4.868624210357666,
      "learning_rate": 3.459945918539801e-05,
      "loss": 0.7009,
      "step": 7790
    },
    {
      "epoch": 1.2909632571996028,
      "grad_norm": 4.863439559936523,
      "learning_rate": 3.457833361500761e-05,
      "loss": 0.5457,
      "step": 7800
    },
    {
      "epoch": 1.2926183382985768,
      "grad_norm": 7.449807643890381,
      "learning_rate": 3.455720804461721e-05,
      "loss": 0.5762,
      "step": 7810
    },
    {
      "epoch": 1.2942734193975505,
      "grad_norm": 7.996222019195557,
      "learning_rate": 3.4536082474226805e-05,
      "loss": 0.57,
      "step": 7820
    },
    {
      "epoch": 1.2959285004965242,
      "grad_norm": 6.643186569213867,
      "learning_rate": 3.4514956903836403e-05,
      "loss": 0.7628,
      "step": 7830
    },
    {
      "epoch": 1.2975835815954981,
      "grad_norm": 5.923222064971924,
      "learning_rate": 3.4493831333446e-05,
      "loss": 0.903,
      "step": 7840
    },
    {
      "epoch": 1.299238662694472,
      "grad_norm": 5.2382426261901855,
      "learning_rate": 3.44727057630556e-05,
      "loss": 0.6136,
      "step": 7850
    },
    {
      "epoch": 1.3008937437934458,
      "grad_norm": 11.171913146972656,
      "learning_rate": 3.4451580192665205e-05,
      "loss": 0.7373,
      "step": 7860
    },
    {
      "epoch": 1.3025488248924197,
      "grad_norm": 6.880643844604492,
      "learning_rate": 3.4430454622274804e-05,
      "loss": 0.7795,
      "step": 7870
    },
    {
      "epoch": 1.3042039059913937,
      "grad_norm": 7.071228504180908,
      "learning_rate": 3.44093290518844e-05,
      "loss": 0.5885,
      "step": 7880
    },
    {
      "epoch": 1.3058589870903674,
      "grad_norm": 8.743435859680176,
      "learning_rate": 3.4388203481494e-05,
      "loss": 0.8623,
      "step": 7890
    },
    {
      "epoch": 1.3075140681893413,
      "grad_norm": 9.068041801452637,
      "learning_rate": 3.43670779111036e-05,
      "loss": 0.577,
      "step": 7900
    },
    {
      "epoch": 1.309169149288315,
      "grad_norm": 3.258715867996216,
      "learning_rate": 3.43459523407132e-05,
      "loss": 0.5601,
      "step": 7910
    },
    {
      "epoch": 1.310824230387289,
      "grad_norm": 9.578812599182129,
      "learning_rate": 3.4324826770322796e-05,
      "loss": 0.9134,
      "step": 7920
    },
    {
      "epoch": 1.3124793114862627,
      "grad_norm": 3.5132293701171875,
      "learning_rate": 3.43037011999324e-05,
      "loss": 0.7028,
      "step": 7930
    },
    {
      "epoch": 1.3141343925852367,
      "grad_norm": 7.409117221832275,
      "learning_rate": 3.4282575629542e-05,
      "loss": 0.7711,
      "step": 7940
    },
    {
      "epoch": 1.3157894736842106,
      "grad_norm": 4.13960075378418,
      "learning_rate": 3.42614500591516e-05,
      "loss": 0.7439,
      "step": 7950
    },
    {
      "epoch": 1.3174445547831843,
      "grad_norm": 8.696565628051758,
      "learning_rate": 3.42403244887612e-05,
      "loss": 0.6265,
      "step": 7960
    },
    {
      "epoch": 1.3190996358821583,
      "grad_norm": 6.771448612213135,
      "learning_rate": 3.42191989183708e-05,
      "loss": 0.7314,
      "step": 7970
    },
    {
      "epoch": 1.320754716981132,
      "grad_norm": 8.623295783996582,
      "learning_rate": 3.41980733479804e-05,
      "loss": 0.7624,
      "step": 7980
    },
    {
      "epoch": 1.322409798080106,
      "grad_norm": 7.38088846206665,
      "learning_rate": 3.417694777759e-05,
      "loss": 0.6535,
      "step": 7990
    },
    {
      "epoch": 1.3240648791790797,
      "grad_norm": 8.992539405822754,
      "learning_rate": 3.4155822207199596e-05,
      "loss": 0.9065,
      "step": 8000
    },
    {
      "epoch": 1.3257199602780536,
      "grad_norm": 9.134363174438477,
      "learning_rate": 3.4134696636809195e-05,
      "loss": 0.9034,
      "step": 8010
    },
    {
      "epoch": 1.3273750413770276,
      "grad_norm": 1.1238981485366821,
      "learning_rate": 3.411357106641879e-05,
      "loss": 0.8046,
      "step": 8020
    },
    {
      "epoch": 1.3290301224760013,
      "grad_norm": 10.448195457458496,
      "learning_rate": 3.40924454960284e-05,
      "loss": 0.7542,
      "step": 8030
    },
    {
      "epoch": 1.3306852035749752,
      "grad_norm": 3.9831955432891846,
      "learning_rate": 3.4071319925638e-05,
      "loss": 0.4939,
      "step": 8040
    },
    {
      "epoch": 1.3323402846739492,
      "grad_norm": 2.7046799659729004,
      "learning_rate": 3.4050194355247595e-05,
      "loss": 0.7428,
      "step": 8050
    },
    {
      "epoch": 1.3339953657729229,
      "grad_norm": 5.377749919891357,
      "learning_rate": 3.402906878485719e-05,
      "loss": 0.4885,
      "step": 8060
    },
    {
      "epoch": 1.3356504468718966,
      "grad_norm": 4.824620723724365,
      "learning_rate": 3.400794321446679e-05,
      "loss": 0.623,
      "step": 8070
    },
    {
      "epoch": 1.3373055279708705,
      "grad_norm": 12.578839302062988,
      "learning_rate": 3.398681764407639e-05,
      "loss": 0.8313,
      "step": 8080
    },
    {
      "epoch": 1.3389606090698445,
      "grad_norm": 12.096033096313477,
      "learning_rate": 3.396569207368599e-05,
      "loss": 0.7571,
      "step": 8090
    },
    {
      "epoch": 1.3406156901688182,
      "grad_norm": 7.6149373054504395,
      "learning_rate": 3.394456650329559e-05,
      "loss": 0.654,
      "step": 8100
    },
    {
      "epoch": 1.3422707712677922,
      "grad_norm": 2.936904191970825,
      "learning_rate": 3.392344093290519e-05,
      "loss": 0.6815,
      "step": 8110
    },
    {
      "epoch": 1.343925852366766,
      "grad_norm": 4.763810634613037,
      "learning_rate": 3.390231536251479e-05,
      "loss": 0.5479,
      "step": 8120
    },
    {
      "epoch": 1.3455809334657398,
      "grad_norm": 4.923865795135498,
      "learning_rate": 3.388118979212439e-05,
      "loss": 0.6902,
      "step": 8130
    },
    {
      "epoch": 1.3472360145647135,
      "grad_norm": 8.708702087402344,
      "learning_rate": 3.386006422173399e-05,
      "loss": 0.6929,
      "step": 8140
    },
    {
      "epoch": 1.3488910956636875,
      "grad_norm": 11.107270240783691,
      "learning_rate": 3.3838938651343586e-05,
      "loss": 0.5298,
      "step": 8150
    },
    {
      "epoch": 1.3505461767626614,
      "grad_norm": 7.445730686187744,
      "learning_rate": 3.3817813080953184e-05,
      "loss": 0.559,
      "step": 8160
    },
    {
      "epoch": 1.3522012578616351,
      "grad_norm": 9.255708694458008,
      "learning_rate": 3.379668751056278e-05,
      "loss": 1.0339,
      "step": 8170
    },
    {
      "epoch": 1.353856338960609,
      "grad_norm": 3.9299139976501465,
      "learning_rate": 3.377556194017239e-05,
      "loss": 0.7434,
      "step": 8180
    },
    {
      "epoch": 1.355511420059583,
      "grad_norm": 10.15609073638916,
      "learning_rate": 3.3754436369781986e-05,
      "loss": 0.6892,
      "step": 8190
    },
    {
      "epoch": 1.3571665011585567,
      "grad_norm": 7.29582405090332,
      "learning_rate": 3.3733310799391584e-05,
      "loss": 0.7144,
      "step": 8200
    },
    {
      "epoch": 1.3588215822575307,
      "grad_norm": 11.464619636535645,
      "learning_rate": 3.371218522900119e-05,
      "loss": 0.6704,
      "step": 8210
    },
    {
      "epoch": 1.3604766633565044,
      "grad_norm": 7.276268005371094,
      "learning_rate": 3.369105965861079e-05,
      "loss": 0.8337,
      "step": 8220
    },
    {
      "epoch": 1.3621317444554784,
      "grad_norm": 6.070382118225098,
      "learning_rate": 3.3669934088220386e-05,
      "loss": 0.7527,
      "step": 8230
    },
    {
      "epoch": 1.363786825554452,
      "grad_norm": 9.705778121948242,
      "learning_rate": 3.3648808517829985e-05,
      "loss": 0.7929,
      "step": 8240
    },
    {
      "epoch": 1.365441906653426,
      "grad_norm": 15.465862274169922,
      "learning_rate": 3.362768294743958e-05,
      "loss": 0.5911,
      "step": 8250
    },
    {
      "epoch": 1.3670969877524,
      "grad_norm": 11.270302772521973,
      "learning_rate": 3.360655737704918e-05,
      "loss": 0.5383,
      "step": 8260
    },
    {
      "epoch": 1.3687520688513737,
      "grad_norm": 12.403617858886719,
      "learning_rate": 3.358543180665878e-05,
      "loss": 0.6322,
      "step": 8270
    },
    {
      "epoch": 1.3704071499503476,
      "grad_norm": 4.541569232940674,
      "learning_rate": 3.3564306236268385e-05,
      "loss": 0.9737,
      "step": 8280
    },
    {
      "epoch": 1.3720622310493213,
      "grad_norm": 10.168715476989746,
      "learning_rate": 3.354318066587798e-05,
      "loss": 0.6595,
      "step": 8290
    },
    {
      "epoch": 1.3737173121482953,
      "grad_norm": 4.585226535797119,
      "learning_rate": 3.352205509548758e-05,
      "loss": 0.7951,
      "step": 8300
    },
    {
      "epoch": 1.375372393247269,
      "grad_norm": 5.445230484008789,
      "learning_rate": 3.350092952509718e-05,
      "loss": 0.6488,
      "step": 8310
    },
    {
      "epoch": 1.377027474346243,
      "grad_norm": 10.251328468322754,
      "learning_rate": 3.347980395470678e-05,
      "loss": 0.8905,
      "step": 8320
    },
    {
      "epoch": 1.378682555445217,
      "grad_norm": 14.049677848815918,
      "learning_rate": 3.345867838431638e-05,
      "loss": 0.6802,
      "step": 8330
    },
    {
      "epoch": 1.3803376365441906,
      "grad_norm": 2.1121113300323486,
      "learning_rate": 3.3437552813925975e-05,
      "loss": 0.5865,
      "step": 8340
    },
    {
      "epoch": 1.3819927176431646,
      "grad_norm": 5.866801738739014,
      "learning_rate": 3.3416427243535574e-05,
      "loss": 0.6492,
      "step": 8350
    },
    {
      "epoch": 1.3836477987421385,
      "grad_norm": 11.19753646850586,
      "learning_rate": 3.339530167314517e-05,
      "loss": 0.7341,
      "step": 8360
    },
    {
      "epoch": 1.3853028798411122,
      "grad_norm": 6.973863124847412,
      "learning_rate": 3.337417610275478e-05,
      "loss": 0.9516,
      "step": 8370
    },
    {
      "epoch": 1.386957960940086,
      "grad_norm": 10.838778495788574,
      "learning_rate": 3.3353050532364376e-05,
      "loss": 0.753,
      "step": 8380
    },
    {
      "epoch": 1.3886130420390599,
      "grad_norm": 7.206505298614502,
      "learning_rate": 3.3331924961973974e-05,
      "loss": 0.7378,
      "step": 8390
    },
    {
      "epoch": 1.3902681231380338,
      "grad_norm": 1.5579450130462646,
      "learning_rate": 3.331079939158357e-05,
      "loss": 0.6214,
      "step": 8400
    },
    {
      "epoch": 1.3919232042370075,
      "grad_norm": 4.728989601135254,
      "learning_rate": 3.328967382119317e-05,
      "loss": 0.8094,
      "step": 8410
    },
    {
      "epoch": 1.3935782853359815,
      "grad_norm": 3.0324511528015137,
      "learning_rate": 3.326854825080277e-05,
      "loss": 0.6956,
      "step": 8420
    },
    {
      "epoch": 1.3952333664349554,
      "grad_norm": 4.911746501922607,
      "learning_rate": 3.3247422680412374e-05,
      "loss": 0.7483,
      "step": 8430
    },
    {
      "epoch": 1.3968884475339292,
      "grad_norm": 8.132606506347656,
      "learning_rate": 3.322629711002197e-05,
      "loss": 0.9202,
      "step": 8440
    },
    {
      "epoch": 1.3985435286329029,
      "grad_norm": 6.673929214477539,
      "learning_rate": 3.320517153963157e-05,
      "loss": 0.7503,
      "step": 8450
    },
    {
      "epoch": 1.4001986097318768,
      "grad_norm": 8.220962524414062,
      "learning_rate": 3.3184045969241176e-05,
      "loss": 0.982,
      "step": 8460
    },
    {
      "epoch": 1.4018536908308508,
      "grad_norm": 7.146113872528076,
      "learning_rate": 3.3162920398850775e-05,
      "loss": 0.683,
      "step": 8470
    },
    {
      "epoch": 1.4035087719298245,
      "grad_norm": 1.4004188776016235,
      "learning_rate": 3.314179482846037e-05,
      "loss": 0.6845,
      "step": 8480
    },
    {
      "epoch": 1.4051638530287984,
      "grad_norm": 7.756066799163818,
      "learning_rate": 3.312066925806997e-05,
      "loss": 0.7475,
      "step": 8490
    },
    {
      "epoch": 1.4068189341277724,
      "grad_norm": 3.549748659133911,
      "learning_rate": 3.309954368767957e-05,
      "loss": 0.7445,
      "step": 8500
    },
    {
      "epoch": 1.408474015226746,
      "grad_norm": 18.235383987426758,
      "learning_rate": 3.307841811728917e-05,
      "loss": 0.3978,
      "step": 8510
    },
    {
      "epoch": 1.41012909632572,
      "grad_norm": 9.851861000061035,
      "learning_rate": 3.3057292546898766e-05,
      "loss": 0.6751,
      "step": 8520
    },
    {
      "epoch": 1.4117841774246938,
      "grad_norm": 3.5988290309906006,
      "learning_rate": 3.3036166976508365e-05,
      "loss": 0.6763,
      "step": 8530
    },
    {
      "epoch": 1.4134392585236677,
      "grad_norm": 8.003803253173828,
      "learning_rate": 3.301504140611797e-05,
      "loss": 0.4507,
      "step": 8540
    },
    {
      "epoch": 1.4150943396226414,
      "grad_norm": 4.885424613952637,
      "learning_rate": 3.299391583572757e-05,
      "loss": 0.5641,
      "step": 8550
    },
    {
      "epoch": 1.4167494207216154,
      "grad_norm": 5.5367021560668945,
      "learning_rate": 3.297279026533717e-05,
      "loss": 0.6532,
      "step": 8560
    },
    {
      "epoch": 1.4184045018205893,
      "grad_norm": 10.978961944580078,
      "learning_rate": 3.2951664694946765e-05,
      "loss": 0.7133,
      "step": 8570
    },
    {
      "epoch": 1.420059582919563,
      "grad_norm": 8.509760856628418,
      "learning_rate": 3.2930539124556364e-05,
      "loss": 0.7281,
      "step": 8580
    },
    {
      "epoch": 1.421714664018537,
      "grad_norm": 6.757058143615723,
      "learning_rate": 3.290941355416596e-05,
      "loss": 0.9651,
      "step": 8590
    },
    {
      "epoch": 1.4233697451175107,
      "grad_norm": 7.2605977058410645,
      "learning_rate": 3.288828798377556e-05,
      "loss": 0.7355,
      "step": 8600
    },
    {
      "epoch": 1.4250248262164846,
      "grad_norm": 7.227848052978516,
      "learning_rate": 3.286716241338516e-05,
      "loss": 0.8116,
      "step": 8610
    },
    {
      "epoch": 1.4266799073154584,
      "grad_norm": 9.899287223815918,
      "learning_rate": 3.2846036842994764e-05,
      "loss": 0.7734,
      "step": 8620
    },
    {
      "epoch": 1.4283349884144323,
      "grad_norm": 2.3421802520751953,
      "learning_rate": 3.282491127260436e-05,
      "loss": 0.6687,
      "step": 8630
    },
    {
      "epoch": 1.4299900695134062,
      "grad_norm": 7.323288440704346,
      "learning_rate": 3.280378570221396e-05,
      "loss": 0.8188,
      "step": 8640
    },
    {
      "epoch": 1.43164515061238,
      "grad_norm": 11.453943252563477,
      "learning_rate": 3.278266013182356e-05,
      "loss": 0.5248,
      "step": 8650
    },
    {
      "epoch": 1.433300231711354,
      "grad_norm": 2.5173587799072266,
      "learning_rate": 3.276153456143316e-05,
      "loss": 0.9087,
      "step": 8660
    },
    {
      "epoch": 1.4349553128103278,
      "grad_norm": 6.204174518585205,
      "learning_rate": 3.274040899104276e-05,
      "loss": 0.6051,
      "step": 8670
    },
    {
      "epoch": 1.4366103939093016,
      "grad_norm": 6.306761264801025,
      "learning_rate": 3.271928342065236e-05,
      "loss": 0.8298,
      "step": 8680
    },
    {
      "epoch": 1.4382654750082753,
      "grad_norm": 1.920698881149292,
      "learning_rate": 3.269815785026196e-05,
      "loss": 0.4473,
      "step": 8690
    },
    {
      "epoch": 1.4399205561072492,
      "grad_norm": 15.927009582519531,
      "learning_rate": 3.267703227987156e-05,
      "loss": 0.5123,
      "step": 8700
    },
    {
      "epoch": 1.4415756372062232,
      "grad_norm": 3.754730463027954,
      "learning_rate": 3.265590670948116e-05,
      "loss": 0.7872,
      "step": 8710
    },
    {
      "epoch": 1.443230718305197,
      "grad_norm": 11.307397842407227,
      "learning_rate": 3.263478113909076e-05,
      "loss": 0.6433,
      "step": 8720
    },
    {
      "epoch": 1.4448857994041708,
      "grad_norm": 9.302504539489746,
      "learning_rate": 3.261365556870036e-05,
      "loss": 0.7069,
      "step": 8730
    },
    {
      "epoch": 1.4465408805031448,
      "grad_norm": 14.311107635498047,
      "learning_rate": 3.259252999830996e-05,
      "loss": 0.9768,
      "step": 8740
    },
    {
      "epoch": 1.4481959616021185,
      "grad_norm": 4.593368053436279,
      "learning_rate": 3.2571404427919556e-05,
      "loss": 0.879,
      "step": 8750
    },
    {
      "epoch": 1.4498510427010922,
      "grad_norm": 4.305541515350342,
      "learning_rate": 3.2550278857529155e-05,
      "loss": 0.9398,
      "step": 8760
    },
    {
      "epoch": 1.4515061238000662,
      "grad_norm": 4.297641754150391,
      "learning_rate": 3.252915328713875e-05,
      "loss": 0.5903,
      "step": 8770
    },
    {
      "epoch": 1.45316120489904,
      "grad_norm": 12.470890998840332,
      "learning_rate": 3.250802771674835e-05,
      "loss": 0.8105,
      "step": 8780
    },
    {
      "epoch": 1.4548162859980138,
      "grad_norm": 7.720037937164307,
      "learning_rate": 3.248690214635796e-05,
      "loss": 0.8776,
      "step": 8790
    },
    {
      "epoch": 1.4564713670969878,
      "grad_norm": 9.6542329788208,
      "learning_rate": 3.2465776575967555e-05,
      "loss": 0.9836,
      "step": 8800
    },
    {
      "epoch": 1.4581264481959617,
      "grad_norm": 4.910064220428467,
      "learning_rate": 3.2444651005577153e-05,
      "loss": 0.7797,
      "step": 8810
    },
    {
      "epoch": 1.4597815292949354,
      "grad_norm": 5.716066360473633,
      "learning_rate": 3.242352543518675e-05,
      "loss": 0.7438,
      "step": 8820
    },
    {
      "epoch": 1.4614366103939094,
      "grad_norm": 6.514086723327637,
      "learning_rate": 3.240239986479635e-05,
      "loss": 0.447,
      "step": 8830
    },
    {
      "epoch": 1.463091691492883,
      "grad_norm": 6.78853702545166,
      "learning_rate": 3.238127429440595e-05,
      "loss": 0.6912,
      "step": 8840
    },
    {
      "epoch": 1.464746772591857,
      "grad_norm": 10.129416465759277,
      "learning_rate": 3.236014872401555e-05,
      "loss": 0.7126,
      "step": 8850
    },
    {
      "epoch": 1.4664018536908308,
      "grad_norm": 4.35906982421875,
      "learning_rate": 3.2339023153625145e-05,
      "loss": 0.8034,
      "step": 8860
    },
    {
      "epoch": 1.4680569347898047,
      "grad_norm": 5.9490156173706055,
      "learning_rate": 3.2317897583234744e-05,
      "loss": 0.6337,
      "step": 8870
    },
    {
      "epoch": 1.4697120158887786,
      "grad_norm": 9.260380744934082,
      "learning_rate": 3.229677201284435e-05,
      "loss": 0.8101,
      "step": 8880
    },
    {
      "epoch": 1.4713670969877524,
      "grad_norm": 8.19864273071289,
      "learning_rate": 3.227564644245395e-05,
      "loss": 0.864,
      "step": 8890
    },
    {
      "epoch": 1.4730221780867263,
      "grad_norm": 2.613029956817627,
      "learning_rate": 3.2254520872063546e-05,
      "loss": 0.8621,
      "step": 8900
    },
    {
      "epoch": 1.4746772591857,
      "grad_norm": 8.01870346069336,
      "learning_rate": 3.2233395301673144e-05,
      "loss": 0.8701,
      "step": 8910
    },
    {
      "epoch": 1.476332340284674,
      "grad_norm": 10.721574783325195,
      "learning_rate": 3.221226973128275e-05,
      "loss": 0.754,
      "step": 8920
    },
    {
      "epoch": 1.4779874213836477,
      "grad_norm": 5.454575538635254,
      "learning_rate": 3.219114416089235e-05,
      "loss": 0.5694,
      "step": 8930
    },
    {
      "epoch": 1.4796425024826216,
      "grad_norm": 7.36234188079834,
      "learning_rate": 3.2170018590501946e-05,
      "loss": 0.4749,
      "step": 8940
    },
    {
      "epoch": 1.4812975835815956,
      "grad_norm": 1.9700318574905396,
      "learning_rate": 3.2148893020111544e-05,
      "loss": 0.6338,
      "step": 8950
    },
    {
      "epoch": 1.4829526646805693,
      "grad_norm": 2.5864171981811523,
      "learning_rate": 3.212776744972115e-05,
      "loss": 0.5675,
      "step": 8960
    },
    {
      "epoch": 1.4846077457795432,
      "grad_norm": 8.842609405517578,
      "learning_rate": 3.210664187933075e-05,
      "loss": 0.7437,
      "step": 8970
    },
    {
      "epoch": 1.4862628268785172,
      "grad_norm": 1.9071290493011475,
      "learning_rate": 3.2085516308940346e-05,
      "loss": 0.7452,
      "step": 8980
    },
    {
      "epoch": 1.487917907977491,
      "grad_norm": 7.120420455932617,
      "learning_rate": 3.2064390738549945e-05,
      "loss": 0.8067,
      "step": 8990
    },
    {
      "epoch": 1.4895729890764646,
      "grad_norm": 10.867350578308105,
      "learning_rate": 3.204326516815954e-05,
      "loss": 0.7316,
      "step": 9000
    },
    {
      "epoch": 1.4912280701754386,
      "grad_norm": 8.026422500610352,
      "learning_rate": 3.202213959776914e-05,
      "loss": 0.618,
      "step": 9010
    },
    {
      "epoch": 1.4928831512744125,
      "grad_norm": 7.291450500488281,
      "learning_rate": 3.200101402737874e-05,
      "loss": 0.8085,
      "step": 9020
    },
    {
      "epoch": 1.4945382323733862,
      "grad_norm": 6.884203910827637,
      "learning_rate": 3.197988845698834e-05,
      "loss": 0.7561,
      "step": 9030
    },
    {
      "epoch": 1.4961933134723602,
      "grad_norm": 7.941955089569092,
      "learning_rate": 3.1958762886597937e-05,
      "loss": 0.8584,
      "step": 9040
    },
    {
      "epoch": 1.4978483945713341,
      "grad_norm": 7.501947402954102,
      "learning_rate": 3.193763731620754e-05,
      "loss": 0.6451,
      "step": 9050
    },
    {
      "epoch": 1.4995034756703078,
      "grad_norm": 7.740591526031494,
      "learning_rate": 3.191651174581714e-05,
      "loss": 0.7219,
      "step": 9060
    },
    {
      "epoch": 1.5011585567692816,
      "grad_norm": 6.809789180755615,
      "learning_rate": 3.189538617542674e-05,
      "loss": 0.49,
      "step": 9070
    },
    {
      "epoch": 1.5028136378682555,
      "grad_norm": 5.050835609436035,
      "learning_rate": 3.187426060503634e-05,
      "loss": 0.594,
      "step": 9080
    },
    {
      "epoch": 1.5044687189672294,
      "grad_norm": 2.7834410667419434,
      "learning_rate": 3.1853135034645935e-05,
      "loss": 0.7009,
      "step": 9090
    },
    {
      "epoch": 1.5061238000662032,
      "grad_norm": 2.291382074356079,
      "learning_rate": 3.1832009464255534e-05,
      "loss": 0.8801,
      "step": 9100
    },
    {
      "epoch": 1.507778881165177,
      "grad_norm": 6.343038558959961,
      "learning_rate": 3.181088389386513e-05,
      "loss": 0.711,
      "step": 9110
    },
    {
      "epoch": 1.509433962264151,
      "grad_norm": 0.8495331406593323,
      "learning_rate": 3.178975832347473e-05,
      "loss": 0.5127,
      "step": 9120
    },
    {
      "epoch": 1.5110890433631248,
      "grad_norm": 10.79688835144043,
      "learning_rate": 3.1768632753084336e-05,
      "loss": 0.6089,
      "step": 9130
    },
    {
      "epoch": 1.5127441244620985,
      "grad_norm": 12.080694198608398,
      "learning_rate": 3.1747507182693934e-05,
      "loss": 0.7613,
      "step": 9140
    },
    {
      "epoch": 1.5143992055610727,
      "grad_norm": 6.024794578552246,
      "learning_rate": 3.172638161230353e-05,
      "loss": 0.7234,
      "step": 9150
    },
    {
      "epoch": 1.5160542866600464,
      "grad_norm": 7.405523777008057,
      "learning_rate": 3.170525604191313e-05,
      "loss": 0.6078,
      "step": 9160
    },
    {
      "epoch": 1.51770936775902,
      "grad_norm": 12.264163970947266,
      "learning_rate": 3.1684130471522736e-05,
      "loss": 0.5984,
      "step": 9170
    },
    {
      "epoch": 1.519364448857994,
      "grad_norm": 3.736060380935669,
      "learning_rate": 3.1663004901132334e-05,
      "loss": 0.3852,
      "step": 9180
    },
    {
      "epoch": 1.521019529956968,
      "grad_norm": 6.72822380065918,
      "learning_rate": 3.164187933074193e-05,
      "loss": 0.883,
      "step": 9190
    },
    {
      "epoch": 1.5226746110559417,
      "grad_norm": 7.77552604675293,
      "learning_rate": 3.162075376035153e-05,
      "loss": 0.8055,
      "step": 9200
    },
    {
      "epoch": 1.5243296921549154,
      "grad_norm": 10.337677001953125,
      "learning_rate": 3.159962818996113e-05,
      "loss": 0.6877,
      "step": 9210
    },
    {
      "epoch": 1.5259847732538896,
      "grad_norm": 12.734128952026367,
      "learning_rate": 3.1578502619570735e-05,
      "loss": 0.6629,
      "step": 9220
    },
    {
      "epoch": 1.5276398543528633,
      "grad_norm": 6.0270466804504395,
      "learning_rate": 3.155737704918033e-05,
      "loss": 0.5955,
      "step": 9230
    },
    {
      "epoch": 1.529294935451837,
      "grad_norm": 6.096181392669678,
      "learning_rate": 3.153625147878993e-05,
      "loss": 0.7124,
      "step": 9240
    },
    {
      "epoch": 1.530950016550811,
      "grad_norm": 0.8235666751861572,
      "learning_rate": 3.151512590839953e-05,
      "loss": 0.8935,
      "step": 9250
    },
    {
      "epoch": 1.532605097649785,
      "grad_norm": 2.73970103263855,
      "learning_rate": 3.149400033800913e-05,
      "loss": 0.6333,
      "step": 9260
    },
    {
      "epoch": 1.5342601787487586,
      "grad_norm": 6.722254276275635,
      "learning_rate": 3.1472874767618727e-05,
      "loss": 0.8239,
      "step": 9270
    },
    {
      "epoch": 1.5359152598477326,
      "grad_norm": 9.30932331085205,
      "learning_rate": 3.1451749197228325e-05,
      "loss": 0.8248,
      "step": 9280
    },
    {
      "epoch": 1.5375703409467065,
      "grad_norm": 9.749568939208984,
      "learning_rate": 3.143062362683792e-05,
      "loss": 0.6414,
      "step": 9290
    },
    {
      "epoch": 1.5392254220456802,
      "grad_norm": 8.180120468139648,
      "learning_rate": 3.140949805644753e-05,
      "loss": 0.965,
      "step": 9300
    },
    {
      "epoch": 1.540880503144654,
      "grad_norm": 9.049437522888184,
      "learning_rate": 3.138837248605713e-05,
      "loss": 0.6975,
      "step": 9310
    },
    {
      "epoch": 1.542535584243628,
      "grad_norm": 8.12508773803711,
      "learning_rate": 3.1367246915666725e-05,
      "loss": 0.3964,
      "step": 9320
    },
    {
      "epoch": 1.5441906653426019,
      "grad_norm": 4.61114501953125,
      "learning_rate": 3.1346121345276324e-05,
      "loss": 0.6575,
      "step": 9330
    },
    {
      "epoch": 1.5458457464415756,
      "grad_norm": 10.077768325805664,
      "learning_rate": 3.132499577488592e-05,
      "loss": 0.4421,
      "step": 9340
    },
    {
      "epoch": 1.5475008275405495,
      "grad_norm": 4.7632832527160645,
      "learning_rate": 3.130387020449552e-05,
      "loss": 0.7692,
      "step": 9350
    },
    {
      "epoch": 1.5491559086395235,
      "grad_norm": 7.77985954284668,
      "learning_rate": 3.128274463410512e-05,
      "loss": 0.6718,
      "step": 9360
    },
    {
      "epoch": 1.5508109897384972,
      "grad_norm": 2.9344115257263184,
      "learning_rate": 3.126161906371472e-05,
      "loss": 0.4033,
      "step": 9370
    },
    {
      "epoch": 1.552466070837471,
      "grad_norm": 4.305725574493408,
      "learning_rate": 3.1240493493324316e-05,
      "loss": 0.8023,
      "step": 9380
    },
    {
      "epoch": 1.5541211519364448,
      "grad_norm": 4.766107082366943,
      "learning_rate": 3.121936792293392e-05,
      "loss": 0.6151,
      "step": 9390
    },
    {
      "epoch": 1.5557762330354188,
      "grad_norm": 9.306041717529297,
      "learning_rate": 3.119824235254352e-05,
      "loss": 0.6257,
      "step": 9400
    },
    {
      "epoch": 1.5574313141343925,
      "grad_norm": 10.810465812683105,
      "learning_rate": 3.117711678215312e-05,
      "loss": 0.5932,
      "step": 9410
    },
    {
      "epoch": 1.5590863952333665,
      "grad_norm": 8.182621002197266,
      "learning_rate": 3.115599121176272e-05,
      "loss": 0.8744,
      "step": 9420
    },
    {
      "epoch": 1.5607414763323404,
      "grad_norm": 2.9907193183898926,
      "learning_rate": 3.113486564137232e-05,
      "loss": 0.5322,
      "step": 9430
    },
    {
      "epoch": 1.5623965574313141,
      "grad_norm": 7.934456825256348,
      "learning_rate": 3.111374007098192e-05,
      "loss": 0.778,
      "step": 9440
    },
    {
      "epoch": 1.5640516385302878,
      "grad_norm": 5.309648036956787,
      "learning_rate": 3.109261450059152e-05,
      "loss": 0.6976,
      "step": 9450
    },
    {
      "epoch": 1.5657067196292618,
      "grad_norm": 7.6517014503479,
      "learning_rate": 3.1071488930201116e-05,
      "loss": 0.5514,
      "step": 9460
    },
    {
      "epoch": 1.5673618007282357,
      "grad_norm": 3.3165783882141113,
      "learning_rate": 3.105036335981072e-05,
      "loss": 0.8949,
      "step": 9470
    },
    {
      "epoch": 1.5690168818272094,
      "grad_norm": 4.233617782592773,
      "learning_rate": 3.102923778942032e-05,
      "loss": 0.7675,
      "step": 9480
    },
    {
      "epoch": 1.5706719629261834,
      "grad_norm": 6.936057090759277,
      "learning_rate": 3.100811221902992e-05,
      "loss": 0.9981,
      "step": 9490
    },
    {
      "epoch": 1.5723270440251573,
      "grad_norm": 15.3574800491333,
      "learning_rate": 3.0986986648639516e-05,
      "loss": 0.5301,
      "step": 9500
    },
    {
      "epoch": 1.573982125124131,
      "grad_norm": 2.9328434467315674,
      "learning_rate": 3.0965861078249115e-05,
      "loss": 0.6899,
      "step": 9510
    },
    {
      "epoch": 1.5756372062231048,
      "grad_norm": 11.261765480041504,
      "learning_rate": 3.094473550785871e-05,
      "loss": 0.6668,
      "step": 9520
    },
    {
      "epoch": 1.577292287322079,
      "grad_norm": 1.2260777950286865,
      "learning_rate": 3.092360993746831e-05,
      "loss": 0.8024,
      "step": 9530
    },
    {
      "epoch": 1.5789473684210527,
      "grad_norm": 7.238890647888184,
      "learning_rate": 3.090248436707791e-05,
      "loss": 0.6302,
      "step": 9540
    },
    {
      "epoch": 1.5806024495200264,
      "grad_norm": 7.61883544921875,
      "learning_rate": 3.088135879668751e-05,
      "loss": 0.8253,
      "step": 9550
    },
    {
      "epoch": 1.5822575306190003,
      "grad_norm": 2.462460517883301,
      "learning_rate": 3.0860233226297114e-05,
      "loss": 0.5426,
      "step": 9560
    },
    {
      "epoch": 1.5839126117179743,
      "grad_norm": 7.409249305725098,
      "learning_rate": 3.083910765590671e-05,
      "loss": 0.726,
      "step": 9570
    },
    {
      "epoch": 1.585567692816948,
      "grad_norm": 6.947539329528809,
      "learning_rate": 3.081798208551631e-05,
      "loss": 0.5104,
      "step": 9580
    },
    {
      "epoch": 1.587222773915922,
      "grad_norm": 10.484869003295898,
      "learning_rate": 3.079685651512591e-05,
      "loss": 0.6416,
      "step": 9590
    },
    {
      "epoch": 1.5888778550148959,
      "grad_norm": 11.86658000946045,
      "learning_rate": 3.077573094473551e-05,
      "loss": 0.6706,
      "step": 9600
    },
    {
      "epoch": 1.5905329361138696,
      "grad_norm": 5.477184295654297,
      "learning_rate": 3.0754605374345105e-05,
      "loss": 0.4897,
      "step": 9610
    },
    {
      "epoch": 1.5921880172128433,
      "grad_norm": 15.1127290725708,
      "learning_rate": 3.0733479803954704e-05,
      "loss": 0.6982,
      "step": 9620
    },
    {
      "epoch": 1.5938430983118173,
      "grad_norm": 9.494717597961426,
      "learning_rate": 3.07123542335643e-05,
      "loss": 0.8047,
      "step": 9630
    },
    {
      "epoch": 1.5954981794107912,
      "grad_norm": 6.2123918533325195,
      "learning_rate": 3.069122866317391e-05,
      "loss": 0.6251,
      "step": 9640
    },
    {
      "epoch": 1.597153260509765,
      "grad_norm": 9.660652160644531,
      "learning_rate": 3.0670103092783506e-05,
      "loss": 0.4401,
      "step": 9650
    },
    {
      "epoch": 1.5988083416087389,
      "grad_norm": 5.053604602813721,
      "learning_rate": 3.0648977522393104e-05,
      "loss": 0.7239,
      "step": 9660
    },
    {
      "epoch": 1.6004634227077128,
      "grad_norm": 5.540187358856201,
      "learning_rate": 3.062785195200271e-05,
      "loss": 0.6388,
      "step": 9670
    },
    {
      "epoch": 1.6021185038066865,
      "grad_norm": 6.465723991394043,
      "learning_rate": 3.060672638161231e-05,
      "loss": 0.7553,
      "step": 9680
    },
    {
      "epoch": 1.6037735849056602,
      "grad_norm": 7.752254486083984,
      "learning_rate": 3.0585600811221906e-05,
      "loss": 1.0778,
      "step": 9690
    },
    {
      "epoch": 1.6054286660046342,
      "grad_norm": 3.387632369995117,
      "learning_rate": 3.0564475240831504e-05,
      "loss": 0.5099,
      "step": 9700
    },
    {
      "epoch": 1.6070837471036081,
      "grad_norm": 6.56606912612915,
      "learning_rate": 3.05433496704411e-05,
      "loss": 0.7547,
      "step": 9710
    },
    {
      "epoch": 1.6087388282025818,
      "grad_norm": 8.981496810913086,
      "learning_rate": 3.05222241000507e-05,
      "loss": 0.8972,
      "step": 9720
    },
    {
      "epoch": 1.6103939093015558,
      "grad_norm": 7.170958995819092,
      "learning_rate": 3.0501098529660306e-05,
      "loss": 0.7515,
      "step": 9730
    },
    {
      "epoch": 1.6120489904005297,
      "grad_norm": 6.075172424316406,
      "learning_rate": 3.0479972959269905e-05,
      "loss": 0.4415,
      "step": 9740
    },
    {
      "epoch": 1.6137040714995035,
      "grad_norm": 8.406197547912598,
      "learning_rate": 3.0458847388879503e-05,
      "loss": 1.1495,
      "step": 9750
    },
    {
      "epoch": 1.6153591525984772,
      "grad_norm": 8.271085739135742,
      "learning_rate": 3.04377218184891e-05,
      "loss": 0.9551,
      "step": 9760
    },
    {
      "epoch": 1.6170142336974511,
      "grad_norm": 13.846830368041992,
      "learning_rate": 3.04165962480987e-05,
      "loss": 0.4994,
      "step": 9770
    },
    {
      "epoch": 1.618669314796425,
      "grad_norm": 3.861544370651245,
      "learning_rate": 3.0395470677708298e-05,
      "loss": 0.8627,
      "step": 9780
    },
    {
      "epoch": 1.6203243958953988,
      "grad_norm": 6.19158411026001,
      "learning_rate": 3.0374345107317897e-05,
      "loss": 0.7339,
      "step": 9790
    },
    {
      "epoch": 1.6219794769943727,
      "grad_norm": 9.577089309692383,
      "learning_rate": 3.0353219536927495e-05,
      "loss": 0.582,
      "step": 9800
    },
    {
      "epoch": 1.6236345580933467,
      "grad_norm": 10.113604545593262,
      "learning_rate": 3.03320939665371e-05,
      "loss": 0.8538,
      "step": 9810
    },
    {
      "epoch": 1.6252896391923204,
      "grad_norm": 3.098356246948242,
      "learning_rate": 3.03109683961467e-05,
      "loss": 0.6652,
      "step": 9820
    },
    {
      "epoch": 1.626944720291294,
      "grad_norm": 7.157524585723877,
      "learning_rate": 3.0289842825756297e-05,
      "loss": 0.5616,
      "step": 9830
    },
    {
      "epoch": 1.6285998013902683,
      "grad_norm": 6.366507530212402,
      "learning_rate": 3.02687172553659e-05,
      "loss": 0.8181,
      "step": 9840
    },
    {
      "epoch": 1.630254882489242,
      "grad_norm": 12.922650337219238,
      "learning_rate": 3.0247591684975497e-05,
      "loss": 0.7802,
      "step": 9850
    },
    {
      "epoch": 1.6319099635882157,
      "grad_norm": 16.5743408203125,
      "learning_rate": 3.0226466114585096e-05,
      "loss": 1.0233,
      "step": 9860
    },
    {
      "epoch": 1.6335650446871897,
      "grad_norm": 7.718776226043701,
      "learning_rate": 3.0205340544194694e-05,
      "loss": 0.7096,
      "step": 9870
    },
    {
      "epoch": 1.6352201257861636,
      "grad_norm": 2.238013982772827,
      "learning_rate": 3.0184214973804292e-05,
      "loss": 0.5655,
      "step": 9880
    },
    {
      "epoch": 1.6368752068851373,
      "grad_norm": 2.145017147064209,
      "learning_rate": 3.016308940341389e-05,
      "loss": 0.644,
      "step": 9890
    },
    {
      "epoch": 1.6385302879841113,
      "grad_norm": 6.705390930175781,
      "learning_rate": 3.0141963833023496e-05,
      "loss": 0.9079,
      "step": 9900
    },
    {
      "epoch": 1.6401853690830852,
      "grad_norm": 6.203497409820557,
      "learning_rate": 3.0120838262633094e-05,
      "loss": 0.5354,
      "step": 9910
    },
    {
      "epoch": 1.641840450182059,
      "grad_norm": 8.337357521057129,
      "learning_rate": 3.0099712692242693e-05,
      "loss": 0.5797,
      "step": 9920
    },
    {
      "epoch": 1.6434955312810327,
      "grad_norm": 3.86504864692688,
      "learning_rate": 3.007858712185229e-05,
      "loss": 0.77,
      "step": 9930
    },
    {
      "epoch": 1.6451506123800066,
      "grad_norm": 5.899746894836426,
      "learning_rate": 3.005746155146189e-05,
      "loss": 0.8102,
      "step": 9940
    },
    {
      "epoch": 1.6468056934789805,
      "grad_norm": 2.7032933235168457,
      "learning_rate": 3.003633598107149e-05,
      "loss": 0.5984,
      "step": 9950
    },
    {
      "epoch": 1.6484607745779543,
      "grad_norm": 8.290793418884277,
      "learning_rate": 3.001521041068109e-05,
      "loss": 0.7087,
      "step": 9960
    },
    {
      "epoch": 1.6501158556769282,
      "grad_norm": 2.233654260635376,
      "learning_rate": 2.9994084840290688e-05,
      "loss": 0.5565,
      "step": 9970
    },
    {
      "epoch": 1.6517709367759021,
      "grad_norm": 5.616755962371826,
      "learning_rate": 2.9972959269900293e-05,
      "loss": 0.6678,
      "step": 9980
    },
    {
      "epoch": 1.6534260178748759,
      "grad_norm": 12.885627746582031,
      "learning_rate": 2.995183369950989e-05,
      "loss": 0.8661,
      "step": 9990
    },
    {
      "epoch": 1.6550810989738496,
      "grad_norm": 9.563692092895508,
      "learning_rate": 2.993070812911949e-05,
      "loss": 0.9002,
      "step": 10000
    },
    {
      "epoch": 1.6567361800728235,
      "grad_norm": 9.282737731933594,
      "learning_rate": 2.9909582558729088e-05,
      "loss": 0.6887,
      "step": 10010
    },
    {
      "epoch": 1.6583912611717975,
      "grad_norm": 6.997591018676758,
      "learning_rate": 2.9888456988338687e-05,
      "loss": 0.4435,
      "step": 10020
    },
    {
      "epoch": 1.6600463422707712,
      "grad_norm": 4.3927178382873535,
      "learning_rate": 2.9867331417948285e-05,
      "loss": 0.7941,
      "step": 10030
    },
    {
      "epoch": 1.6617014233697451,
      "grad_norm": 2.315178871154785,
      "learning_rate": 2.9846205847557883e-05,
      "loss": 0.6008,
      "step": 10040
    },
    {
      "epoch": 1.663356504468719,
      "grad_norm": 7.453789234161377,
      "learning_rate": 2.9825080277167482e-05,
      "loss": 0.6472,
      "step": 10050
    },
    {
      "epoch": 1.6650115855676928,
      "grad_norm": 5.14705228805542,
      "learning_rate": 2.9803954706777084e-05,
      "loss": 0.838,
      "step": 10060
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 9.856420516967773,
      "learning_rate": 2.9782829136386685e-05,
      "loss": 0.8189,
      "step": 10070
    },
    {
      "epoch": 1.6683217477656405,
      "grad_norm": 14.185420989990234,
      "learning_rate": 2.9761703565996284e-05,
      "loss": 0.7048,
      "step": 10080
    },
    {
      "epoch": 1.6699768288646144,
      "grad_norm": 2.209510087966919,
      "learning_rate": 2.9740577995605885e-05,
      "loss": 0.5872,
      "step": 10090
    },
    {
      "epoch": 1.6716319099635881,
      "grad_norm": 4.505244731903076,
      "learning_rate": 2.9719452425215484e-05,
      "loss": 0.4807,
      "step": 10100
    },
    {
      "epoch": 1.673286991062562,
      "grad_norm": 9.42475700378418,
      "learning_rate": 2.9698326854825082e-05,
      "loss": 0.7362,
      "step": 10110
    },
    {
      "epoch": 1.674942072161536,
      "grad_norm": 4.169875144958496,
      "learning_rate": 2.967720128443468e-05,
      "loss": 0.8299,
      "step": 10120
    },
    {
      "epoch": 1.6765971532605097,
      "grad_norm": 7.596154689788818,
      "learning_rate": 2.965607571404428e-05,
      "loss": 0.79,
      "step": 10130
    },
    {
      "epoch": 1.6782522343594835,
      "grad_norm": 8.331493377685547,
      "learning_rate": 2.9634950143653877e-05,
      "loss": 0.706,
      "step": 10140
    },
    {
      "epoch": 1.6799073154584576,
      "grad_norm": 10.557329177856445,
      "learning_rate": 2.9613824573263483e-05,
      "loss": 0.7606,
      "step": 10150
    },
    {
      "epoch": 1.6815623965574313,
      "grad_norm": 6.832432746887207,
      "learning_rate": 2.959269900287308e-05,
      "loss": 0.6861,
      "step": 10160
    },
    {
      "epoch": 1.683217477656405,
      "grad_norm": 8.423654556274414,
      "learning_rate": 2.957157343248268e-05,
      "loss": 0.8292,
      "step": 10170
    },
    {
      "epoch": 1.684872558755379,
      "grad_norm": 11.796192169189453,
      "learning_rate": 2.9550447862092278e-05,
      "loss": 0.7123,
      "step": 10180
    },
    {
      "epoch": 1.686527639854353,
      "grad_norm": 7.814089775085449,
      "learning_rate": 2.9529322291701876e-05,
      "loss": 0.7668,
      "step": 10190
    },
    {
      "epoch": 1.6881827209533267,
      "grad_norm": 9.458212852478027,
      "learning_rate": 2.9508196721311478e-05,
      "loss": 0.7608,
      "step": 10200
    },
    {
      "epoch": 1.6898378020523006,
      "grad_norm": 1.9778622388839722,
      "learning_rate": 2.9487071150921076e-05,
      "loss": 0.5303,
      "step": 10210
    },
    {
      "epoch": 1.6914928831512746,
      "grad_norm": 0.45171818137168884,
      "learning_rate": 2.9465945580530675e-05,
      "loss": 0.6155,
      "step": 10220
    },
    {
      "epoch": 1.6931479642502483,
      "grad_norm": 9.554718971252441,
      "learning_rate": 2.9444820010140273e-05,
      "loss": 0.8481,
      "step": 10230
    },
    {
      "epoch": 1.694803045349222,
      "grad_norm": 5.126983642578125,
      "learning_rate": 2.9423694439749878e-05,
      "loss": 0.4612,
      "step": 10240
    },
    {
      "epoch": 1.696458126448196,
      "grad_norm": 10.4384183883667,
      "learning_rate": 2.9402568869359477e-05,
      "loss": 0.653,
      "step": 10250
    },
    {
      "epoch": 1.6981132075471699,
      "grad_norm": 9.860486030578613,
      "learning_rate": 2.9381443298969075e-05,
      "loss": 0.6952,
      "step": 10260
    },
    {
      "epoch": 1.6997682886461436,
      "grad_norm": 6.370824813842773,
      "learning_rate": 2.9360317728578673e-05,
      "loss": 0.9798,
      "step": 10270
    },
    {
      "epoch": 1.7014233697451175,
      "grad_norm": 1.6601736545562744,
      "learning_rate": 2.933919215818827e-05,
      "loss": 0.5498,
      "step": 10280
    },
    {
      "epoch": 1.7030784508440915,
      "grad_norm": 5.674802780151367,
      "learning_rate": 2.931806658779787e-05,
      "loss": 0.7001,
      "step": 10290
    },
    {
      "epoch": 1.7047335319430652,
      "grad_norm": 10.558813095092773,
      "learning_rate": 2.929694101740747e-05,
      "loss": 0.6487,
      "step": 10300
    },
    {
      "epoch": 1.706388613042039,
      "grad_norm": 10.952089309692383,
      "learning_rate": 2.927581544701707e-05,
      "loss": 0.5384,
      "step": 10310
    },
    {
      "epoch": 1.7080436941410129,
      "grad_norm": 4.31552791595459,
      "learning_rate": 2.9254689876626672e-05,
      "loss": 0.4352,
      "step": 10320
    },
    {
      "epoch": 1.7096987752399868,
      "grad_norm": 9.620162963867188,
      "learning_rate": 2.923356430623627e-05,
      "loss": 0.5812,
      "step": 10330
    },
    {
      "epoch": 1.7113538563389605,
      "grad_norm": 7.864400863647461,
      "learning_rate": 2.9212438735845872e-05,
      "loss": 0.8178,
      "step": 10340
    },
    {
      "epoch": 1.7130089374379345,
      "grad_norm": 6.937893867492676,
      "learning_rate": 2.919131316545547e-05,
      "loss": 0.5319,
      "step": 10350
    },
    {
      "epoch": 1.7146640185369084,
      "grad_norm": 14.069920539855957,
      "learning_rate": 2.917018759506507e-05,
      "loss": 0.903,
      "step": 10360
    },
    {
      "epoch": 1.7163190996358821,
      "grad_norm": 14.14964771270752,
      "learning_rate": 2.9149062024674667e-05,
      "loss": 0.9481,
      "step": 10370
    },
    {
      "epoch": 1.7179741807348559,
      "grad_norm": 8.848390579223633,
      "learning_rate": 2.9127936454284266e-05,
      "loss": 0.7393,
      "step": 10380
    },
    {
      "epoch": 1.7196292618338298,
      "grad_norm": 5.903340816497803,
      "learning_rate": 2.9106810883893864e-05,
      "loss": 0.7587,
      "step": 10390
    },
    {
      "epoch": 1.7212843429328037,
      "grad_norm": 8.207732200622559,
      "learning_rate": 2.9085685313503462e-05,
      "loss": 0.5722,
      "step": 10400
    },
    {
      "epoch": 1.7229394240317775,
      "grad_norm": 3.095677614212036,
      "learning_rate": 2.9064559743113068e-05,
      "loss": 0.4849,
      "step": 10410
    },
    {
      "epoch": 1.7245945051307514,
      "grad_norm": 6.148995399475098,
      "learning_rate": 2.9043434172722666e-05,
      "loss": 0.7504,
      "step": 10420
    },
    {
      "epoch": 1.7262495862297254,
      "grad_norm": 11.436935424804688,
      "learning_rate": 2.9022308602332264e-05,
      "loss": 0.7367,
      "step": 10430
    },
    {
      "epoch": 1.727904667328699,
      "grad_norm": 5.623143196105957,
      "learning_rate": 2.9001183031941863e-05,
      "loss": 0.6789,
      "step": 10440
    },
    {
      "epoch": 1.7295597484276728,
      "grad_norm": 11.208179473876953,
      "learning_rate": 2.8980057461551465e-05,
      "loss": 0.6612,
      "step": 10450
    },
    {
      "epoch": 1.731214829526647,
      "grad_norm": 3.2015810012817383,
      "learning_rate": 2.8958931891161063e-05,
      "loss": 0.8846,
      "step": 10460
    },
    {
      "epoch": 1.7328699106256207,
      "grad_norm": 14.068859100341797,
      "learning_rate": 2.893780632077066e-05,
      "loss": 0.8057,
      "step": 10470
    },
    {
      "epoch": 1.7345249917245944,
      "grad_norm": 3.396934986114502,
      "learning_rate": 2.891668075038026e-05,
      "loss": 0.7057,
      "step": 10480
    },
    {
      "epoch": 1.7361800728235683,
      "grad_norm": 7.737791061401367,
      "learning_rate": 2.8895555179989865e-05,
      "loss": 0.5418,
      "step": 10490
    },
    {
      "epoch": 1.7378351539225423,
      "grad_norm": 14.853691101074219,
      "learning_rate": 2.8874429609599463e-05,
      "loss": 0.8471,
      "step": 10500
    },
    {
      "epoch": 1.739490235021516,
      "grad_norm": 1.7635552883148193,
      "learning_rate": 2.885330403920906e-05,
      "loss": 0.705,
      "step": 10510
    },
    {
      "epoch": 1.74114531612049,
      "grad_norm": 9.503922462463379,
      "learning_rate": 2.883217846881866e-05,
      "loss": 0.9683,
      "step": 10520
    },
    {
      "epoch": 1.742800397219464,
      "grad_norm": 7.787638187408447,
      "learning_rate": 2.881105289842826e-05,
      "loss": 0.6567,
      "step": 10530
    },
    {
      "epoch": 1.7444554783184376,
      "grad_norm": 13.376498222351074,
      "learning_rate": 2.8789927328037857e-05,
      "loss": 0.7,
      "step": 10540
    },
    {
      "epoch": 1.7461105594174113,
      "grad_norm": 8.31912612915039,
      "learning_rate": 2.8768801757647455e-05,
      "loss": 0.7447,
      "step": 10550
    },
    {
      "epoch": 1.7477656405163853,
      "grad_norm": 6.077436923980713,
      "learning_rate": 2.8747676187257057e-05,
      "loss": 0.5459,
      "step": 10560
    },
    {
      "epoch": 1.7494207216153592,
      "grad_norm": 6.657055377960205,
      "learning_rate": 2.8726550616866655e-05,
      "loss": 0.7315,
      "step": 10570
    },
    {
      "epoch": 1.751075802714333,
      "grad_norm": 7.677982807159424,
      "learning_rate": 2.8705425046476257e-05,
      "loss": 0.7693,
      "step": 10580
    },
    {
      "epoch": 1.7527308838133069,
      "grad_norm": 5.541990756988525,
      "learning_rate": 2.868429947608586e-05,
      "loss": 0.6465,
      "step": 10590
    },
    {
      "epoch": 1.7543859649122808,
      "grad_norm": 6.058269023895264,
      "learning_rate": 2.8663173905695457e-05,
      "loss": 0.811,
      "step": 10600
    },
    {
      "epoch": 1.7560410460112545,
      "grad_norm": 3.1122348308563232,
      "learning_rate": 2.8642048335305056e-05,
      "loss": 0.8179,
      "step": 10610
    },
    {
      "epoch": 1.7576961271102283,
      "grad_norm": 2.5586354732513428,
      "learning_rate": 2.8620922764914654e-05,
      "loss": 0.75,
      "step": 10620
    },
    {
      "epoch": 1.7593512082092022,
      "grad_norm": 5.883629322052002,
      "learning_rate": 2.8599797194524252e-05,
      "loss": 0.7485,
      "step": 10630
    },
    {
      "epoch": 1.7610062893081762,
      "grad_norm": 4.991631031036377,
      "learning_rate": 2.857867162413385e-05,
      "loss": 0.7489,
      "step": 10640
    },
    {
      "epoch": 1.7626613704071499,
      "grad_norm": 7.49458646774292,
      "learning_rate": 2.855754605374345e-05,
      "loss": 0.5526,
      "step": 10650
    },
    {
      "epoch": 1.7643164515061238,
      "grad_norm": 4.664979457855225,
      "learning_rate": 2.8536420483353054e-05,
      "loss": 0.8245,
      "step": 10660
    },
    {
      "epoch": 1.7659715326050978,
      "grad_norm": 3.9022204875946045,
      "learning_rate": 2.8515294912962653e-05,
      "loss": 0.5719,
      "step": 10670
    },
    {
      "epoch": 1.7676266137040715,
      "grad_norm": 7.6836771965026855,
      "learning_rate": 2.849416934257225e-05,
      "loss": 0.5764,
      "step": 10680
    },
    {
      "epoch": 1.7692816948030452,
      "grad_norm": 2.5252983570098877,
      "learning_rate": 2.847304377218185e-05,
      "loss": 0.5396,
      "step": 10690
    },
    {
      "epoch": 1.7709367759020191,
      "grad_norm": 6.438911437988281,
      "learning_rate": 2.845191820179145e-05,
      "loss": 0.8521,
      "step": 10700
    },
    {
      "epoch": 1.772591857000993,
      "grad_norm": 8.692585945129395,
      "learning_rate": 2.843079263140105e-05,
      "loss": 0.7903,
      "step": 10710
    },
    {
      "epoch": 1.7742469380999668,
      "grad_norm": 4.1693434715271,
      "learning_rate": 2.8409667061010648e-05,
      "loss": 0.5676,
      "step": 10720
    },
    {
      "epoch": 1.7759020191989408,
      "grad_norm": 7.894397258758545,
      "learning_rate": 2.8388541490620246e-05,
      "loss": 0.5731,
      "step": 10730
    },
    {
      "epoch": 1.7775571002979147,
      "grad_norm": 11.88364315032959,
      "learning_rate": 2.8367415920229845e-05,
      "loss": 0.7537,
      "step": 10740
    },
    {
      "epoch": 1.7792121813968884,
      "grad_norm": 6.327468395233154,
      "learning_rate": 2.834629034983945e-05,
      "loss": 0.5111,
      "step": 10750
    },
    {
      "epoch": 1.7808672624958621,
      "grad_norm": 9.538745880126953,
      "learning_rate": 2.8325164779449048e-05,
      "loss": 1.0039,
      "step": 10760
    },
    {
      "epoch": 1.7825223435948363,
      "grad_norm": 5.892218589782715,
      "learning_rate": 2.8304039209058647e-05,
      "loss": 0.4347,
      "step": 10770
    },
    {
      "epoch": 1.78417742469381,
      "grad_norm": 5.04967737197876,
      "learning_rate": 2.8282913638668245e-05,
      "loss": 0.7211,
      "step": 10780
    },
    {
      "epoch": 1.7858325057927837,
      "grad_norm": 11.380403518676758,
      "learning_rate": 2.8261788068277843e-05,
      "loss": 0.65,
      "step": 10790
    },
    {
      "epoch": 1.7874875868917577,
      "grad_norm": 9.272041320800781,
      "learning_rate": 2.8240662497887442e-05,
      "loss": 0.6879,
      "step": 10800
    },
    {
      "epoch": 1.7891426679907316,
      "grad_norm": 4.747367858886719,
      "learning_rate": 2.8219536927497044e-05,
      "loss": 0.3976,
      "step": 10810
    },
    {
      "epoch": 1.7907977490897053,
      "grad_norm": 7.258157253265381,
      "learning_rate": 2.8198411357106642e-05,
      "loss": 0.5679,
      "step": 10820
    },
    {
      "epoch": 1.7924528301886793,
      "grad_norm": 6.686504364013672,
      "learning_rate": 2.8177285786716244e-05,
      "loss": 0.7579,
      "step": 10830
    },
    {
      "epoch": 1.7941079112876532,
      "grad_norm": 10.540482521057129,
      "learning_rate": 2.8156160216325846e-05,
      "loss": 0.4968,
      "step": 10840
    },
    {
      "epoch": 1.795762992386627,
      "grad_norm": 9.53823184967041,
      "learning_rate": 2.8135034645935444e-05,
      "loss": 0.582,
      "step": 10850
    },
    {
      "epoch": 1.7974180734856007,
      "grad_norm": 4.284958839416504,
      "learning_rate": 2.8113909075545042e-05,
      "loss": 0.6028,
      "step": 10860
    },
    {
      "epoch": 1.7990731545845746,
      "grad_norm": 9.711718559265137,
      "learning_rate": 2.809278350515464e-05,
      "loss": 0.7532,
      "step": 10870
    },
    {
      "epoch": 1.8007282356835486,
      "grad_norm": 6.902862548828125,
      "learning_rate": 2.807165793476424e-05,
      "loss": 0.809,
      "step": 10880
    },
    {
      "epoch": 1.8023833167825223,
      "grad_norm": 8.179754257202148,
      "learning_rate": 2.8050532364373837e-05,
      "loss": 0.6302,
      "step": 10890
    },
    {
      "epoch": 1.8040383978814962,
      "grad_norm": 9.080558776855469,
      "learning_rate": 2.8029406793983436e-05,
      "loss": 0.7332,
      "step": 10900
    },
    {
      "epoch": 1.8056934789804702,
      "grad_norm": 12.572257995605469,
      "learning_rate": 2.8008281223593034e-05,
      "loss": 1.3787,
      "step": 10910
    },
    {
      "epoch": 1.8073485600794439,
      "grad_norm": 0.821594774723053,
      "learning_rate": 2.798715565320264e-05,
      "loss": 0.8835,
      "step": 10920
    },
    {
      "epoch": 1.8090036411784176,
      "grad_norm": 3.8074162006378174,
      "learning_rate": 2.7966030082812238e-05,
      "loss": 0.7603,
      "step": 10930
    },
    {
      "epoch": 1.8106587222773916,
      "grad_norm": 2.790959596633911,
      "learning_rate": 2.794490451242184e-05,
      "loss": 0.6041,
      "step": 10940
    },
    {
      "epoch": 1.8123138033763655,
      "grad_norm": 6.163257122039795,
      "learning_rate": 2.7923778942031438e-05,
      "loss": 0.8702,
      "step": 10950
    },
    {
      "epoch": 1.8139688844753392,
      "grad_norm": 7.015653610229492,
      "learning_rate": 2.7902653371641036e-05,
      "loss": 0.6859,
      "step": 10960
    },
    {
      "epoch": 1.8156239655743132,
      "grad_norm": 8.134598731994629,
      "learning_rate": 2.7881527801250635e-05,
      "loss": 0.5471,
      "step": 10970
    },
    {
      "epoch": 1.817279046673287,
      "grad_norm": 9.133872985839844,
      "learning_rate": 2.7860402230860233e-05,
      "loss": 0.5379,
      "step": 10980
    },
    {
      "epoch": 1.8189341277722608,
      "grad_norm": 6.082035541534424,
      "learning_rate": 2.783927666046983e-05,
      "loss": 0.59,
      "step": 10990
    },
    {
      "epoch": 1.8205892088712345,
      "grad_norm": 2.086956024169922,
      "learning_rate": 2.7818151090079437e-05,
      "loss": 0.7053,
      "step": 11000
    },
    {
      "epoch": 1.8222442899702085,
      "grad_norm": 3.9870545864105225,
      "learning_rate": 2.7797025519689035e-05,
      "loss": 0.5741,
      "step": 11010
    },
    {
      "epoch": 1.8238993710691824,
      "grad_norm": 7.210365295410156,
      "learning_rate": 2.7775899949298633e-05,
      "loss": 0.5523,
      "step": 11020
    },
    {
      "epoch": 1.8255544521681561,
      "grad_norm": 5.898279190063477,
      "learning_rate": 2.7754774378908232e-05,
      "loss": 0.6738,
      "step": 11030
    },
    {
      "epoch": 1.82720953326713,
      "grad_norm": 3.7487051486968994,
      "learning_rate": 2.773364880851783e-05,
      "loss": 0.5874,
      "step": 11040
    },
    {
      "epoch": 1.828864614366104,
      "grad_norm": 6.380067348480225,
      "learning_rate": 2.7712523238127432e-05,
      "loss": 0.6087,
      "step": 11050
    },
    {
      "epoch": 1.8305196954650778,
      "grad_norm": 6.079387187957764,
      "learning_rate": 2.769139766773703e-05,
      "loss": 0.6415,
      "step": 11060
    },
    {
      "epoch": 1.8321747765640515,
      "grad_norm": 7.479349613189697,
      "learning_rate": 2.767027209734663e-05,
      "loss": 0.7015,
      "step": 11070
    },
    {
      "epoch": 1.8338298576630256,
      "grad_norm": 8.856660842895508,
      "learning_rate": 2.7649146526956227e-05,
      "loss": 0.6407,
      "step": 11080
    },
    {
      "epoch": 1.8354849387619994,
      "grad_norm": 5.084643363952637,
      "learning_rate": 2.7628020956565832e-05,
      "loss": 0.5616,
      "step": 11090
    },
    {
      "epoch": 1.837140019860973,
      "grad_norm": 10.033227920532227,
      "learning_rate": 2.760689538617543e-05,
      "loss": 0.5345,
      "step": 11100
    },
    {
      "epoch": 1.838795100959947,
      "grad_norm": 7.844731330871582,
      "learning_rate": 2.758576981578503e-05,
      "loss": 0.5746,
      "step": 11110
    },
    {
      "epoch": 1.840450182058921,
      "grad_norm": 2.716636896133423,
      "learning_rate": 2.7564644245394627e-05,
      "loss": 0.5816,
      "step": 11120
    },
    {
      "epoch": 1.8421052631578947,
      "grad_norm": 3.7201716899871826,
      "learning_rate": 2.7543518675004226e-05,
      "loss": 0.6663,
      "step": 11130
    },
    {
      "epoch": 1.8437603442568686,
      "grad_norm": 9.72002124786377,
      "learning_rate": 2.7522393104613824e-05,
      "loss": 0.6247,
      "step": 11140
    },
    {
      "epoch": 1.8454154253558426,
      "grad_norm": 4.3175225257873535,
      "learning_rate": 2.7501267534223423e-05,
      "loss": 0.731,
      "step": 11150
    },
    {
      "epoch": 1.8470705064548163,
      "grad_norm": 5.636064052581787,
      "learning_rate": 2.7480141963833024e-05,
      "loss": 0.797,
      "step": 11160
    },
    {
      "epoch": 1.84872558755379,
      "grad_norm": 7.097209453582764,
      "learning_rate": 2.7459016393442626e-05,
      "loss": 0.8246,
      "step": 11170
    },
    {
      "epoch": 1.850380668652764,
      "grad_norm": 2.3647985458374023,
      "learning_rate": 2.7437890823052224e-05,
      "loss": 0.5222,
      "step": 11180
    },
    {
      "epoch": 1.852035749751738,
      "grad_norm": 7.887512683868408,
      "learning_rate": 2.7416765252661826e-05,
      "loss": 0.6683,
      "step": 11190
    },
    {
      "epoch": 1.8536908308507116,
      "grad_norm": 4.718008518218994,
      "learning_rate": 2.7395639682271425e-05,
      "loss": 0.423,
      "step": 11200
    },
    {
      "epoch": 1.8553459119496856,
      "grad_norm": 4.46136474609375,
      "learning_rate": 2.7374514111881023e-05,
      "loss": 0.5828,
      "step": 11210
    },
    {
      "epoch": 1.8570009930486595,
      "grad_norm": 9.49305248260498,
      "learning_rate": 2.735338854149062e-05,
      "loss": 0.4998,
      "step": 11220
    },
    {
      "epoch": 1.8586560741476332,
      "grad_norm": 12.336441993713379,
      "learning_rate": 2.733226297110022e-05,
      "loss": 0.872,
      "step": 11230
    },
    {
      "epoch": 1.860311155246607,
      "grad_norm": 16.26667022705078,
      "learning_rate": 2.7311137400709818e-05,
      "loss": 0.8469,
      "step": 11240
    },
    {
      "epoch": 1.861966236345581,
      "grad_norm": 6.5281267166137695,
      "learning_rate": 2.7290011830319416e-05,
      "loss": 0.6596,
      "step": 11250
    },
    {
      "epoch": 1.8636213174445548,
      "grad_norm": 3.4119770526885986,
      "learning_rate": 2.726888625992902e-05,
      "loss": 0.5131,
      "step": 11260
    },
    {
      "epoch": 1.8652763985435286,
      "grad_norm": 6.9927077293396,
      "learning_rate": 2.724776068953862e-05,
      "loss": 0.459,
      "step": 11270
    },
    {
      "epoch": 1.8669314796425025,
      "grad_norm": 4.523588180541992,
      "learning_rate": 2.722663511914822e-05,
      "loss": 0.5144,
      "step": 11280
    },
    {
      "epoch": 1.8685865607414764,
      "grad_norm": 6.561376571655273,
      "learning_rate": 2.7205509548757817e-05,
      "loss": 0.9042,
      "step": 11290
    },
    {
      "epoch": 1.8702416418404502,
      "grad_norm": 1.4205429553985596,
      "learning_rate": 2.718438397836742e-05,
      "loss": 0.697,
      "step": 11300
    },
    {
      "epoch": 1.8718967229394239,
      "grad_norm": 3.553602933883667,
      "learning_rate": 2.7163258407977017e-05,
      "loss": 0.8697,
      "step": 11310
    },
    {
      "epoch": 1.8735518040383978,
      "grad_norm": 7.370333671569824,
      "learning_rate": 2.7142132837586615e-05,
      "loss": 0.4877,
      "step": 11320
    },
    {
      "epoch": 1.8752068851373718,
      "grad_norm": 12.96839714050293,
      "learning_rate": 2.7121007267196214e-05,
      "loss": 0.7989,
      "step": 11330
    },
    {
      "epoch": 1.8768619662363455,
      "grad_norm": 7.851128578186035,
      "learning_rate": 2.709988169680582e-05,
      "loss": 0.6133,
      "step": 11340
    },
    {
      "epoch": 1.8785170473353194,
      "grad_norm": 3.7234365940093994,
      "learning_rate": 2.7078756126415417e-05,
      "loss": 0.8014,
      "step": 11350
    },
    {
      "epoch": 1.8801721284342934,
      "grad_norm": 7.6572136878967285,
      "learning_rate": 2.7057630556025016e-05,
      "loss": 1.1146,
      "step": 11360
    },
    {
      "epoch": 1.881827209533267,
      "grad_norm": 15.97938060760498,
      "learning_rate": 2.7036504985634614e-05,
      "loss": 0.6419,
      "step": 11370
    },
    {
      "epoch": 1.8834822906322408,
      "grad_norm": 5.039325714111328,
      "learning_rate": 2.7015379415244212e-05,
      "loss": 0.6463,
      "step": 11380
    },
    {
      "epoch": 1.885137371731215,
      "grad_norm": 3.1931204795837402,
      "learning_rate": 2.699425384485381e-05,
      "loss": 0.6616,
      "step": 11390
    },
    {
      "epoch": 1.8867924528301887,
      "grad_norm": 15.371889114379883,
      "learning_rate": 2.697312827446341e-05,
      "loss": 0.7662,
      "step": 11400
    },
    {
      "epoch": 1.8884475339291624,
      "grad_norm": 4.939863681793213,
      "learning_rate": 2.695200270407301e-05,
      "loss": 0.6993,
      "step": 11410
    },
    {
      "epoch": 1.8901026150281364,
      "grad_norm": 9.487849235534668,
      "learning_rate": 2.693087713368261e-05,
      "loss": 0.5206,
      "step": 11420
    },
    {
      "epoch": 1.8917576961271103,
      "grad_norm": 7.005796909332275,
      "learning_rate": 2.690975156329221e-05,
      "loss": 0.6504,
      "step": 11430
    },
    {
      "epoch": 1.893412777226084,
      "grad_norm": 8.260921478271484,
      "learning_rate": 2.6888625992901813e-05,
      "loss": 0.4978,
      "step": 11440
    },
    {
      "epoch": 1.895067858325058,
      "grad_norm": 7.263406753540039,
      "learning_rate": 2.686750042251141e-05,
      "loss": 0.6031,
      "step": 11450
    },
    {
      "epoch": 1.896722939424032,
      "grad_norm": 7.041758060455322,
      "learning_rate": 2.684637485212101e-05,
      "loss": 0.7448,
      "step": 11460
    },
    {
      "epoch": 1.8983780205230056,
      "grad_norm": 8.920317649841309,
      "learning_rate": 2.6825249281730608e-05,
      "loss": 0.6412,
      "step": 11470
    },
    {
      "epoch": 1.9000331016219794,
      "grad_norm": 7.251124382019043,
      "learning_rate": 2.6804123711340206e-05,
      "loss": 0.6119,
      "step": 11480
    },
    {
      "epoch": 1.9016881827209533,
      "grad_norm": 9.376360893249512,
      "learning_rate": 2.6782998140949805e-05,
      "loss": 0.5737,
      "step": 11490
    },
    {
      "epoch": 1.9033432638199272,
      "grad_norm": 11.813860893249512,
      "learning_rate": 2.6761872570559403e-05,
      "loss": 0.7317,
      "step": 11500
    },
    {
      "epoch": 1.904998344918901,
      "grad_norm": 3.818513870239258,
      "learning_rate": 2.674074700016901e-05,
      "loss": 0.6721,
      "step": 11510
    },
    {
      "epoch": 1.906653426017875,
      "grad_norm": 9.76960563659668,
      "learning_rate": 2.6719621429778607e-05,
      "loss": 0.9387,
      "step": 11520
    },
    {
      "epoch": 1.9083085071168489,
      "grad_norm": 7.79152774810791,
      "learning_rate": 2.6698495859388205e-05,
      "loss": 0.7683,
      "step": 11530
    },
    {
      "epoch": 1.9099635882158226,
      "grad_norm": 9.979121208190918,
      "learning_rate": 2.6677370288997803e-05,
      "loss": 0.7389,
      "step": 11540
    },
    {
      "epoch": 1.9116186693147963,
      "grad_norm": 4.865985870361328,
      "learning_rate": 2.6656244718607405e-05,
      "loss": 0.968,
      "step": 11550
    },
    {
      "epoch": 1.9132737504137702,
      "grad_norm": 7.992015361785889,
      "learning_rate": 2.6635119148217004e-05,
      "loss": 0.8791,
      "step": 11560
    },
    {
      "epoch": 1.9149288315127442,
      "grad_norm": 6.297119617462158,
      "learning_rate": 2.6613993577826602e-05,
      "loss": 0.7482,
      "step": 11570
    },
    {
      "epoch": 1.916583912611718,
      "grad_norm": 2.8887088298797607,
      "learning_rate": 2.65928680074362e-05,
      "loss": 0.5483,
      "step": 11580
    },
    {
      "epoch": 1.9182389937106918,
      "grad_norm": 10.49492359161377,
      "learning_rate": 2.65717424370458e-05,
      "loss": 0.5371,
      "step": 11590
    },
    {
      "epoch": 1.9198940748096658,
      "grad_norm": 6.460445880889893,
      "learning_rate": 2.6550616866655404e-05,
      "loss": 0.7356,
      "step": 11600
    },
    {
      "epoch": 1.9215491559086395,
      "grad_norm": 9.367919921875,
      "learning_rate": 2.6529491296265002e-05,
      "loss": 0.7875,
      "step": 11610
    },
    {
      "epoch": 1.9232042370076132,
      "grad_norm": 6.753652572631836,
      "learning_rate": 2.65083657258746e-05,
      "loss": 0.6686,
      "step": 11620
    },
    {
      "epoch": 1.9248593181065872,
      "grad_norm": 10.88228702545166,
      "learning_rate": 2.64872401554842e-05,
      "loss": 0.8549,
      "step": 11630
    },
    {
      "epoch": 1.9265143992055611,
      "grad_norm": 6.009291648864746,
      "learning_rate": 2.6466114585093797e-05,
      "loss": 0.8383,
      "step": 11640
    },
    {
      "epoch": 1.9281694803045348,
      "grad_norm": 5.272893905639648,
      "learning_rate": 2.6444989014703396e-05,
      "loss": 0.6214,
      "step": 11650
    },
    {
      "epoch": 1.9298245614035088,
      "grad_norm": 9.108641624450684,
      "learning_rate": 2.6423863444312998e-05,
      "loss": 0.6079,
      "step": 11660
    },
    {
      "epoch": 1.9314796425024827,
      "grad_norm": 4.889091491699219,
      "learning_rate": 2.6402737873922596e-05,
      "loss": 0.6088,
      "step": 11670
    },
    {
      "epoch": 1.9331347236014564,
      "grad_norm": 5.582423686981201,
      "learning_rate": 2.6381612303532198e-05,
      "loss": 0.4524,
      "step": 11680
    },
    {
      "epoch": 1.9347898047004302,
      "grad_norm": 11.567883491516113,
      "learning_rate": 2.63604867331418e-05,
      "loss": 0.6744,
      "step": 11690
    },
    {
      "epoch": 1.9364448857994043,
      "grad_norm": 4.840481758117676,
      "learning_rate": 2.6339361162751398e-05,
      "loss": 0.5944,
      "step": 11700
    },
    {
      "epoch": 1.938099966898378,
      "grad_norm": 5.520872592926025,
      "learning_rate": 2.6318235592360996e-05,
      "loss": 0.9857,
      "step": 11710
    },
    {
      "epoch": 1.9397550479973518,
      "grad_norm": 10.392516136169434,
      "learning_rate": 2.6297110021970595e-05,
      "loss": 0.6162,
      "step": 11720
    },
    {
      "epoch": 1.9414101290963257,
      "grad_norm": 4.256656169891357,
      "learning_rate": 2.6275984451580193e-05,
      "loss": 0.782,
      "step": 11730
    },
    {
      "epoch": 1.9430652101952997,
      "grad_norm": 3.5730466842651367,
      "learning_rate": 2.625485888118979e-05,
      "loss": 0.4999,
      "step": 11740
    },
    {
      "epoch": 1.9447202912942734,
      "grad_norm": 5.9669623374938965,
      "learning_rate": 2.623373331079939e-05,
      "loss": 0.7186,
      "step": 11750
    },
    {
      "epoch": 1.9463753723932473,
      "grad_norm": 4.916635513305664,
      "learning_rate": 2.6212607740408988e-05,
      "loss": 0.8722,
      "step": 11760
    },
    {
      "epoch": 1.9480304534922213,
      "grad_norm": 2.7786076068878174,
      "learning_rate": 2.6191482170018593e-05,
      "loss": 0.811,
      "step": 11770
    },
    {
      "epoch": 1.949685534591195,
      "grad_norm": 8.172378540039062,
      "learning_rate": 2.6170356599628192e-05,
      "loss": 0.783,
      "step": 11780
    },
    {
      "epoch": 1.9513406156901687,
      "grad_norm": 3.4159371852874756,
      "learning_rate": 2.614923102923779e-05,
      "loss": 0.6533,
      "step": 11790
    },
    {
      "epoch": 1.9529956967891426,
      "grad_norm": 1.6024651527404785,
      "learning_rate": 2.6128105458847392e-05,
      "loss": 0.9008,
      "step": 11800
    },
    {
      "epoch": 1.9546507778881166,
      "grad_norm": 6.659937381744385,
      "learning_rate": 2.610697988845699e-05,
      "loss": 0.401,
      "step": 11810
    },
    {
      "epoch": 1.9563058589870903,
      "grad_norm": 5.116189479827881,
      "learning_rate": 2.608585431806659e-05,
      "loss": 0.692,
      "step": 11820
    },
    {
      "epoch": 1.9579609400860642,
      "grad_norm": 12.277504920959473,
      "learning_rate": 2.6064728747676187e-05,
      "loss": 0.6657,
      "step": 11830
    },
    {
      "epoch": 1.9596160211850382,
      "grad_norm": 10.450069427490234,
      "learning_rate": 2.6043603177285785e-05,
      "loss": 0.7935,
      "step": 11840
    },
    {
      "epoch": 1.961271102284012,
      "grad_norm": 6.333822250366211,
      "learning_rate": 2.602247760689539e-05,
      "loss": 0.7396,
      "step": 11850
    },
    {
      "epoch": 1.9629261833829856,
      "grad_norm": 9.002862930297852,
      "learning_rate": 2.600135203650499e-05,
      "loss": 0.4987,
      "step": 11860
    },
    {
      "epoch": 1.9645812644819596,
      "grad_norm": 4.263606071472168,
      "learning_rate": 2.5980226466114587e-05,
      "loss": 0.7053,
      "step": 11870
    },
    {
      "epoch": 1.9662363455809335,
      "grad_norm": 6.319537162780762,
      "learning_rate": 2.5959100895724186e-05,
      "loss": 0.6639,
      "step": 11880
    },
    {
      "epoch": 1.9678914266799072,
      "grad_norm": 6.470550537109375,
      "learning_rate": 2.5937975325333784e-05,
      "loss": 0.653,
      "step": 11890
    },
    {
      "epoch": 1.9695465077788812,
      "grad_norm": 17.440753936767578,
      "learning_rate": 2.5916849754943383e-05,
      "loss": 1.0208,
      "step": 11900
    },
    {
      "epoch": 1.9712015888778551,
      "grad_norm": 7.6418232917785645,
      "learning_rate": 2.5895724184552984e-05,
      "loss": 0.6385,
      "step": 11910
    },
    {
      "epoch": 1.9728566699768288,
      "grad_norm": 8.591548919677734,
      "learning_rate": 2.5874598614162583e-05,
      "loss": 0.5961,
      "step": 11920
    },
    {
      "epoch": 1.9745117510758026,
      "grad_norm": 4.126323223114014,
      "learning_rate": 2.585347304377218e-05,
      "loss": 0.4409,
      "step": 11930
    },
    {
      "epoch": 1.9761668321747765,
      "grad_norm": 1.949657917022705,
      "learning_rate": 2.5832347473381786e-05,
      "loss": 0.704,
      "step": 11940
    },
    {
      "epoch": 1.9778219132737505,
      "grad_norm": 8.87740707397461,
      "learning_rate": 2.5811221902991385e-05,
      "loss": 0.7217,
      "step": 11950
    },
    {
      "epoch": 1.9794769943727242,
      "grad_norm": 5.372537612915039,
      "learning_rate": 2.5790096332600983e-05,
      "loss": 0.5814,
      "step": 11960
    },
    {
      "epoch": 1.9811320754716981,
      "grad_norm": 3.392500162124634,
      "learning_rate": 2.576897076221058e-05,
      "loss": 0.52,
      "step": 11970
    },
    {
      "epoch": 1.982787156570672,
      "grad_norm": 5.720313549041748,
      "learning_rate": 2.574784519182018e-05,
      "loss": 0.6135,
      "step": 11980
    },
    {
      "epoch": 1.9844422376696458,
      "grad_norm": 15.157927513122559,
      "learning_rate": 2.5726719621429778e-05,
      "loss": 0.9472,
      "step": 11990
    },
    {
      "epoch": 1.9860973187686195,
      "grad_norm": 7.303770542144775,
      "learning_rate": 2.5705594051039377e-05,
      "loss": 0.8612,
      "step": 12000
    },
    {
      "epoch": 1.9877523998675937,
      "grad_norm": 9.365641593933105,
      "learning_rate": 2.5684468480648975e-05,
      "loss": 1.095,
      "step": 12010
    },
    {
      "epoch": 1.9894074809665674,
      "grad_norm": 10.799443244934082,
      "learning_rate": 2.566334291025858e-05,
      "loss": 0.6967,
      "step": 12020
    },
    {
      "epoch": 1.991062562065541,
      "grad_norm": 4.058383464813232,
      "learning_rate": 2.564221733986818e-05,
      "loss": 0.6169,
      "step": 12030
    },
    {
      "epoch": 1.992717643164515,
      "grad_norm": 5.2603373527526855,
      "learning_rate": 2.5621091769477777e-05,
      "loss": 0.4979,
      "step": 12040
    },
    {
      "epoch": 1.994372724263489,
      "grad_norm": 4.266921520233154,
      "learning_rate": 2.559996619908738e-05,
      "loss": 0.5869,
      "step": 12050
    },
    {
      "epoch": 1.9960278053624627,
      "grad_norm": 7.452713966369629,
      "learning_rate": 2.5578840628696977e-05,
      "loss": 0.5867,
      "step": 12060
    },
    {
      "epoch": 1.9976828864614367,
      "grad_norm": 9.311662673950195,
      "learning_rate": 2.5557715058306575e-05,
      "loss": 0.7754,
      "step": 12070
    },
    {
      "epoch": 1.9993379675604106,
      "grad_norm": 6.503630638122559,
      "learning_rate": 2.5536589487916174e-05,
      "loss": 0.4524,
      "step": 12080
    },
    {
      "epoch": 2.0009930486593843,
      "grad_norm": 5.143138408660889,
      "learning_rate": 2.5515463917525772e-05,
      "loss": 0.9485,
      "step": 12090
    },
    {
      "epoch": 2.002648129758358,
      "grad_norm": 12.57167911529541,
      "learning_rate": 2.549433834713537e-05,
      "loss": 0.6995,
      "step": 12100
    },
    {
      "epoch": 2.004303210857332,
      "grad_norm": 6.110803604125977,
      "learning_rate": 2.5473212776744976e-05,
      "loss": 0.5488,
      "step": 12110
    },
    {
      "epoch": 2.005958291956306,
      "grad_norm": 2.2789018154144287,
      "learning_rate": 2.5452087206354574e-05,
      "loss": 0.4726,
      "step": 12120
    },
    {
      "epoch": 2.0076133730552796,
      "grad_norm": 5.685675144195557,
      "learning_rate": 2.5430961635964172e-05,
      "loss": 0.8198,
      "step": 12130
    },
    {
      "epoch": 2.0092684541542534,
      "grad_norm": 13.751742362976074,
      "learning_rate": 2.540983606557377e-05,
      "loss": 0.5648,
      "step": 12140
    },
    {
      "epoch": 2.0109235352532275,
      "grad_norm": 4.847718238830566,
      "learning_rate": 2.5388710495183373e-05,
      "loss": 0.55,
      "step": 12150
    },
    {
      "epoch": 2.0125786163522013,
      "grad_norm": 3.4423885345458984,
      "learning_rate": 2.536758492479297e-05,
      "loss": 0.5455,
      "step": 12160
    },
    {
      "epoch": 2.014233697451175,
      "grad_norm": 14.159363746643066,
      "learning_rate": 2.534645935440257e-05,
      "loss": 1.0336,
      "step": 12170
    },
    {
      "epoch": 2.015888778550149,
      "grad_norm": 11.10091781616211,
      "learning_rate": 2.5325333784012168e-05,
      "loss": 0.7534,
      "step": 12180
    },
    {
      "epoch": 2.017543859649123,
      "grad_norm": 1.812164545059204,
      "learning_rate": 2.5304208213621773e-05,
      "loss": 0.7432,
      "step": 12190
    },
    {
      "epoch": 2.0191989407480966,
      "grad_norm": 9.428847312927246,
      "learning_rate": 2.528308264323137e-05,
      "loss": 0.9476,
      "step": 12200
    },
    {
      "epoch": 2.0208540218470703,
      "grad_norm": 5.482785224914551,
      "learning_rate": 2.526195707284097e-05,
      "loss": 0.6576,
      "step": 12210
    },
    {
      "epoch": 2.0225091029460445,
      "grad_norm": 5.117173194885254,
      "learning_rate": 2.5240831502450568e-05,
      "loss": 0.6559,
      "step": 12220
    },
    {
      "epoch": 2.024164184045018,
      "grad_norm": 5.9809489250183105,
      "learning_rate": 2.5219705932060166e-05,
      "loss": 0.5725,
      "step": 12230
    },
    {
      "epoch": 2.025819265143992,
      "grad_norm": 6.737792015075684,
      "learning_rate": 2.5198580361669765e-05,
      "loss": 0.9227,
      "step": 12240
    },
    {
      "epoch": 2.027474346242966,
      "grad_norm": 12.14630126953125,
      "learning_rate": 2.5177454791279363e-05,
      "loss": 0.7601,
      "step": 12250
    },
    {
      "epoch": 2.02912942734194,
      "grad_norm": 3.288093090057373,
      "learning_rate": 2.5156329220888965e-05,
      "loss": 0.4459,
      "step": 12260
    },
    {
      "epoch": 2.0307845084409135,
      "grad_norm": 9.579849243164062,
      "learning_rate": 2.5135203650498563e-05,
      "loss": 0.7549,
      "step": 12270
    },
    {
      "epoch": 2.0324395895398872,
      "grad_norm": 6.380425453186035,
      "learning_rate": 2.5114078080108165e-05,
      "loss": 0.6142,
      "step": 12280
    },
    {
      "epoch": 2.0340946706388614,
      "grad_norm": 6.776975631713867,
      "learning_rate": 2.5092952509717767e-05,
      "loss": 0.4758,
      "step": 12290
    },
    {
      "epoch": 2.035749751737835,
      "grad_norm": 2.6169750690460205,
      "learning_rate": 2.5071826939327365e-05,
      "loss": 0.4746,
      "step": 12300
    },
    {
      "epoch": 2.037404832836809,
      "grad_norm": 11.645594596862793,
      "learning_rate": 2.5050701368936964e-05,
      "loss": 0.7003,
      "step": 12310
    },
    {
      "epoch": 2.039059913935783,
      "grad_norm": 1.6118006706237793,
      "learning_rate": 2.5029575798546562e-05,
      "loss": 0.6515,
      "step": 12320
    },
    {
      "epoch": 2.0407149950347567,
      "grad_norm": 8.22458553314209,
      "learning_rate": 2.500845022815616e-05,
      "loss": 0.5659,
      "step": 12330
    },
    {
      "epoch": 2.0423700761337304,
      "grad_norm": 6.070284843444824,
      "learning_rate": 2.4987324657765762e-05,
      "loss": 0.5566,
      "step": 12340
    },
    {
      "epoch": 2.0440251572327046,
      "grad_norm": 7.893292427062988,
      "learning_rate": 2.496619908737536e-05,
      "loss": 0.8031,
      "step": 12350
    },
    {
      "epoch": 2.0456802383316783,
      "grad_norm": 9.852980613708496,
      "learning_rate": 2.494507351698496e-05,
      "loss": 0.4343,
      "step": 12360
    },
    {
      "epoch": 2.047335319430652,
      "grad_norm": 13.492615699768066,
      "learning_rate": 2.4923947946594557e-05,
      "loss": 0.6567,
      "step": 12370
    },
    {
      "epoch": 2.0489904005296258,
      "grad_norm": 19.66620635986328,
      "learning_rate": 2.4902822376204156e-05,
      "loss": 0.8166,
      "step": 12380
    },
    {
      "epoch": 2.0506454816286,
      "grad_norm": 8.559783935546875,
      "learning_rate": 2.4881696805813758e-05,
      "loss": 0.5352,
      "step": 12390
    },
    {
      "epoch": 2.0523005627275737,
      "grad_norm": 1.5116767883300781,
      "learning_rate": 2.486057123542336e-05,
      "loss": 0.5582,
      "step": 12400
    },
    {
      "epoch": 2.0539556438265474,
      "grad_norm": 9.787927627563477,
      "learning_rate": 2.4839445665032958e-05,
      "loss": 0.6791,
      "step": 12410
    },
    {
      "epoch": 2.0556107249255215,
      "grad_norm": 5.483076095581055,
      "learning_rate": 2.4818320094642556e-05,
      "loss": 0.5165,
      "step": 12420
    },
    {
      "epoch": 2.0572658060244953,
      "grad_norm": 9.593470573425293,
      "learning_rate": 2.4797194524252158e-05,
      "loss": 0.5415,
      "step": 12430
    },
    {
      "epoch": 2.058920887123469,
      "grad_norm": 10.82944107055664,
      "learning_rate": 2.4776068953861756e-05,
      "loss": 0.6853,
      "step": 12440
    },
    {
      "epoch": 2.0605759682224427,
      "grad_norm": 8.793251037597656,
      "learning_rate": 2.4754943383471355e-05,
      "loss": 0.7958,
      "step": 12450
    },
    {
      "epoch": 2.062231049321417,
      "grad_norm": 15.514778137207031,
      "learning_rate": 2.4733817813080953e-05,
      "loss": 0.8556,
      "step": 12460
    },
    {
      "epoch": 2.0638861304203906,
      "grad_norm": 9.100138664245605,
      "learning_rate": 2.4712692242690555e-05,
      "loss": 0.7183,
      "step": 12470
    },
    {
      "epoch": 2.0655412115193643,
      "grad_norm": 1.601211428642273,
      "learning_rate": 2.4691566672300153e-05,
      "loss": 0.4878,
      "step": 12480
    },
    {
      "epoch": 2.0671962926183385,
      "grad_norm": 11.017544746398926,
      "learning_rate": 2.467044110190975e-05,
      "loss": 0.5721,
      "step": 12490
    },
    {
      "epoch": 2.068851373717312,
      "grad_norm": 7.533577919006348,
      "learning_rate": 2.464931553151935e-05,
      "loss": 0.5369,
      "step": 12500
    },
    {
      "epoch": 2.070506454816286,
      "grad_norm": 9.263910293579102,
      "learning_rate": 2.4628189961128952e-05,
      "loss": 0.5733,
      "step": 12510
    },
    {
      "epoch": 2.0721615359152596,
      "grad_norm": 5.43499231338501,
      "learning_rate": 2.460706439073855e-05,
      "loss": 0.5601,
      "step": 12520
    },
    {
      "epoch": 2.073816617014234,
      "grad_norm": 9.444048881530762,
      "learning_rate": 2.4585938820348152e-05,
      "loss": 0.5857,
      "step": 12530
    },
    {
      "epoch": 2.0754716981132075,
      "grad_norm": 6.828461170196533,
      "learning_rate": 2.456481324995775e-05,
      "loss": 0.641,
      "step": 12540
    },
    {
      "epoch": 2.0771267792121813,
      "grad_norm": 8.050933837890625,
      "learning_rate": 2.454368767956735e-05,
      "loss": 0.6539,
      "step": 12550
    },
    {
      "epoch": 2.0787818603111554,
      "grad_norm": 11.32827091217041,
      "learning_rate": 2.452256210917695e-05,
      "loss": 0.698,
      "step": 12560
    },
    {
      "epoch": 2.080436941410129,
      "grad_norm": 4.64754581451416,
      "learning_rate": 2.450143653878655e-05,
      "loss": 0.6328,
      "step": 12570
    },
    {
      "epoch": 2.082092022509103,
      "grad_norm": 2.866670608520508,
      "learning_rate": 2.4480310968396147e-05,
      "loss": 0.6017,
      "step": 12580
    },
    {
      "epoch": 2.0837471036080766,
      "grad_norm": 8.213902473449707,
      "learning_rate": 2.4459185398005746e-05,
      "loss": 0.4507,
      "step": 12590
    },
    {
      "epoch": 2.0854021847070507,
      "grad_norm": 12.46671199798584,
      "learning_rate": 2.4438059827615347e-05,
      "loss": 0.5089,
      "step": 12600
    },
    {
      "epoch": 2.0870572658060245,
      "grad_norm": 6.603592872619629,
      "learning_rate": 2.4416934257224946e-05,
      "loss": 0.7173,
      "step": 12610
    },
    {
      "epoch": 2.088712346904998,
      "grad_norm": 7.276727199554443,
      "learning_rate": 2.4395808686834544e-05,
      "loss": 0.5643,
      "step": 12620
    },
    {
      "epoch": 2.0903674280039723,
      "grad_norm": 0.8849647641181946,
      "learning_rate": 2.4374683116444142e-05,
      "loss": 0.8599,
      "step": 12630
    },
    {
      "epoch": 2.092022509102946,
      "grad_norm": 6.37057638168335,
      "learning_rate": 2.4353557546053744e-05,
      "loss": 0.7528,
      "step": 12640
    },
    {
      "epoch": 2.09367759020192,
      "grad_norm": 5.3119120597839355,
      "learning_rate": 2.4332431975663346e-05,
      "loss": 0.738,
      "step": 12650
    },
    {
      "epoch": 2.095332671300894,
      "grad_norm": 8.213275909423828,
      "learning_rate": 2.4311306405272944e-05,
      "loss": 1.0757,
      "step": 12660
    },
    {
      "epoch": 2.0969877523998677,
      "grad_norm": 4.72123384475708,
      "learning_rate": 2.4290180834882543e-05,
      "loss": 0.4509,
      "step": 12670
    },
    {
      "epoch": 2.0986428334988414,
      "grad_norm": 8.459196090698242,
      "learning_rate": 2.4269055264492145e-05,
      "loss": 0.8699,
      "step": 12680
    },
    {
      "epoch": 2.100297914597815,
      "grad_norm": 6.703444480895996,
      "learning_rate": 2.4247929694101743e-05,
      "loss": 0.4957,
      "step": 12690
    },
    {
      "epoch": 2.1019529956967893,
      "grad_norm": 6.101469993591309,
      "learning_rate": 2.422680412371134e-05,
      "loss": 0.6574,
      "step": 12700
    },
    {
      "epoch": 2.103608076795763,
      "grad_norm": 5.333622932434082,
      "learning_rate": 2.420567855332094e-05,
      "loss": 0.5378,
      "step": 12710
    },
    {
      "epoch": 2.1052631578947367,
      "grad_norm": 19.068145751953125,
      "learning_rate": 2.4184552982930538e-05,
      "loss": 0.48,
      "step": 12720
    },
    {
      "epoch": 2.106918238993711,
      "grad_norm": 11.487529754638672,
      "learning_rate": 2.416342741254014e-05,
      "loss": 0.4406,
      "step": 12730
    },
    {
      "epoch": 2.1085733200926846,
      "grad_norm": 7.915150165557861,
      "learning_rate": 2.4142301842149738e-05,
      "loss": 0.6731,
      "step": 12740
    },
    {
      "epoch": 2.1102284011916583,
      "grad_norm": 17.524168014526367,
      "learning_rate": 2.4121176271759337e-05,
      "loss": 0.7173,
      "step": 12750
    },
    {
      "epoch": 2.111883482290632,
      "grad_norm": 4.899764537811279,
      "learning_rate": 2.410005070136894e-05,
      "loss": 0.5679,
      "step": 12760
    },
    {
      "epoch": 2.113538563389606,
      "grad_norm": 3.07222056388855,
      "learning_rate": 2.407892513097854e-05,
      "loss": 0.8013,
      "step": 12770
    },
    {
      "epoch": 2.11519364448858,
      "grad_norm": 9.451594352722168,
      "learning_rate": 2.405779956058814e-05,
      "loss": 0.8526,
      "step": 12780
    },
    {
      "epoch": 2.1168487255875537,
      "grad_norm": 9.054977416992188,
      "learning_rate": 2.4036673990197737e-05,
      "loss": 0.7764,
      "step": 12790
    },
    {
      "epoch": 2.118503806686528,
      "grad_norm": 10.639378547668457,
      "learning_rate": 2.4015548419807335e-05,
      "loss": 0.5784,
      "step": 12800
    },
    {
      "epoch": 2.1201588877855015,
      "grad_norm": 16.33058738708496,
      "learning_rate": 2.3994422849416937e-05,
      "loss": 0.8155,
      "step": 12810
    },
    {
      "epoch": 2.1218139688844753,
      "grad_norm": 7.6870598793029785,
      "learning_rate": 2.3973297279026535e-05,
      "loss": 0.7089,
      "step": 12820
    },
    {
      "epoch": 2.123469049983449,
      "grad_norm": 5.305695056915283,
      "learning_rate": 2.3952171708636134e-05,
      "loss": 0.6047,
      "step": 12830
    },
    {
      "epoch": 2.125124131082423,
      "grad_norm": 2.975097179412842,
      "learning_rate": 2.3931046138245732e-05,
      "loss": 0.4882,
      "step": 12840
    },
    {
      "epoch": 2.126779212181397,
      "grad_norm": 7.396853446960449,
      "learning_rate": 2.3909920567855334e-05,
      "loss": 0.5802,
      "step": 12850
    },
    {
      "epoch": 2.1284342932803706,
      "grad_norm": 7.184856414794922,
      "learning_rate": 2.3888794997464932e-05,
      "loss": 0.5902,
      "step": 12860
    },
    {
      "epoch": 2.1300893743793448,
      "grad_norm": 13.981529235839844,
      "learning_rate": 2.386766942707453e-05,
      "loss": 0.8044,
      "step": 12870
    },
    {
      "epoch": 2.1317444554783185,
      "grad_norm": 7.011593818664551,
      "learning_rate": 2.3846543856684133e-05,
      "loss": 0.3445,
      "step": 12880
    },
    {
      "epoch": 2.133399536577292,
      "grad_norm": 9.505497932434082,
      "learning_rate": 2.382541828629373e-05,
      "loss": 0.7144,
      "step": 12890
    },
    {
      "epoch": 2.135054617676266,
      "grad_norm": 2.257108449935913,
      "learning_rate": 2.3804292715903333e-05,
      "loss": 0.4945,
      "step": 12900
    },
    {
      "epoch": 2.13670969877524,
      "grad_norm": 3.6385114192962646,
      "learning_rate": 2.378316714551293e-05,
      "loss": 0.6557,
      "step": 12910
    },
    {
      "epoch": 2.138364779874214,
      "grad_norm": 7.175348281860352,
      "learning_rate": 2.376204157512253e-05,
      "loss": 0.6809,
      "step": 12920
    },
    {
      "epoch": 2.1400198609731875,
      "grad_norm": 7.542967319488525,
      "learning_rate": 2.3740916004732128e-05,
      "loss": 0.6302,
      "step": 12930
    },
    {
      "epoch": 2.1416749420721617,
      "grad_norm": 8.54224681854248,
      "learning_rate": 2.371979043434173e-05,
      "loss": 0.904,
      "step": 12940
    },
    {
      "epoch": 2.1433300231711354,
      "grad_norm": 1.4422818422317505,
      "learning_rate": 2.3698664863951328e-05,
      "loss": 0.655,
      "step": 12950
    },
    {
      "epoch": 2.144985104270109,
      "grad_norm": 5.229928493499756,
      "learning_rate": 2.3677539293560926e-05,
      "loss": 0.5897,
      "step": 12960
    },
    {
      "epoch": 2.1466401853690833,
      "grad_norm": 9.104884147644043,
      "learning_rate": 2.3656413723170525e-05,
      "loss": 0.5498,
      "step": 12970
    },
    {
      "epoch": 2.148295266468057,
      "grad_norm": 10.995546340942383,
      "learning_rate": 2.3635288152780127e-05,
      "loss": 0.845,
      "step": 12980
    },
    {
      "epoch": 2.1499503475670307,
      "grad_norm": 6.361529350280762,
      "learning_rate": 2.3614162582389725e-05,
      "loss": 0.6733,
      "step": 12990
    },
    {
      "epoch": 2.1516054286660045,
      "grad_norm": 0.38223451375961304,
      "learning_rate": 2.3593037011999323e-05,
      "loss": 0.6755,
      "step": 13000
    },
    {
      "epoch": 2.1532605097649786,
      "grad_norm": 9.588419914245605,
      "learning_rate": 2.3571911441608925e-05,
      "loss": 0.677,
      "step": 13010
    },
    {
      "epoch": 2.1549155908639523,
      "grad_norm": 4.898079872131348,
      "learning_rate": 2.3550785871218527e-05,
      "loss": 0.5209,
      "step": 13020
    },
    {
      "epoch": 2.156570671962926,
      "grad_norm": 4.295017719268799,
      "learning_rate": 2.3529660300828125e-05,
      "loss": 0.3492,
      "step": 13030
    },
    {
      "epoch": 2.1582257530619002,
      "grad_norm": 11.922882080078125,
      "learning_rate": 2.3508534730437724e-05,
      "loss": 0.6769,
      "step": 13040
    },
    {
      "epoch": 2.159880834160874,
      "grad_norm": 7.1274614334106445,
      "learning_rate": 2.3487409160047322e-05,
      "loss": 0.6076,
      "step": 13050
    },
    {
      "epoch": 2.1615359152598477,
      "grad_norm": 9.482973098754883,
      "learning_rate": 2.346628358965692e-05,
      "loss": 0.6062,
      "step": 13060
    },
    {
      "epoch": 2.1631909963588214,
      "grad_norm": 2.1896820068359375,
      "learning_rate": 2.3445158019266522e-05,
      "loss": 0.5286,
      "step": 13070
    },
    {
      "epoch": 2.1648460774577956,
      "grad_norm": 6.167243480682373,
      "learning_rate": 2.342403244887612e-05,
      "loss": 0.9114,
      "step": 13080
    },
    {
      "epoch": 2.1665011585567693,
      "grad_norm": 4.880818843841553,
      "learning_rate": 2.340290687848572e-05,
      "loss": 0.3147,
      "step": 13090
    },
    {
      "epoch": 2.168156239655743,
      "grad_norm": 9.225013732910156,
      "learning_rate": 2.3381781308095317e-05,
      "loss": 0.7916,
      "step": 13100
    },
    {
      "epoch": 2.169811320754717,
      "grad_norm": 7.458749771118164,
      "learning_rate": 2.336065573770492e-05,
      "loss": 0.6615,
      "step": 13110
    },
    {
      "epoch": 2.171466401853691,
      "grad_norm": 9.527663230895996,
      "learning_rate": 2.3339530167314517e-05,
      "loss": 0.6756,
      "step": 13120
    },
    {
      "epoch": 2.1731214829526646,
      "grad_norm": 11.071285247802734,
      "learning_rate": 2.331840459692412e-05,
      "loss": 0.8219,
      "step": 13130
    },
    {
      "epoch": 2.1747765640516383,
      "grad_norm": 6.970534324645996,
      "learning_rate": 2.3297279026533718e-05,
      "loss": 0.5271,
      "step": 13140
    },
    {
      "epoch": 2.1764316451506125,
      "grad_norm": 9.485676765441895,
      "learning_rate": 2.327615345614332e-05,
      "loss": 0.7407,
      "step": 13150
    },
    {
      "epoch": 2.178086726249586,
      "grad_norm": 3.9959166049957275,
      "learning_rate": 2.3255027885752918e-05,
      "loss": 0.5515,
      "step": 13160
    },
    {
      "epoch": 2.17974180734856,
      "grad_norm": 3.066304922103882,
      "learning_rate": 2.3233902315362516e-05,
      "loss": 0.3869,
      "step": 13170
    },
    {
      "epoch": 2.181396888447534,
      "grad_norm": 5.823259353637695,
      "learning_rate": 2.3212776744972115e-05,
      "loss": 0.3909,
      "step": 13180
    },
    {
      "epoch": 2.183051969546508,
      "grad_norm": 8.340502738952637,
      "learning_rate": 2.3191651174581716e-05,
      "loss": 0.4774,
      "step": 13190
    },
    {
      "epoch": 2.1847070506454815,
      "grad_norm": 7.913177967071533,
      "learning_rate": 2.3170525604191315e-05,
      "loss": 0.4581,
      "step": 13200
    },
    {
      "epoch": 2.1863621317444553,
      "grad_norm": 4.392423152923584,
      "learning_rate": 2.3149400033800913e-05,
      "loss": 0.5632,
      "step": 13210
    },
    {
      "epoch": 2.1880172128434294,
      "grad_norm": 5.3960771560668945,
      "learning_rate": 2.312827446341051e-05,
      "loss": 0.4453,
      "step": 13220
    },
    {
      "epoch": 2.189672293942403,
      "grad_norm": 4.433848857879639,
      "learning_rate": 2.310714889302011e-05,
      "loss": 0.7487,
      "step": 13230
    },
    {
      "epoch": 2.191327375041377,
      "grad_norm": 1.108404278755188,
      "learning_rate": 2.308602332262971e-05,
      "loss": 0.5732,
      "step": 13240
    },
    {
      "epoch": 2.192982456140351,
      "grad_norm": 13.03085994720459,
      "learning_rate": 2.306489775223931e-05,
      "loss": 0.4058,
      "step": 13250
    },
    {
      "epoch": 2.1946375372393248,
      "grad_norm": 7.7609663009643555,
      "learning_rate": 2.3043772181848912e-05,
      "loss": 0.4648,
      "step": 13260
    },
    {
      "epoch": 2.1962926183382985,
      "grad_norm": 6.320751190185547,
      "learning_rate": 2.302264661145851e-05,
      "loss": 0.8297,
      "step": 13270
    },
    {
      "epoch": 2.1979476994372726,
      "grad_norm": 7.078385353088379,
      "learning_rate": 2.3001521041068112e-05,
      "loss": 0.5302,
      "step": 13280
    },
    {
      "epoch": 2.1996027805362464,
      "grad_norm": 1.2695754766464233,
      "learning_rate": 2.298039547067771e-05,
      "loss": 0.7415,
      "step": 13290
    },
    {
      "epoch": 2.20125786163522,
      "grad_norm": 7.02971076965332,
      "learning_rate": 2.295926990028731e-05,
      "loss": 0.7157,
      "step": 13300
    },
    {
      "epoch": 2.202912942734194,
      "grad_norm": 9.628299713134766,
      "learning_rate": 2.2938144329896907e-05,
      "loss": 0.5877,
      "step": 13310
    },
    {
      "epoch": 2.204568023833168,
      "grad_norm": 10.41122817993164,
      "learning_rate": 2.291701875950651e-05,
      "loss": 0.788,
      "step": 13320
    },
    {
      "epoch": 2.2062231049321417,
      "grad_norm": 9.158013343811035,
      "learning_rate": 2.2895893189116107e-05,
      "loss": 0.5389,
      "step": 13330
    },
    {
      "epoch": 2.2078781860311154,
      "grad_norm": 13.282702445983887,
      "learning_rate": 2.2874767618725706e-05,
      "loss": 0.6921,
      "step": 13340
    },
    {
      "epoch": 2.2095332671300896,
      "grad_norm": 2.025907039642334,
      "learning_rate": 2.2853642048335304e-05,
      "loss": 0.438,
      "step": 13350
    },
    {
      "epoch": 2.2111883482290633,
      "grad_norm": 12.74975872039795,
      "learning_rate": 2.2832516477944906e-05,
      "loss": 0.6234,
      "step": 13360
    },
    {
      "epoch": 2.212843429328037,
      "grad_norm": 6.582385063171387,
      "learning_rate": 2.2811390907554504e-05,
      "loss": 0.5551,
      "step": 13370
    },
    {
      "epoch": 2.2144985104270107,
      "grad_norm": 8.639120101928711,
      "learning_rate": 2.2790265337164106e-05,
      "loss": 0.8059,
      "step": 13380
    },
    {
      "epoch": 2.216153591525985,
      "grad_norm": 4.3446807861328125,
      "learning_rate": 2.2769139766773704e-05,
      "loss": 0.6399,
      "step": 13390
    },
    {
      "epoch": 2.2178086726249586,
      "grad_norm": 3.619426965713501,
      "learning_rate": 2.2748014196383303e-05,
      "loss": 0.5011,
      "step": 13400
    },
    {
      "epoch": 2.2194637537239323,
      "grad_norm": 6.72926664352417,
      "learning_rate": 2.2726888625992904e-05,
      "loss": 0.4498,
      "step": 13410
    },
    {
      "epoch": 2.2211188348229065,
      "grad_norm": 8.792291641235352,
      "learning_rate": 2.2705763055602503e-05,
      "loss": 0.6661,
      "step": 13420
    },
    {
      "epoch": 2.2227739159218802,
      "grad_norm": 1.8787288665771484,
      "learning_rate": 2.26846374852121e-05,
      "loss": 0.5008,
      "step": 13430
    },
    {
      "epoch": 2.224428997020854,
      "grad_norm": 8.74829387664795,
      "learning_rate": 2.26635119148217e-05,
      "loss": 0.5652,
      "step": 13440
    },
    {
      "epoch": 2.2260840781198277,
      "grad_norm": 11.251057624816895,
      "learning_rate": 2.26423863444313e-05,
      "loss": 0.676,
      "step": 13450
    },
    {
      "epoch": 2.227739159218802,
      "grad_norm": 7.536021709442139,
      "learning_rate": 2.26212607740409e-05,
      "loss": 0.6098,
      "step": 13460
    },
    {
      "epoch": 2.2293942403177756,
      "grad_norm": 9.606626510620117,
      "learning_rate": 2.2600135203650498e-05,
      "loss": 0.7046,
      "step": 13470
    },
    {
      "epoch": 2.2310493214167493,
      "grad_norm": 10.4229097366333,
      "learning_rate": 2.2579009633260097e-05,
      "loss": 0.7989,
      "step": 13480
    },
    {
      "epoch": 2.2327044025157234,
      "grad_norm": 4.638187885284424,
      "learning_rate": 2.25578840628697e-05,
      "loss": 0.5837,
      "step": 13490
    },
    {
      "epoch": 2.234359483614697,
      "grad_norm": 10.353434562683105,
      "learning_rate": 2.25367584924793e-05,
      "loss": 0.4938,
      "step": 13500
    },
    {
      "epoch": 2.236014564713671,
      "grad_norm": 0.3662639856338501,
      "learning_rate": 2.25156329220889e-05,
      "loss": 0.633,
      "step": 13510
    },
    {
      "epoch": 2.2376696458126446,
      "grad_norm": 4.5210795402526855,
      "learning_rate": 2.2494507351698497e-05,
      "loss": 0.4336,
      "step": 13520
    },
    {
      "epoch": 2.2393247269116188,
      "grad_norm": 7.731559753417969,
      "learning_rate": 2.24733817813081e-05,
      "loss": 0.5727,
      "step": 13530
    },
    {
      "epoch": 2.2409798080105925,
      "grad_norm": 16.090728759765625,
      "learning_rate": 2.2452256210917697e-05,
      "loss": 0.6484,
      "step": 13540
    },
    {
      "epoch": 2.242634889109566,
      "grad_norm": 8.082454681396484,
      "learning_rate": 2.2431130640527295e-05,
      "loss": 0.9649,
      "step": 13550
    },
    {
      "epoch": 2.2442899702085404,
      "grad_norm": 12.144664764404297,
      "learning_rate": 2.2410005070136894e-05,
      "loss": 0.4418,
      "step": 13560
    },
    {
      "epoch": 2.245945051307514,
      "grad_norm": 9.5991849899292,
      "learning_rate": 2.2388879499746492e-05,
      "loss": 0.5569,
      "step": 13570
    },
    {
      "epoch": 2.247600132406488,
      "grad_norm": 3.5887773036956787,
      "learning_rate": 2.2367753929356094e-05,
      "loss": 0.6636,
      "step": 13580
    },
    {
      "epoch": 2.249255213505462,
      "grad_norm": 5.042080879211426,
      "learning_rate": 2.2346628358965692e-05,
      "loss": 0.6122,
      "step": 13590
    },
    {
      "epoch": 2.2509102946044357,
      "grad_norm": 7.918135166168213,
      "learning_rate": 2.232550278857529e-05,
      "loss": 0.6689,
      "step": 13600
    },
    {
      "epoch": 2.2525653757034094,
      "grad_norm": 8.002357482910156,
      "learning_rate": 2.2304377218184892e-05,
      "loss": 0.4207,
      "step": 13610
    },
    {
      "epoch": 2.254220456802383,
      "grad_norm": 6.4409308433532715,
      "learning_rate": 2.228325164779449e-05,
      "loss": 0.3744,
      "step": 13620
    },
    {
      "epoch": 2.2558755379013573,
      "grad_norm": 6.8521623611450195,
      "learning_rate": 2.2262126077404093e-05,
      "loss": 0.609,
      "step": 13630
    },
    {
      "epoch": 2.257530619000331,
      "grad_norm": 13.002952575683594,
      "learning_rate": 2.224100050701369e-05,
      "loss": 0.5582,
      "step": 13640
    },
    {
      "epoch": 2.2591857000993047,
      "grad_norm": 1.8713867664337158,
      "learning_rate": 2.221987493662329e-05,
      "loss": 0.5789,
      "step": 13650
    },
    {
      "epoch": 2.2608407811982785,
      "grad_norm": 14.237205505371094,
      "learning_rate": 2.219874936623289e-05,
      "loss": 0.6738,
      "step": 13660
    },
    {
      "epoch": 2.2624958622972526,
      "grad_norm": 15.801751136779785,
      "learning_rate": 2.217762379584249e-05,
      "loss": 0.7351,
      "step": 13670
    },
    {
      "epoch": 2.2641509433962264,
      "grad_norm": 8.614109992980957,
      "learning_rate": 2.2156498225452088e-05,
      "loss": 0.5271,
      "step": 13680
    },
    {
      "epoch": 2.2658060244952,
      "grad_norm": 7.915506362915039,
      "learning_rate": 2.2135372655061686e-05,
      "loss": 0.4634,
      "step": 13690
    },
    {
      "epoch": 2.2674611055941742,
      "grad_norm": 8.758493423461914,
      "learning_rate": 2.2114247084671288e-05,
      "loss": 0.781,
      "step": 13700
    },
    {
      "epoch": 2.269116186693148,
      "grad_norm": 5.591545104980469,
      "learning_rate": 2.2093121514280886e-05,
      "loss": 0.5078,
      "step": 13710
    },
    {
      "epoch": 2.2707712677921217,
      "grad_norm": 4.436402320861816,
      "learning_rate": 2.2071995943890485e-05,
      "loss": 0.7415,
      "step": 13720
    },
    {
      "epoch": 2.272426348891096,
      "grad_norm": 8.306211471557617,
      "learning_rate": 2.2050870373500083e-05,
      "loss": 0.8071,
      "step": 13730
    },
    {
      "epoch": 2.2740814299900696,
      "grad_norm": 10.801403999328613,
      "learning_rate": 2.2029744803109685e-05,
      "loss": 0.5877,
      "step": 13740
    },
    {
      "epoch": 2.2757365110890433,
      "grad_norm": 10.663542747497559,
      "learning_rate": 2.2008619232719287e-05,
      "loss": 0.5529,
      "step": 13750
    },
    {
      "epoch": 2.2773915921880175,
      "grad_norm": 9.147966384887695,
      "learning_rate": 2.1987493662328885e-05,
      "loss": 0.7592,
      "step": 13760
    },
    {
      "epoch": 2.279046673286991,
      "grad_norm": 10.519007682800293,
      "learning_rate": 2.1966368091938484e-05,
      "loss": 0.6765,
      "step": 13770
    },
    {
      "epoch": 2.280701754385965,
      "grad_norm": 9.309673309326172,
      "learning_rate": 2.1945242521548082e-05,
      "loss": 0.7776,
      "step": 13780
    },
    {
      "epoch": 2.2823568354849386,
      "grad_norm": 5.826881408691406,
      "learning_rate": 2.1924116951157684e-05,
      "loss": 0.6498,
      "step": 13790
    },
    {
      "epoch": 2.284011916583913,
      "grad_norm": 4.7942118644714355,
      "learning_rate": 2.1902991380767282e-05,
      "loss": 0.5363,
      "step": 13800
    },
    {
      "epoch": 2.2856669976828865,
      "grad_norm": 15.153264045715332,
      "learning_rate": 2.188186581037688e-05,
      "loss": 0.8684,
      "step": 13810
    },
    {
      "epoch": 2.2873220787818602,
      "grad_norm": 8.63984489440918,
      "learning_rate": 2.186074023998648e-05,
      "loss": 0.6676,
      "step": 13820
    },
    {
      "epoch": 2.288977159880834,
      "grad_norm": 11.197067260742188,
      "learning_rate": 2.183961466959608e-05,
      "loss": 0.5107,
      "step": 13830
    },
    {
      "epoch": 2.290632240979808,
      "grad_norm": 7.896108150482178,
      "learning_rate": 2.181848909920568e-05,
      "loss": 0.4993,
      "step": 13840
    },
    {
      "epoch": 2.292287322078782,
      "grad_norm": 13.843343734741211,
      "learning_rate": 2.1797363528815277e-05,
      "loss": 0.7243,
      "step": 13850
    },
    {
      "epoch": 2.2939424031777556,
      "grad_norm": 8.772977828979492,
      "learning_rate": 2.177623795842488e-05,
      "loss": 0.7388,
      "step": 13860
    },
    {
      "epoch": 2.2955974842767297,
      "grad_norm": 7.105792999267578,
      "learning_rate": 2.175511238803448e-05,
      "loss": 0.4938,
      "step": 13870
    },
    {
      "epoch": 2.2972525653757034,
      "grad_norm": 4.582803726196289,
      "learning_rate": 2.173398681764408e-05,
      "loss": 0.6496,
      "step": 13880
    },
    {
      "epoch": 2.298907646474677,
      "grad_norm": 3.72663950920105,
      "learning_rate": 2.1712861247253678e-05,
      "loss": 0.821,
      "step": 13890
    },
    {
      "epoch": 2.3005627275736513,
      "grad_norm": 4.0633978843688965,
      "learning_rate": 2.1691735676863276e-05,
      "loss": 0.6937,
      "step": 13900
    },
    {
      "epoch": 2.302217808672625,
      "grad_norm": 1.6835578680038452,
      "learning_rate": 2.1670610106472874e-05,
      "loss": 0.968,
      "step": 13910
    },
    {
      "epoch": 2.3038728897715988,
      "grad_norm": 8.172380447387695,
      "learning_rate": 2.1649484536082476e-05,
      "loss": 0.5874,
      "step": 13920
    },
    {
      "epoch": 2.3055279708705725,
      "grad_norm": 5.603165626525879,
      "learning_rate": 2.1628358965692075e-05,
      "loss": 0.8133,
      "step": 13930
    },
    {
      "epoch": 2.3071830519695466,
      "grad_norm": 2.296903610229492,
      "learning_rate": 2.1607233395301673e-05,
      "loss": 0.5117,
      "step": 13940
    },
    {
      "epoch": 2.3088381330685204,
      "grad_norm": 3.433790922164917,
      "learning_rate": 2.158610782491127e-05,
      "loss": 0.7219,
      "step": 13950
    },
    {
      "epoch": 2.310493214167494,
      "grad_norm": 6.168792247772217,
      "learning_rate": 2.1564982254520873e-05,
      "loss": 0.5958,
      "step": 13960
    },
    {
      "epoch": 2.312148295266468,
      "grad_norm": 14.452911376953125,
      "learning_rate": 2.154385668413047e-05,
      "loss": 0.6363,
      "step": 13970
    },
    {
      "epoch": 2.313803376365442,
      "grad_norm": 6.123717784881592,
      "learning_rate": 2.1522731113740073e-05,
      "loss": 0.5493,
      "step": 13980
    },
    {
      "epoch": 2.3154584574644157,
      "grad_norm": 5.903880596160889,
      "learning_rate": 2.1501605543349672e-05,
      "loss": 0.7145,
      "step": 13990
    },
    {
      "epoch": 2.3171135385633894,
      "grad_norm": 5.8749680519104,
      "learning_rate": 2.1480479972959273e-05,
      "loss": 0.5426,
      "step": 14000
    },
    {
      "epoch": 2.3187686196623636,
      "grad_norm": 7.532459735870361,
      "learning_rate": 2.1459354402568872e-05,
      "loss": 0.6161,
      "step": 14010
    },
    {
      "epoch": 2.3204237007613373,
      "grad_norm": 5.321092128753662,
      "learning_rate": 2.143822883217847e-05,
      "loss": 0.5104,
      "step": 14020
    },
    {
      "epoch": 2.322078781860311,
      "grad_norm": 11.370227813720703,
      "learning_rate": 2.141710326178807e-05,
      "loss": 0.8395,
      "step": 14030
    },
    {
      "epoch": 2.323733862959285,
      "grad_norm": 0.7402486205101013,
      "learning_rate": 2.139597769139767e-05,
      "loss": 0.4725,
      "step": 14040
    },
    {
      "epoch": 2.325388944058259,
      "grad_norm": 4.379803657531738,
      "learning_rate": 2.137485212100727e-05,
      "loss": 0.8716,
      "step": 14050
    },
    {
      "epoch": 2.3270440251572326,
      "grad_norm": 1.7059239149093628,
      "learning_rate": 2.1353726550616867e-05,
      "loss": 0.3366,
      "step": 14060
    },
    {
      "epoch": 2.328699106256207,
      "grad_norm": 7.963507175445557,
      "learning_rate": 2.1332600980226466e-05,
      "loss": 0.8131,
      "step": 14070
    },
    {
      "epoch": 2.3303541873551805,
      "grad_norm": 10.179487228393555,
      "learning_rate": 2.1311475409836064e-05,
      "loss": 0.5779,
      "step": 14080
    },
    {
      "epoch": 2.3320092684541542,
      "grad_norm": 3.0391411781311035,
      "learning_rate": 2.1290349839445666e-05,
      "loss": 0.7338,
      "step": 14090
    },
    {
      "epoch": 2.333664349553128,
      "grad_norm": 6.557188034057617,
      "learning_rate": 2.1269224269055264e-05,
      "loss": 0.5208,
      "step": 14100
    },
    {
      "epoch": 2.335319430652102,
      "grad_norm": 3.2459499835968018,
      "learning_rate": 2.1248098698664866e-05,
      "loss": 0.9565,
      "step": 14110
    },
    {
      "epoch": 2.336974511751076,
      "grad_norm": 6.293309211730957,
      "learning_rate": 2.1226973128274464e-05,
      "loss": 0.731,
      "step": 14120
    },
    {
      "epoch": 2.3386295928500496,
      "grad_norm": 10.665974617004395,
      "learning_rate": 2.1205847557884066e-05,
      "loss": 0.7704,
      "step": 14130
    },
    {
      "epoch": 2.3402846739490233,
      "grad_norm": 5.423940658569336,
      "learning_rate": 2.1184721987493664e-05,
      "loss": 0.5436,
      "step": 14140
    },
    {
      "epoch": 2.3419397550479975,
      "grad_norm": 8.765881538391113,
      "learning_rate": 2.1163596417103263e-05,
      "loss": 0.7135,
      "step": 14150
    },
    {
      "epoch": 2.343594836146971,
      "grad_norm": 2.277557611465454,
      "learning_rate": 2.114247084671286e-05,
      "loss": 0.4462,
      "step": 14160
    },
    {
      "epoch": 2.345249917245945,
      "grad_norm": 6.194435119628906,
      "learning_rate": 2.1121345276322463e-05,
      "loss": 0.632,
      "step": 14170
    },
    {
      "epoch": 2.346904998344919,
      "grad_norm": 7.072089195251465,
      "learning_rate": 2.110021970593206e-05,
      "loss": 0.3701,
      "step": 14180
    },
    {
      "epoch": 2.348560079443893,
      "grad_norm": 12.563509941101074,
      "learning_rate": 2.107909413554166e-05,
      "loss": 0.6144,
      "step": 14190
    },
    {
      "epoch": 2.3502151605428665,
      "grad_norm": 5.652981758117676,
      "learning_rate": 2.1057968565151258e-05,
      "loss": 0.7027,
      "step": 14200
    },
    {
      "epoch": 2.3518702416418407,
      "grad_norm": 5.572032928466797,
      "learning_rate": 2.103684299476086e-05,
      "loss": 0.6688,
      "step": 14210
    },
    {
      "epoch": 2.3535253227408144,
      "grad_norm": 9.274613380432129,
      "learning_rate": 2.1015717424370458e-05,
      "loss": 0.5486,
      "step": 14220
    },
    {
      "epoch": 2.355180403839788,
      "grad_norm": 10.010333061218262,
      "learning_rate": 2.099459185398006e-05,
      "loss": 0.5205,
      "step": 14230
    },
    {
      "epoch": 2.356835484938762,
      "grad_norm": 13.318350791931152,
      "learning_rate": 2.097346628358966e-05,
      "loss": 0.6547,
      "step": 14240
    },
    {
      "epoch": 2.358490566037736,
      "grad_norm": 4.498316764831543,
      "learning_rate": 2.0952340713199257e-05,
      "loss": 0.3663,
      "step": 14250
    },
    {
      "epoch": 2.3601456471367097,
      "grad_norm": 7.748896598815918,
      "learning_rate": 2.093121514280886e-05,
      "loss": 0.5204,
      "step": 14260
    },
    {
      "epoch": 2.3618007282356834,
      "grad_norm": 7.990764141082764,
      "learning_rate": 2.0910089572418457e-05,
      "loss": 0.5611,
      "step": 14270
    },
    {
      "epoch": 2.363455809334657,
      "grad_norm": 7.809503555297852,
      "learning_rate": 2.0888964002028055e-05,
      "loss": 0.5752,
      "step": 14280
    },
    {
      "epoch": 2.3651108904336313,
      "grad_norm": 4.948361396789551,
      "learning_rate": 2.0867838431637654e-05,
      "loss": 0.7291,
      "step": 14290
    },
    {
      "epoch": 2.366765971532605,
      "grad_norm": 0.7698342800140381,
      "learning_rate": 2.0846712861247255e-05,
      "loss": 0.3881,
      "step": 14300
    },
    {
      "epoch": 2.3684210526315788,
      "grad_norm": 5.419276237487793,
      "learning_rate": 2.0825587290856854e-05,
      "loss": 0.4729,
      "step": 14310
    },
    {
      "epoch": 2.370076133730553,
      "grad_norm": 20.576087951660156,
      "learning_rate": 2.0804461720466452e-05,
      "loss": 0.7596,
      "step": 14320
    },
    {
      "epoch": 2.3717312148295266,
      "grad_norm": 5.749523162841797,
      "learning_rate": 2.078333615007605e-05,
      "loss": 0.7438,
      "step": 14330
    },
    {
      "epoch": 2.3733862959285004,
      "grad_norm": 8.38940715789795,
      "learning_rate": 2.0762210579685652e-05,
      "loss": 0.5347,
      "step": 14340
    },
    {
      "epoch": 2.3750413770274745,
      "grad_norm": 12.02057933807373,
      "learning_rate": 2.074108500929525e-05,
      "loss": 0.7013,
      "step": 14350
    },
    {
      "epoch": 2.3766964581264483,
      "grad_norm": 5.198694705963135,
      "learning_rate": 2.0719959438904853e-05,
      "loss": 0.8944,
      "step": 14360
    },
    {
      "epoch": 2.378351539225422,
      "grad_norm": 8.762221336364746,
      "learning_rate": 2.069883386851445e-05,
      "loss": 0.6271,
      "step": 14370
    },
    {
      "epoch": 2.380006620324396,
      "grad_norm": 4.581204414367676,
      "learning_rate": 2.0677708298124053e-05,
      "loss": 0.6009,
      "step": 14380
    },
    {
      "epoch": 2.38166170142337,
      "grad_norm": 1.78107488155365,
      "learning_rate": 2.065658272773365e-05,
      "loss": 0.8006,
      "step": 14390
    },
    {
      "epoch": 2.3833167825223436,
      "grad_norm": 18.377304077148438,
      "learning_rate": 2.063545715734325e-05,
      "loss": 0.7916,
      "step": 14400
    },
    {
      "epoch": 2.3849718636213173,
      "grad_norm": 8.341451644897461,
      "learning_rate": 2.0614331586952848e-05,
      "loss": 0.7456,
      "step": 14410
    },
    {
      "epoch": 2.3866269447202915,
      "grad_norm": 15.370765686035156,
      "learning_rate": 2.0593206016562446e-05,
      "loss": 0.574,
      "step": 14420
    },
    {
      "epoch": 2.388282025819265,
      "grad_norm": 9.58170223236084,
      "learning_rate": 2.0572080446172048e-05,
      "loss": 0.8919,
      "step": 14430
    },
    {
      "epoch": 2.389937106918239,
      "grad_norm": 3.734553337097168,
      "learning_rate": 2.0550954875781646e-05,
      "loss": 0.6042,
      "step": 14440
    },
    {
      "epoch": 2.3915921880172126,
      "grad_norm": 6.296032428741455,
      "learning_rate": 2.0529829305391245e-05,
      "loss": 0.5494,
      "step": 14450
    },
    {
      "epoch": 2.393247269116187,
      "grad_norm": 5.003880500793457,
      "learning_rate": 2.0508703735000847e-05,
      "loss": 0.7609,
      "step": 14460
    },
    {
      "epoch": 2.3949023502151605,
      "grad_norm": 2.7977471351623535,
      "learning_rate": 2.0487578164610445e-05,
      "loss": 0.4949,
      "step": 14470
    },
    {
      "epoch": 2.3965574313141342,
      "grad_norm": 4.802281379699707,
      "learning_rate": 2.0466452594220047e-05,
      "loss": 0.5808,
      "step": 14480
    },
    {
      "epoch": 2.3982125124131084,
      "grad_norm": 6.903770446777344,
      "learning_rate": 2.0445327023829645e-05,
      "loss": 0.4562,
      "step": 14490
    },
    {
      "epoch": 2.399867593512082,
      "grad_norm": 1.8155772686004639,
      "learning_rate": 2.0424201453439243e-05,
      "loss": 0.7469,
      "step": 14500
    }
  ],
  "logging_steps": 10,
  "max_steps": 24168,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 2417,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.085801104825344e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
