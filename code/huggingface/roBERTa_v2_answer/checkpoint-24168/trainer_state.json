{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 24168,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016550810989738498,
      "grad_norm": 10.491287231445312,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 3.6997,
      "step": 10
    },
    {
      "epoch": 0.0033101621979476996,
      "grad_norm": 8.1528959274292,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 4.0751,
      "step": 20
    },
    {
      "epoch": 0.004965243296921549,
      "grad_norm": 11.886052131652832,
      "learning_rate": 3e-06,
      "loss": 3.9032,
      "step": 30
    },
    {
      "epoch": 0.006620324395895399,
      "grad_norm": 5.415994167327881,
      "learning_rate": 4.000000000000001e-06,
      "loss": 3.4249,
      "step": 40
    },
    {
      "epoch": 0.00827540549486925,
      "grad_norm": 12.988325119018555,
      "learning_rate": 5e-06,
      "loss": 3.5436,
      "step": 50
    },
    {
      "epoch": 0.009930486593843098,
      "grad_norm": 11.426213264465332,
      "learning_rate": 6e-06,
      "loss": 4.8924,
      "step": 60
    },
    {
      "epoch": 0.011585567692816948,
      "grad_norm": 9.009767532348633,
      "learning_rate": 7.000000000000001e-06,
      "loss": 4.0708,
      "step": 70
    },
    {
      "epoch": 0.013240648791790799,
      "grad_norm": 5.40336275100708,
      "learning_rate": 8.000000000000001e-06,
      "loss": 3.6271,
      "step": 80
    },
    {
      "epoch": 0.014895729890764648,
      "grad_norm": 14.442776679992676,
      "learning_rate": 9e-06,
      "loss": 3.5951,
      "step": 90
    },
    {
      "epoch": 0.0165508109897385,
      "grad_norm": 7.636654853820801,
      "learning_rate": 1e-05,
      "loss": 3.6199,
      "step": 100
    },
    {
      "epoch": 0.018205892088712348,
      "grad_norm": 7.5129075050354,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 3.2084,
      "step": 110
    },
    {
      "epoch": 0.019860973187686197,
      "grad_norm": 10.015198707580566,
      "learning_rate": 1.2e-05,
      "loss": 3.9251,
      "step": 120
    },
    {
      "epoch": 0.021516054286660046,
      "grad_norm": 13.5173978805542,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 4.1535,
      "step": 130
    },
    {
      "epoch": 0.023171135385633895,
      "grad_norm": 8.180093765258789,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 3.5428,
      "step": 140
    },
    {
      "epoch": 0.024826216484607744,
      "grad_norm": 2.851651906967163,
      "learning_rate": 1.5e-05,
      "loss": 2.7841,
      "step": 150
    },
    {
      "epoch": 0.026481297583581597,
      "grad_norm": 11.961166381835938,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 2.3978,
      "step": 160
    },
    {
      "epoch": 0.028136378682555446,
      "grad_norm": 8.412672996520996,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 2.4668,
      "step": 170
    },
    {
      "epoch": 0.029791459781529295,
      "grad_norm": 14.415621757507324,
      "learning_rate": 1.8e-05,
      "loss": 2.8025,
      "step": 180
    },
    {
      "epoch": 0.031446540880503145,
      "grad_norm": 12.371260643005371,
      "learning_rate": 1.9e-05,
      "loss": 2.2191,
      "step": 190
    },
    {
      "epoch": 0.033101621979477,
      "grad_norm": 15.451358795166016,
      "learning_rate": 2e-05,
      "loss": 2.3608,
      "step": 200
    },
    {
      "epoch": 0.03475670307845084,
      "grad_norm": 7.22649621963501,
      "learning_rate": 2.1e-05,
      "loss": 1.8181,
      "step": 210
    },
    {
      "epoch": 0.036411784177424696,
      "grad_norm": 4.971599578857422,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.7533,
      "step": 220
    },
    {
      "epoch": 0.03806686527639854,
      "grad_norm": 4.988760948181152,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 1.8505,
      "step": 230
    },
    {
      "epoch": 0.039721946375372394,
      "grad_norm": 3.332304000854492,
      "learning_rate": 2.4e-05,
      "loss": 1.8816,
      "step": 240
    },
    {
      "epoch": 0.04137702747434624,
      "grad_norm": 7.057027339935303,
      "learning_rate": 2.5e-05,
      "loss": 1.6548,
      "step": 250
    },
    {
      "epoch": 0.04303210857332009,
      "grad_norm": 7.131735324859619,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.9387,
      "step": 260
    },
    {
      "epoch": 0.044687189672293945,
      "grad_norm": 8.290486335754395,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.8058,
      "step": 270
    },
    {
      "epoch": 0.04634227077126779,
      "grad_norm": 5.917972087860107,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.8249,
      "step": 280
    },
    {
      "epoch": 0.04799735187024164,
      "grad_norm": 6.318717002868652,
      "learning_rate": 2.9e-05,
      "loss": 1.2006,
      "step": 290
    },
    {
      "epoch": 0.04965243296921549,
      "grad_norm": 5.791667938232422,
      "learning_rate": 3e-05,
      "loss": 2.1717,
      "step": 300
    },
    {
      "epoch": 0.05130751406818934,
      "grad_norm": 5.035686492919922,
      "learning_rate": 3.1e-05,
      "loss": 1.9212,
      "step": 310
    },
    {
      "epoch": 0.052962595167163194,
      "grad_norm": 7.047187805175781,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.7182,
      "step": 320
    },
    {
      "epoch": 0.05461767626613704,
      "grad_norm": 5.455808639526367,
      "learning_rate": 3.3e-05,
      "loss": 1.352,
      "step": 330
    },
    {
      "epoch": 0.05627275736511089,
      "grad_norm": 7.1284308433532715,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.2526,
      "step": 340
    },
    {
      "epoch": 0.05792783846408474,
      "grad_norm": 2.6936628818511963,
      "learning_rate": 3.5e-05,
      "loss": 1.4086,
      "step": 350
    },
    {
      "epoch": 0.05958291956305859,
      "grad_norm": 6.265352249145508,
      "learning_rate": 3.6e-05,
      "loss": 1.4256,
      "step": 360
    },
    {
      "epoch": 0.06123800066203244,
      "grad_norm": 6.356632709503174,
      "learning_rate": 3.7e-05,
      "loss": 1.6221,
      "step": 370
    },
    {
      "epoch": 0.06289308176100629,
      "grad_norm": 4.151512145996094,
      "learning_rate": 3.8e-05,
      "loss": 1.2793,
      "step": 380
    },
    {
      "epoch": 0.06454816285998013,
      "grad_norm": 5.9260993003845215,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.6107,
      "step": 390
    },
    {
      "epoch": 0.066203243958954,
      "grad_norm": 4.526922225952148,
      "learning_rate": 4e-05,
      "loss": 1.5396,
      "step": 400
    },
    {
      "epoch": 0.06785832505792784,
      "grad_norm": 5.142249584197998,
      "learning_rate": 4.1e-05,
      "loss": 1.5912,
      "step": 410
    },
    {
      "epoch": 0.06951340615690169,
      "grad_norm": 5.746337413787842,
      "learning_rate": 4.2e-05,
      "loss": 1.4797,
      "step": 420
    },
    {
      "epoch": 0.07116848725587553,
      "grad_norm": 2.304018497467041,
      "learning_rate": 4.3e-05,
      "loss": 0.9609,
      "step": 430
    },
    {
      "epoch": 0.07282356835484939,
      "grad_norm": 6.13914155960083,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.1815,
      "step": 440
    },
    {
      "epoch": 0.07447864945382324,
      "grad_norm": 6.120832443237305,
      "learning_rate": 4.5e-05,
      "loss": 1.6981,
      "step": 450
    },
    {
      "epoch": 0.07613373055279708,
      "grad_norm": 3.058312177658081,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.3897,
      "step": 460
    },
    {
      "epoch": 0.07778881165177094,
      "grad_norm": 7.008513927459717,
      "learning_rate": 4.7e-05,
      "loss": 0.8884,
      "step": 470
    },
    {
      "epoch": 0.07944389275074479,
      "grad_norm": 8.14610481262207,
      "learning_rate": 4.8e-05,
      "loss": 1.8417,
      "step": 480
    },
    {
      "epoch": 0.08109897384971863,
      "grad_norm": 9.467963218688965,
      "learning_rate": 4.9e-05,
      "loss": 1.4363,
      "step": 490
    },
    {
      "epoch": 0.08275405494869248,
      "grad_norm": 6.290310859680176,
      "learning_rate": 5e-05,
      "loss": 1.2156,
      "step": 500
    },
    {
      "epoch": 0.08440913604766634,
      "grad_norm": 10.386557579040527,
      "learning_rate": 4.99788744296096e-05,
      "loss": 1.0915,
      "step": 510
    },
    {
      "epoch": 0.08606421714664018,
      "grad_norm": 4.732298851013184,
      "learning_rate": 4.99577488592192e-05,
      "loss": 1.0337,
      "step": 520
    },
    {
      "epoch": 0.08771929824561403,
      "grad_norm": 7.166783809661865,
      "learning_rate": 4.99366232888288e-05,
      "loss": 1.3134,
      "step": 530
    },
    {
      "epoch": 0.08937437934458789,
      "grad_norm": 8.70981502532959,
      "learning_rate": 4.9915497718438396e-05,
      "loss": 1.4855,
      "step": 540
    },
    {
      "epoch": 0.09102946044356174,
      "grad_norm": 9.945058822631836,
      "learning_rate": 4.9894372148048e-05,
      "loss": 1.2601,
      "step": 550
    },
    {
      "epoch": 0.09268454154253558,
      "grad_norm": 3.8860156536102295,
      "learning_rate": 4.98732465776576e-05,
      "loss": 0.9946,
      "step": 560
    },
    {
      "epoch": 0.09433962264150944,
      "grad_norm": 5.446142673492432,
      "learning_rate": 4.98521210072672e-05,
      "loss": 1.2979,
      "step": 570
    },
    {
      "epoch": 0.09599470374048329,
      "grad_norm": 13.52455997467041,
      "learning_rate": 4.9830995436876796e-05,
      "loss": 1.1829,
      "step": 580
    },
    {
      "epoch": 0.09764978483945713,
      "grad_norm": 6.01290225982666,
      "learning_rate": 4.9809869866486395e-05,
      "loss": 1.4549,
      "step": 590
    },
    {
      "epoch": 0.09930486593843098,
      "grad_norm": 6.11596155166626,
      "learning_rate": 4.978874429609599e-05,
      "loss": 1.1775,
      "step": 600
    },
    {
      "epoch": 0.10095994703740484,
      "grad_norm": 7.768533706665039,
      "learning_rate": 4.97676187257056e-05,
      "loss": 1.5284,
      "step": 610
    },
    {
      "epoch": 0.10261502813637868,
      "grad_norm": 4.7729973793029785,
      "learning_rate": 4.9746493155315197e-05,
      "loss": 0.9855,
      "step": 620
    },
    {
      "epoch": 0.10427010923535253,
      "grad_norm": 6.364742755889893,
      "learning_rate": 4.9725367584924795e-05,
      "loss": 1.0399,
      "step": 630
    },
    {
      "epoch": 0.10592519033432639,
      "grad_norm": 9.551055908203125,
      "learning_rate": 4.97042420145344e-05,
      "loss": 1.2766,
      "step": 640
    },
    {
      "epoch": 0.10758027143330023,
      "grad_norm": 5.833217144012451,
      "learning_rate": 4.9683116444144e-05,
      "loss": 1.3197,
      "step": 650
    },
    {
      "epoch": 0.10923535253227408,
      "grad_norm": 9.327737808227539,
      "learning_rate": 4.96619908737536e-05,
      "loss": 1.2004,
      "step": 660
    },
    {
      "epoch": 0.11089043363124793,
      "grad_norm": 6.029761791229248,
      "learning_rate": 4.9640865303363195e-05,
      "loss": 1.1285,
      "step": 670
    },
    {
      "epoch": 0.11254551473022179,
      "grad_norm": 7.959981918334961,
      "learning_rate": 4.9619739732972794e-05,
      "loss": 1.3077,
      "step": 680
    },
    {
      "epoch": 0.11420059582919563,
      "grad_norm": 7.7021260261535645,
      "learning_rate": 4.959861416258239e-05,
      "loss": 1.2503,
      "step": 690
    },
    {
      "epoch": 0.11585567692816948,
      "grad_norm": 4.773941516876221,
      "learning_rate": 4.957748859219199e-05,
      "loss": 1.2603,
      "step": 700
    },
    {
      "epoch": 0.11751075802714334,
      "grad_norm": 11.288993835449219,
      "learning_rate": 4.955636302180159e-05,
      "loss": 0.8977,
      "step": 710
    },
    {
      "epoch": 0.11916583912611718,
      "grad_norm": 9.984114646911621,
      "learning_rate": 4.9535237451411194e-05,
      "loss": 0.9651,
      "step": 720
    },
    {
      "epoch": 0.12082092022509103,
      "grad_norm": 4.741395473480225,
      "learning_rate": 4.951411188102079e-05,
      "loss": 1.508,
      "step": 730
    },
    {
      "epoch": 0.12247600132406487,
      "grad_norm": 6.255035400390625,
      "learning_rate": 4.949298631063039e-05,
      "loss": 0.9833,
      "step": 740
    },
    {
      "epoch": 0.12413108242303873,
      "grad_norm": 4.81515645980835,
      "learning_rate": 4.947186074023999e-05,
      "loss": 1.0659,
      "step": 750
    },
    {
      "epoch": 0.12578616352201258,
      "grad_norm": 8.93841552734375,
      "learning_rate": 4.945073516984959e-05,
      "loss": 1.0845,
      "step": 760
    },
    {
      "epoch": 0.12744124462098644,
      "grad_norm": 11.186384201049805,
      "learning_rate": 4.9429609599459186e-05,
      "loss": 1.0993,
      "step": 770
    },
    {
      "epoch": 0.12909632571996027,
      "grad_norm": 4.88622522354126,
      "learning_rate": 4.9408484029068784e-05,
      "loss": 1.2958,
      "step": 780
    },
    {
      "epoch": 0.13075140681893413,
      "grad_norm": 8.505717277526855,
      "learning_rate": 4.938735845867838e-05,
      "loss": 1.3057,
      "step": 790
    },
    {
      "epoch": 0.132406487917908,
      "grad_norm": 7.886797904968262,
      "learning_rate": 4.936623288828799e-05,
      "loss": 1.2592,
      "step": 800
    },
    {
      "epoch": 0.13406156901688182,
      "grad_norm": 5.561014652252197,
      "learning_rate": 4.9345107317897586e-05,
      "loss": 1.1112,
      "step": 810
    },
    {
      "epoch": 0.13571665011585568,
      "grad_norm": 7.795823574066162,
      "learning_rate": 4.9323981747507185e-05,
      "loss": 1.1049,
      "step": 820
    },
    {
      "epoch": 0.13737173121482954,
      "grad_norm": 4.678593158721924,
      "learning_rate": 4.930285617711678e-05,
      "loss": 0.9318,
      "step": 830
    },
    {
      "epoch": 0.13902681231380337,
      "grad_norm": 6.312258720397949,
      "learning_rate": 4.928173060672638e-05,
      "loss": 0.8961,
      "step": 840
    },
    {
      "epoch": 0.14068189341277723,
      "grad_norm": 5.5256123542785645,
      "learning_rate": 4.926060503633598e-05,
      "loss": 1.058,
      "step": 850
    },
    {
      "epoch": 0.14233697451175106,
      "grad_norm": 9.251810073852539,
      "learning_rate": 4.9239479465945585e-05,
      "loss": 1.0689,
      "step": 860
    },
    {
      "epoch": 0.14399205561072492,
      "grad_norm": 10.534529685974121,
      "learning_rate": 4.921835389555518e-05,
      "loss": 1.2571,
      "step": 870
    },
    {
      "epoch": 0.14564713670969878,
      "grad_norm": 8.464441299438477,
      "learning_rate": 4.919722832516478e-05,
      "loss": 0.7042,
      "step": 880
    },
    {
      "epoch": 0.14730221780867261,
      "grad_norm": 6.656115531921387,
      "learning_rate": 4.917610275477439e-05,
      "loss": 1.1147,
      "step": 890
    },
    {
      "epoch": 0.14895729890764647,
      "grad_norm": 5.315909385681152,
      "learning_rate": 4.9154977184383985e-05,
      "loss": 1.085,
      "step": 900
    },
    {
      "epoch": 0.15061238000662033,
      "grad_norm": 6.3923444747924805,
      "learning_rate": 4.9133851613993584e-05,
      "loss": 0.8607,
      "step": 910
    },
    {
      "epoch": 0.15226746110559417,
      "grad_norm": 1.0524730682373047,
      "learning_rate": 4.911272604360318e-05,
      "loss": 1.4105,
      "step": 920
    },
    {
      "epoch": 0.15392254220456802,
      "grad_norm": 8.541412353515625,
      "learning_rate": 4.909160047321278e-05,
      "loss": 1.2395,
      "step": 930
    },
    {
      "epoch": 0.15557762330354188,
      "grad_norm": 8.274399757385254,
      "learning_rate": 4.907047490282238e-05,
      "loss": 1.2217,
      "step": 940
    },
    {
      "epoch": 0.15723270440251572,
      "grad_norm": 12.976698875427246,
      "learning_rate": 4.904934933243198e-05,
      "loss": 0.9044,
      "step": 950
    },
    {
      "epoch": 0.15888778550148958,
      "grad_norm": 9.225919723510742,
      "learning_rate": 4.9028223762041575e-05,
      "loss": 1.6052,
      "step": 960
    },
    {
      "epoch": 0.16054286660046344,
      "grad_norm": 10.446148872375488,
      "learning_rate": 4.900709819165118e-05,
      "loss": 1.1844,
      "step": 970
    },
    {
      "epoch": 0.16219794769943727,
      "grad_norm": 7.481561660766602,
      "learning_rate": 4.898597262126078e-05,
      "loss": 0.9881,
      "step": 980
    },
    {
      "epoch": 0.16385302879841113,
      "grad_norm": 6.5786919593811035,
      "learning_rate": 4.896484705087038e-05,
      "loss": 1.2164,
      "step": 990
    },
    {
      "epoch": 0.16550810989738496,
      "grad_norm": 7.9727582931518555,
      "learning_rate": 4.8943721480479976e-05,
      "loss": 0.9195,
      "step": 1000
    },
    {
      "epoch": 0.16716319099635882,
      "grad_norm": 5.710793972015381,
      "learning_rate": 4.8922595910089574e-05,
      "loss": 1.2506,
      "step": 1010
    },
    {
      "epoch": 0.16881827209533268,
      "grad_norm": 5.972925662994385,
      "learning_rate": 4.890147033969917e-05,
      "loss": 0.9573,
      "step": 1020
    },
    {
      "epoch": 0.1704733531943065,
      "grad_norm": 4.672503471374512,
      "learning_rate": 4.888034476930877e-05,
      "loss": 1.076,
      "step": 1030
    },
    {
      "epoch": 0.17212843429328037,
      "grad_norm": 8.415233612060547,
      "learning_rate": 4.885921919891837e-05,
      "loss": 0.8532,
      "step": 1040
    },
    {
      "epoch": 0.17378351539225423,
      "grad_norm": 5.87423038482666,
      "learning_rate": 4.883809362852797e-05,
      "loss": 1.0938,
      "step": 1050
    },
    {
      "epoch": 0.17543859649122806,
      "grad_norm": 7.390071868896484,
      "learning_rate": 4.881696805813757e-05,
      "loss": 0.9239,
      "step": 1060
    },
    {
      "epoch": 0.17709367759020192,
      "grad_norm": 7.279072284698486,
      "learning_rate": 4.879584248774717e-05,
      "loss": 1.4614,
      "step": 1070
    },
    {
      "epoch": 0.17874875868917578,
      "grad_norm": 7.712642669677734,
      "learning_rate": 4.877471691735677e-05,
      "loss": 1.2552,
      "step": 1080
    },
    {
      "epoch": 0.1804038397881496,
      "grad_norm": 5.452831745147705,
      "learning_rate": 4.875359134696637e-05,
      "loss": 1.0082,
      "step": 1090
    },
    {
      "epoch": 0.18205892088712347,
      "grad_norm": 3.5975940227508545,
      "learning_rate": 4.8732465776575966e-05,
      "loss": 1.0337,
      "step": 1100
    },
    {
      "epoch": 0.18371400198609733,
      "grad_norm": 5.293908596038818,
      "learning_rate": 4.871134020618557e-05,
      "loss": 1.1785,
      "step": 1110
    },
    {
      "epoch": 0.18536908308507116,
      "grad_norm": 6.499624252319336,
      "learning_rate": 4.869021463579517e-05,
      "loss": 0.9932,
      "step": 1120
    },
    {
      "epoch": 0.18702416418404502,
      "grad_norm": 6.528020858764648,
      "learning_rate": 4.866908906540477e-05,
      "loss": 1.0532,
      "step": 1130
    },
    {
      "epoch": 0.18867924528301888,
      "grad_norm": 6.886061668395996,
      "learning_rate": 4.8647963495014373e-05,
      "loss": 1.2608,
      "step": 1140
    },
    {
      "epoch": 0.1903343263819927,
      "grad_norm": 1.0990080833435059,
      "learning_rate": 4.862683792462397e-05,
      "loss": 0.6045,
      "step": 1150
    },
    {
      "epoch": 0.19198940748096657,
      "grad_norm": 9.182982444763184,
      "learning_rate": 4.860571235423357e-05,
      "loss": 1.1212,
      "step": 1160
    },
    {
      "epoch": 0.1936444885799404,
      "grad_norm": 12.97887134552002,
      "learning_rate": 4.858458678384317e-05,
      "loss": 1.2021,
      "step": 1170
    },
    {
      "epoch": 0.19529956967891426,
      "grad_norm": 10.343672752380371,
      "learning_rate": 4.856346121345277e-05,
      "loss": 0.9037,
      "step": 1180
    },
    {
      "epoch": 0.19695465077788812,
      "grad_norm": 5.573896884918213,
      "learning_rate": 4.8542335643062365e-05,
      "loss": 1.181,
      "step": 1190
    },
    {
      "epoch": 0.19860973187686196,
      "grad_norm": 4.694684982299805,
      "learning_rate": 4.8521210072671964e-05,
      "loss": 1.2865,
      "step": 1200
    },
    {
      "epoch": 0.20026481297583582,
      "grad_norm": 6.90242862701416,
      "learning_rate": 4.850008450228156e-05,
      "loss": 1.0798,
      "step": 1210
    },
    {
      "epoch": 0.20191989407480967,
      "grad_norm": 6.095139980316162,
      "learning_rate": 4.847895893189116e-05,
      "loss": 1.0511,
      "step": 1220
    },
    {
      "epoch": 0.2035749751737835,
      "grad_norm": 7.662085056304932,
      "learning_rate": 4.8457833361500766e-05,
      "loss": 1.1618,
      "step": 1230
    },
    {
      "epoch": 0.20523005627275737,
      "grad_norm": 4.982565879821777,
      "learning_rate": 4.8436707791110364e-05,
      "loss": 0.8293,
      "step": 1240
    },
    {
      "epoch": 0.20688513737173123,
      "grad_norm": 11.322824478149414,
      "learning_rate": 4.841558222071996e-05,
      "loss": 0.8927,
      "step": 1250
    },
    {
      "epoch": 0.20854021847070506,
      "grad_norm": 4.266307353973389,
      "learning_rate": 4.839445665032956e-05,
      "loss": 0.7087,
      "step": 1260
    },
    {
      "epoch": 0.21019529956967892,
      "grad_norm": 9.22838306427002,
      "learning_rate": 4.837333107993916e-05,
      "loss": 1.0479,
      "step": 1270
    },
    {
      "epoch": 0.21185038066865278,
      "grad_norm": 14.849045753479004,
      "learning_rate": 4.835220550954876e-05,
      "loss": 0.9063,
      "step": 1280
    },
    {
      "epoch": 0.2135054617676266,
      "grad_norm": 12.852745056152344,
      "learning_rate": 4.8331079939158356e-05,
      "loss": 0.9111,
      "step": 1290
    },
    {
      "epoch": 0.21516054286660047,
      "grad_norm": 10.388618469238281,
      "learning_rate": 4.8309954368767954e-05,
      "loss": 0.8381,
      "step": 1300
    },
    {
      "epoch": 0.2168156239655743,
      "grad_norm": 8.935135841369629,
      "learning_rate": 4.828882879837756e-05,
      "loss": 0.793,
      "step": 1310
    },
    {
      "epoch": 0.21847070506454816,
      "grad_norm": 9.99417495727539,
      "learning_rate": 4.826770322798716e-05,
      "loss": 1.4309,
      "step": 1320
    },
    {
      "epoch": 0.22012578616352202,
      "grad_norm": 9.451557159423828,
      "learning_rate": 4.8246577657596756e-05,
      "loss": 1.148,
      "step": 1330
    },
    {
      "epoch": 0.22178086726249585,
      "grad_norm": 5.033353328704834,
      "learning_rate": 4.8225452087206355e-05,
      "loss": 0.7394,
      "step": 1340
    },
    {
      "epoch": 0.2234359483614697,
      "grad_norm": 7.628906726837158,
      "learning_rate": 4.820432651681595e-05,
      "loss": 0.7979,
      "step": 1350
    },
    {
      "epoch": 0.22509102946044357,
      "grad_norm": 4.09216833114624,
      "learning_rate": 4.818320094642556e-05,
      "loss": 0.9801,
      "step": 1360
    },
    {
      "epoch": 0.2267461105594174,
      "grad_norm": 9.092968940734863,
      "learning_rate": 4.816207537603516e-05,
      "loss": 1.5788,
      "step": 1370
    },
    {
      "epoch": 0.22840119165839126,
      "grad_norm": 8.166152000427246,
      "learning_rate": 4.8140949805644755e-05,
      "loss": 0.7949,
      "step": 1380
    },
    {
      "epoch": 0.23005627275736512,
      "grad_norm": 4.641820907592773,
      "learning_rate": 4.811982423525435e-05,
      "loss": 1.2087,
      "step": 1390
    },
    {
      "epoch": 0.23171135385633895,
      "grad_norm": 7.541827201843262,
      "learning_rate": 4.809869866486396e-05,
      "loss": 1.0402,
      "step": 1400
    },
    {
      "epoch": 0.2333664349553128,
      "grad_norm": 4.966800689697266,
      "learning_rate": 4.807757309447356e-05,
      "loss": 1.2821,
      "step": 1410
    },
    {
      "epoch": 0.23502151605428667,
      "grad_norm": 4.65952730178833,
      "learning_rate": 4.8056447524083155e-05,
      "loss": 1.1426,
      "step": 1420
    },
    {
      "epoch": 0.2366765971532605,
      "grad_norm": 7.216237545013428,
      "learning_rate": 4.8035321953692754e-05,
      "loss": 0.898,
      "step": 1430
    },
    {
      "epoch": 0.23833167825223436,
      "grad_norm": 9.235370635986328,
      "learning_rate": 4.801419638330235e-05,
      "loss": 0.9671,
      "step": 1440
    },
    {
      "epoch": 0.2399867593512082,
      "grad_norm": 4.024963855743408,
      "learning_rate": 4.799307081291195e-05,
      "loss": 0.6373,
      "step": 1450
    },
    {
      "epoch": 0.24164184045018205,
      "grad_norm": 6.676187038421631,
      "learning_rate": 4.797194524252155e-05,
      "loss": 1.0034,
      "step": 1460
    },
    {
      "epoch": 0.24329692154915591,
      "grad_norm": 6.9953413009643555,
      "learning_rate": 4.795081967213115e-05,
      "loss": 1.2088,
      "step": 1470
    },
    {
      "epoch": 0.24495200264812975,
      "grad_norm": 2.9078562259674072,
      "learning_rate": 4.792969410174075e-05,
      "loss": 0.8398,
      "step": 1480
    },
    {
      "epoch": 0.2466070837471036,
      "grad_norm": 6.16707706451416,
      "learning_rate": 4.790856853135035e-05,
      "loss": 1.1536,
      "step": 1490
    },
    {
      "epoch": 0.24826216484607747,
      "grad_norm": 4.324840545654297,
      "learning_rate": 4.788744296095995e-05,
      "loss": 1.0408,
      "step": 1500
    },
    {
      "epoch": 0.2499172459450513,
      "grad_norm": 15.712027549743652,
      "learning_rate": 4.786631739056955e-05,
      "loss": 0.9953,
      "step": 1510
    },
    {
      "epoch": 0.25157232704402516,
      "grad_norm": 9.069661140441895,
      "learning_rate": 4.7845191820179146e-05,
      "loss": 1.2565,
      "step": 1520
    },
    {
      "epoch": 0.253227408142999,
      "grad_norm": 12.16580581665039,
      "learning_rate": 4.7824066249788744e-05,
      "loss": 0.9488,
      "step": 1530
    },
    {
      "epoch": 0.2548824892419729,
      "grad_norm": 13.393353462219238,
      "learning_rate": 4.780294067939834e-05,
      "loss": 0.733,
      "step": 1540
    },
    {
      "epoch": 0.2565375703409467,
      "grad_norm": 3.2282142639160156,
      "learning_rate": 4.778181510900794e-05,
      "loss": 0.852,
      "step": 1550
    },
    {
      "epoch": 0.25819265143992054,
      "grad_norm": 6.335540294647217,
      "learning_rate": 4.776068953861754e-05,
      "loss": 1.0715,
      "step": 1560
    },
    {
      "epoch": 0.2598477325388944,
      "grad_norm": 9.485562324523926,
      "learning_rate": 4.7739563968227145e-05,
      "loss": 0.7638,
      "step": 1570
    },
    {
      "epoch": 0.26150281363786826,
      "grad_norm": 5.468998908996582,
      "learning_rate": 4.771843839783674e-05,
      "loss": 0.796,
      "step": 1580
    },
    {
      "epoch": 0.2631578947368421,
      "grad_norm": 4.316401481628418,
      "learning_rate": 4.769731282744634e-05,
      "loss": 0.9601,
      "step": 1590
    },
    {
      "epoch": 0.264812975835816,
      "grad_norm": 8.861205101013184,
      "learning_rate": 4.767618725705594e-05,
      "loss": 0.824,
      "step": 1600
    },
    {
      "epoch": 0.2664680569347898,
      "grad_norm": 5.394019603729248,
      "learning_rate": 4.7655061686665545e-05,
      "loss": 0.8,
      "step": 1610
    },
    {
      "epoch": 0.26812313803376364,
      "grad_norm": 6.36236047744751,
      "learning_rate": 4.763393611627514e-05,
      "loss": 1.3162,
      "step": 1620
    },
    {
      "epoch": 0.26977821913273753,
      "grad_norm": 5.691561222076416,
      "learning_rate": 4.761281054588474e-05,
      "loss": 0.7874,
      "step": 1630
    },
    {
      "epoch": 0.27143330023171136,
      "grad_norm": 11.217209815979004,
      "learning_rate": 4.759168497549434e-05,
      "loss": 1.2582,
      "step": 1640
    },
    {
      "epoch": 0.2730883813306852,
      "grad_norm": 11.502766609191895,
      "learning_rate": 4.7570559405103945e-05,
      "loss": 0.9681,
      "step": 1650
    },
    {
      "epoch": 0.2747434624296591,
      "grad_norm": 5.960086822509766,
      "learning_rate": 4.7549433834713544e-05,
      "loss": 0.9101,
      "step": 1660
    },
    {
      "epoch": 0.2763985435286329,
      "grad_norm": 4.755646228790283,
      "learning_rate": 4.752830826432314e-05,
      "loss": 1.1065,
      "step": 1670
    },
    {
      "epoch": 0.27805362462760674,
      "grad_norm": 5.744128227233887,
      "learning_rate": 4.750718269393274e-05,
      "loss": 0.7898,
      "step": 1680
    },
    {
      "epoch": 0.2797087057265806,
      "grad_norm": 6.398251056671143,
      "learning_rate": 4.748605712354234e-05,
      "loss": 0.881,
      "step": 1690
    },
    {
      "epoch": 0.28136378682555446,
      "grad_norm": 11.929609298706055,
      "learning_rate": 4.746493155315194e-05,
      "loss": 1.1886,
      "step": 1700
    },
    {
      "epoch": 0.2830188679245283,
      "grad_norm": 11.165188789367676,
      "learning_rate": 4.7443805982761536e-05,
      "loss": 1.3147,
      "step": 1710
    },
    {
      "epoch": 0.2846739490235021,
      "grad_norm": 7.1000471115112305,
      "learning_rate": 4.7422680412371134e-05,
      "loss": 1.0023,
      "step": 1720
    },
    {
      "epoch": 0.286329030122476,
      "grad_norm": 8.47039794921875,
      "learning_rate": 4.740155484198073e-05,
      "loss": 0.9653,
      "step": 1730
    },
    {
      "epoch": 0.28798411122144985,
      "grad_norm": 6.560060024261475,
      "learning_rate": 4.738042927159034e-05,
      "loss": 1.1021,
      "step": 1740
    },
    {
      "epoch": 0.2896391923204237,
      "grad_norm": 13.052764892578125,
      "learning_rate": 4.7359303701199936e-05,
      "loss": 1.2055,
      "step": 1750
    },
    {
      "epoch": 0.29129427341939756,
      "grad_norm": 3.893538475036621,
      "learning_rate": 4.7338178130809534e-05,
      "loss": 1.0219,
      "step": 1760
    },
    {
      "epoch": 0.2929493545183714,
      "grad_norm": 4.123079776763916,
      "learning_rate": 4.731705256041913e-05,
      "loss": 1.0483,
      "step": 1770
    },
    {
      "epoch": 0.29460443561734523,
      "grad_norm": 8.050045013427734,
      "learning_rate": 4.729592699002873e-05,
      "loss": 1.1029,
      "step": 1780
    },
    {
      "epoch": 0.2962595167163191,
      "grad_norm": 5.392127513885498,
      "learning_rate": 4.727480141963833e-05,
      "loss": 0.8663,
      "step": 1790
    },
    {
      "epoch": 0.29791459781529295,
      "grad_norm": 7.382465362548828,
      "learning_rate": 4.725367584924793e-05,
      "loss": 0.6713,
      "step": 1800
    },
    {
      "epoch": 0.2995696789142668,
      "grad_norm": 8.146446228027344,
      "learning_rate": 4.7232550278857526e-05,
      "loss": 1.055,
      "step": 1810
    },
    {
      "epoch": 0.30122476001324067,
      "grad_norm": 8.0614013671875,
      "learning_rate": 4.721142470846713e-05,
      "loss": 1.318,
      "step": 1820
    },
    {
      "epoch": 0.3028798411122145,
      "grad_norm": 3.2113471031188965,
      "learning_rate": 4.719029913807673e-05,
      "loss": 0.8232,
      "step": 1830
    },
    {
      "epoch": 0.30453492221118833,
      "grad_norm": 4.718456268310547,
      "learning_rate": 4.716917356768633e-05,
      "loss": 0.8375,
      "step": 1840
    },
    {
      "epoch": 0.3061900033101622,
      "grad_norm": 5.668194770812988,
      "learning_rate": 4.714804799729593e-05,
      "loss": 0.8218,
      "step": 1850
    },
    {
      "epoch": 0.30784508440913605,
      "grad_norm": 8.703760147094727,
      "learning_rate": 4.712692242690553e-05,
      "loss": 1.3189,
      "step": 1860
    },
    {
      "epoch": 0.3095001655081099,
      "grad_norm": 6.846735954284668,
      "learning_rate": 4.710579685651513e-05,
      "loss": 1.0485,
      "step": 1870
    },
    {
      "epoch": 0.31115524660708377,
      "grad_norm": 9.82274055480957,
      "learning_rate": 4.708467128612473e-05,
      "loss": 1.0404,
      "step": 1880
    },
    {
      "epoch": 0.3128103277060576,
      "grad_norm": 5.211826801300049,
      "learning_rate": 4.706354571573433e-05,
      "loss": 1.2764,
      "step": 1890
    },
    {
      "epoch": 0.31446540880503143,
      "grad_norm": 6.238792896270752,
      "learning_rate": 4.7042420145343925e-05,
      "loss": 0.9796,
      "step": 1900
    },
    {
      "epoch": 0.3161204899040053,
      "grad_norm": 14.759909629821777,
      "learning_rate": 4.702129457495353e-05,
      "loss": 1.1424,
      "step": 1910
    },
    {
      "epoch": 0.31777557100297915,
      "grad_norm": 8.369919776916504,
      "learning_rate": 4.700016900456313e-05,
      "loss": 1.0176,
      "step": 1920
    },
    {
      "epoch": 0.319430652101953,
      "grad_norm": 11.669900894165039,
      "learning_rate": 4.697904343417273e-05,
      "loss": 0.9761,
      "step": 1930
    },
    {
      "epoch": 0.32108573320092687,
      "grad_norm": 12.827033996582031,
      "learning_rate": 4.6957917863782325e-05,
      "loss": 0.9359,
      "step": 1940
    },
    {
      "epoch": 0.3227408142999007,
      "grad_norm": 7.350112438201904,
      "learning_rate": 4.6936792293391924e-05,
      "loss": 0.8662,
      "step": 1950
    },
    {
      "epoch": 0.32439589539887453,
      "grad_norm": 9.767650604248047,
      "learning_rate": 4.691566672300152e-05,
      "loss": 0.9339,
      "step": 1960
    },
    {
      "epoch": 0.3260509764978484,
      "grad_norm": 6.722286701202393,
      "learning_rate": 4.689454115261112e-05,
      "loss": 0.8763,
      "step": 1970
    },
    {
      "epoch": 0.32770605759682225,
      "grad_norm": 5.984364986419678,
      "learning_rate": 4.687341558222072e-05,
      "loss": 0.6197,
      "step": 1980
    },
    {
      "epoch": 0.3293611386957961,
      "grad_norm": 8.830606460571289,
      "learning_rate": 4.6852290011830324e-05,
      "loss": 0.7596,
      "step": 1990
    },
    {
      "epoch": 0.3310162197947699,
      "grad_norm": 10.46382999420166,
      "learning_rate": 4.683116444143992e-05,
      "loss": 0.9028,
      "step": 2000
    },
    {
      "epoch": 0.3326713008937438,
      "grad_norm": 4.29202127456665,
      "learning_rate": 4.681003887104952e-05,
      "loss": 0.9622,
      "step": 2010
    },
    {
      "epoch": 0.33432638199271764,
      "grad_norm": 5.198490619659424,
      "learning_rate": 4.678891330065912e-05,
      "loss": 1.1256,
      "step": 2020
    },
    {
      "epoch": 0.33598146309169147,
      "grad_norm": 7.314266204833984,
      "learning_rate": 4.676778773026872e-05,
      "loss": 0.7641,
      "step": 2030
    },
    {
      "epoch": 0.33763654419066536,
      "grad_norm": 7.8869853019714355,
      "learning_rate": 4.6746662159878316e-05,
      "loss": 1.215,
      "step": 2040
    },
    {
      "epoch": 0.3392916252896392,
      "grad_norm": 4.416752338409424,
      "learning_rate": 4.6725536589487914e-05,
      "loss": 0.9831,
      "step": 2050
    },
    {
      "epoch": 0.340946706388613,
      "grad_norm": 7.837658405303955,
      "learning_rate": 4.670441101909751e-05,
      "loss": 1.278,
      "step": 2060
    },
    {
      "epoch": 0.3426017874875869,
      "grad_norm": 11.287732124328613,
      "learning_rate": 4.668328544870712e-05,
      "loss": 1.1857,
      "step": 2070
    },
    {
      "epoch": 0.34425686858656074,
      "grad_norm": 6.965438365936279,
      "learning_rate": 4.6662159878316716e-05,
      "loss": 1.3615,
      "step": 2080
    },
    {
      "epoch": 0.34591194968553457,
      "grad_norm": 12.797938346862793,
      "learning_rate": 4.6641034307926315e-05,
      "loss": 1.0073,
      "step": 2090
    },
    {
      "epoch": 0.34756703078450846,
      "grad_norm": 7.017601013183594,
      "learning_rate": 4.661990873753592e-05,
      "loss": 0.9076,
      "step": 2100
    },
    {
      "epoch": 0.3492221118834823,
      "grad_norm": 15.2490234375,
      "learning_rate": 4.659878316714552e-05,
      "loss": 0.9688,
      "step": 2110
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 6.6677656173706055,
      "learning_rate": 4.657765759675512e-05,
      "loss": 0.8588,
      "step": 2120
    },
    {
      "epoch": 0.35253227408143,
      "grad_norm": 2.888070821762085,
      "learning_rate": 4.6556532026364715e-05,
      "loss": 1.1205,
      "step": 2130
    },
    {
      "epoch": 0.35418735518040384,
      "grad_norm": 4.525887489318848,
      "learning_rate": 4.6535406455974313e-05,
      "loss": 0.7531,
      "step": 2140
    },
    {
      "epoch": 0.35584243627937767,
      "grad_norm": 13.461175918579102,
      "learning_rate": 4.651428088558391e-05,
      "loss": 1.1408,
      "step": 2150
    },
    {
      "epoch": 0.35749751737835156,
      "grad_norm": 4.689554214477539,
      "learning_rate": 4.649315531519352e-05,
      "loss": 1.0282,
      "step": 2160
    },
    {
      "epoch": 0.3591525984773254,
      "grad_norm": 4.277727127075195,
      "learning_rate": 4.6472029744803115e-05,
      "loss": 1.1317,
      "step": 2170
    },
    {
      "epoch": 0.3608076795762992,
      "grad_norm": 5.21662712097168,
      "learning_rate": 4.6450904174412714e-05,
      "loss": 0.9825,
      "step": 2180
    },
    {
      "epoch": 0.3624627606752731,
      "grad_norm": 13.37752914428711,
      "learning_rate": 4.642977860402231e-05,
      "loss": 1.1966,
      "step": 2190
    },
    {
      "epoch": 0.36411784177424694,
      "grad_norm": 5.78953218460083,
      "learning_rate": 4.640865303363191e-05,
      "loss": 0.9366,
      "step": 2200
    },
    {
      "epoch": 0.3657729228732208,
      "grad_norm": 14.460609436035156,
      "learning_rate": 4.638752746324151e-05,
      "loss": 1.1944,
      "step": 2210
    },
    {
      "epoch": 0.36742800397219466,
      "grad_norm": 6.542474269866943,
      "learning_rate": 4.636640189285111e-05,
      "loss": 0.9912,
      "step": 2220
    },
    {
      "epoch": 0.3690830850711685,
      "grad_norm": 6.891678333282471,
      "learning_rate": 4.6345276322460706e-05,
      "loss": 0.761,
      "step": 2230
    },
    {
      "epoch": 0.3707381661701423,
      "grad_norm": 14.132966041564941,
      "learning_rate": 4.6324150752070304e-05,
      "loss": 1.1693,
      "step": 2240
    },
    {
      "epoch": 0.3723932472691162,
      "grad_norm": 3.1992459297180176,
      "learning_rate": 4.630302518167991e-05,
      "loss": 1.1172,
      "step": 2250
    },
    {
      "epoch": 0.37404832836809004,
      "grad_norm": 4.0023322105407715,
      "learning_rate": 4.628189961128951e-05,
      "loss": 0.7275,
      "step": 2260
    },
    {
      "epoch": 0.3757034094670639,
      "grad_norm": 10.657421112060547,
      "learning_rate": 4.6260774040899106e-05,
      "loss": 0.6986,
      "step": 2270
    },
    {
      "epoch": 0.37735849056603776,
      "grad_norm": 14.291274070739746,
      "learning_rate": 4.6239648470508704e-05,
      "loss": 0.7852,
      "step": 2280
    },
    {
      "epoch": 0.3790135716650116,
      "grad_norm": 5.616158962249756,
      "learning_rate": 4.62185229001183e-05,
      "loss": 1.0561,
      "step": 2290
    },
    {
      "epoch": 0.3806686527639854,
      "grad_norm": 9.056979179382324,
      "learning_rate": 4.61973973297279e-05,
      "loss": 0.9167,
      "step": 2300
    },
    {
      "epoch": 0.38232373386295926,
      "grad_norm": 10.916108131408691,
      "learning_rate": 4.61762717593375e-05,
      "loss": 1.1045,
      "step": 2310
    },
    {
      "epoch": 0.38397881496193315,
      "grad_norm": 7.0765581130981445,
      "learning_rate": 4.6155146188947105e-05,
      "loss": 0.7871,
      "step": 2320
    },
    {
      "epoch": 0.385633896060907,
      "grad_norm": 13.04115104675293,
      "learning_rate": 4.61340206185567e-05,
      "loss": 0.6958,
      "step": 2330
    },
    {
      "epoch": 0.3872889771598808,
      "grad_norm": 6.533575534820557,
      "learning_rate": 4.61128950481663e-05,
      "loss": 0.9052,
      "step": 2340
    },
    {
      "epoch": 0.3889440582588547,
      "grad_norm": 6.860217571258545,
      "learning_rate": 4.6091769477775907e-05,
      "loss": 0.8782,
      "step": 2350
    },
    {
      "epoch": 0.39059913935782853,
      "grad_norm": 13.65745735168457,
      "learning_rate": 4.6070643907385505e-05,
      "loss": 0.9703,
      "step": 2360
    },
    {
      "epoch": 0.39225422045680236,
      "grad_norm": 3.47294545173645,
      "learning_rate": 4.60495183369951e-05,
      "loss": 0.664,
      "step": 2370
    },
    {
      "epoch": 0.39390930155577625,
      "grad_norm": 11.261844635009766,
      "learning_rate": 4.60283927666047e-05,
      "loss": 1.1676,
      "step": 2380
    },
    {
      "epoch": 0.3955643826547501,
      "grad_norm": 11.997441291809082,
      "learning_rate": 4.60072671962143e-05,
      "loss": 1.0542,
      "step": 2390
    },
    {
      "epoch": 0.3972194637537239,
      "grad_norm": 6.871723651885986,
      "learning_rate": 4.59861416258239e-05,
      "loss": 0.9611,
      "step": 2400
    },
    {
      "epoch": 0.3988745448526978,
      "grad_norm": 5.230261325836182,
      "learning_rate": 4.59650160554335e-05,
      "loss": 0.7119,
      "step": 2410
    },
    {
      "epoch": 0.40052962595167163,
      "grad_norm": 4.8493499755859375,
      "learning_rate": 4.59438904850431e-05,
      "loss": 1.0232,
      "step": 2420
    },
    {
      "epoch": 0.40218470705064546,
      "grad_norm": 6.881191730499268,
      "learning_rate": 4.59227649146527e-05,
      "loss": 0.9283,
      "step": 2430
    },
    {
      "epoch": 0.40383978814961935,
      "grad_norm": 6.700901031494141,
      "learning_rate": 4.59016393442623e-05,
      "loss": 0.8026,
      "step": 2440
    },
    {
      "epoch": 0.4054948692485932,
      "grad_norm": 6.372035503387451,
      "learning_rate": 4.58805137738719e-05,
      "loss": 0.7256,
      "step": 2450
    },
    {
      "epoch": 0.407149950347567,
      "grad_norm": 4.700034141540527,
      "learning_rate": 4.5859388203481496e-05,
      "loss": 1.1296,
      "step": 2460
    },
    {
      "epoch": 0.4088050314465409,
      "grad_norm": 1.616067886352539,
      "learning_rate": 4.5838262633091094e-05,
      "loss": 0.6541,
      "step": 2470
    },
    {
      "epoch": 0.41046011254551473,
      "grad_norm": 9.335246086120605,
      "learning_rate": 4.581713706270069e-05,
      "loss": 0.688,
      "step": 2480
    },
    {
      "epoch": 0.41211519364448856,
      "grad_norm": 8.422526359558105,
      "learning_rate": 4.579601149231029e-05,
      "loss": 0.7278,
      "step": 2490
    },
    {
      "epoch": 0.41377027474346245,
      "grad_norm": 9.080416679382324,
      "learning_rate": 4.5774885921919896e-05,
      "loss": 0.9465,
      "step": 2500
    },
    {
      "epoch": 0.4154253558424363,
      "grad_norm": 6.632617950439453,
      "learning_rate": 4.5753760351529494e-05,
      "loss": 0.9072,
      "step": 2510
    },
    {
      "epoch": 0.4170804369414101,
      "grad_norm": 3.826263904571533,
      "learning_rate": 4.573263478113909e-05,
      "loss": 0.9312,
      "step": 2520
    },
    {
      "epoch": 0.418735518040384,
      "grad_norm": 2.985973596572876,
      "learning_rate": 4.571150921074869e-05,
      "loss": 0.7634,
      "step": 2530
    },
    {
      "epoch": 0.42039059913935783,
      "grad_norm": 9.88821029663086,
      "learning_rate": 4.569038364035829e-05,
      "loss": 0.8229,
      "step": 2540
    },
    {
      "epoch": 0.42204568023833167,
      "grad_norm": 6.639096260070801,
      "learning_rate": 4.566925806996789e-05,
      "loss": 0.7482,
      "step": 2550
    },
    {
      "epoch": 0.42370076133730555,
      "grad_norm": 8.494214057922363,
      "learning_rate": 4.5648132499577486e-05,
      "loss": 0.7668,
      "step": 2560
    },
    {
      "epoch": 0.4253558424362794,
      "grad_norm": 2.4266209602355957,
      "learning_rate": 4.562700692918709e-05,
      "loss": 0.7687,
      "step": 2570
    },
    {
      "epoch": 0.4270109235352532,
      "grad_norm": 12.757013320922852,
      "learning_rate": 4.560588135879669e-05,
      "loss": 0.8125,
      "step": 2580
    },
    {
      "epoch": 0.4286660046342271,
      "grad_norm": 8.268671989440918,
      "learning_rate": 4.558475578840629e-05,
      "loss": 0.9711,
      "step": 2590
    },
    {
      "epoch": 0.43032108573320094,
      "grad_norm": 6.398922920227051,
      "learning_rate": 4.556363021801589e-05,
      "loss": 0.7699,
      "step": 2600
    },
    {
      "epoch": 0.43197616683217477,
      "grad_norm": 6.036654949188232,
      "learning_rate": 4.554250464762549e-05,
      "loss": 0.9779,
      "step": 2610
    },
    {
      "epoch": 0.4336312479311486,
      "grad_norm": 2.2752764225006104,
      "learning_rate": 4.552137907723509e-05,
      "loss": 0.6848,
      "step": 2620
    },
    {
      "epoch": 0.4352863290301225,
      "grad_norm": 6.142755031585693,
      "learning_rate": 4.550025350684469e-05,
      "loss": 0.832,
      "step": 2630
    },
    {
      "epoch": 0.4369414101290963,
      "grad_norm": 3.3886430263519287,
      "learning_rate": 4.547912793645429e-05,
      "loss": 0.9572,
      "step": 2640
    },
    {
      "epoch": 0.43859649122807015,
      "grad_norm": 2.131927013397217,
      "learning_rate": 4.5458002366063885e-05,
      "loss": 1.1079,
      "step": 2650
    },
    {
      "epoch": 0.44025157232704404,
      "grad_norm": 10.101700782775879,
      "learning_rate": 4.5436876795673484e-05,
      "loss": 0.9348,
      "step": 2660
    },
    {
      "epoch": 0.44190665342601787,
      "grad_norm": 7.716516494750977,
      "learning_rate": 4.541575122528309e-05,
      "loss": 0.9035,
      "step": 2670
    },
    {
      "epoch": 0.4435617345249917,
      "grad_norm": 8.790989875793457,
      "learning_rate": 4.539462565489269e-05,
      "loss": 0.9221,
      "step": 2680
    },
    {
      "epoch": 0.4452168156239656,
      "grad_norm": 7.028026580810547,
      "learning_rate": 4.5373500084502286e-05,
      "loss": 0.7573,
      "step": 2690
    },
    {
      "epoch": 0.4468718967229394,
      "grad_norm": 6.49647331237793,
      "learning_rate": 4.5352374514111884e-05,
      "loss": 0.913,
      "step": 2700
    },
    {
      "epoch": 0.44852697782191325,
      "grad_norm": 8.924870491027832,
      "learning_rate": 4.533124894372148e-05,
      "loss": 1.1755,
      "step": 2710
    },
    {
      "epoch": 0.45018205892088714,
      "grad_norm": 6.205325603485107,
      "learning_rate": 4.531012337333108e-05,
      "loss": 0.7005,
      "step": 2720
    },
    {
      "epoch": 0.45183714001986097,
      "grad_norm": 12.115555763244629,
      "learning_rate": 4.528899780294068e-05,
      "loss": 0.6898,
      "step": 2730
    },
    {
      "epoch": 0.4534922211188348,
      "grad_norm": 5.578763008117676,
      "learning_rate": 4.526787223255028e-05,
      "loss": 0.8865,
      "step": 2740
    },
    {
      "epoch": 0.4551473022178087,
      "grad_norm": 9.250011444091797,
      "learning_rate": 4.5246746662159876e-05,
      "loss": 0.9077,
      "step": 2750
    },
    {
      "epoch": 0.4568023833167825,
      "grad_norm": 6.993340492248535,
      "learning_rate": 4.522562109176948e-05,
      "loss": 1.1887,
      "step": 2760
    },
    {
      "epoch": 0.45845746441575635,
      "grad_norm": 5.107805252075195,
      "learning_rate": 4.520449552137908e-05,
      "loss": 0.8029,
      "step": 2770
    },
    {
      "epoch": 0.46011254551473024,
      "grad_norm": 4.955021858215332,
      "learning_rate": 4.518336995098868e-05,
      "loss": 0.9129,
      "step": 2780
    },
    {
      "epoch": 0.4617676266137041,
      "grad_norm": 5.654029369354248,
      "learning_rate": 4.5162244380598276e-05,
      "loss": 1.0356,
      "step": 2790
    },
    {
      "epoch": 0.4634227077126779,
      "grad_norm": 7.8578104972839355,
      "learning_rate": 4.5141118810207875e-05,
      "loss": 0.9541,
      "step": 2800
    },
    {
      "epoch": 0.4650777888116518,
      "grad_norm": 2.639310836791992,
      "learning_rate": 4.511999323981747e-05,
      "loss": 0.8483,
      "step": 2810
    },
    {
      "epoch": 0.4667328699106256,
      "grad_norm": 11.16014289855957,
      "learning_rate": 4.509886766942708e-05,
      "loss": 0.603,
      "step": 2820
    },
    {
      "epoch": 0.46838795100959946,
      "grad_norm": 9.685157775878906,
      "learning_rate": 4.5077742099036676e-05,
      "loss": 0.7042,
      "step": 2830
    },
    {
      "epoch": 0.47004303210857334,
      "grad_norm": 6.205262184143066,
      "learning_rate": 4.505661652864628e-05,
      "loss": 0.8134,
      "step": 2840
    },
    {
      "epoch": 0.4716981132075472,
      "grad_norm": 6.234001159667969,
      "learning_rate": 4.503549095825588e-05,
      "loss": 0.8736,
      "step": 2850
    },
    {
      "epoch": 0.473353194306521,
      "grad_norm": 6.411564350128174,
      "learning_rate": 4.501436538786548e-05,
      "loss": 0.9473,
      "step": 2860
    },
    {
      "epoch": 0.4750082754054949,
      "grad_norm": 9.144033432006836,
      "learning_rate": 4.499323981747508e-05,
      "loss": 0.8906,
      "step": 2870
    },
    {
      "epoch": 0.4766633565044687,
      "grad_norm": 8.112288475036621,
      "learning_rate": 4.4972114247084675e-05,
      "loss": 1.0941,
      "step": 2880
    },
    {
      "epoch": 0.47831843760344256,
      "grad_norm": 3.6994872093200684,
      "learning_rate": 4.4950988676694274e-05,
      "loss": 0.9745,
      "step": 2890
    },
    {
      "epoch": 0.4799735187024164,
      "grad_norm": 6.30675745010376,
      "learning_rate": 4.492986310630387e-05,
      "loss": 0.8931,
      "step": 2900
    },
    {
      "epoch": 0.4816285998013903,
      "grad_norm": 8.197927474975586,
      "learning_rate": 4.490873753591347e-05,
      "loss": 0.6748,
      "step": 2910
    },
    {
      "epoch": 0.4832836809003641,
      "grad_norm": 8.401453018188477,
      "learning_rate": 4.488761196552307e-05,
      "loss": 1.115,
      "step": 2920
    },
    {
      "epoch": 0.48493876199933794,
      "grad_norm": 6.450730323791504,
      "learning_rate": 4.4866486395132674e-05,
      "loss": 0.9554,
      "step": 2930
    },
    {
      "epoch": 0.48659384309831183,
      "grad_norm": 4.708397388458252,
      "learning_rate": 4.484536082474227e-05,
      "loss": 0.7489,
      "step": 2940
    },
    {
      "epoch": 0.48824892419728566,
      "grad_norm": 8.569350242614746,
      "learning_rate": 4.482423525435187e-05,
      "loss": 0.7748,
      "step": 2950
    },
    {
      "epoch": 0.4899040052962595,
      "grad_norm": 4.660645484924316,
      "learning_rate": 4.480310968396147e-05,
      "loss": 0.8927,
      "step": 2960
    },
    {
      "epoch": 0.4915590863952334,
      "grad_norm": 3.760030746459961,
      "learning_rate": 4.478198411357107e-05,
      "loss": 0.9522,
      "step": 2970
    },
    {
      "epoch": 0.4932141674942072,
      "grad_norm": 7.2157769203186035,
      "learning_rate": 4.4760858543180666e-05,
      "loss": 0.7931,
      "step": 2980
    },
    {
      "epoch": 0.49486924859318104,
      "grad_norm": 5.604398250579834,
      "learning_rate": 4.4739732972790264e-05,
      "loss": 1.1067,
      "step": 2990
    },
    {
      "epoch": 0.49652432969215493,
      "grad_norm": 4.288139820098877,
      "learning_rate": 4.471860740239986e-05,
      "loss": 0.8416,
      "step": 3000
    },
    {
      "epoch": 0.49817941079112876,
      "grad_norm": 3.1991305351257324,
      "learning_rate": 4.469748183200947e-05,
      "loss": 0.7996,
      "step": 3010
    },
    {
      "epoch": 0.4998344918901026,
      "grad_norm": 7.078308582305908,
      "learning_rate": 4.4676356261619066e-05,
      "loss": 1.0293,
      "step": 3020
    },
    {
      "epoch": 0.5014895729890765,
      "grad_norm": 6.171448230743408,
      "learning_rate": 4.4655230691228664e-05,
      "loss": 1.0262,
      "step": 3030
    },
    {
      "epoch": 0.5031446540880503,
      "grad_norm": 7.430249214172363,
      "learning_rate": 4.463410512083826e-05,
      "loss": 0.7363,
      "step": 3040
    },
    {
      "epoch": 0.5047997351870241,
      "grad_norm": 6.61668062210083,
      "learning_rate": 4.461297955044786e-05,
      "loss": 0.903,
      "step": 3050
    },
    {
      "epoch": 0.506454816285998,
      "grad_norm": 7.731870174407959,
      "learning_rate": 4.4591853980057466e-05,
      "loss": 0.8504,
      "step": 3060
    },
    {
      "epoch": 0.5081098973849718,
      "grad_norm": 7.338734149932861,
      "learning_rate": 4.4570728409667065e-05,
      "loss": 0.8842,
      "step": 3070
    },
    {
      "epoch": 0.5097649784839458,
      "grad_norm": 4.170329570770264,
      "learning_rate": 4.454960283927666e-05,
      "loss": 1.0367,
      "step": 3080
    },
    {
      "epoch": 0.5114200595829196,
      "grad_norm": 11.470917701721191,
      "learning_rate": 4.452847726888626e-05,
      "loss": 0.9406,
      "step": 3090
    },
    {
      "epoch": 0.5130751406818934,
      "grad_norm": 4.3268632888793945,
      "learning_rate": 4.450735169849587e-05,
      "loss": 0.6569,
      "step": 3100
    },
    {
      "epoch": 0.5147302217808672,
      "grad_norm": 6.205569744110107,
      "learning_rate": 4.4486226128105465e-05,
      "loss": 0.7701,
      "step": 3110
    },
    {
      "epoch": 0.5163853028798411,
      "grad_norm": 9.798870086669922,
      "learning_rate": 4.4465100557715063e-05,
      "loss": 0.7788,
      "step": 3120
    },
    {
      "epoch": 0.5180403839788149,
      "grad_norm": 7.253307819366455,
      "learning_rate": 4.444397498732466e-05,
      "loss": 1.2477,
      "step": 3130
    },
    {
      "epoch": 0.5196954650777889,
      "grad_norm": 7.995439052581787,
      "learning_rate": 4.442284941693426e-05,
      "loss": 1.0995,
      "step": 3140
    },
    {
      "epoch": 0.5213505461767627,
      "grad_norm": 3.534459114074707,
      "learning_rate": 4.440172384654386e-05,
      "loss": 0.8614,
      "step": 3150
    },
    {
      "epoch": 0.5230056272757365,
      "grad_norm": 6.06298303604126,
      "learning_rate": 4.438059827615346e-05,
      "loss": 1.0413,
      "step": 3160
    },
    {
      "epoch": 0.5246607083747103,
      "grad_norm": 10.025978088378906,
      "learning_rate": 4.4359472705763055e-05,
      "loss": 0.7898,
      "step": 3170
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 6.205329418182373,
      "learning_rate": 4.433834713537266e-05,
      "loss": 0.8492,
      "step": 3180
    },
    {
      "epoch": 0.527970870572658,
      "grad_norm": 3.651817798614502,
      "learning_rate": 4.431722156498226e-05,
      "loss": 0.734,
      "step": 3190
    },
    {
      "epoch": 0.529625951671632,
      "grad_norm": 6.25150203704834,
      "learning_rate": 4.429609599459186e-05,
      "loss": 1.1334,
      "step": 3200
    },
    {
      "epoch": 0.5312810327706058,
      "grad_norm": 8.238245964050293,
      "learning_rate": 4.4274970424201456e-05,
      "loss": 0.7096,
      "step": 3210
    },
    {
      "epoch": 0.5329361138695796,
      "grad_norm": 8.685776710510254,
      "learning_rate": 4.4253844853811054e-05,
      "loss": 0.9741,
      "step": 3220
    },
    {
      "epoch": 0.5345911949685535,
      "grad_norm": 8.368990898132324,
      "learning_rate": 4.423271928342065e-05,
      "loss": 1.2481,
      "step": 3230
    },
    {
      "epoch": 0.5362462760675273,
      "grad_norm": 9.404903411865234,
      "learning_rate": 4.421159371303025e-05,
      "loss": 0.8451,
      "step": 3240
    },
    {
      "epoch": 0.5379013571665011,
      "grad_norm": 5.186917781829834,
      "learning_rate": 4.419046814263985e-05,
      "loss": 1.1256,
      "step": 3250
    },
    {
      "epoch": 0.5395564382654751,
      "grad_norm": 1.7970277070999146,
      "learning_rate": 4.416934257224945e-05,
      "loss": 0.8035,
      "step": 3260
    },
    {
      "epoch": 0.5412115193644489,
      "grad_norm": 5.78540563583374,
      "learning_rate": 4.414821700185905e-05,
      "loss": 0.9053,
      "step": 3270
    },
    {
      "epoch": 0.5428666004634227,
      "grad_norm": 4.145542621612549,
      "learning_rate": 4.412709143146865e-05,
      "loss": 0.773,
      "step": 3280
    },
    {
      "epoch": 0.5445216815623966,
      "grad_norm": 6.171741008758545,
      "learning_rate": 4.410596586107825e-05,
      "loss": 1.0764,
      "step": 3290
    },
    {
      "epoch": 0.5461767626613704,
      "grad_norm": 1.7947089672088623,
      "learning_rate": 4.408484029068785e-05,
      "loss": 0.538,
      "step": 3300
    },
    {
      "epoch": 0.5478318437603442,
      "grad_norm": 6.015474796295166,
      "learning_rate": 4.406371472029745e-05,
      "loss": 0.6116,
      "step": 3310
    },
    {
      "epoch": 0.5494869248593182,
      "grad_norm": 5.405176162719727,
      "learning_rate": 4.404258914990705e-05,
      "loss": 0.7102,
      "step": 3320
    },
    {
      "epoch": 0.551142005958292,
      "grad_norm": 8.834525108337402,
      "learning_rate": 4.402146357951665e-05,
      "loss": 0.7622,
      "step": 3330
    },
    {
      "epoch": 0.5527970870572658,
      "grad_norm": 5.83921480178833,
      "learning_rate": 4.400033800912625e-05,
      "loss": 0.938,
      "step": 3340
    },
    {
      "epoch": 0.5544521681562397,
      "grad_norm": 6.873477458953857,
      "learning_rate": 4.397921243873585e-05,
      "loss": 0.703,
      "step": 3350
    },
    {
      "epoch": 0.5561072492552135,
      "grad_norm": 6.438570499420166,
      "learning_rate": 4.395808686834545e-05,
      "loss": 0.8803,
      "step": 3360
    },
    {
      "epoch": 0.5577623303541873,
      "grad_norm": 6.389456748962402,
      "learning_rate": 4.393696129795505e-05,
      "loss": 0.9387,
      "step": 3370
    },
    {
      "epoch": 0.5594174114531612,
      "grad_norm": 8.315299987792969,
      "learning_rate": 4.391583572756465e-05,
      "loss": 0.7676,
      "step": 3380
    },
    {
      "epoch": 0.5610724925521351,
      "grad_norm": 8.758903503417969,
      "learning_rate": 4.389471015717425e-05,
      "loss": 0.7509,
      "step": 3390
    },
    {
      "epoch": 0.5627275736511089,
      "grad_norm": 2.8985164165496826,
      "learning_rate": 4.3873584586783845e-05,
      "loss": 0.5929,
      "step": 3400
    },
    {
      "epoch": 0.5643826547500828,
      "grad_norm": 11.930763244628906,
      "learning_rate": 4.3852459016393444e-05,
      "loss": 0.9522,
      "step": 3410
    },
    {
      "epoch": 0.5660377358490566,
      "grad_norm": 9.046439170837402,
      "learning_rate": 4.383133344600304e-05,
      "loss": 0.8276,
      "step": 3420
    },
    {
      "epoch": 0.5676928169480304,
      "grad_norm": 6.21189546585083,
      "learning_rate": 4.381020787561264e-05,
      "loss": 0.8181,
      "step": 3430
    },
    {
      "epoch": 0.5693478980470043,
      "grad_norm": 8.005921363830566,
      "learning_rate": 4.3789082305222246e-05,
      "loss": 0.8295,
      "step": 3440
    },
    {
      "epoch": 0.5710029791459782,
      "grad_norm": 5.560092449188232,
      "learning_rate": 4.3767956734831844e-05,
      "loss": 0.8907,
      "step": 3450
    },
    {
      "epoch": 0.572658060244952,
      "grad_norm": 5.807984828948975,
      "learning_rate": 4.374683116444144e-05,
      "loss": 0.6233,
      "step": 3460
    },
    {
      "epoch": 0.5743131413439259,
      "grad_norm": 6.971253871917725,
      "learning_rate": 4.372570559405104e-05,
      "loss": 0.9676,
      "step": 3470
    },
    {
      "epoch": 0.5759682224428997,
      "grad_norm": 9.718036651611328,
      "learning_rate": 4.370458002366064e-05,
      "loss": 0.8843,
      "step": 3480
    },
    {
      "epoch": 0.5776233035418735,
      "grad_norm": 4.739887714385986,
      "learning_rate": 4.368345445327024e-05,
      "loss": 0.6793,
      "step": 3490
    },
    {
      "epoch": 0.5792783846408474,
      "grad_norm": 6.5463690757751465,
      "learning_rate": 4.3662328882879836e-05,
      "loss": 0.8926,
      "step": 3500
    },
    {
      "epoch": 0.5809334657398213,
      "grad_norm": 8.068760871887207,
      "learning_rate": 4.3641203312489434e-05,
      "loss": 0.7175,
      "step": 3510
    },
    {
      "epoch": 0.5825885468387951,
      "grad_norm": 4.675145626068115,
      "learning_rate": 4.362007774209904e-05,
      "loss": 1.0869,
      "step": 3520
    },
    {
      "epoch": 0.584243627937769,
      "grad_norm": 7.863040924072266,
      "learning_rate": 4.359895217170864e-05,
      "loss": 0.7445,
      "step": 3530
    },
    {
      "epoch": 0.5858987090367428,
      "grad_norm": 8.852269172668457,
      "learning_rate": 4.3577826601318236e-05,
      "loss": 1.3173,
      "step": 3540
    },
    {
      "epoch": 0.5875537901357166,
      "grad_norm": 5.696547985076904,
      "learning_rate": 4.3556701030927835e-05,
      "loss": 0.9989,
      "step": 3550
    },
    {
      "epoch": 0.5892088712346905,
      "grad_norm": 10.914084434509277,
      "learning_rate": 4.353557546053744e-05,
      "loss": 0.7949,
      "step": 3560
    },
    {
      "epoch": 0.5908639523336644,
      "grad_norm": 5.762917518615723,
      "learning_rate": 4.351444989014704e-05,
      "loss": 0.754,
      "step": 3570
    },
    {
      "epoch": 0.5925190334326382,
      "grad_norm": 7.852947235107422,
      "learning_rate": 4.3493324319756637e-05,
      "loss": 0.7189,
      "step": 3580
    },
    {
      "epoch": 0.5941741145316121,
      "grad_norm": 3.6804656982421875,
      "learning_rate": 4.3472198749366235e-05,
      "loss": 0.9927,
      "step": 3590
    },
    {
      "epoch": 0.5958291956305859,
      "grad_norm": 4.671380043029785,
      "learning_rate": 4.345107317897583e-05,
      "loss": 0.7421,
      "step": 3600
    },
    {
      "epoch": 0.5974842767295597,
      "grad_norm": 4.10764217376709,
      "learning_rate": 4.342994760858544e-05,
      "loss": 0.5662,
      "step": 3610
    },
    {
      "epoch": 0.5991393578285336,
      "grad_norm": 14.370808601379395,
      "learning_rate": 4.340882203819504e-05,
      "loss": 0.8655,
      "step": 3620
    },
    {
      "epoch": 0.6007944389275075,
      "grad_norm": 9.864126205444336,
      "learning_rate": 4.3387696467804635e-05,
      "loss": 0.9969,
      "step": 3630
    },
    {
      "epoch": 0.6024495200264813,
      "grad_norm": 4.301313877105713,
      "learning_rate": 4.3366570897414234e-05,
      "loss": 0.9638,
      "step": 3640
    },
    {
      "epoch": 0.6041046011254552,
      "grad_norm": 4.02168083190918,
      "learning_rate": 4.334544532702383e-05,
      "loss": 0.5628,
      "step": 3650
    },
    {
      "epoch": 0.605759682224429,
      "grad_norm": 9.22834300994873,
      "learning_rate": 4.332431975663343e-05,
      "loss": 0.7957,
      "step": 3660
    },
    {
      "epoch": 0.6074147633234028,
      "grad_norm": 6.008138656616211,
      "learning_rate": 4.330319418624303e-05,
      "loss": 1.0057,
      "step": 3670
    },
    {
      "epoch": 0.6090698444223767,
      "grad_norm": 7.340017795562744,
      "learning_rate": 4.328206861585263e-05,
      "loss": 0.8517,
      "step": 3680
    },
    {
      "epoch": 0.6107249255213505,
      "grad_norm": 7.928074359893799,
      "learning_rate": 4.326094304546223e-05,
      "loss": 0.7449,
      "step": 3690
    },
    {
      "epoch": 0.6123800066203244,
      "grad_norm": 8.329083442687988,
      "learning_rate": 4.323981747507183e-05,
      "loss": 0.7467,
      "step": 3700
    },
    {
      "epoch": 0.6140350877192983,
      "grad_norm": 10.044578552246094,
      "learning_rate": 4.321869190468143e-05,
      "loss": 0.7991,
      "step": 3710
    },
    {
      "epoch": 0.6156901688182721,
      "grad_norm": 1.9473140239715576,
      "learning_rate": 4.319756633429103e-05,
      "loss": 0.6097,
      "step": 3720
    },
    {
      "epoch": 0.6173452499172459,
      "grad_norm": 1.8311842679977417,
      "learning_rate": 4.3176440763900626e-05,
      "loss": 0.7742,
      "step": 3730
    },
    {
      "epoch": 0.6190003310162198,
      "grad_norm": 6.810498237609863,
      "learning_rate": 4.3155315193510224e-05,
      "loss": 0.7616,
      "step": 3740
    },
    {
      "epoch": 0.6206554121151936,
      "grad_norm": 11.013484001159668,
      "learning_rate": 4.313418962311982e-05,
      "loss": 0.7351,
      "step": 3750
    },
    {
      "epoch": 0.6223104932141675,
      "grad_norm": 6.745355606079102,
      "learning_rate": 4.311306405272942e-05,
      "loss": 1.1457,
      "step": 3760
    },
    {
      "epoch": 0.6239655743131414,
      "grad_norm": 6.882542133331299,
      "learning_rate": 4.309193848233902e-05,
      "loss": 0.7859,
      "step": 3770
    },
    {
      "epoch": 0.6256206554121152,
      "grad_norm": 15.17281436920166,
      "learning_rate": 4.3070812911948625e-05,
      "loss": 0.8339,
      "step": 3780
    },
    {
      "epoch": 0.627275736511089,
      "grad_norm": 6.302368640899658,
      "learning_rate": 4.304968734155822e-05,
      "loss": 0.844,
      "step": 3790
    },
    {
      "epoch": 0.6289308176100629,
      "grad_norm": 12.103489875793457,
      "learning_rate": 4.302856177116782e-05,
      "loss": 0.8042,
      "step": 3800
    },
    {
      "epoch": 0.6305858987090367,
      "grad_norm": 9.590091705322266,
      "learning_rate": 4.3007436200777426e-05,
      "loss": 0.8798,
      "step": 3810
    },
    {
      "epoch": 0.6322409798080106,
      "grad_norm": 8.327310562133789,
      "learning_rate": 4.2986310630387025e-05,
      "loss": 0.7716,
      "step": 3820
    },
    {
      "epoch": 0.6338960609069845,
      "grad_norm": 8.551024436950684,
      "learning_rate": 4.296518505999662e-05,
      "loss": 0.8283,
      "step": 3830
    },
    {
      "epoch": 0.6355511420059583,
      "grad_norm": 8.548646926879883,
      "learning_rate": 4.294405948960622e-05,
      "loss": 0.7693,
      "step": 3840
    },
    {
      "epoch": 0.6372062231049321,
      "grad_norm": 7.225947380065918,
      "learning_rate": 4.292293391921582e-05,
      "loss": 1.1526,
      "step": 3850
    },
    {
      "epoch": 0.638861304203906,
      "grad_norm": 7.045748233795166,
      "learning_rate": 4.2901808348825425e-05,
      "loss": 0.6072,
      "step": 3860
    },
    {
      "epoch": 0.6405163853028798,
      "grad_norm": 4.800227165222168,
      "learning_rate": 4.2880682778435024e-05,
      "loss": 0.9431,
      "step": 3870
    },
    {
      "epoch": 0.6421714664018537,
      "grad_norm": 5.048583984375,
      "learning_rate": 4.285955720804462e-05,
      "loss": 0.8312,
      "step": 3880
    },
    {
      "epoch": 0.6438265475008276,
      "grad_norm": 6.927045822143555,
      "learning_rate": 4.283843163765422e-05,
      "loss": 0.685,
      "step": 3890
    },
    {
      "epoch": 0.6454816285998014,
      "grad_norm": 5.825406074523926,
      "learning_rate": 4.281730606726382e-05,
      "loss": 0.761,
      "step": 3900
    },
    {
      "epoch": 0.6471367096987752,
      "grad_norm": 4.584779262542725,
      "learning_rate": 4.279618049687342e-05,
      "loss": 0.7183,
      "step": 3910
    },
    {
      "epoch": 0.6487917907977491,
      "grad_norm": 5.699444770812988,
      "learning_rate": 4.2775054926483015e-05,
      "loss": 0.7682,
      "step": 3920
    },
    {
      "epoch": 0.6504468718967229,
      "grad_norm": 6.175193786621094,
      "learning_rate": 4.2753929356092614e-05,
      "loss": 0.7666,
      "step": 3930
    },
    {
      "epoch": 0.6521019529956968,
      "grad_norm": 6.399385929107666,
      "learning_rate": 4.273280378570221e-05,
      "loss": 1.0835,
      "step": 3940
    },
    {
      "epoch": 0.6537570340946707,
      "grad_norm": 4.850484371185303,
      "learning_rate": 4.271167821531182e-05,
      "loss": 0.9761,
      "step": 3950
    },
    {
      "epoch": 0.6554121151936445,
      "grad_norm": 7.150592803955078,
      "learning_rate": 4.2690552644921416e-05,
      "loss": 1.0394,
      "step": 3960
    },
    {
      "epoch": 0.6570671962926183,
      "grad_norm": 6.800861835479736,
      "learning_rate": 4.2669427074531014e-05,
      "loss": 0.8463,
      "step": 3970
    },
    {
      "epoch": 0.6587222773915922,
      "grad_norm": 8.591506004333496,
      "learning_rate": 4.264830150414061e-05,
      "loss": 1.0708,
      "step": 3980
    },
    {
      "epoch": 0.660377358490566,
      "grad_norm": 10.600196838378906,
      "learning_rate": 4.262717593375021e-05,
      "loss": 0.8306,
      "step": 3990
    },
    {
      "epoch": 0.6620324395895398,
      "grad_norm": 6.348047256469727,
      "learning_rate": 4.260605036335981e-05,
      "loss": 1.1668,
      "step": 4000
    },
    {
      "epoch": 0.6636875206885138,
      "grad_norm": 5.897114276885986,
      "learning_rate": 4.258492479296941e-05,
      "loss": 0.5943,
      "step": 4010
    },
    {
      "epoch": 0.6653426017874876,
      "grad_norm": 12.60709285736084,
      "learning_rate": 4.2563799222579006e-05,
      "loss": 0.7178,
      "step": 4020
    },
    {
      "epoch": 0.6669976828864614,
      "grad_norm": 7.019259452819824,
      "learning_rate": 4.254267365218861e-05,
      "loss": 0.845,
      "step": 4030
    },
    {
      "epoch": 0.6686527639854353,
      "grad_norm": 7.819041728973389,
      "learning_rate": 4.252154808179821e-05,
      "loss": 0.898,
      "step": 4040
    },
    {
      "epoch": 0.6703078450844091,
      "grad_norm": 9.637151718139648,
      "learning_rate": 4.2500422511407815e-05,
      "loss": 0.7292,
      "step": 4050
    },
    {
      "epoch": 0.6719629261833829,
      "grad_norm": 7.209911823272705,
      "learning_rate": 4.247929694101741e-05,
      "loss": 1.0762,
      "step": 4060
    },
    {
      "epoch": 0.6736180072823569,
      "grad_norm": 8.248425483703613,
      "learning_rate": 4.245817137062701e-05,
      "loss": 0.7516,
      "step": 4070
    },
    {
      "epoch": 0.6752730883813307,
      "grad_norm": 9.18157958984375,
      "learning_rate": 4.243704580023661e-05,
      "loss": 1.0679,
      "step": 4080
    },
    {
      "epoch": 0.6769281694803045,
      "grad_norm": 9.46638011932373,
      "learning_rate": 4.241592022984621e-05,
      "loss": 0.9436,
      "step": 4090
    },
    {
      "epoch": 0.6785832505792784,
      "grad_norm": 9.864697456359863,
      "learning_rate": 4.239479465945581e-05,
      "loss": 1.05,
      "step": 4100
    },
    {
      "epoch": 0.6802383316782522,
      "grad_norm": 3.716219186782837,
      "learning_rate": 4.2373669089065405e-05,
      "loss": 0.7355,
      "step": 4110
    },
    {
      "epoch": 0.681893412777226,
      "grad_norm": 5.667409896850586,
      "learning_rate": 4.235254351867501e-05,
      "loss": 0.8628,
      "step": 4120
    },
    {
      "epoch": 0.6835484938762,
      "grad_norm": 9.33585262298584,
      "learning_rate": 4.233141794828461e-05,
      "loss": 0.8877,
      "step": 4130
    },
    {
      "epoch": 0.6852035749751738,
      "grad_norm": 5.699236869812012,
      "learning_rate": 4.231029237789421e-05,
      "loss": 0.9821,
      "step": 4140
    },
    {
      "epoch": 0.6868586560741476,
      "grad_norm": 7.901264190673828,
      "learning_rate": 4.2289166807503805e-05,
      "loss": 1.0466,
      "step": 4150
    },
    {
      "epoch": 0.6885137371731215,
      "grad_norm": 9.64258861541748,
      "learning_rate": 4.2268041237113404e-05,
      "loss": 1.2131,
      "step": 4160
    },
    {
      "epoch": 0.6901688182720953,
      "grad_norm": 2.1907119750976562,
      "learning_rate": 4.2246915666723e-05,
      "loss": 0.5354,
      "step": 4170
    },
    {
      "epoch": 0.6918238993710691,
      "grad_norm": 9.380793571472168,
      "learning_rate": 4.22257900963326e-05,
      "loss": 0.8112,
      "step": 4180
    },
    {
      "epoch": 0.6934789804700431,
      "grad_norm": 7.183408260345459,
      "learning_rate": 4.22046645259422e-05,
      "loss": 0.723,
      "step": 4190
    },
    {
      "epoch": 0.6951340615690169,
      "grad_norm": 3.154494047164917,
      "learning_rate": 4.2183538955551804e-05,
      "loss": 1.0919,
      "step": 4200
    },
    {
      "epoch": 0.6967891426679907,
      "grad_norm": 7.608426570892334,
      "learning_rate": 4.21624133851614e-05,
      "loss": 0.8934,
      "step": 4210
    },
    {
      "epoch": 0.6984442237669646,
      "grad_norm": 4.74393367767334,
      "learning_rate": 4.2141287814771e-05,
      "loss": 0.8568,
      "step": 4220
    },
    {
      "epoch": 0.7000993048659384,
      "grad_norm": 2.195302724838257,
      "learning_rate": 4.21201622443806e-05,
      "loss": 0.8487,
      "step": 4230
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 11.308738708496094,
      "learning_rate": 4.20990366739902e-05,
      "loss": 0.8214,
      "step": 4240
    },
    {
      "epoch": 0.7034094670638862,
      "grad_norm": 4.068066120147705,
      "learning_rate": 4.2077911103599796e-05,
      "loss": 0.6701,
      "step": 4250
    },
    {
      "epoch": 0.70506454816286,
      "grad_norm": 11.175095558166504,
      "learning_rate": 4.2056785533209394e-05,
      "loss": 0.8972,
      "step": 4260
    },
    {
      "epoch": 0.7067196292618338,
      "grad_norm": 7.744206428527832,
      "learning_rate": 4.2035659962819e-05,
      "loss": 0.7442,
      "step": 4270
    },
    {
      "epoch": 0.7083747103608077,
      "grad_norm": 8.913176536560059,
      "learning_rate": 4.20145343924286e-05,
      "loss": 0.7186,
      "step": 4280
    },
    {
      "epoch": 0.7100297914597815,
      "grad_norm": 7.044890403747559,
      "learning_rate": 4.1993408822038196e-05,
      "loss": 0.8237,
      "step": 4290
    },
    {
      "epoch": 0.7116848725587553,
      "grad_norm": 16.207719802856445,
      "learning_rate": 4.19722832516478e-05,
      "loss": 0.9432,
      "step": 4300
    },
    {
      "epoch": 0.7133399536577292,
      "grad_norm": 9.865875244140625,
      "learning_rate": 4.19511576812574e-05,
      "loss": 0.8535,
      "step": 4310
    },
    {
      "epoch": 0.7149950347567031,
      "grad_norm": 7.980419635772705,
      "learning_rate": 4.1930032110867e-05,
      "loss": 0.9213,
      "step": 4320
    },
    {
      "epoch": 0.716650115855677,
      "grad_norm": 4.096696853637695,
      "learning_rate": 4.1908906540476597e-05,
      "loss": 0.6829,
      "step": 4330
    },
    {
      "epoch": 0.7183051969546508,
      "grad_norm": 10.01707935333252,
      "learning_rate": 4.1887780970086195e-05,
      "loss": 0.6344,
      "step": 4340
    },
    {
      "epoch": 0.7199602780536246,
      "grad_norm": 10.038146018981934,
      "learning_rate": 4.186665539969579e-05,
      "loss": 0.5451,
      "step": 4350
    },
    {
      "epoch": 0.7216153591525984,
      "grad_norm": 10.179122924804688,
      "learning_rate": 4.184552982930539e-05,
      "loss": 1.0201,
      "step": 4360
    },
    {
      "epoch": 0.7232704402515723,
      "grad_norm": 5.868716716766357,
      "learning_rate": 4.1824404258915e-05,
      "loss": 0.811,
      "step": 4370
    },
    {
      "epoch": 0.7249255213505462,
      "grad_norm": 4.631462097167969,
      "learning_rate": 4.1803278688524595e-05,
      "loss": 0.4404,
      "step": 4380
    },
    {
      "epoch": 0.72658060244952,
      "grad_norm": 5.620421886444092,
      "learning_rate": 4.1782153118134194e-05,
      "loss": 0.8852,
      "step": 4390
    },
    {
      "epoch": 0.7282356835484939,
      "grad_norm": 10.263690948486328,
      "learning_rate": 4.176102754774379e-05,
      "loss": 1.0185,
      "step": 4400
    },
    {
      "epoch": 0.7298907646474677,
      "grad_norm": 24.96410369873047,
      "learning_rate": 4.173990197735339e-05,
      "loss": 0.8385,
      "step": 4410
    },
    {
      "epoch": 0.7315458457464415,
      "grad_norm": 9.493979454040527,
      "learning_rate": 4.171877640696299e-05,
      "loss": 0.5377,
      "step": 4420
    },
    {
      "epoch": 0.7332009268454154,
      "grad_norm": 7.22935676574707,
      "learning_rate": 4.169765083657259e-05,
      "loss": 0.6193,
      "step": 4430
    },
    {
      "epoch": 0.7348560079443893,
      "grad_norm": 7.223391056060791,
      "learning_rate": 4.1676525266182186e-05,
      "loss": 0.9331,
      "step": 4440
    },
    {
      "epoch": 0.7365110890433632,
      "grad_norm": 6.107539176940918,
      "learning_rate": 4.1655399695791784e-05,
      "loss": 0.8986,
      "step": 4450
    },
    {
      "epoch": 0.738166170142337,
      "grad_norm": 3.726116180419922,
      "learning_rate": 4.163427412540139e-05,
      "loss": 0.6729,
      "step": 4460
    },
    {
      "epoch": 0.7398212512413108,
      "grad_norm": 11.27258586883545,
      "learning_rate": 4.161314855501099e-05,
      "loss": 0.8042,
      "step": 4470
    },
    {
      "epoch": 0.7414763323402846,
      "grad_norm": 6.260288238525391,
      "learning_rate": 4.1592022984620586e-05,
      "loss": 0.8616,
      "step": 4480
    },
    {
      "epoch": 0.7431314134392585,
      "grad_norm": 6.1774821281433105,
      "learning_rate": 4.1570897414230184e-05,
      "loss": 0.8522,
      "step": 4490
    },
    {
      "epoch": 0.7447864945382324,
      "grad_norm": 8.04418659210205,
      "learning_rate": 4.154977184383978e-05,
      "loss": 0.6581,
      "step": 4500
    },
    {
      "epoch": 0.7464415756372063,
      "grad_norm": 8.601615905761719,
      "learning_rate": 4.152864627344938e-05,
      "loss": 0.872,
      "step": 4510
    },
    {
      "epoch": 0.7480966567361801,
      "grad_norm": 3.2736144065856934,
      "learning_rate": 4.1507520703058986e-05,
      "loss": 0.8518,
      "step": 4520
    },
    {
      "epoch": 0.7497517378351539,
      "grad_norm": 6.955022811889648,
      "learning_rate": 4.1486395132668585e-05,
      "loss": 0.8147,
      "step": 4530
    },
    {
      "epoch": 0.7514068189341278,
      "grad_norm": 13.691156387329102,
      "learning_rate": 4.146526956227818e-05,
      "loss": 0.724,
      "step": 4540
    },
    {
      "epoch": 0.7530619000331016,
      "grad_norm": 7.433265686035156,
      "learning_rate": 4.144414399188779e-05,
      "loss": 1.1402,
      "step": 4550
    },
    {
      "epoch": 0.7547169811320755,
      "grad_norm": 3.2315666675567627,
      "learning_rate": 4.1423018421497387e-05,
      "loss": 0.7936,
      "step": 4560
    },
    {
      "epoch": 0.7563720622310494,
      "grad_norm": 7.950416564941406,
      "learning_rate": 4.1401892851106985e-05,
      "loss": 0.765,
      "step": 4570
    },
    {
      "epoch": 0.7580271433300232,
      "grad_norm": 7.682269096374512,
      "learning_rate": 4.138076728071658e-05,
      "loss": 0.791,
      "step": 4580
    },
    {
      "epoch": 0.759682224428997,
      "grad_norm": 6.168712139129639,
      "learning_rate": 4.135964171032618e-05,
      "loss": 0.6551,
      "step": 4590
    },
    {
      "epoch": 0.7613373055279709,
      "grad_norm": 13.752392768859863,
      "learning_rate": 4.133851613993578e-05,
      "loss": 1.1661,
      "step": 4600
    },
    {
      "epoch": 0.7629923866269447,
      "grad_norm": 4.9881720542907715,
      "learning_rate": 4.131739056954538e-05,
      "loss": 1.0483,
      "step": 4610
    },
    {
      "epoch": 0.7646474677259185,
      "grad_norm": 3.2760379314422607,
      "learning_rate": 4.129626499915498e-05,
      "loss": 0.6487,
      "step": 4620
    },
    {
      "epoch": 0.7663025488248925,
      "grad_norm": 5.3312764167785645,
      "learning_rate": 4.127513942876458e-05,
      "loss": 0.8175,
      "step": 4630
    },
    {
      "epoch": 0.7679576299238663,
      "grad_norm": 7.6243977546691895,
      "learning_rate": 4.125401385837418e-05,
      "loss": 0.8402,
      "step": 4640
    },
    {
      "epoch": 0.7696127110228401,
      "grad_norm": 0.8809918165206909,
      "learning_rate": 4.123288828798378e-05,
      "loss": 0.4948,
      "step": 4650
    },
    {
      "epoch": 0.771267792121814,
      "grad_norm": 12.90571117401123,
      "learning_rate": 4.121176271759338e-05,
      "loss": 0.8692,
      "step": 4660
    },
    {
      "epoch": 0.7729228732207878,
      "grad_norm": 4.1047682762146,
      "learning_rate": 4.1190637147202976e-05,
      "loss": 1.0653,
      "step": 4670
    },
    {
      "epoch": 0.7745779543197616,
      "grad_norm": 6.952812671661377,
      "learning_rate": 4.1169511576812574e-05,
      "loss": 0.8876,
      "step": 4680
    },
    {
      "epoch": 0.7762330354187356,
      "grad_norm": 3.454697847366333,
      "learning_rate": 4.114838600642217e-05,
      "loss": 0.3122,
      "step": 4690
    },
    {
      "epoch": 0.7778881165177094,
      "grad_norm": 9.471778869628906,
      "learning_rate": 4.112726043603177e-05,
      "loss": 0.8549,
      "step": 4700
    },
    {
      "epoch": 0.7795431976166832,
      "grad_norm": 3.304105520248413,
      "learning_rate": 4.1106134865641376e-05,
      "loss": 0.7305,
      "step": 4710
    },
    {
      "epoch": 0.7811982787156571,
      "grad_norm": 5.420029163360596,
      "learning_rate": 4.1085009295250974e-05,
      "loss": 1.0034,
      "step": 4720
    },
    {
      "epoch": 0.7828533598146309,
      "grad_norm": 6.436787128448486,
      "learning_rate": 4.106388372486057e-05,
      "loss": 0.7558,
      "step": 4730
    },
    {
      "epoch": 0.7845084409136047,
      "grad_norm": 9.845918655395508,
      "learning_rate": 4.104275815447017e-05,
      "loss": 0.8401,
      "step": 4740
    },
    {
      "epoch": 0.7861635220125787,
      "grad_norm": 13.204890251159668,
      "learning_rate": 4.102163258407977e-05,
      "loss": 1.0185,
      "step": 4750
    },
    {
      "epoch": 0.7878186031115525,
      "grad_norm": 9.306239128112793,
      "learning_rate": 4.100050701368937e-05,
      "loss": 0.8975,
      "step": 4760
    },
    {
      "epoch": 0.7894736842105263,
      "grad_norm": 7.704019546508789,
      "learning_rate": 4.097938144329897e-05,
      "loss": 0.7853,
      "step": 4770
    },
    {
      "epoch": 0.7911287653095002,
      "grad_norm": 6.60499382019043,
      "learning_rate": 4.095825587290857e-05,
      "loss": 1.0487,
      "step": 4780
    },
    {
      "epoch": 0.792783846408474,
      "grad_norm": 4.177285671234131,
      "learning_rate": 4.093713030251817e-05,
      "loss": 0.7792,
      "step": 4790
    },
    {
      "epoch": 0.7944389275074478,
      "grad_norm": 7.4618821144104,
      "learning_rate": 4.0916004732127775e-05,
      "loss": 0.7785,
      "step": 4800
    },
    {
      "epoch": 0.7960940086064218,
      "grad_norm": 6.4297332763671875,
      "learning_rate": 4.089487916173737e-05,
      "loss": 0.7934,
      "step": 4810
    },
    {
      "epoch": 0.7977490897053956,
      "grad_norm": 6.923448085784912,
      "learning_rate": 4.087375359134697e-05,
      "loss": 0.819,
      "step": 4820
    },
    {
      "epoch": 0.7994041708043694,
      "grad_norm": 9.160228729248047,
      "learning_rate": 4.085262802095657e-05,
      "loss": 1.0765,
      "step": 4830
    },
    {
      "epoch": 0.8010592519033433,
      "grad_norm": 7.735018253326416,
      "learning_rate": 4.083150245056617e-05,
      "loss": 0.8465,
      "step": 4840
    },
    {
      "epoch": 0.8027143330023171,
      "grad_norm": 3.1149091720581055,
      "learning_rate": 4.081037688017577e-05,
      "loss": 0.7236,
      "step": 4850
    },
    {
      "epoch": 0.8043694141012909,
      "grad_norm": 4.222911357879639,
      "learning_rate": 4.0789251309785365e-05,
      "loss": 0.5268,
      "step": 4860
    },
    {
      "epoch": 0.8060244952002649,
      "grad_norm": 5.7403059005737305,
      "learning_rate": 4.0768125739394963e-05,
      "loss": 0.955,
      "step": 4870
    },
    {
      "epoch": 0.8076795762992387,
      "grad_norm": 6.928130149841309,
      "learning_rate": 4.074700016900457e-05,
      "loss": 0.7802,
      "step": 4880
    },
    {
      "epoch": 0.8093346573982125,
      "grad_norm": 6.3596696853637695,
      "learning_rate": 4.072587459861417e-05,
      "loss": 0.5101,
      "step": 4890
    },
    {
      "epoch": 0.8109897384971864,
      "grad_norm": 4.623471260070801,
      "learning_rate": 4.0704749028223765e-05,
      "loss": 0.9866,
      "step": 4900
    },
    {
      "epoch": 0.8126448195961602,
      "grad_norm": 5.588627815246582,
      "learning_rate": 4.0683623457833364e-05,
      "loss": 0.6054,
      "step": 4910
    },
    {
      "epoch": 0.814299900695134,
      "grad_norm": 4.1112141609191895,
      "learning_rate": 4.066249788744296e-05,
      "loss": 0.8039,
      "step": 4920
    },
    {
      "epoch": 0.8159549817941079,
      "grad_norm": 5.37169075012207,
      "learning_rate": 4.064137231705256e-05,
      "loss": 0.572,
      "step": 4930
    },
    {
      "epoch": 0.8176100628930818,
      "grad_norm": 9.34904956817627,
      "learning_rate": 4.062024674666216e-05,
      "loss": 0.8807,
      "step": 4940
    },
    {
      "epoch": 0.8192651439920556,
      "grad_norm": 9.200342178344727,
      "learning_rate": 4.059912117627176e-05,
      "loss": 0.8793,
      "step": 4950
    },
    {
      "epoch": 0.8209202250910295,
      "grad_norm": 8.0599365234375,
      "learning_rate": 4.0577995605881356e-05,
      "loss": 0.9513,
      "step": 4960
    },
    {
      "epoch": 0.8225753061900033,
      "grad_norm": 4.9278178215026855,
      "learning_rate": 4.055687003549096e-05,
      "loss": 0.5799,
      "step": 4970
    },
    {
      "epoch": 0.8242303872889771,
      "grad_norm": 2.4664244651794434,
      "learning_rate": 4.053574446510056e-05,
      "loss": 0.7996,
      "step": 4980
    },
    {
      "epoch": 0.825885468387951,
      "grad_norm": 7.465128421783447,
      "learning_rate": 4.051461889471016e-05,
      "loss": 0.9056,
      "step": 4990
    },
    {
      "epoch": 0.8275405494869249,
      "grad_norm": 2.758134365081787,
      "learning_rate": 4.0493493324319756e-05,
      "loss": 0.7909,
      "step": 5000
    },
    {
      "epoch": 0.8291956305858987,
      "grad_norm": 6.137884616851807,
      "learning_rate": 4.0472367753929354e-05,
      "loss": 0.7415,
      "step": 5010
    },
    {
      "epoch": 0.8308507116848726,
      "grad_norm": 4.549850940704346,
      "learning_rate": 4.045124218353896e-05,
      "loss": 0.8621,
      "step": 5020
    },
    {
      "epoch": 0.8325057927838464,
      "grad_norm": 3.003980875015259,
      "learning_rate": 4.043011661314856e-05,
      "loss": 0.7899,
      "step": 5030
    },
    {
      "epoch": 0.8341608738828202,
      "grad_norm": 6.8583197593688965,
      "learning_rate": 4.0408991042758156e-05,
      "loss": 0.6648,
      "step": 5040
    },
    {
      "epoch": 0.8358159549817941,
      "grad_norm": 16.9272403717041,
      "learning_rate": 4.038786547236776e-05,
      "loss": 0.7334,
      "step": 5050
    },
    {
      "epoch": 0.837471036080768,
      "grad_norm": 11.164928436279297,
      "learning_rate": 4.036673990197736e-05,
      "loss": 1.0891,
      "step": 5060
    },
    {
      "epoch": 0.8391261171797418,
      "grad_norm": 2.0962514877319336,
      "learning_rate": 4.034561433158696e-05,
      "loss": 0.8106,
      "step": 5070
    },
    {
      "epoch": 0.8407811982787157,
      "grad_norm": 8.039371490478516,
      "learning_rate": 4.032448876119656e-05,
      "loss": 0.9451,
      "step": 5080
    },
    {
      "epoch": 0.8424362793776895,
      "grad_norm": 6.283297061920166,
      "learning_rate": 4.0303363190806155e-05,
      "loss": 0.9477,
      "step": 5090
    },
    {
      "epoch": 0.8440913604766633,
      "grad_norm": 10.314887046813965,
      "learning_rate": 4.0282237620415753e-05,
      "loss": 0.8392,
      "step": 5100
    },
    {
      "epoch": 0.8457464415756372,
      "grad_norm": 7.8519673347473145,
      "learning_rate": 4.026111205002535e-05,
      "loss": 1.1044,
      "step": 5110
    },
    {
      "epoch": 0.8474015226746111,
      "grad_norm": 11.174814224243164,
      "learning_rate": 4.023998647963495e-05,
      "loss": 1.1346,
      "step": 5120
    },
    {
      "epoch": 0.8490566037735849,
      "grad_norm": 10.319960594177246,
      "learning_rate": 4.021886090924455e-05,
      "loss": 0.8132,
      "step": 5130
    },
    {
      "epoch": 0.8507116848725588,
      "grad_norm": 5.573333263397217,
      "learning_rate": 4.0197735338854154e-05,
      "loss": 0.6748,
      "step": 5140
    },
    {
      "epoch": 0.8523667659715326,
      "grad_norm": 9.64225959777832,
      "learning_rate": 4.017660976846375e-05,
      "loss": 0.839,
      "step": 5150
    },
    {
      "epoch": 0.8540218470705064,
      "grad_norm": 4.789793491363525,
      "learning_rate": 4.015548419807335e-05,
      "loss": 0.8002,
      "step": 5160
    },
    {
      "epoch": 0.8556769281694803,
      "grad_norm": 5.123698711395264,
      "learning_rate": 4.013435862768295e-05,
      "loss": 0.6224,
      "step": 5170
    },
    {
      "epoch": 0.8573320092684542,
      "grad_norm": 7.579616069793701,
      "learning_rate": 4.011323305729255e-05,
      "loss": 0.9422,
      "step": 5180
    },
    {
      "epoch": 0.858987090367428,
      "grad_norm": 10.534358978271484,
      "learning_rate": 4.0092107486902146e-05,
      "loss": 0.4776,
      "step": 5190
    },
    {
      "epoch": 0.8606421714664019,
      "grad_norm": 6.348382472991943,
      "learning_rate": 4.0070981916511744e-05,
      "loss": 0.643,
      "step": 5200
    },
    {
      "epoch": 0.8622972525653757,
      "grad_norm": 12.83508014678955,
      "learning_rate": 4.004985634612134e-05,
      "loss": 1.2666,
      "step": 5210
    },
    {
      "epoch": 0.8639523336643495,
      "grad_norm": 6.381442070007324,
      "learning_rate": 4.002873077573095e-05,
      "loss": 0.8589,
      "step": 5220
    },
    {
      "epoch": 0.8656074147633234,
      "grad_norm": 11.411808013916016,
      "learning_rate": 4.0007605205340546e-05,
      "loss": 0.8776,
      "step": 5230
    },
    {
      "epoch": 0.8672624958622972,
      "grad_norm": 7.441321849822998,
      "learning_rate": 3.9986479634950144e-05,
      "loss": 0.7178,
      "step": 5240
    },
    {
      "epoch": 0.8689175769612711,
      "grad_norm": 8.345239639282227,
      "learning_rate": 3.996535406455974e-05,
      "loss": 0.7912,
      "step": 5250
    },
    {
      "epoch": 0.870572658060245,
      "grad_norm": 7.41257381439209,
      "learning_rate": 3.994422849416935e-05,
      "loss": 0.7871,
      "step": 5260
    },
    {
      "epoch": 0.8722277391592188,
      "grad_norm": 5.282618999481201,
      "learning_rate": 3.9923102923778946e-05,
      "loss": 0.5823,
      "step": 5270
    },
    {
      "epoch": 0.8738828202581926,
      "grad_norm": 9.641045570373535,
      "learning_rate": 3.9901977353388545e-05,
      "loss": 0.788,
      "step": 5280
    },
    {
      "epoch": 0.8755379013571665,
      "grad_norm": 6.7407426834106445,
      "learning_rate": 3.988085178299814e-05,
      "loss": 0.8002,
      "step": 5290
    },
    {
      "epoch": 0.8771929824561403,
      "grad_norm": 6.914491176605225,
      "learning_rate": 3.985972621260774e-05,
      "loss": 0.7538,
      "step": 5300
    },
    {
      "epoch": 0.8788480635551142,
      "grad_norm": 6.258886814117432,
      "learning_rate": 3.9838600642217347e-05,
      "loss": 0.9445,
      "step": 5310
    },
    {
      "epoch": 0.8805031446540881,
      "grad_norm": 3.8856136798858643,
      "learning_rate": 3.9817475071826945e-05,
      "loss": 0.5205,
      "step": 5320
    },
    {
      "epoch": 0.8821582257530619,
      "grad_norm": 3.695891857147217,
      "learning_rate": 3.979634950143654e-05,
      "loss": 0.6468,
      "step": 5330
    },
    {
      "epoch": 0.8838133068520357,
      "grad_norm": 6.838665962219238,
      "learning_rate": 3.977522393104614e-05,
      "loss": 0.8835,
      "step": 5340
    },
    {
      "epoch": 0.8854683879510096,
      "grad_norm": 4.32612419128418,
      "learning_rate": 3.975409836065574e-05,
      "loss": 0.6721,
      "step": 5350
    },
    {
      "epoch": 0.8871234690499834,
      "grad_norm": 6.689059257507324,
      "learning_rate": 3.973297279026534e-05,
      "loss": 0.9238,
      "step": 5360
    },
    {
      "epoch": 0.8887785501489573,
      "grad_norm": 8.288667678833008,
      "learning_rate": 3.971184721987494e-05,
      "loss": 0.9151,
      "step": 5370
    },
    {
      "epoch": 0.8904336312479312,
      "grad_norm": 3.008657932281494,
      "learning_rate": 3.9690721649484535e-05,
      "loss": 0.7425,
      "step": 5380
    },
    {
      "epoch": 0.892088712346905,
      "grad_norm": 6.7204084396362305,
      "learning_rate": 3.966959607909414e-05,
      "loss": 1.02,
      "step": 5390
    },
    {
      "epoch": 0.8937437934458788,
      "grad_norm": 8.979735374450684,
      "learning_rate": 3.964847050870374e-05,
      "loss": 0.645,
      "step": 5400
    },
    {
      "epoch": 0.8953988745448527,
      "grad_norm": 3.6422133445739746,
      "learning_rate": 3.962734493831334e-05,
      "loss": 0.6467,
      "step": 5410
    },
    {
      "epoch": 0.8970539556438265,
      "grad_norm": 11.633221626281738,
      "learning_rate": 3.9606219367922936e-05,
      "loss": 0.8949,
      "step": 5420
    },
    {
      "epoch": 0.8987090367428004,
      "grad_norm": 5.105805397033691,
      "learning_rate": 3.9585093797532534e-05,
      "loss": 0.8518,
      "step": 5430
    },
    {
      "epoch": 0.9003641178417743,
      "grad_norm": 6.702571392059326,
      "learning_rate": 3.956396822714213e-05,
      "loss": 0.6298,
      "step": 5440
    },
    {
      "epoch": 0.9020191989407481,
      "grad_norm": 3.891049385070801,
      "learning_rate": 3.954284265675173e-05,
      "loss": 0.5076,
      "step": 5450
    },
    {
      "epoch": 0.9036742800397219,
      "grad_norm": 7.306512832641602,
      "learning_rate": 3.952171708636133e-05,
      "loss": 0.7208,
      "step": 5460
    },
    {
      "epoch": 0.9053293611386958,
      "grad_norm": 8.937118530273438,
      "learning_rate": 3.950059151597093e-05,
      "loss": 0.7317,
      "step": 5470
    },
    {
      "epoch": 0.9069844422376696,
      "grad_norm": 9.849403381347656,
      "learning_rate": 3.947946594558053e-05,
      "loss": 1.0634,
      "step": 5480
    },
    {
      "epoch": 0.9086395233366436,
      "grad_norm": 4.4751129150390625,
      "learning_rate": 3.945834037519013e-05,
      "loss": 0.7869,
      "step": 5490
    },
    {
      "epoch": 0.9102946044356174,
      "grad_norm": 9.121310234069824,
      "learning_rate": 3.943721480479973e-05,
      "loss": 1.2411,
      "step": 5500
    },
    {
      "epoch": 0.9119496855345912,
      "grad_norm": 4.661601543426514,
      "learning_rate": 3.9416089234409335e-05,
      "loss": 0.797,
      "step": 5510
    },
    {
      "epoch": 0.913604766633565,
      "grad_norm": 9.620807647705078,
      "learning_rate": 3.939496366401893e-05,
      "loss": 0.8303,
      "step": 5520
    },
    {
      "epoch": 0.9152598477325389,
      "grad_norm": 8.210502624511719,
      "learning_rate": 3.937383809362853e-05,
      "loss": 0.6475,
      "step": 5530
    },
    {
      "epoch": 0.9169149288315127,
      "grad_norm": 6.49641752243042,
      "learning_rate": 3.935271252323813e-05,
      "loss": 1.0418,
      "step": 5540
    },
    {
      "epoch": 0.9185700099304865,
      "grad_norm": 5.502089977264404,
      "learning_rate": 3.933158695284773e-05,
      "loss": 0.8705,
      "step": 5550
    },
    {
      "epoch": 0.9202250910294605,
      "grad_norm": 5.70926570892334,
      "learning_rate": 3.931046138245733e-05,
      "loss": 0.6527,
      "step": 5560
    },
    {
      "epoch": 0.9218801721284343,
      "grad_norm": 8.291305541992188,
      "learning_rate": 3.928933581206693e-05,
      "loss": 0.7902,
      "step": 5570
    },
    {
      "epoch": 0.9235352532274081,
      "grad_norm": 3.212561845779419,
      "learning_rate": 3.926821024167653e-05,
      "loss": 0.5282,
      "step": 5580
    },
    {
      "epoch": 0.925190334326382,
      "grad_norm": 13.752019882202148,
      "learning_rate": 3.924708467128613e-05,
      "loss": 1.0058,
      "step": 5590
    },
    {
      "epoch": 0.9268454154253558,
      "grad_norm": 9.438868522644043,
      "learning_rate": 3.922595910089573e-05,
      "loss": 1.0936,
      "step": 5600
    },
    {
      "epoch": 0.9285004965243296,
      "grad_norm": 3.17415189743042,
      "learning_rate": 3.9204833530505325e-05,
      "loss": 0.8907,
      "step": 5610
    },
    {
      "epoch": 0.9301555776233036,
      "grad_norm": 6.175893306732178,
      "learning_rate": 3.9183707960114924e-05,
      "loss": 0.6455,
      "step": 5620
    },
    {
      "epoch": 0.9318106587222774,
      "grad_norm": 8.105611801147461,
      "learning_rate": 3.916258238972452e-05,
      "loss": 0.7816,
      "step": 5630
    },
    {
      "epoch": 0.9334657398212513,
      "grad_norm": 6.192255020141602,
      "learning_rate": 3.914145681933412e-05,
      "loss": 0.8329,
      "step": 5640
    },
    {
      "epoch": 0.9351208209202251,
      "grad_norm": 7.72561502456665,
      "learning_rate": 3.9120331248943725e-05,
      "loss": 0.7949,
      "step": 5650
    },
    {
      "epoch": 0.9367759020191989,
      "grad_norm": 2.6464052200317383,
      "learning_rate": 3.9099205678553324e-05,
      "loss": 0.9183,
      "step": 5660
    },
    {
      "epoch": 0.9384309831181727,
      "grad_norm": 7.433149337768555,
      "learning_rate": 3.907808010816292e-05,
      "loss": 0.5919,
      "step": 5670
    },
    {
      "epoch": 0.9400860642171467,
      "grad_norm": 5.4882283210754395,
      "learning_rate": 3.905695453777252e-05,
      "loss": 0.5833,
      "step": 5680
    },
    {
      "epoch": 0.9417411453161205,
      "grad_norm": 7.088761329650879,
      "learning_rate": 3.903582896738212e-05,
      "loss": 0.669,
      "step": 5690
    },
    {
      "epoch": 0.9433962264150944,
      "grad_norm": 8.576591491699219,
      "learning_rate": 3.901470339699172e-05,
      "loss": 0.7211,
      "step": 5700
    },
    {
      "epoch": 0.9450513075140682,
      "grad_norm": 13.387581825256348,
      "learning_rate": 3.8993577826601316e-05,
      "loss": 0.8661,
      "step": 5710
    },
    {
      "epoch": 0.946706388613042,
      "grad_norm": 5.815371990203857,
      "learning_rate": 3.8972452256210914e-05,
      "loss": 0.5873,
      "step": 5720
    },
    {
      "epoch": 0.9483614697120158,
      "grad_norm": 3.7529213428497314,
      "learning_rate": 3.895132668582052e-05,
      "loss": 0.7352,
      "step": 5730
    },
    {
      "epoch": 0.9500165508109898,
      "grad_norm": 3.91986346244812,
      "learning_rate": 3.893020111543012e-05,
      "loss": 0.8345,
      "step": 5740
    },
    {
      "epoch": 0.9516716319099636,
      "grad_norm": 4.097564220428467,
      "learning_rate": 3.8909075545039716e-05,
      "loss": 0.8311,
      "step": 5750
    },
    {
      "epoch": 0.9533267130089375,
      "grad_norm": 5.831536769866943,
      "learning_rate": 3.888794997464932e-05,
      "loss": 0.651,
      "step": 5760
    },
    {
      "epoch": 0.9549817941079113,
      "grad_norm": 3.1209757328033447,
      "learning_rate": 3.886682440425892e-05,
      "loss": 0.9341,
      "step": 5770
    },
    {
      "epoch": 0.9566368752068851,
      "grad_norm": 8.368700981140137,
      "learning_rate": 3.884569883386852e-05,
      "loss": 0.565,
      "step": 5780
    },
    {
      "epoch": 0.958291956305859,
      "grad_norm": 4.004268169403076,
      "learning_rate": 3.8824573263478116e-05,
      "loss": 0.6752,
      "step": 5790
    },
    {
      "epoch": 0.9599470374048328,
      "grad_norm": 6.96585750579834,
      "learning_rate": 3.8803447693087715e-05,
      "loss": 0.6275,
      "step": 5800
    },
    {
      "epoch": 0.9616021185038067,
      "grad_norm": 11.93501091003418,
      "learning_rate": 3.878232212269731e-05,
      "loss": 0.8464,
      "step": 5810
    },
    {
      "epoch": 0.9632571996027806,
      "grad_norm": 7.300952434539795,
      "learning_rate": 3.876119655230692e-05,
      "loss": 1.0381,
      "step": 5820
    },
    {
      "epoch": 0.9649122807017544,
      "grad_norm": 3.9323360919952393,
      "learning_rate": 3.874007098191652e-05,
      "loss": 0.798,
      "step": 5830
    },
    {
      "epoch": 0.9665673618007282,
      "grad_norm": 8.626023292541504,
      "learning_rate": 3.8718945411526115e-05,
      "loss": 0.8782,
      "step": 5840
    },
    {
      "epoch": 0.968222442899702,
      "grad_norm": 9.177884101867676,
      "learning_rate": 3.8697819841135713e-05,
      "loss": 0.6883,
      "step": 5850
    },
    {
      "epoch": 0.9698775239986759,
      "grad_norm": 5.639456272125244,
      "learning_rate": 3.867669427074531e-05,
      "loss": 0.8211,
      "step": 5860
    },
    {
      "epoch": 0.9715326050976498,
      "grad_norm": 2.318056106567383,
      "learning_rate": 3.865556870035491e-05,
      "loss": 0.5665,
      "step": 5870
    },
    {
      "epoch": 0.9731876861966237,
      "grad_norm": 9.1118803024292,
      "learning_rate": 3.863444312996451e-05,
      "loss": 0.8282,
      "step": 5880
    },
    {
      "epoch": 0.9748427672955975,
      "grad_norm": 4.295466899871826,
      "learning_rate": 3.861331755957411e-05,
      "loss": 0.6521,
      "step": 5890
    },
    {
      "epoch": 0.9764978483945713,
      "grad_norm": 11.82414722442627,
      "learning_rate": 3.859219198918371e-05,
      "loss": 0.7619,
      "step": 5900
    },
    {
      "epoch": 0.9781529294935452,
      "grad_norm": 2.394678831100464,
      "learning_rate": 3.857106641879331e-05,
      "loss": 1.0147,
      "step": 5910
    },
    {
      "epoch": 0.979808010592519,
      "grad_norm": 8.692333221435547,
      "learning_rate": 3.854994084840291e-05,
      "loss": 0.5116,
      "step": 5920
    },
    {
      "epoch": 0.9814630916914929,
      "grad_norm": 7.333255767822266,
      "learning_rate": 3.852881527801251e-05,
      "loss": 0.965,
      "step": 5930
    },
    {
      "epoch": 0.9831181727904668,
      "grad_norm": 7.43820333480835,
      "learning_rate": 3.8507689707622106e-05,
      "loss": 0.7462,
      "step": 5940
    },
    {
      "epoch": 0.9847732538894406,
      "grad_norm": 4.187070369720459,
      "learning_rate": 3.8486564137231704e-05,
      "loss": 0.8058,
      "step": 5950
    },
    {
      "epoch": 0.9864283349884144,
      "grad_norm": 3.2813329696655273,
      "learning_rate": 3.84654385668413e-05,
      "loss": 0.6014,
      "step": 5960
    },
    {
      "epoch": 0.9880834160873883,
      "grad_norm": 4.6697096824646,
      "learning_rate": 3.84443129964509e-05,
      "loss": 0.7311,
      "step": 5970
    },
    {
      "epoch": 0.9897384971863621,
      "grad_norm": 7.3077898025512695,
      "learning_rate": 3.8423187426060506e-05,
      "loss": 0.7493,
      "step": 5980
    },
    {
      "epoch": 0.991393578285336,
      "grad_norm": 9.459609985351562,
      "learning_rate": 3.8402061855670104e-05,
      "loss": 0.7926,
      "step": 5990
    },
    {
      "epoch": 0.9930486593843099,
      "grad_norm": 9.969450950622559,
      "learning_rate": 3.83809362852797e-05,
      "loss": 0.8097,
      "step": 6000
    },
    {
      "epoch": 0.9947037404832837,
      "grad_norm": 12.550638198852539,
      "learning_rate": 3.835981071488931e-05,
      "loss": 0.7239,
      "step": 6010
    },
    {
      "epoch": 0.9963588215822575,
      "grad_norm": 7.678092002868652,
      "learning_rate": 3.8338685144498906e-05,
      "loss": 0.7383,
      "step": 6020
    },
    {
      "epoch": 0.9980139026812314,
      "grad_norm": 4.705316543579102,
      "learning_rate": 3.8317559574108505e-05,
      "loss": 0.7249,
      "step": 6030
    },
    {
      "epoch": 0.9996689837802052,
      "grad_norm": 11.151215553283691,
      "learning_rate": 3.82964340037181e-05,
      "loss": 0.6281,
      "step": 6040
    },
    {
      "epoch": 1.001324064879179,
      "grad_norm": 6.518406867980957,
      "learning_rate": 3.82753084333277e-05,
      "loss": 0.9236,
      "step": 6050
    },
    {
      "epoch": 1.002979145978153,
      "grad_norm": 2.786750555038452,
      "learning_rate": 3.82541828629373e-05,
      "loss": 0.6503,
      "step": 6060
    },
    {
      "epoch": 1.0046342270771267,
      "grad_norm": 4.405419826507568,
      "learning_rate": 3.8233057292546905e-05,
      "loss": 0.6991,
      "step": 6070
    },
    {
      "epoch": 1.0062893081761006,
      "grad_norm": 9.034991264343262,
      "learning_rate": 3.8211931722156503e-05,
      "loss": 0.6314,
      "step": 6080
    },
    {
      "epoch": 1.0079443892750746,
      "grad_norm": 8.976570129394531,
      "learning_rate": 3.81908061517661e-05,
      "loss": 0.7638,
      "step": 6090
    },
    {
      "epoch": 1.0095994703740483,
      "grad_norm": 11.087831497192383,
      "learning_rate": 3.81696805813757e-05,
      "loss": 0.7247,
      "step": 6100
    },
    {
      "epoch": 1.0112545514730222,
      "grad_norm": 7.215321063995361,
      "learning_rate": 3.81485550109853e-05,
      "loss": 0.7285,
      "step": 6110
    },
    {
      "epoch": 1.012909632571996,
      "grad_norm": 5.415118217468262,
      "learning_rate": 3.81274294405949e-05,
      "loss": 0.7109,
      "step": 6120
    },
    {
      "epoch": 1.01456471367097,
      "grad_norm": 2.8213400840759277,
      "learning_rate": 3.8106303870204495e-05,
      "loss": 0.5543,
      "step": 6130
    },
    {
      "epoch": 1.0162197947699436,
      "grad_norm": 7.7865824699401855,
      "learning_rate": 3.8085178299814094e-05,
      "loss": 0.5905,
      "step": 6140
    },
    {
      "epoch": 1.0178748758689176,
      "grad_norm": 8.589634895324707,
      "learning_rate": 3.806405272942369e-05,
      "loss": 0.5962,
      "step": 6150
    },
    {
      "epoch": 1.0195299569678915,
      "grad_norm": 9.689987182617188,
      "learning_rate": 3.80429271590333e-05,
      "loss": 0.6255,
      "step": 6160
    },
    {
      "epoch": 1.0211850380668652,
      "grad_norm": 6.403554916381836,
      "learning_rate": 3.8021801588642896e-05,
      "loss": 0.783,
      "step": 6170
    },
    {
      "epoch": 1.0228401191658392,
      "grad_norm": 10.512937545776367,
      "learning_rate": 3.8000676018252494e-05,
      "loss": 0.808,
      "step": 6180
    },
    {
      "epoch": 1.0244952002648129,
      "grad_norm": 6.386028289794922,
      "learning_rate": 3.797955044786209e-05,
      "loss": 0.9294,
      "step": 6190
    },
    {
      "epoch": 1.0261502813637868,
      "grad_norm": 12.173444747924805,
      "learning_rate": 3.795842487747169e-05,
      "loss": 1.0584,
      "step": 6200
    },
    {
      "epoch": 1.0278053624627608,
      "grad_norm": 10.3660249710083,
      "learning_rate": 3.793729930708129e-05,
      "loss": 0.5132,
      "step": 6210
    },
    {
      "epoch": 1.0294604435617345,
      "grad_norm": 5.885777950286865,
      "learning_rate": 3.791617373669089e-05,
      "loss": 0.8248,
      "step": 6220
    },
    {
      "epoch": 1.0311155246607084,
      "grad_norm": 11.990609169006348,
      "learning_rate": 3.789504816630049e-05,
      "loss": 0.6344,
      "step": 6230
    },
    {
      "epoch": 1.0327706057596822,
      "grad_norm": 8.579591751098633,
      "learning_rate": 3.787392259591009e-05,
      "loss": 1.164,
      "step": 6240
    },
    {
      "epoch": 1.034425686858656,
      "grad_norm": 4.441565990447998,
      "learning_rate": 3.785279702551969e-05,
      "loss": 0.8635,
      "step": 6250
    },
    {
      "epoch": 1.0360807679576298,
      "grad_norm": 5.815997123718262,
      "learning_rate": 3.7831671455129295e-05,
      "loss": 0.8239,
      "step": 6260
    },
    {
      "epoch": 1.0377358490566038,
      "grad_norm": 4.0193376541137695,
      "learning_rate": 3.781054588473889e-05,
      "loss": 0.5734,
      "step": 6270
    },
    {
      "epoch": 1.0393909301555777,
      "grad_norm": 2.8566458225250244,
      "learning_rate": 3.778942031434849e-05,
      "loss": 0.7098,
      "step": 6280
    },
    {
      "epoch": 1.0410460112545514,
      "grad_norm": 4.620614528656006,
      "learning_rate": 3.776829474395809e-05,
      "loss": 0.7029,
      "step": 6290
    },
    {
      "epoch": 1.0427010923535254,
      "grad_norm": 5.069474697113037,
      "learning_rate": 3.774716917356769e-05,
      "loss": 0.5052,
      "step": 6300
    },
    {
      "epoch": 1.044356173452499,
      "grad_norm": 10.774474143981934,
      "learning_rate": 3.7726043603177287e-05,
      "loss": 1.0114,
      "step": 6310
    },
    {
      "epoch": 1.046011254551473,
      "grad_norm": 6.714102745056152,
      "learning_rate": 3.7704918032786885e-05,
      "loss": 0.604,
      "step": 6320
    },
    {
      "epoch": 1.047666335650447,
      "grad_norm": 7.08482551574707,
      "learning_rate": 3.768379246239649e-05,
      "loss": 0.9567,
      "step": 6330
    },
    {
      "epoch": 1.0493214167494207,
      "grad_norm": 3.9563658237457275,
      "learning_rate": 3.766266689200609e-05,
      "loss": 0.4573,
      "step": 6340
    },
    {
      "epoch": 1.0509764978483946,
      "grad_norm": 5.296104431152344,
      "learning_rate": 3.764154132161569e-05,
      "loss": 0.5425,
      "step": 6350
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 11.595404624938965,
      "learning_rate": 3.7620415751225285e-05,
      "loss": 0.6648,
      "step": 6360
    },
    {
      "epoch": 1.0542866600463423,
      "grad_norm": 4.364054203033447,
      "learning_rate": 3.7599290180834884e-05,
      "loss": 0.6395,
      "step": 6370
    },
    {
      "epoch": 1.055941741145316,
      "grad_norm": 2.6158525943756104,
      "learning_rate": 3.757816461044448e-05,
      "loss": 0.6826,
      "step": 6380
    },
    {
      "epoch": 1.05759682224429,
      "grad_norm": 6.0076117515563965,
      "learning_rate": 3.755703904005408e-05,
      "loss": 0.7826,
      "step": 6390
    },
    {
      "epoch": 1.059251903343264,
      "grad_norm": 8.394646644592285,
      "learning_rate": 3.753591346966368e-05,
      "loss": 0.6281,
      "step": 6400
    },
    {
      "epoch": 1.0609069844422376,
      "grad_norm": 8.699710845947266,
      "learning_rate": 3.7514787899273284e-05,
      "loss": 0.7419,
      "step": 6410
    },
    {
      "epoch": 1.0625620655412116,
      "grad_norm": 15.326727867126465,
      "learning_rate": 3.749366232888288e-05,
      "loss": 0.698,
      "step": 6420
    },
    {
      "epoch": 1.0642171466401853,
      "grad_norm": 9.417495727539062,
      "learning_rate": 3.747253675849248e-05,
      "loss": 0.8707,
      "step": 6430
    },
    {
      "epoch": 1.0658722277391592,
      "grad_norm": 15.093852996826172,
      "learning_rate": 3.745141118810208e-05,
      "loss": 0.6771,
      "step": 6440
    },
    {
      "epoch": 1.067527308838133,
      "grad_norm": 5.133484363555908,
      "learning_rate": 3.743028561771168e-05,
      "loss": 0.8722,
      "step": 6450
    },
    {
      "epoch": 1.069182389937107,
      "grad_norm": 3.690852403640747,
      "learning_rate": 3.7409160047321276e-05,
      "loss": 0.9093,
      "step": 6460
    },
    {
      "epoch": 1.0708374710360808,
      "grad_norm": 4.166110992431641,
      "learning_rate": 3.738803447693088e-05,
      "loss": 0.6697,
      "step": 6470
    },
    {
      "epoch": 1.0724925521350546,
      "grad_norm": 10.393726348876953,
      "learning_rate": 3.736690890654048e-05,
      "loss": 0.8661,
      "step": 6480
    },
    {
      "epoch": 1.0741476332340285,
      "grad_norm": 3.2394423484802246,
      "learning_rate": 3.734578333615008e-05,
      "loss": 0.7986,
      "step": 6490
    },
    {
      "epoch": 1.0758027143330022,
      "grad_norm": 5.046095848083496,
      "learning_rate": 3.732465776575968e-05,
      "loss": 0.8333,
      "step": 6500
    },
    {
      "epoch": 1.0774577954319762,
      "grad_norm": 7.35997200012207,
      "learning_rate": 3.730353219536928e-05,
      "loss": 0.8187,
      "step": 6510
    },
    {
      "epoch": 1.0791128765309501,
      "grad_norm": 3.15278959274292,
      "learning_rate": 3.728240662497888e-05,
      "loss": 0.7423,
      "step": 6520
    },
    {
      "epoch": 1.0807679576299238,
      "grad_norm": 5.886096477508545,
      "learning_rate": 3.726128105458848e-05,
      "loss": 0.5739,
      "step": 6530
    },
    {
      "epoch": 1.0824230387288978,
      "grad_norm": 8.116938591003418,
      "learning_rate": 3.7240155484198076e-05,
      "loss": 0.6777,
      "step": 6540
    },
    {
      "epoch": 1.0840781198278715,
      "grad_norm": 8.417621612548828,
      "learning_rate": 3.7219029913807675e-05,
      "loss": 0.8631,
      "step": 6550
    },
    {
      "epoch": 1.0857332009268454,
      "grad_norm": 7.217428684234619,
      "learning_rate": 3.719790434341727e-05,
      "loss": 0.8336,
      "step": 6560
    },
    {
      "epoch": 1.0873882820258192,
      "grad_norm": 1.7699183225631714,
      "learning_rate": 3.717677877302687e-05,
      "loss": 0.6011,
      "step": 6570
    },
    {
      "epoch": 1.089043363124793,
      "grad_norm": 4.5065083503723145,
      "learning_rate": 3.715565320263648e-05,
      "loss": 0.5588,
      "step": 6580
    },
    {
      "epoch": 1.090698444223767,
      "grad_norm": 8.196365356445312,
      "learning_rate": 3.7134527632246075e-05,
      "loss": 0.5082,
      "step": 6590
    },
    {
      "epoch": 1.0923535253227408,
      "grad_norm": 12.140345573425293,
      "learning_rate": 3.7113402061855674e-05,
      "loss": 0.8012,
      "step": 6600
    },
    {
      "epoch": 1.0940086064217147,
      "grad_norm": 6.861212730407715,
      "learning_rate": 3.709227649146527e-05,
      "loss": 0.8159,
      "step": 6610
    },
    {
      "epoch": 1.0956636875206884,
      "grad_norm": 0.9522998332977295,
      "learning_rate": 3.707115092107487e-05,
      "loss": 0.4879,
      "step": 6620
    },
    {
      "epoch": 1.0973187686196624,
      "grad_norm": 4.11716890335083,
      "learning_rate": 3.705002535068447e-05,
      "loss": 0.636,
      "step": 6630
    },
    {
      "epoch": 1.0989738497186363,
      "grad_norm": 4.967156887054443,
      "learning_rate": 3.702889978029407e-05,
      "loss": 0.7844,
      "step": 6640
    },
    {
      "epoch": 1.10062893081761,
      "grad_norm": 9.6195650100708,
      "learning_rate": 3.7007774209903665e-05,
      "loss": 0.5735,
      "step": 6650
    },
    {
      "epoch": 1.102284011916584,
      "grad_norm": 8.064162254333496,
      "learning_rate": 3.6986648639513264e-05,
      "loss": 0.7889,
      "step": 6660
    },
    {
      "epoch": 1.1039390930155577,
      "grad_norm": 2.745148181915283,
      "learning_rate": 3.696552306912287e-05,
      "loss": 0.9982,
      "step": 6670
    },
    {
      "epoch": 1.1055941741145316,
      "grad_norm": 12.541773796081543,
      "learning_rate": 3.694439749873247e-05,
      "loss": 0.8908,
      "step": 6680
    },
    {
      "epoch": 1.1072492552135054,
      "grad_norm": 7.391824722290039,
      "learning_rate": 3.6923271928342066e-05,
      "loss": 0.6141,
      "step": 6690
    },
    {
      "epoch": 1.1089043363124793,
      "grad_norm": 4.546953201293945,
      "learning_rate": 3.6902146357951664e-05,
      "loss": 0.4683,
      "step": 6700
    },
    {
      "epoch": 1.1105594174114533,
      "grad_norm": 10.516761779785156,
      "learning_rate": 3.688102078756126e-05,
      "loss": 0.88,
      "step": 6710
    },
    {
      "epoch": 1.112214498510427,
      "grad_norm": 5.7534284591674805,
      "learning_rate": 3.685989521717087e-05,
      "loss": 0.6763,
      "step": 6720
    },
    {
      "epoch": 1.113869579609401,
      "grad_norm": 8.477033615112305,
      "learning_rate": 3.6838769646780466e-05,
      "loss": 0.6031,
      "step": 6730
    },
    {
      "epoch": 1.1155246607083746,
      "grad_norm": 10.461743354797363,
      "learning_rate": 3.6817644076390064e-05,
      "loss": 0.8644,
      "step": 6740
    },
    {
      "epoch": 1.1171797418073486,
      "grad_norm": 4.804139137268066,
      "learning_rate": 3.679651850599967e-05,
      "loss": 1.0702,
      "step": 6750
    },
    {
      "epoch": 1.1188348229063223,
      "grad_norm": 11.583818435668945,
      "learning_rate": 3.677539293560927e-05,
      "loss": 1.0061,
      "step": 6760
    },
    {
      "epoch": 1.1204899040052962,
      "grad_norm": 4.435029983520508,
      "learning_rate": 3.6754267365218866e-05,
      "loss": 0.9434,
      "step": 6770
    },
    {
      "epoch": 1.1221449851042702,
      "grad_norm": 5.4313788414001465,
      "learning_rate": 3.6733141794828465e-05,
      "loss": 0.5003,
      "step": 6780
    },
    {
      "epoch": 1.123800066203244,
      "grad_norm": 7.3584794998168945,
      "learning_rate": 3.671201622443806e-05,
      "loss": 0.6557,
      "step": 6790
    },
    {
      "epoch": 1.1254551473022179,
      "grad_norm": 3.4365134239196777,
      "learning_rate": 3.669089065404766e-05,
      "loss": 0.6309,
      "step": 6800
    },
    {
      "epoch": 1.1271102284011916,
      "grad_norm": 10.767348289489746,
      "learning_rate": 3.666976508365726e-05,
      "loss": 0.7167,
      "step": 6810
    },
    {
      "epoch": 1.1287653095001655,
      "grad_norm": 12.988149642944336,
      "learning_rate": 3.664863951326686e-05,
      "loss": 0.8825,
      "step": 6820
    },
    {
      "epoch": 1.1304203905991392,
      "grad_norm": 5.682524681091309,
      "learning_rate": 3.662751394287646e-05,
      "loss": 0.4676,
      "step": 6830
    },
    {
      "epoch": 1.1320754716981132,
      "grad_norm": 10.703333854675293,
      "learning_rate": 3.660638837248606e-05,
      "loss": 0.6629,
      "step": 6840
    },
    {
      "epoch": 1.1337305527970871,
      "grad_norm": 7.927700996398926,
      "learning_rate": 3.658526280209566e-05,
      "loss": 0.7073,
      "step": 6850
    },
    {
      "epoch": 1.1353856338960608,
      "grad_norm": 8.109436988830566,
      "learning_rate": 3.656413723170526e-05,
      "loss": 0.6591,
      "step": 6860
    },
    {
      "epoch": 1.1370407149950348,
      "grad_norm": 6.961367607116699,
      "learning_rate": 3.654301166131486e-05,
      "loss": 1.0573,
      "step": 6870
    },
    {
      "epoch": 1.1386957960940087,
      "grad_norm": 7.393187046051025,
      "learning_rate": 3.6521886090924455e-05,
      "loss": 0.5463,
      "step": 6880
    },
    {
      "epoch": 1.1403508771929824,
      "grad_norm": 3.9798266887664795,
      "learning_rate": 3.6500760520534054e-05,
      "loss": 0.7738,
      "step": 6890
    },
    {
      "epoch": 1.1420059582919564,
      "grad_norm": 5.561964988708496,
      "learning_rate": 3.647963495014365e-05,
      "loss": 0.6792,
      "step": 6900
    },
    {
      "epoch": 1.1436610393909301,
      "grad_norm": 8.877525329589844,
      "learning_rate": 3.645850937975325e-05,
      "loss": 0.7545,
      "step": 6910
    },
    {
      "epoch": 1.145316120489904,
      "grad_norm": 8.257837295532227,
      "learning_rate": 3.6437383809362856e-05,
      "loss": 0.655,
      "step": 6920
    },
    {
      "epoch": 1.1469712015888778,
      "grad_norm": 3.726146936416626,
      "learning_rate": 3.6416258238972454e-05,
      "loss": 0.7954,
      "step": 6930
    },
    {
      "epoch": 1.1486262826878517,
      "grad_norm": 8.134098052978516,
      "learning_rate": 3.639513266858205e-05,
      "loss": 0.6725,
      "step": 6940
    },
    {
      "epoch": 1.1502813637868257,
      "grad_norm": 8.043886184692383,
      "learning_rate": 3.637400709819165e-05,
      "loss": 0.8112,
      "step": 6950
    },
    {
      "epoch": 1.1519364448857994,
      "grad_norm": 9.990250587463379,
      "learning_rate": 3.635288152780125e-05,
      "loss": 1.2451,
      "step": 6960
    },
    {
      "epoch": 1.1535915259847733,
      "grad_norm": 5.516153812408447,
      "learning_rate": 3.6331755957410854e-05,
      "loss": 0.5153,
      "step": 6970
    },
    {
      "epoch": 1.155246607083747,
      "grad_norm": 1.6596720218658447,
      "learning_rate": 3.631063038702045e-05,
      "loss": 0.7634,
      "step": 6980
    },
    {
      "epoch": 1.156901688182721,
      "grad_norm": 4.818734645843506,
      "learning_rate": 3.628950481663005e-05,
      "loss": 0.7347,
      "step": 6990
    },
    {
      "epoch": 1.1585567692816947,
      "grad_norm": 14.623958587646484,
      "learning_rate": 3.626837924623965e-05,
      "loss": 0.5582,
      "step": 7000
    },
    {
      "epoch": 1.1602118503806687,
      "grad_norm": 4.6976237297058105,
      "learning_rate": 3.6247253675849255e-05,
      "loss": 0.7431,
      "step": 7010
    },
    {
      "epoch": 1.1618669314796426,
      "grad_norm": 6.487015724182129,
      "learning_rate": 3.622612810545885e-05,
      "loss": 0.5759,
      "step": 7020
    },
    {
      "epoch": 1.1635220125786163,
      "grad_norm": 15.384269714355469,
      "learning_rate": 3.620500253506845e-05,
      "loss": 0.8358,
      "step": 7030
    },
    {
      "epoch": 1.1651770936775903,
      "grad_norm": 10.690239906311035,
      "learning_rate": 3.618387696467805e-05,
      "loss": 0.8372,
      "step": 7040
    },
    {
      "epoch": 1.166832174776564,
      "grad_norm": 9.670157432556152,
      "learning_rate": 3.616275139428765e-05,
      "loss": 0.8187,
      "step": 7050
    },
    {
      "epoch": 1.168487255875538,
      "grad_norm": 10.020424842834473,
      "learning_rate": 3.614162582389725e-05,
      "loss": 0.7706,
      "step": 7060
    },
    {
      "epoch": 1.1701423369745116,
      "grad_norm": 10.113683700561523,
      "learning_rate": 3.6120500253506845e-05,
      "loss": 0.9841,
      "step": 7070
    },
    {
      "epoch": 1.1717974180734856,
      "grad_norm": 11.158884048461914,
      "learning_rate": 3.6099374683116443e-05,
      "loss": 0.8783,
      "step": 7080
    },
    {
      "epoch": 1.1734524991724595,
      "grad_norm": 5.935790538787842,
      "learning_rate": 3.607824911272605e-05,
      "loss": 0.6582,
      "step": 7090
    },
    {
      "epoch": 1.1751075802714332,
      "grad_norm": 10.57275676727295,
      "learning_rate": 3.605712354233565e-05,
      "loss": 0.7496,
      "step": 7100
    },
    {
      "epoch": 1.1767626613704072,
      "grad_norm": 2.951037645339966,
      "learning_rate": 3.6035997971945245e-05,
      "loss": 0.6413,
      "step": 7110
    },
    {
      "epoch": 1.178417742469381,
      "grad_norm": 10.563003540039062,
      "learning_rate": 3.6014872401554844e-05,
      "loss": 0.9732,
      "step": 7120
    },
    {
      "epoch": 1.1800728235683549,
      "grad_norm": 7.732124328613281,
      "learning_rate": 3.599374683116444e-05,
      "loss": 0.9223,
      "step": 7130
    },
    {
      "epoch": 1.1817279046673286,
      "grad_norm": 6.886355876922607,
      "learning_rate": 3.597262126077404e-05,
      "loss": 0.5639,
      "step": 7140
    },
    {
      "epoch": 1.1833829857663025,
      "grad_norm": 7.257668495178223,
      "learning_rate": 3.595149569038364e-05,
      "loss": 0.6674,
      "step": 7150
    },
    {
      "epoch": 1.1850380668652765,
      "grad_norm": 4.526784420013428,
      "learning_rate": 3.593037011999324e-05,
      "loss": 0.8718,
      "step": 7160
    },
    {
      "epoch": 1.1866931479642502,
      "grad_norm": 9.535927772521973,
      "learning_rate": 3.5909244549602836e-05,
      "loss": 0.7178,
      "step": 7170
    },
    {
      "epoch": 1.1883482290632241,
      "grad_norm": 6.364920139312744,
      "learning_rate": 3.588811897921244e-05,
      "loss": 0.6439,
      "step": 7180
    },
    {
      "epoch": 1.190003310162198,
      "grad_norm": 6.341578483581543,
      "learning_rate": 3.586699340882204e-05,
      "loss": 0.5866,
      "step": 7190
    },
    {
      "epoch": 1.1916583912611718,
      "grad_norm": 8.837087631225586,
      "learning_rate": 3.584586783843164e-05,
      "loss": 0.5328,
      "step": 7200
    },
    {
      "epoch": 1.1933134723601457,
      "grad_norm": 3.193737745285034,
      "learning_rate": 3.5824742268041236e-05,
      "loss": 0.6661,
      "step": 7210
    },
    {
      "epoch": 1.1949685534591195,
      "grad_norm": 7.678332328796387,
      "learning_rate": 3.580361669765084e-05,
      "loss": 0.6165,
      "step": 7220
    },
    {
      "epoch": 1.1966236345580934,
      "grad_norm": 5.2279253005981445,
      "learning_rate": 3.578249112726044e-05,
      "loss": 1.0299,
      "step": 7230
    },
    {
      "epoch": 1.1982787156570671,
      "grad_norm": 9.129426956176758,
      "learning_rate": 3.576136555687004e-05,
      "loss": 0.9512,
      "step": 7240
    },
    {
      "epoch": 1.199933796756041,
      "grad_norm": 6.320796012878418,
      "learning_rate": 3.5740239986479636e-05,
      "loss": 1.0282,
      "step": 7250
    },
    {
      "epoch": 1.201588877855015,
      "grad_norm": 7.459604740142822,
      "learning_rate": 3.571911441608924e-05,
      "loss": 0.8481,
      "step": 7260
    },
    {
      "epoch": 1.2032439589539887,
      "grad_norm": 3.3185272216796875,
      "learning_rate": 3.569798884569884e-05,
      "loss": 0.4843,
      "step": 7270
    },
    {
      "epoch": 1.2048990400529627,
      "grad_norm": 11.688035011291504,
      "learning_rate": 3.567686327530844e-05,
      "loss": 0.9265,
      "step": 7280
    },
    {
      "epoch": 1.2065541211519364,
      "grad_norm": 6.99226713180542,
      "learning_rate": 3.5655737704918037e-05,
      "loss": 0.5674,
      "step": 7290
    },
    {
      "epoch": 1.2082092022509103,
      "grad_norm": 6.594771385192871,
      "learning_rate": 3.5634612134527635e-05,
      "loss": 0.6439,
      "step": 7300
    },
    {
      "epoch": 1.209864283349884,
      "grad_norm": 6.251147747039795,
      "learning_rate": 3.561348656413723e-05,
      "loss": 0.5264,
      "step": 7310
    },
    {
      "epoch": 1.211519364448858,
      "grad_norm": 10.47301959991455,
      "learning_rate": 3.559236099374683e-05,
      "loss": 0.7328,
      "step": 7320
    },
    {
      "epoch": 1.213174445547832,
      "grad_norm": 5.256150245666504,
      "learning_rate": 3.557123542335643e-05,
      "loss": 0.6593,
      "step": 7330
    },
    {
      "epoch": 1.2148295266468057,
      "grad_norm": 12.11567211151123,
      "learning_rate": 3.555010985296603e-05,
      "loss": 1.1692,
      "step": 7340
    },
    {
      "epoch": 1.2164846077457796,
      "grad_norm": 9.258344650268555,
      "learning_rate": 3.5528984282575634e-05,
      "loss": 0.6069,
      "step": 7350
    },
    {
      "epoch": 1.2181396888447533,
      "grad_norm": 9.085851669311523,
      "learning_rate": 3.550785871218523e-05,
      "loss": 0.6817,
      "step": 7360
    },
    {
      "epoch": 1.2197947699437273,
      "grad_norm": 8.504349708557129,
      "learning_rate": 3.548673314179483e-05,
      "loss": 0.958,
      "step": 7370
    },
    {
      "epoch": 1.221449851042701,
      "grad_norm": 8.018280029296875,
      "learning_rate": 3.546560757140443e-05,
      "loss": 0.4337,
      "step": 7380
    },
    {
      "epoch": 1.223104932141675,
      "grad_norm": 7.995080947875977,
      "learning_rate": 3.544448200101403e-05,
      "loss": 0.6597,
      "step": 7390
    },
    {
      "epoch": 1.2247600132406489,
      "grad_norm": 10.100285530090332,
      "learning_rate": 3.5423356430623626e-05,
      "loss": 0.7213,
      "step": 7400
    },
    {
      "epoch": 1.2264150943396226,
      "grad_norm": 4.597739219665527,
      "learning_rate": 3.5402230860233224e-05,
      "loss": 0.5943,
      "step": 7410
    },
    {
      "epoch": 1.2280701754385965,
      "grad_norm": 8.804723739624023,
      "learning_rate": 3.538110528984282e-05,
      "loss": 0.7117,
      "step": 7420
    },
    {
      "epoch": 1.2297252565375703,
      "grad_norm": 2.448058843612671,
      "learning_rate": 3.535997971945243e-05,
      "loss": 0.6533,
      "step": 7430
    },
    {
      "epoch": 1.2313803376365442,
      "grad_norm": 8.154659271240234,
      "learning_rate": 3.5338854149062026e-05,
      "loss": 0.739,
      "step": 7440
    },
    {
      "epoch": 1.233035418735518,
      "grad_norm": 3.411768674850464,
      "learning_rate": 3.5317728578671624e-05,
      "loss": 0.7088,
      "step": 7450
    },
    {
      "epoch": 1.2346904998344919,
      "grad_norm": 4.676724433898926,
      "learning_rate": 3.529660300828123e-05,
      "loss": 0.8394,
      "step": 7460
    },
    {
      "epoch": 1.2363455809334658,
      "grad_norm": 10.319320678710938,
      "learning_rate": 3.527547743789083e-05,
      "loss": 0.9726,
      "step": 7470
    },
    {
      "epoch": 1.2380006620324395,
      "grad_norm": 4.5372796058654785,
      "learning_rate": 3.5254351867500426e-05,
      "loss": 0.729,
      "step": 7480
    },
    {
      "epoch": 1.2396557431314135,
      "grad_norm": 7.622683048248291,
      "learning_rate": 3.5233226297110025e-05,
      "loss": 0.8399,
      "step": 7490
    },
    {
      "epoch": 1.2413108242303874,
      "grad_norm": 7.711987018585205,
      "learning_rate": 3.521210072671962e-05,
      "loss": 0.7103,
      "step": 7500
    },
    {
      "epoch": 1.2429659053293611,
      "grad_norm": 6.814263820648193,
      "learning_rate": 3.519097515632922e-05,
      "loss": 0.9604,
      "step": 7510
    },
    {
      "epoch": 1.244620986428335,
      "grad_norm": 7.50216817855835,
      "learning_rate": 3.5169849585938826e-05,
      "loss": 0.5048,
      "step": 7520
    },
    {
      "epoch": 1.2462760675273088,
      "grad_norm": 5.681636333465576,
      "learning_rate": 3.5148724015548425e-05,
      "loss": 0.7858,
      "step": 7530
    },
    {
      "epoch": 1.2479311486262827,
      "grad_norm": 9.040925025939941,
      "learning_rate": 3.512759844515802e-05,
      "loss": 0.8666,
      "step": 7540
    },
    {
      "epoch": 1.2495862297252565,
      "grad_norm": 5.31890869140625,
      "learning_rate": 3.510647287476762e-05,
      "loss": 0.6261,
      "step": 7550
    },
    {
      "epoch": 1.2512413108242304,
      "grad_norm": 9.894919395446777,
      "learning_rate": 3.508534730437722e-05,
      "loss": 0.8547,
      "step": 7560
    },
    {
      "epoch": 1.2528963919232043,
      "grad_norm": 3.8524532318115234,
      "learning_rate": 3.506422173398682e-05,
      "loss": 0.4473,
      "step": 7570
    },
    {
      "epoch": 1.254551473022178,
      "grad_norm": 5.985054969787598,
      "learning_rate": 3.504309616359642e-05,
      "loss": 0.5111,
      "step": 7580
    },
    {
      "epoch": 1.256206554121152,
      "grad_norm": 7.695760726928711,
      "learning_rate": 3.5021970593206015e-05,
      "loss": 0.7445,
      "step": 7590
    },
    {
      "epoch": 1.2578616352201257,
      "grad_norm": 6.626453399658203,
      "learning_rate": 3.500084502281562e-05,
      "loss": 0.616,
      "step": 7600
    },
    {
      "epoch": 1.2595167163190997,
      "grad_norm": 13.192097663879395,
      "learning_rate": 3.497971945242522e-05,
      "loss": 0.6814,
      "step": 7610
    },
    {
      "epoch": 1.2611717974180734,
      "grad_norm": 11.941505432128906,
      "learning_rate": 3.495859388203482e-05,
      "loss": 1.1657,
      "step": 7620
    },
    {
      "epoch": 1.2628268785170473,
      "grad_norm": 10.585745811462402,
      "learning_rate": 3.4937468311644415e-05,
      "loss": 0.6979,
      "step": 7630
    },
    {
      "epoch": 1.2644819596160213,
      "grad_norm": 8.929100036621094,
      "learning_rate": 3.4916342741254014e-05,
      "loss": 0.9371,
      "step": 7640
    },
    {
      "epoch": 1.266137040714995,
      "grad_norm": 7.016721725463867,
      "learning_rate": 3.489521717086361e-05,
      "loss": 0.7197,
      "step": 7650
    },
    {
      "epoch": 1.267792121813969,
      "grad_norm": 8.017862319946289,
      "learning_rate": 3.487409160047321e-05,
      "loss": 0.7189,
      "step": 7660
    },
    {
      "epoch": 1.2694472029129427,
      "grad_norm": 13.833950996398926,
      "learning_rate": 3.485296603008281e-05,
      "loss": 0.9093,
      "step": 7670
    },
    {
      "epoch": 1.2711022840119166,
      "grad_norm": 4.734326362609863,
      "learning_rate": 3.4831840459692414e-05,
      "loss": 0.6067,
      "step": 7680
    },
    {
      "epoch": 1.2727573651108903,
      "grad_norm": 1.7852694988250732,
      "learning_rate": 3.481071488930201e-05,
      "loss": 0.5751,
      "step": 7690
    },
    {
      "epoch": 1.2744124462098643,
      "grad_norm": 3.5768258571624756,
      "learning_rate": 3.478958931891161e-05,
      "loss": 0.781,
      "step": 7700
    },
    {
      "epoch": 1.2760675273088382,
      "grad_norm": 8.414502143859863,
      "learning_rate": 3.4768463748521216e-05,
      "loss": 0.6276,
      "step": 7710
    },
    {
      "epoch": 1.277722608407812,
      "grad_norm": 9.705824851989746,
      "learning_rate": 3.4747338178130814e-05,
      "loss": 0.5048,
      "step": 7720
    },
    {
      "epoch": 1.2793776895067859,
      "grad_norm": 6.068521022796631,
      "learning_rate": 3.472621260774041e-05,
      "loss": 0.7538,
      "step": 7730
    },
    {
      "epoch": 1.2810327706057598,
      "grad_norm": 12.727599143981934,
      "learning_rate": 3.470508703735001e-05,
      "loss": 0.9761,
      "step": 7740
    },
    {
      "epoch": 1.2826878517047335,
      "grad_norm": 9.110688209533691,
      "learning_rate": 3.468396146695961e-05,
      "loss": 0.6823,
      "step": 7750
    },
    {
      "epoch": 1.2843429328037073,
      "grad_norm": 9.145857810974121,
      "learning_rate": 3.466283589656921e-05,
      "loss": 1.009,
      "step": 7760
    },
    {
      "epoch": 1.2859980139026812,
      "grad_norm": 3.2220211029052734,
      "learning_rate": 3.464171032617881e-05,
      "loss": 0.614,
      "step": 7770
    },
    {
      "epoch": 1.2876530950016551,
      "grad_norm": 4.339253902435303,
      "learning_rate": 3.462058475578841e-05,
      "loss": 0.6776,
      "step": 7780
    },
    {
      "epoch": 1.2893081761006289,
      "grad_norm": 4.868624210357666,
      "learning_rate": 3.459945918539801e-05,
      "loss": 0.7009,
      "step": 7790
    },
    {
      "epoch": 1.2909632571996028,
      "grad_norm": 4.863439559936523,
      "learning_rate": 3.457833361500761e-05,
      "loss": 0.5457,
      "step": 7800
    },
    {
      "epoch": 1.2926183382985768,
      "grad_norm": 7.449807643890381,
      "learning_rate": 3.455720804461721e-05,
      "loss": 0.5762,
      "step": 7810
    },
    {
      "epoch": 1.2942734193975505,
      "grad_norm": 7.996222019195557,
      "learning_rate": 3.4536082474226805e-05,
      "loss": 0.57,
      "step": 7820
    },
    {
      "epoch": 1.2959285004965242,
      "grad_norm": 6.643186569213867,
      "learning_rate": 3.4514956903836403e-05,
      "loss": 0.7628,
      "step": 7830
    },
    {
      "epoch": 1.2975835815954981,
      "grad_norm": 5.923222064971924,
      "learning_rate": 3.4493831333446e-05,
      "loss": 0.903,
      "step": 7840
    },
    {
      "epoch": 1.299238662694472,
      "grad_norm": 5.2382426261901855,
      "learning_rate": 3.44727057630556e-05,
      "loss": 0.6136,
      "step": 7850
    },
    {
      "epoch": 1.3008937437934458,
      "grad_norm": 11.171913146972656,
      "learning_rate": 3.4451580192665205e-05,
      "loss": 0.7373,
      "step": 7860
    },
    {
      "epoch": 1.3025488248924197,
      "grad_norm": 6.880643844604492,
      "learning_rate": 3.4430454622274804e-05,
      "loss": 0.7795,
      "step": 7870
    },
    {
      "epoch": 1.3042039059913937,
      "grad_norm": 7.071228504180908,
      "learning_rate": 3.44093290518844e-05,
      "loss": 0.5885,
      "step": 7880
    },
    {
      "epoch": 1.3058589870903674,
      "grad_norm": 8.743435859680176,
      "learning_rate": 3.4388203481494e-05,
      "loss": 0.8623,
      "step": 7890
    },
    {
      "epoch": 1.3075140681893413,
      "grad_norm": 9.068041801452637,
      "learning_rate": 3.43670779111036e-05,
      "loss": 0.577,
      "step": 7900
    },
    {
      "epoch": 1.309169149288315,
      "grad_norm": 3.258715867996216,
      "learning_rate": 3.43459523407132e-05,
      "loss": 0.5601,
      "step": 7910
    },
    {
      "epoch": 1.310824230387289,
      "grad_norm": 9.578812599182129,
      "learning_rate": 3.4324826770322796e-05,
      "loss": 0.9134,
      "step": 7920
    },
    {
      "epoch": 1.3124793114862627,
      "grad_norm": 3.5132293701171875,
      "learning_rate": 3.43037011999324e-05,
      "loss": 0.7028,
      "step": 7930
    },
    {
      "epoch": 1.3141343925852367,
      "grad_norm": 7.409117221832275,
      "learning_rate": 3.4282575629542e-05,
      "loss": 0.7711,
      "step": 7940
    },
    {
      "epoch": 1.3157894736842106,
      "grad_norm": 4.13960075378418,
      "learning_rate": 3.42614500591516e-05,
      "loss": 0.7439,
      "step": 7950
    },
    {
      "epoch": 1.3174445547831843,
      "grad_norm": 8.696565628051758,
      "learning_rate": 3.42403244887612e-05,
      "loss": 0.6265,
      "step": 7960
    },
    {
      "epoch": 1.3190996358821583,
      "grad_norm": 6.771448612213135,
      "learning_rate": 3.42191989183708e-05,
      "loss": 0.7314,
      "step": 7970
    },
    {
      "epoch": 1.320754716981132,
      "grad_norm": 8.623295783996582,
      "learning_rate": 3.41980733479804e-05,
      "loss": 0.7624,
      "step": 7980
    },
    {
      "epoch": 1.322409798080106,
      "grad_norm": 7.38088846206665,
      "learning_rate": 3.417694777759e-05,
      "loss": 0.6535,
      "step": 7990
    },
    {
      "epoch": 1.3240648791790797,
      "grad_norm": 8.992539405822754,
      "learning_rate": 3.4155822207199596e-05,
      "loss": 0.9065,
      "step": 8000
    },
    {
      "epoch": 1.3257199602780536,
      "grad_norm": 9.134363174438477,
      "learning_rate": 3.4134696636809195e-05,
      "loss": 0.9034,
      "step": 8010
    },
    {
      "epoch": 1.3273750413770276,
      "grad_norm": 1.1238981485366821,
      "learning_rate": 3.411357106641879e-05,
      "loss": 0.8046,
      "step": 8020
    },
    {
      "epoch": 1.3290301224760013,
      "grad_norm": 10.448195457458496,
      "learning_rate": 3.40924454960284e-05,
      "loss": 0.7542,
      "step": 8030
    },
    {
      "epoch": 1.3306852035749752,
      "grad_norm": 3.9831955432891846,
      "learning_rate": 3.4071319925638e-05,
      "loss": 0.4939,
      "step": 8040
    },
    {
      "epoch": 1.3323402846739492,
      "grad_norm": 2.7046799659729004,
      "learning_rate": 3.4050194355247595e-05,
      "loss": 0.7428,
      "step": 8050
    },
    {
      "epoch": 1.3339953657729229,
      "grad_norm": 5.377749919891357,
      "learning_rate": 3.402906878485719e-05,
      "loss": 0.4885,
      "step": 8060
    },
    {
      "epoch": 1.3356504468718966,
      "grad_norm": 4.824620723724365,
      "learning_rate": 3.400794321446679e-05,
      "loss": 0.623,
      "step": 8070
    },
    {
      "epoch": 1.3373055279708705,
      "grad_norm": 12.578839302062988,
      "learning_rate": 3.398681764407639e-05,
      "loss": 0.8313,
      "step": 8080
    },
    {
      "epoch": 1.3389606090698445,
      "grad_norm": 12.096033096313477,
      "learning_rate": 3.396569207368599e-05,
      "loss": 0.7571,
      "step": 8090
    },
    {
      "epoch": 1.3406156901688182,
      "grad_norm": 7.6149373054504395,
      "learning_rate": 3.394456650329559e-05,
      "loss": 0.654,
      "step": 8100
    },
    {
      "epoch": 1.3422707712677922,
      "grad_norm": 2.936904191970825,
      "learning_rate": 3.392344093290519e-05,
      "loss": 0.6815,
      "step": 8110
    },
    {
      "epoch": 1.343925852366766,
      "grad_norm": 4.763810634613037,
      "learning_rate": 3.390231536251479e-05,
      "loss": 0.5479,
      "step": 8120
    },
    {
      "epoch": 1.3455809334657398,
      "grad_norm": 4.923865795135498,
      "learning_rate": 3.388118979212439e-05,
      "loss": 0.6902,
      "step": 8130
    },
    {
      "epoch": 1.3472360145647135,
      "grad_norm": 8.708702087402344,
      "learning_rate": 3.386006422173399e-05,
      "loss": 0.6929,
      "step": 8140
    },
    {
      "epoch": 1.3488910956636875,
      "grad_norm": 11.107270240783691,
      "learning_rate": 3.3838938651343586e-05,
      "loss": 0.5298,
      "step": 8150
    },
    {
      "epoch": 1.3505461767626614,
      "grad_norm": 7.445730686187744,
      "learning_rate": 3.3817813080953184e-05,
      "loss": 0.559,
      "step": 8160
    },
    {
      "epoch": 1.3522012578616351,
      "grad_norm": 9.255708694458008,
      "learning_rate": 3.379668751056278e-05,
      "loss": 1.0339,
      "step": 8170
    },
    {
      "epoch": 1.353856338960609,
      "grad_norm": 3.9299139976501465,
      "learning_rate": 3.377556194017239e-05,
      "loss": 0.7434,
      "step": 8180
    },
    {
      "epoch": 1.355511420059583,
      "grad_norm": 10.15609073638916,
      "learning_rate": 3.3754436369781986e-05,
      "loss": 0.6892,
      "step": 8190
    },
    {
      "epoch": 1.3571665011585567,
      "grad_norm": 7.29582405090332,
      "learning_rate": 3.3733310799391584e-05,
      "loss": 0.7144,
      "step": 8200
    },
    {
      "epoch": 1.3588215822575307,
      "grad_norm": 11.464619636535645,
      "learning_rate": 3.371218522900119e-05,
      "loss": 0.6704,
      "step": 8210
    },
    {
      "epoch": 1.3604766633565044,
      "grad_norm": 7.276268005371094,
      "learning_rate": 3.369105965861079e-05,
      "loss": 0.8337,
      "step": 8220
    },
    {
      "epoch": 1.3621317444554784,
      "grad_norm": 6.070382118225098,
      "learning_rate": 3.3669934088220386e-05,
      "loss": 0.7527,
      "step": 8230
    },
    {
      "epoch": 1.363786825554452,
      "grad_norm": 9.705778121948242,
      "learning_rate": 3.3648808517829985e-05,
      "loss": 0.7929,
      "step": 8240
    },
    {
      "epoch": 1.365441906653426,
      "grad_norm": 15.465862274169922,
      "learning_rate": 3.362768294743958e-05,
      "loss": 0.5911,
      "step": 8250
    },
    {
      "epoch": 1.3670969877524,
      "grad_norm": 11.270302772521973,
      "learning_rate": 3.360655737704918e-05,
      "loss": 0.5383,
      "step": 8260
    },
    {
      "epoch": 1.3687520688513737,
      "grad_norm": 12.403617858886719,
      "learning_rate": 3.358543180665878e-05,
      "loss": 0.6322,
      "step": 8270
    },
    {
      "epoch": 1.3704071499503476,
      "grad_norm": 4.541569232940674,
      "learning_rate": 3.3564306236268385e-05,
      "loss": 0.9737,
      "step": 8280
    },
    {
      "epoch": 1.3720622310493213,
      "grad_norm": 10.168715476989746,
      "learning_rate": 3.354318066587798e-05,
      "loss": 0.6595,
      "step": 8290
    },
    {
      "epoch": 1.3737173121482953,
      "grad_norm": 4.585226535797119,
      "learning_rate": 3.352205509548758e-05,
      "loss": 0.7951,
      "step": 8300
    },
    {
      "epoch": 1.375372393247269,
      "grad_norm": 5.445230484008789,
      "learning_rate": 3.350092952509718e-05,
      "loss": 0.6488,
      "step": 8310
    },
    {
      "epoch": 1.377027474346243,
      "grad_norm": 10.251328468322754,
      "learning_rate": 3.347980395470678e-05,
      "loss": 0.8905,
      "step": 8320
    },
    {
      "epoch": 1.378682555445217,
      "grad_norm": 14.049677848815918,
      "learning_rate": 3.345867838431638e-05,
      "loss": 0.6802,
      "step": 8330
    },
    {
      "epoch": 1.3803376365441906,
      "grad_norm": 2.1121113300323486,
      "learning_rate": 3.3437552813925975e-05,
      "loss": 0.5865,
      "step": 8340
    },
    {
      "epoch": 1.3819927176431646,
      "grad_norm": 5.866801738739014,
      "learning_rate": 3.3416427243535574e-05,
      "loss": 0.6492,
      "step": 8350
    },
    {
      "epoch": 1.3836477987421385,
      "grad_norm": 11.19753646850586,
      "learning_rate": 3.339530167314517e-05,
      "loss": 0.7341,
      "step": 8360
    },
    {
      "epoch": 1.3853028798411122,
      "grad_norm": 6.973863124847412,
      "learning_rate": 3.337417610275478e-05,
      "loss": 0.9516,
      "step": 8370
    },
    {
      "epoch": 1.386957960940086,
      "grad_norm": 10.838778495788574,
      "learning_rate": 3.3353050532364376e-05,
      "loss": 0.753,
      "step": 8380
    },
    {
      "epoch": 1.3886130420390599,
      "grad_norm": 7.206505298614502,
      "learning_rate": 3.3331924961973974e-05,
      "loss": 0.7378,
      "step": 8390
    },
    {
      "epoch": 1.3902681231380338,
      "grad_norm": 1.5579450130462646,
      "learning_rate": 3.331079939158357e-05,
      "loss": 0.6214,
      "step": 8400
    },
    {
      "epoch": 1.3919232042370075,
      "grad_norm": 4.728989601135254,
      "learning_rate": 3.328967382119317e-05,
      "loss": 0.8094,
      "step": 8410
    },
    {
      "epoch": 1.3935782853359815,
      "grad_norm": 3.0324511528015137,
      "learning_rate": 3.326854825080277e-05,
      "loss": 0.6956,
      "step": 8420
    },
    {
      "epoch": 1.3952333664349554,
      "grad_norm": 4.911746501922607,
      "learning_rate": 3.3247422680412374e-05,
      "loss": 0.7483,
      "step": 8430
    },
    {
      "epoch": 1.3968884475339292,
      "grad_norm": 8.132606506347656,
      "learning_rate": 3.322629711002197e-05,
      "loss": 0.9202,
      "step": 8440
    },
    {
      "epoch": 1.3985435286329029,
      "grad_norm": 6.673929214477539,
      "learning_rate": 3.320517153963157e-05,
      "loss": 0.7503,
      "step": 8450
    },
    {
      "epoch": 1.4001986097318768,
      "grad_norm": 8.220962524414062,
      "learning_rate": 3.3184045969241176e-05,
      "loss": 0.982,
      "step": 8460
    },
    {
      "epoch": 1.4018536908308508,
      "grad_norm": 7.146113872528076,
      "learning_rate": 3.3162920398850775e-05,
      "loss": 0.683,
      "step": 8470
    },
    {
      "epoch": 1.4035087719298245,
      "grad_norm": 1.4004188776016235,
      "learning_rate": 3.314179482846037e-05,
      "loss": 0.6845,
      "step": 8480
    },
    {
      "epoch": 1.4051638530287984,
      "grad_norm": 7.756066799163818,
      "learning_rate": 3.312066925806997e-05,
      "loss": 0.7475,
      "step": 8490
    },
    {
      "epoch": 1.4068189341277724,
      "grad_norm": 3.549748659133911,
      "learning_rate": 3.309954368767957e-05,
      "loss": 0.7445,
      "step": 8500
    },
    {
      "epoch": 1.408474015226746,
      "grad_norm": 18.235383987426758,
      "learning_rate": 3.307841811728917e-05,
      "loss": 0.3978,
      "step": 8510
    },
    {
      "epoch": 1.41012909632572,
      "grad_norm": 9.851861000061035,
      "learning_rate": 3.3057292546898766e-05,
      "loss": 0.6751,
      "step": 8520
    },
    {
      "epoch": 1.4117841774246938,
      "grad_norm": 3.5988290309906006,
      "learning_rate": 3.3036166976508365e-05,
      "loss": 0.6763,
      "step": 8530
    },
    {
      "epoch": 1.4134392585236677,
      "grad_norm": 8.003803253173828,
      "learning_rate": 3.301504140611797e-05,
      "loss": 0.4507,
      "step": 8540
    },
    {
      "epoch": 1.4150943396226414,
      "grad_norm": 4.885424613952637,
      "learning_rate": 3.299391583572757e-05,
      "loss": 0.5641,
      "step": 8550
    },
    {
      "epoch": 1.4167494207216154,
      "grad_norm": 5.5367021560668945,
      "learning_rate": 3.297279026533717e-05,
      "loss": 0.6532,
      "step": 8560
    },
    {
      "epoch": 1.4184045018205893,
      "grad_norm": 10.978961944580078,
      "learning_rate": 3.2951664694946765e-05,
      "loss": 0.7133,
      "step": 8570
    },
    {
      "epoch": 1.420059582919563,
      "grad_norm": 8.509760856628418,
      "learning_rate": 3.2930539124556364e-05,
      "loss": 0.7281,
      "step": 8580
    },
    {
      "epoch": 1.421714664018537,
      "grad_norm": 6.757058143615723,
      "learning_rate": 3.290941355416596e-05,
      "loss": 0.9651,
      "step": 8590
    },
    {
      "epoch": 1.4233697451175107,
      "grad_norm": 7.2605977058410645,
      "learning_rate": 3.288828798377556e-05,
      "loss": 0.7355,
      "step": 8600
    },
    {
      "epoch": 1.4250248262164846,
      "grad_norm": 7.227848052978516,
      "learning_rate": 3.286716241338516e-05,
      "loss": 0.8116,
      "step": 8610
    },
    {
      "epoch": 1.4266799073154584,
      "grad_norm": 9.899287223815918,
      "learning_rate": 3.2846036842994764e-05,
      "loss": 0.7734,
      "step": 8620
    },
    {
      "epoch": 1.4283349884144323,
      "grad_norm": 2.3421802520751953,
      "learning_rate": 3.282491127260436e-05,
      "loss": 0.6687,
      "step": 8630
    },
    {
      "epoch": 1.4299900695134062,
      "grad_norm": 7.323288440704346,
      "learning_rate": 3.280378570221396e-05,
      "loss": 0.8188,
      "step": 8640
    },
    {
      "epoch": 1.43164515061238,
      "grad_norm": 11.453943252563477,
      "learning_rate": 3.278266013182356e-05,
      "loss": 0.5248,
      "step": 8650
    },
    {
      "epoch": 1.433300231711354,
      "grad_norm": 2.5173587799072266,
      "learning_rate": 3.276153456143316e-05,
      "loss": 0.9087,
      "step": 8660
    },
    {
      "epoch": 1.4349553128103278,
      "grad_norm": 6.204174518585205,
      "learning_rate": 3.274040899104276e-05,
      "loss": 0.6051,
      "step": 8670
    },
    {
      "epoch": 1.4366103939093016,
      "grad_norm": 6.306761264801025,
      "learning_rate": 3.271928342065236e-05,
      "loss": 0.8298,
      "step": 8680
    },
    {
      "epoch": 1.4382654750082753,
      "grad_norm": 1.920698881149292,
      "learning_rate": 3.269815785026196e-05,
      "loss": 0.4473,
      "step": 8690
    },
    {
      "epoch": 1.4399205561072492,
      "grad_norm": 15.927009582519531,
      "learning_rate": 3.267703227987156e-05,
      "loss": 0.5123,
      "step": 8700
    },
    {
      "epoch": 1.4415756372062232,
      "grad_norm": 3.754730463027954,
      "learning_rate": 3.265590670948116e-05,
      "loss": 0.7872,
      "step": 8710
    },
    {
      "epoch": 1.443230718305197,
      "grad_norm": 11.307397842407227,
      "learning_rate": 3.263478113909076e-05,
      "loss": 0.6433,
      "step": 8720
    },
    {
      "epoch": 1.4448857994041708,
      "grad_norm": 9.302504539489746,
      "learning_rate": 3.261365556870036e-05,
      "loss": 0.7069,
      "step": 8730
    },
    {
      "epoch": 1.4465408805031448,
      "grad_norm": 14.311107635498047,
      "learning_rate": 3.259252999830996e-05,
      "loss": 0.9768,
      "step": 8740
    },
    {
      "epoch": 1.4481959616021185,
      "grad_norm": 4.593368053436279,
      "learning_rate": 3.2571404427919556e-05,
      "loss": 0.879,
      "step": 8750
    },
    {
      "epoch": 1.4498510427010922,
      "grad_norm": 4.305541515350342,
      "learning_rate": 3.2550278857529155e-05,
      "loss": 0.9398,
      "step": 8760
    },
    {
      "epoch": 1.4515061238000662,
      "grad_norm": 4.297641754150391,
      "learning_rate": 3.252915328713875e-05,
      "loss": 0.5903,
      "step": 8770
    },
    {
      "epoch": 1.45316120489904,
      "grad_norm": 12.470890998840332,
      "learning_rate": 3.250802771674835e-05,
      "loss": 0.8105,
      "step": 8780
    },
    {
      "epoch": 1.4548162859980138,
      "grad_norm": 7.720037937164307,
      "learning_rate": 3.248690214635796e-05,
      "loss": 0.8776,
      "step": 8790
    },
    {
      "epoch": 1.4564713670969878,
      "grad_norm": 9.6542329788208,
      "learning_rate": 3.2465776575967555e-05,
      "loss": 0.9836,
      "step": 8800
    },
    {
      "epoch": 1.4581264481959617,
      "grad_norm": 4.910064220428467,
      "learning_rate": 3.2444651005577153e-05,
      "loss": 0.7797,
      "step": 8810
    },
    {
      "epoch": 1.4597815292949354,
      "grad_norm": 5.716066360473633,
      "learning_rate": 3.242352543518675e-05,
      "loss": 0.7438,
      "step": 8820
    },
    {
      "epoch": 1.4614366103939094,
      "grad_norm": 6.514086723327637,
      "learning_rate": 3.240239986479635e-05,
      "loss": 0.447,
      "step": 8830
    },
    {
      "epoch": 1.463091691492883,
      "grad_norm": 6.78853702545166,
      "learning_rate": 3.238127429440595e-05,
      "loss": 0.6912,
      "step": 8840
    },
    {
      "epoch": 1.464746772591857,
      "grad_norm": 10.129416465759277,
      "learning_rate": 3.236014872401555e-05,
      "loss": 0.7126,
      "step": 8850
    },
    {
      "epoch": 1.4664018536908308,
      "grad_norm": 4.35906982421875,
      "learning_rate": 3.2339023153625145e-05,
      "loss": 0.8034,
      "step": 8860
    },
    {
      "epoch": 1.4680569347898047,
      "grad_norm": 5.9490156173706055,
      "learning_rate": 3.2317897583234744e-05,
      "loss": 0.6337,
      "step": 8870
    },
    {
      "epoch": 1.4697120158887786,
      "grad_norm": 9.260380744934082,
      "learning_rate": 3.229677201284435e-05,
      "loss": 0.8101,
      "step": 8880
    },
    {
      "epoch": 1.4713670969877524,
      "grad_norm": 8.19864273071289,
      "learning_rate": 3.227564644245395e-05,
      "loss": 0.864,
      "step": 8890
    },
    {
      "epoch": 1.4730221780867263,
      "grad_norm": 2.613029956817627,
      "learning_rate": 3.2254520872063546e-05,
      "loss": 0.8621,
      "step": 8900
    },
    {
      "epoch": 1.4746772591857,
      "grad_norm": 8.01870346069336,
      "learning_rate": 3.2233395301673144e-05,
      "loss": 0.8701,
      "step": 8910
    },
    {
      "epoch": 1.476332340284674,
      "grad_norm": 10.721574783325195,
      "learning_rate": 3.221226973128275e-05,
      "loss": 0.754,
      "step": 8920
    },
    {
      "epoch": 1.4779874213836477,
      "grad_norm": 5.454575538635254,
      "learning_rate": 3.219114416089235e-05,
      "loss": 0.5694,
      "step": 8930
    },
    {
      "epoch": 1.4796425024826216,
      "grad_norm": 7.36234188079834,
      "learning_rate": 3.2170018590501946e-05,
      "loss": 0.4749,
      "step": 8940
    },
    {
      "epoch": 1.4812975835815956,
      "grad_norm": 1.9700318574905396,
      "learning_rate": 3.2148893020111544e-05,
      "loss": 0.6338,
      "step": 8950
    },
    {
      "epoch": 1.4829526646805693,
      "grad_norm": 2.5864171981811523,
      "learning_rate": 3.212776744972115e-05,
      "loss": 0.5675,
      "step": 8960
    },
    {
      "epoch": 1.4846077457795432,
      "grad_norm": 8.842609405517578,
      "learning_rate": 3.210664187933075e-05,
      "loss": 0.7437,
      "step": 8970
    },
    {
      "epoch": 1.4862628268785172,
      "grad_norm": 1.9071290493011475,
      "learning_rate": 3.2085516308940346e-05,
      "loss": 0.7452,
      "step": 8980
    },
    {
      "epoch": 1.487917907977491,
      "grad_norm": 7.120420455932617,
      "learning_rate": 3.2064390738549945e-05,
      "loss": 0.8067,
      "step": 8990
    },
    {
      "epoch": 1.4895729890764646,
      "grad_norm": 10.867350578308105,
      "learning_rate": 3.204326516815954e-05,
      "loss": 0.7316,
      "step": 9000
    },
    {
      "epoch": 1.4912280701754386,
      "grad_norm": 8.026422500610352,
      "learning_rate": 3.202213959776914e-05,
      "loss": 0.618,
      "step": 9010
    },
    {
      "epoch": 1.4928831512744125,
      "grad_norm": 7.291450500488281,
      "learning_rate": 3.200101402737874e-05,
      "loss": 0.8085,
      "step": 9020
    },
    {
      "epoch": 1.4945382323733862,
      "grad_norm": 6.884203910827637,
      "learning_rate": 3.197988845698834e-05,
      "loss": 0.7561,
      "step": 9030
    },
    {
      "epoch": 1.4961933134723602,
      "grad_norm": 7.941955089569092,
      "learning_rate": 3.1958762886597937e-05,
      "loss": 0.8584,
      "step": 9040
    },
    {
      "epoch": 1.4978483945713341,
      "grad_norm": 7.501947402954102,
      "learning_rate": 3.193763731620754e-05,
      "loss": 0.6451,
      "step": 9050
    },
    {
      "epoch": 1.4995034756703078,
      "grad_norm": 7.740591526031494,
      "learning_rate": 3.191651174581714e-05,
      "loss": 0.7219,
      "step": 9060
    },
    {
      "epoch": 1.5011585567692816,
      "grad_norm": 6.809789180755615,
      "learning_rate": 3.189538617542674e-05,
      "loss": 0.49,
      "step": 9070
    },
    {
      "epoch": 1.5028136378682555,
      "grad_norm": 5.050835609436035,
      "learning_rate": 3.187426060503634e-05,
      "loss": 0.594,
      "step": 9080
    },
    {
      "epoch": 1.5044687189672294,
      "grad_norm": 2.7834410667419434,
      "learning_rate": 3.1853135034645935e-05,
      "loss": 0.7009,
      "step": 9090
    },
    {
      "epoch": 1.5061238000662032,
      "grad_norm": 2.291382074356079,
      "learning_rate": 3.1832009464255534e-05,
      "loss": 0.8801,
      "step": 9100
    },
    {
      "epoch": 1.507778881165177,
      "grad_norm": 6.343038558959961,
      "learning_rate": 3.181088389386513e-05,
      "loss": 0.711,
      "step": 9110
    },
    {
      "epoch": 1.509433962264151,
      "grad_norm": 0.8495331406593323,
      "learning_rate": 3.178975832347473e-05,
      "loss": 0.5127,
      "step": 9120
    },
    {
      "epoch": 1.5110890433631248,
      "grad_norm": 10.79688835144043,
      "learning_rate": 3.1768632753084336e-05,
      "loss": 0.6089,
      "step": 9130
    },
    {
      "epoch": 1.5127441244620985,
      "grad_norm": 12.080694198608398,
      "learning_rate": 3.1747507182693934e-05,
      "loss": 0.7613,
      "step": 9140
    },
    {
      "epoch": 1.5143992055610727,
      "grad_norm": 6.024794578552246,
      "learning_rate": 3.172638161230353e-05,
      "loss": 0.7234,
      "step": 9150
    },
    {
      "epoch": 1.5160542866600464,
      "grad_norm": 7.405523777008057,
      "learning_rate": 3.170525604191313e-05,
      "loss": 0.6078,
      "step": 9160
    },
    {
      "epoch": 1.51770936775902,
      "grad_norm": 12.264163970947266,
      "learning_rate": 3.1684130471522736e-05,
      "loss": 0.5984,
      "step": 9170
    },
    {
      "epoch": 1.519364448857994,
      "grad_norm": 3.736060380935669,
      "learning_rate": 3.1663004901132334e-05,
      "loss": 0.3852,
      "step": 9180
    },
    {
      "epoch": 1.521019529956968,
      "grad_norm": 6.72822380065918,
      "learning_rate": 3.164187933074193e-05,
      "loss": 0.883,
      "step": 9190
    },
    {
      "epoch": 1.5226746110559417,
      "grad_norm": 7.77552604675293,
      "learning_rate": 3.162075376035153e-05,
      "loss": 0.8055,
      "step": 9200
    },
    {
      "epoch": 1.5243296921549154,
      "grad_norm": 10.337677001953125,
      "learning_rate": 3.159962818996113e-05,
      "loss": 0.6877,
      "step": 9210
    },
    {
      "epoch": 1.5259847732538896,
      "grad_norm": 12.734128952026367,
      "learning_rate": 3.1578502619570735e-05,
      "loss": 0.6629,
      "step": 9220
    },
    {
      "epoch": 1.5276398543528633,
      "grad_norm": 6.0270466804504395,
      "learning_rate": 3.155737704918033e-05,
      "loss": 0.5955,
      "step": 9230
    },
    {
      "epoch": 1.529294935451837,
      "grad_norm": 6.096181392669678,
      "learning_rate": 3.153625147878993e-05,
      "loss": 0.7124,
      "step": 9240
    },
    {
      "epoch": 1.530950016550811,
      "grad_norm": 0.8235666751861572,
      "learning_rate": 3.151512590839953e-05,
      "loss": 0.8935,
      "step": 9250
    },
    {
      "epoch": 1.532605097649785,
      "grad_norm": 2.73970103263855,
      "learning_rate": 3.149400033800913e-05,
      "loss": 0.6333,
      "step": 9260
    },
    {
      "epoch": 1.5342601787487586,
      "grad_norm": 6.722254276275635,
      "learning_rate": 3.1472874767618727e-05,
      "loss": 0.8239,
      "step": 9270
    },
    {
      "epoch": 1.5359152598477326,
      "grad_norm": 9.30932331085205,
      "learning_rate": 3.1451749197228325e-05,
      "loss": 0.8248,
      "step": 9280
    },
    {
      "epoch": 1.5375703409467065,
      "grad_norm": 9.749568939208984,
      "learning_rate": 3.143062362683792e-05,
      "loss": 0.6414,
      "step": 9290
    },
    {
      "epoch": 1.5392254220456802,
      "grad_norm": 8.180120468139648,
      "learning_rate": 3.140949805644753e-05,
      "loss": 0.965,
      "step": 9300
    },
    {
      "epoch": 1.540880503144654,
      "grad_norm": 9.049437522888184,
      "learning_rate": 3.138837248605713e-05,
      "loss": 0.6975,
      "step": 9310
    },
    {
      "epoch": 1.542535584243628,
      "grad_norm": 8.12508773803711,
      "learning_rate": 3.1367246915666725e-05,
      "loss": 0.3964,
      "step": 9320
    },
    {
      "epoch": 1.5441906653426019,
      "grad_norm": 4.61114501953125,
      "learning_rate": 3.1346121345276324e-05,
      "loss": 0.6575,
      "step": 9330
    },
    {
      "epoch": 1.5458457464415756,
      "grad_norm": 10.077768325805664,
      "learning_rate": 3.132499577488592e-05,
      "loss": 0.4421,
      "step": 9340
    },
    {
      "epoch": 1.5475008275405495,
      "grad_norm": 4.7632832527160645,
      "learning_rate": 3.130387020449552e-05,
      "loss": 0.7692,
      "step": 9350
    },
    {
      "epoch": 1.5491559086395235,
      "grad_norm": 7.77985954284668,
      "learning_rate": 3.128274463410512e-05,
      "loss": 0.6718,
      "step": 9360
    },
    {
      "epoch": 1.5508109897384972,
      "grad_norm": 2.9344115257263184,
      "learning_rate": 3.126161906371472e-05,
      "loss": 0.4033,
      "step": 9370
    },
    {
      "epoch": 1.552466070837471,
      "grad_norm": 4.305725574493408,
      "learning_rate": 3.1240493493324316e-05,
      "loss": 0.8023,
      "step": 9380
    },
    {
      "epoch": 1.5541211519364448,
      "grad_norm": 4.766107082366943,
      "learning_rate": 3.121936792293392e-05,
      "loss": 0.6151,
      "step": 9390
    },
    {
      "epoch": 1.5557762330354188,
      "grad_norm": 9.306041717529297,
      "learning_rate": 3.119824235254352e-05,
      "loss": 0.6257,
      "step": 9400
    },
    {
      "epoch": 1.5574313141343925,
      "grad_norm": 10.810465812683105,
      "learning_rate": 3.117711678215312e-05,
      "loss": 0.5932,
      "step": 9410
    },
    {
      "epoch": 1.5590863952333665,
      "grad_norm": 8.182621002197266,
      "learning_rate": 3.115599121176272e-05,
      "loss": 0.8744,
      "step": 9420
    },
    {
      "epoch": 1.5607414763323404,
      "grad_norm": 2.9907193183898926,
      "learning_rate": 3.113486564137232e-05,
      "loss": 0.5322,
      "step": 9430
    },
    {
      "epoch": 1.5623965574313141,
      "grad_norm": 7.934456825256348,
      "learning_rate": 3.111374007098192e-05,
      "loss": 0.778,
      "step": 9440
    },
    {
      "epoch": 1.5640516385302878,
      "grad_norm": 5.309648036956787,
      "learning_rate": 3.109261450059152e-05,
      "loss": 0.6976,
      "step": 9450
    },
    {
      "epoch": 1.5657067196292618,
      "grad_norm": 7.6517014503479,
      "learning_rate": 3.1071488930201116e-05,
      "loss": 0.5514,
      "step": 9460
    },
    {
      "epoch": 1.5673618007282357,
      "grad_norm": 3.3165783882141113,
      "learning_rate": 3.105036335981072e-05,
      "loss": 0.8949,
      "step": 9470
    },
    {
      "epoch": 1.5690168818272094,
      "grad_norm": 4.233617782592773,
      "learning_rate": 3.102923778942032e-05,
      "loss": 0.7675,
      "step": 9480
    },
    {
      "epoch": 1.5706719629261834,
      "grad_norm": 6.936057090759277,
      "learning_rate": 3.100811221902992e-05,
      "loss": 0.9981,
      "step": 9490
    },
    {
      "epoch": 1.5723270440251573,
      "grad_norm": 15.3574800491333,
      "learning_rate": 3.0986986648639516e-05,
      "loss": 0.5301,
      "step": 9500
    },
    {
      "epoch": 1.573982125124131,
      "grad_norm": 2.9328434467315674,
      "learning_rate": 3.0965861078249115e-05,
      "loss": 0.6899,
      "step": 9510
    },
    {
      "epoch": 1.5756372062231048,
      "grad_norm": 11.261765480041504,
      "learning_rate": 3.094473550785871e-05,
      "loss": 0.6668,
      "step": 9520
    },
    {
      "epoch": 1.577292287322079,
      "grad_norm": 1.2260777950286865,
      "learning_rate": 3.092360993746831e-05,
      "loss": 0.8024,
      "step": 9530
    },
    {
      "epoch": 1.5789473684210527,
      "grad_norm": 7.238890647888184,
      "learning_rate": 3.090248436707791e-05,
      "loss": 0.6302,
      "step": 9540
    },
    {
      "epoch": 1.5806024495200264,
      "grad_norm": 7.61883544921875,
      "learning_rate": 3.088135879668751e-05,
      "loss": 0.8253,
      "step": 9550
    },
    {
      "epoch": 1.5822575306190003,
      "grad_norm": 2.462460517883301,
      "learning_rate": 3.0860233226297114e-05,
      "loss": 0.5426,
      "step": 9560
    },
    {
      "epoch": 1.5839126117179743,
      "grad_norm": 7.409249305725098,
      "learning_rate": 3.083910765590671e-05,
      "loss": 0.726,
      "step": 9570
    },
    {
      "epoch": 1.585567692816948,
      "grad_norm": 6.947539329528809,
      "learning_rate": 3.081798208551631e-05,
      "loss": 0.5104,
      "step": 9580
    },
    {
      "epoch": 1.587222773915922,
      "grad_norm": 10.484869003295898,
      "learning_rate": 3.079685651512591e-05,
      "loss": 0.6416,
      "step": 9590
    },
    {
      "epoch": 1.5888778550148959,
      "grad_norm": 11.86658000946045,
      "learning_rate": 3.077573094473551e-05,
      "loss": 0.6706,
      "step": 9600
    },
    {
      "epoch": 1.5905329361138696,
      "grad_norm": 5.477184295654297,
      "learning_rate": 3.0754605374345105e-05,
      "loss": 0.4897,
      "step": 9610
    },
    {
      "epoch": 1.5921880172128433,
      "grad_norm": 15.1127290725708,
      "learning_rate": 3.0733479803954704e-05,
      "loss": 0.6982,
      "step": 9620
    },
    {
      "epoch": 1.5938430983118173,
      "grad_norm": 9.494717597961426,
      "learning_rate": 3.07123542335643e-05,
      "loss": 0.8047,
      "step": 9630
    },
    {
      "epoch": 1.5954981794107912,
      "grad_norm": 6.2123918533325195,
      "learning_rate": 3.069122866317391e-05,
      "loss": 0.6251,
      "step": 9640
    },
    {
      "epoch": 1.597153260509765,
      "grad_norm": 9.660652160644531,
      "learning_rate": 3.0670103092783506e-05,
      "loss": 0.4401,
      "step": 9650
    },
    {
      "epoch": 1.5988083416087389,
      "grad_norm": 5.053604602813721,
      "learning_rate": 3.0648977522393104e-05,
      "loss": 0.7239,
      "step": 9660
    },
    {
      "epoch": 1.6004634227077128,
      "grad_norm": 5.540187358856201,
      "learning_rate": 3.062785195200271e-05,
      "loss": 0.6388,
      "step": 9670
    },
    {
      "epoch": 1.6021185038066865,
      "grad_norm": 6.465723991394043,
      "learning_rate": 3.060672638161231e-05,
      "loss": 0.7553,
      "step": 9680
    },
    {
      "epoch": 1.6037735849056602,
      "grad_norm": 7.752254486083984,
      "learning_rate": 3.0585600811221906e-05,
      "loss": 1.0778,
      "step": 9690
    },
    {
      "epoch": 1.6054286660046342,
      "grad_norm": 3.387632369995117,
      "learning_rate": 3.0564475240831504e-05,
      "loss": 0.5099,
      "step": 9700
    },
    {
      "epoch": 1.6070837471036081,
      "grad_norm": 6.56606912612915,
      "learning_rate": 3.05433496704411e-05,
      "loss": 0.7547,
      "step": 9710
    },
    {
      "epoch": 1.6087388282025818,
      "grad_norm": 8.981496810913086,
      "learning_rate": 3.05222241000507e-05,
      "loss": 0.8972,
      "step": 9720
    },
    {
      "epoch": 1.6103939093015558,
      "grad_norm": 7.170958995819092,
      "learning_rate": 3.0501098529660306e-05,
      "loss": 0.7515,
      "step": 9730
    },
    {
      "epoch": 1.6120489904005297,
      "grad_norm": 6.075172424316406,
      "learning_rate": 3.0479972959269905e-05,
      "loss": 0.4415,
      "step": 9740
    },
    {
      "epoch": 1.6137040714995035,
      "grad_norm": 8.406197547912598,
      "learning_rate": 3.0458847388879503e-05,
      "loss": 1.1495,
      "step": 9750
    },
    {
      "epoch": 1.6153591525984772,
      "grad_norm": 8.271085739135742,
      "learning_rate": 3.04377218184891e-05,
      "loss": 0.9551,
      "step": 9760
    },
    {
      "epoch": 1.6170142336974511,
      "grad_norm": 13.846830368041992,
      "learning_rate": 3.04165962480987e-05,
      "loss": 0.4994,
      "step": 9770
    },
    {
      "epoch": 1.618669314796425,
      "grad_norm": 3.861544370651245,
      "learning_rate": 3.0395470677708298e-05,
      "loss": 0.8627,
      "step": 9780
    },
    {
      "epoch": 1.6203243958953988,
      "grad_norm": 6.19158411026001,
      "learning_rate": 3.0374345107317897e-05,
      "loss": 0.7339,
      "step": 9790
    },
    {
      "epoch": 1.6219794769943727,
      "grad_norm": 9.577089309692383,
      "learning_rate": 3.0353219536927495e-05,
      "loss": 0.582,
      "step": 9800
    },
    {
      "epoch": 1.6236345580933467,
      "grad_norm": 10.113604545593262,
      "learning_rate": 3.03320939665371e-05,
      "loss": 0.8538,
      "step": 9810
    },
    {
      "epoch": 1.6252896391923204,
      "grad_norm": 3.098356246948242,
      "learning_rate": 3.03109683961467e-05,
      "loss": 0.6652,
      "step": 9820
    },
    {
      "epoch": 1.626944720291294,
      "grad_norm": 7.157524585723877,
      "learning_rate": 3.0289842825756297e-05,
      "loss": 0.5616,
      "step": 9830
    },
    {
      "epoch": 1.6285998013902683,
      "grad_norm": 6.366507530212402,
      "learning_rate": 3.02687172553659e-05,
      "loss": 0.8181,
      "step": 9840
    },
    {
      "epoch": 1.630254882489242,
      "grad_norm": 12.922650337219238,
      "learning_rate": 3.0247591684975497e-05,
      "loss": 0.7802,
      "step": 9850
    },
    {
      "epoch": 1.6319099635882157,
      "grad_norm": 16.5743408203125,
      "learning_rate": 3.0226466114585096e-05,
      "loss": 1.0233,
      "step": 9860
    },
    {
      "epoch": 1.6335650446871897,
      "grad_norm": 7.718776226043701,
      "learning_rate": 3.0205340544194694e-05,
      "loss": 0.7096,
      "step": 9870
    },
    {
      "epoch": 1.6352201257861636,
      "grad_norm": 2.238013982772827,
      "learning_rate": 3.0184214973804292e-05,
      "loss": 0.5655,
      "step": 9880
    },
    {
      "epoch": 1.6368752068851373,
      "grad_norm": 2.145017147064209,
      "learning_rate": 3.016308940341389e-05,
      "loss": 0.644,
      "step": 9890
    },
    {
      "epoch": 1.6385302879841113,
      "grad_norm": 6.705390930175781,
      "learning_rate": 3.0141963833023496e-05,
      "loss": 0.9079,
      "step": 9900
    },
    {
      "epoch": 1.6401853690830852,
      "grad_norm": 6.203497409820557,
      "learning_rate": 3.0120838262633094e-05,
      "loss": 0.5354,
      "step": 9910
    },
    {
      "epoch": 1.641840450182059,
      "grad_norm": 8.337357521057129,
      "learning_rate": 3.0099712692242693e-05,
      "loss": 0.5797,
      "step": 9920
    },
    {
      "epoch": 1.6434955312810327,
      "grad_norm": 3.86504864692688,
      "learning_rate": 3.007858712185229e-05,
      "loss": 0.77,
      "step": 9930
    },
    {
      "epoch": 1.6451506123800066,
      "grad_norm": 5.899746894836426,
      "learning_rate": 3.005746155146189e-05,
      "loss": 0.8102,
      "step": 9940
    },
    {
      "epoch": 1.6468056934789805,
      "grad_norm": 2.7032933235168457,
      "learning_rate": 3.003633598107149e-05,
      "loss": 0.5984,
      "step": 9950
    },
    {
      "epoch": 1.6484607745779543,
      "grad_norm": 8.290793418884277,
      "learning_rate": 3.001521041068109e-05,
      "loss": 0.7087,
      "step": 9960
    },
    {
      "epoch": 1.6501158556769282,
      "grad_norm": 2.233654260635376,
      "learning_rate": 2.9994084840290688e-05,
      "loss": 0.5565,
      "step": 9970
    },
    {
      "epoch": 1.6517709367759021,
      "grad_norm": 5.616755962371826,
      "learning_rate": 2.9972959269900293e-05,
      "loss": 0.6678,
      "step": 9980
    },
    {
      "epoch": 1.6534260178748759,
      "grad_norm": 12.885627746582031,
      "learning_rate": 2.995183369950989e-05,
      "loss": 0.8661,
      "step": 9990
    },
    {
      "epoch": 1.6550810989738496,
      "grad_norm": 9.563692092895508,
      "learning_rate": 2.993070812911949e-05,
      "loss": 0.9002,
      "step": 10000
    },
    {
      "epoch": 1.6567361800728235,
      "grad_norm": 9.282737731933594,
      "learning_rate": 2.9909582558729088e-05,
      "loss": 0.6887,
      "step": 10010
    },
    {
      "epoch": 1.6583912611717975,
      "grad_norm": 6.997591018676758,
      "learning_rate": 2.9888456988338687e-05,
      "loss": 0.4435,
      "step": 10020
    },
    {
      "epoch": 1.6600463422707712,
      "grad_norm": 4.3927178382873535,
      "learning_rate": 2.9867331417948285e-05,
      "loss": 0.7941,
      "step": 10030
    },
    {
      "epoch": 1.6617014233697451,
      "grad_norm": 2.315178871154785,
      "learning_rate": 2.9846205847557883e-05,
      "loss": 0.6008,
      "step": 10040
    },
    {
      "epoch": 1.663356504468719,
      "grad_norm": 7.453789234161377,
      "learning_rate": 2.9825080277167482e-05,
      "loss": 0.6472,
      "step": 10050
    },
    {
      "epoch": 1.6650115855676928,
      "grad_norm": 5.14705228805542,
      "learning_rate": 2.9803954706777084e-05,
      "loss": 0.838,
      "step": 10060
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 9.856420516967773,
      "learning_rate": 2.9782829136386685e-05,
      "loss": 0.8189,
      "step": 10070
    },
    {
      "epoch": 1.6683217477656405,
      "grad_norm": 14.185420989990234,
      "learning_rate": 2.9761703565996284e-05,
      "loss": 0.7048,
      "step": 10080
    },
    {
      "epoch": 1.6699768288646144,
      "grad_norm": 2.209510087966919,
      "learning_rate": 2.9740577995605885e-05,
      "loss": 0.5872,
      "step": 10090
    },
    {
      "epoch": 1.6716319099635881,
      "grad_norm": 4.505244731903076,
      "learning_rate": 2.9719452425215484e-05,
      "loss": 0.4807,
      "step": 10100
    },
    {
      "epoch": 1.673286991062562,
      "grad_norm": 9.42475700378418,
      "learning_rate": 2.9698326854825082e-05,
      "loss": 0.7362,
      "step": 10110
    },
    {
      "epoch": 1.674942072161536,
      "grad_norm": 4.169875144958496,
      "learning_rate": 2.967720128443468e-05,
      "loss": 0.8299,
      "step": 10120
    },
    {
      "epoch": 1.6765971532605097,
      "grad_norm": 7.596154689788818,
      "learning_rate": 2.965607571404428e-05,
      "loss": 0.79,
      "step": 10130
    },
    {
      "epoch": 1.6782522343594835,
      "grad_norm": 8.331493377685547,
      "learning_rate": 2.9634950143653877e-05,
      "loss": 0.706,
      "step": 10140
    },
    {
      "epoch": 1.6799073154584576,
      "grad_norm": 10.557329177856445,
      "learning_rate": 2.9613824573263483e-05,
      "loss": 0.7606,
      "step": 10150
    },
    {
      "epoch": 1.6815623965574313,
      "grad_norm": 6.832432746887207,
      "learning_rate": 2.959269900287308e-05,
      "loss": 0.6861,
      "step": 10160
    },
    {
      "epoch": 1.683217477656405,
      "grad_norm": 8.423654556274414,
      "learning_rate": 2.957157343248268e-05,
      "loss": 0.8292,
      "step": 10170
    },
    {
      "epoch": 1.684872558755379,
      "grad_norm": 11.796192169189453,
      "learning_rate": 2.9550447862092278e-05,
      "loss": 0.7123,
      "step": 10180
    },
    {
      "epoch": 1.686527639854353,
      "grad_norm": 7.814089775085449,
      "learning_rate": 2.9529322291701876e-05,
      "loss": 0.7668,
      "step": 10190
    },
    {
      "epoch": 1.6881827209533267,
      "grad_norm": 9.458212852478027,
      "learning_rate": 2.9508196721311478e-05,
      "loss": 0.7608,
      "step": 10200
    },
    {
      "epoch": 1.6898378020523006,
      "grad_norm": 1.9778622388839722,
      "learning_rate": 2.9487071150921076e-05,
      "loss": 0.5303,
      "step": 10210
    },
    {
      "epoch": 1.6914928831512746,
      "grad_norm": 0.45171818137168884,
      "learning_rate": 2.9465945580530675e-05,
      "loss": 0.6155,
      "step": 10220
    },
    {
      "epoch": 1.6931479642502483,
      "grad_norm": 9.554718971252441,
      "learning_rate": 2.9444820010140273e-05,
      "loss": 0.8481,
      "step": 10230
    },
    {
      "epoch": 1.694803045349222,
      "grad_norm": 5.126983642578125,
      "learning_rate": 2.9423694439749878e-05,
      "loss": 0.4612,
      "step": 10240
    },
    {
      "epoch": 1.696458126448196,
      "grad_norm": 10.4384183883667,
      "learning_rate": 2.9402568869359477e-05,
      "loss": 0.653,
      "step": 10250
    },
    {
      "epoch": 1.6981132075471699,
      "grad_norm": 9.860486030578613,
      "learning_rate": 2.9381443298969075e-05,
      "loss": 0.6952,
      "step": 10260
    },
    {
      "epoch": 1.6997682886461436,
      "grad_norm": 6.370824813842773,
      "learning_rate": 2.9360317728578673e-05,
      "loss": 0.9798,
      "step": 10270
    },
    {
      "epoch": 1.7014233697451175,
      "grad_norm": 1.6601736545562744,
      "learning_rate": 2.933919215818827e-05,
      "loss": 0.5498,
      "step": 10280
    },
    {
      "epoch": 1.7030784508440915,
      "grad_norm": 5.674802780151367,
      "learning_rate": 2.931806658779787e-05,
      "loss": 0.7001,
      "step": 10290
    },
    {
      "epoch": 1.7047335319430652,
      "grad_norm": 10.558813095092773,
      "learning_rate": 2.929694101740747e-05,
      "loss": 0.6487,
      "step": 10300
    },
    {
      "epoch": 1.706388613042039,
      "grad_norm": 10.952089309692383,
      "learning_rate": 2.927581544701707e-05,
      "loss": 0.5384,
      "step": 10310
    },
    {
      "epoch": 1.7080436941410129,
      "grad_norm": 4.31552791595459,
      "learning_rate": 2.9254689876626672e-05,
      "loss": 0.4352,
      "step": 10320
    },
    {
      "epoch": 1.7096987752399868,
      "grad_norm": 9.620162963867188,
      "learning_rate": 2.923356430623627e-05,
      "loss": 0.5812,
      "step": 10330
    },
    {
      "epoch": 1.7113538563389605,
      "grad_norm": 7.864400863647461,
      "learning_rate": 2.9212438735845872e-05,
      "loss": 0.8178,
      "step": 10340
    },
    {
      "epoch": 1.7130089374379345,
      "grad_norm": 6.937893867492676,
      "learning_rate": 2.919131316545547e-05,
      "loss": 0.5319,
      "step": 10350
    },
    {
      "epoch": 1.7146640185369084,
      "grad_norm": 14.069920539855957,
      "learning_rate": 2.917018759506507e-05,
      "loss": 0.903,
      "step": 10360
    },
    {
      "epoch": 1.7163190996358821,
      "grad_norm": 14.14964771270752,
      "learning_rate": 2.9149062024674667e-05,
      "loss": 0.9481,
      "step": 10370
    },
    {
      "epoch": 1.7179741807348559,
      "grad_norm": 8.848390579223633,
      "learning_rate": 2.9127936454284266e-05,
      "loss": 0.7393,
      "step": 10380
    },
    {
      "epoch": 1.7196292618338298,
      "grad_norm": 5.903340816497803,
      "learning_rate": 2.9106810883893864e-05,
      "loss": 0.7587,
      "step": 10390
    },
    {
      "epoch": 1.7212843429328037,
      "grad_norm": 8.207732200622559,
      "learning_rate": 2.9085685313503462e-05,
      "loss": 0.5722,
      "step": 10400
    },
    {
      "epoch": 1.7229394240317775,
      "grad_norm": 3.095677614212036,
      "learning_rate": 2.9064559743113068e-05,
      "loss": 0.4849,
      "step": 10410
    },
    {
      "epoch": 1.7245945051307514,
      "grad_norm": 6.148995399475098,
      "learning_rate": 2.9043434172722666e-05,
      "loss": 0.7504,
      "step": 10420
    },
    {
      "epoch": 1.7262495862297254,
      "grad_norm": 11.436935424804688,
      "learning_rate": 2.9022308602332264e-05,
      "loss": 0.7367,
      "step": 10430
    },
    {
      "epoch": 1.727904667328699,
      "grad_norm": 5.623143196105957,
      "learning_rate": 2.9001183031941863e-05,
      "loss": 0.6789,
      "step": 10440
    },
    {
      "epoch": 1.7295597484276728,
      "grad_norm": 11.208179473876953,
      "learning_rate": 2.8980057461551465e-05,
      "loss": 0.6612,
      "step": 10450
    },
    {
      "epoch": 1.731214829526647,
      "grad_norm": 3.2015810012817383,
      "learning_rate": 2.8958931891161063e-05,
      "loss": 0.8846,
      "step": 10460
    },
    {
      "epoch": 1.7328699106256207,
      "grad_norm": 14.068859100341797,
      "learning_rate": 2.893780632077066e-05,
      "loss": 0.8057,
      "step": 10470
    },
    {
      "epoch": 1.7345249917245944,
      "grad_norm": 3.396934986114502,
      "learning_rate": 2.891668075038026e-05,
      "loss": 0.7057,
      "step": 10480
    },
    {
      "epoch": 1.7361800728235683,
      "grad_norm": 7.737791061401367,
      "learning_rate": 2.8895555179989865e-05,
      "loss": 0.5418,
      "step": 10490
    },
    {
      "epoch": 1.7378351539225423,
      "grad_norm": 14.853691101074219,
      "learning_rate": 2.8874429609599463e-05,
      "loss": 0.8471,
      "step": 10500
    },
    {
      "epoch": 1.739490235021516,
      "grad_norm": 1.7635552883148193,
      "learning_rate": 2.885330403920906e-05,
      "loss": 0.705,
      "step": 10510
    },
    {
      "epoch": 1.74114531612049,
      "grad_norm": 9.503922462463379,
      "learning_rate": 2.883217846881866e-05,
      "loss": 0.9683,
      "step": 10520
    },
    {
      "epoch": 1.742800397219464,
      "grad_norm": 7.787638187408447,
      "learning_rate": 2.881105289842826e-05,
      "loss": 0.6567,
      "step": 10530
    },
    {
      "epoch": 1.7444554783184376,
      "grad_norm": 13.376498222351074,
      "learning_rate": 2.8789927328037857e-05,
      "loss": 0.7,
      "step": 10540
    },
    {
      "epoch": 1.7461105594174113,
      "grad_norm": 8.31912612915039,
      "learning_rate": 2.8768801757647455e-05,
      "loss": 0.7447,
      "step": 10550
    },
    {
      "epoch": 1.7477656405163853,
      "grad_norm": 6.077436923980713,
      "learning_rate": 2.8747676187257057e-05,
      "loss": 0.5459,
      "step": 10560
    },
    {
      "epoch": 1.7494207216153592,
      "grad_norm": 6.657055377960205,
      "learning_rate": 2.8726550616866655e-05,
      "loss": 0.7315,
      "step": 10570
    },
    {
      "epoch": 1.751075802714333,
      "grad_norm": 7.677982807159424,
      "learning_rate": 2.8705425046476257e-05,
      "loss": 0.7693,
      "step": 10580
    },
    {
      "epoch": 1.7527308838133069,
      "grad_norm": 5.541990756988525,
      "learning_rate": 2.868429947608586e-05,
      "loss": 0.6465,
      "step": 10590
    },
    {
      "epoch": 1.7543859649122808,
      "grad_norm": 6.058269023895264,
      "learning_rate": 2.8663173905695457e-05,
      "loss": 0.811,
      "step": 10600
    },
    {
      "epoch": 1.7560410460112545,
      "grad_norm": 3.1122348308563232,
      "learning_rate": 2.8642048335305056e-05,
      "loss": 0.8179,
      "step": 10610
    },
    {
      "epoch": 1.7576961271102283,
      "grad_norm": 2.5586354732513428,
      "learning_rate": 2.8620922764914654e-05,
      "loss": 0.75,
      "step": 10620
    },
    {
      "epoch": 1.7593512082092022,
      "grad_norm": 5.883629322052002,
      "learning_rate": 2.8599797194524252e-05,
      "loss": 0.7485,
      "step": 10630
    },
    {
      "epoch": 1.7610062893081762,
      "grad_norm": 4.991631031036377,
      "learning_rate": 2.857867162413385e-05,
      "loss": 0.7489,
      "step": 10640
    },
    {
      "epoch": 1.7626613704071499,
      "grad_norm": 7.49458646774292,
      "learning_rate": 2.855754605374345e-05,
      "loss": 0.5526,
      "step": 10650
    },
    {
      "epoch": 1.7643164515061238,
      "grad_norm": 4.664979457855225,
      "learning_rate": 2.8536420483353054e-05,
      "loss": 0.8245,
      "step": 10660
    },
    {
      "epoch": 1.7659715326050978,
      "grad_norm": 3.9022204875946045,
      "learning_rate": 2.8515294912962653e-05,
      "loss": 0.5719,
      "step": 10670
    },
    {
      "epoch": 1.7676266137040715,
      "grad_norm": 7.6836771965026855,
      "learning_rate": 2.849416934257225e-05,
      "loss": 0.5764,
      "step": 10680
    },
    {
      "epoch": 1.7692816948030452,
      "grad_norm": 2.5252983570098877,
      "learning_rate": 2.847304377218185e-05,
      "loss": 0.5396,
      "step": 10690
    },
    {
      "epoch": 1.7709367759020191,
      "grad_norm": 6.438911437988281,
      "learning_rate": 2.845191820179145e-05,
      "loss": 0.8521,
      "step": 10700
    },
    {
      "epoch": 1.772591857000993,
      "grad_norm": 8.692585945129395,
      "learning_rate": 2.843079263140105e-05,
      "loss": 0.7903,
      "step": 10710
    },
    {
      "epoch": 1.7742469380999668,
      "grad_norm": 4.1693434715271,
      "learning_rate": 2.8409667061010648e-05,
      "loss": 0.5676,
      "step": 10720
    },
    {
      "epoch": 1.7759020191989408,
      "grad_norm": 7.894397258758545,
      "learning_rate": 2.8388541490620246e-05,
      "loss": 0.5731,
      "step": 10730
    },
    {
      "epoch": 1.7775571002979147,
      "grad_norm": 11.88364315032959,
      "learning_rate": 2.8367415920229845e-05,
      "loss": 0.7537,
      "step": 10740
    },
    {
      "epoch": 1.7792121813968884,
      "grad_norm": 6.327468395233154,
      "learning_rate": 2.834629034983945e-05,
      "loss": 0.5111,
      "step": 10750
    },
    {
      "epoch": 1.7808672624958621,
      "grad_norm": 9.538745880126953,
      "learning_rate": 2.8325164779449048e-05,
      "loss": 1.0039,
      "step": 10760
    },
    {
      "epoch": 1.7825223435948363,
      "grad_norm": 5.892218589782715,
      "learning_rate": 2.8304039209058647e-05,
      "loss": 0.4347,
      "step": 10770
    },
    {
      "epoch": 1.78417742469381,
      "grad_norm": 5.04967737197876,
      "learning_rate": 2.8282913638668245e-05,
      "loss": 0.7211,
      "step": 10780
    },
    {
      "epoch": 1.7858325057927837,
      "grad_norm": 11.380403518676758,
      "learning_rate": 2.8261788068277843e-05,
      "loss": 0.65,
      "step": 10790
    },
    {
      "epoch": 1.7874875868917577,
      "grad_norm": 9.272041320800781,
      "learning_rate": 2.8240662497887442e-05,
      "loss": 0.6879,
      "step": 10800
    },
    {
      "epoch": 1.7891426679907316,
      "grad_norm": 4.747367858886719,
      "learning_rate": 2.8219536927497044e-05,
      "loss": 0.3976,
      "step": 10810
    },
    {
      "epoch": 1.7907977490897053,
      "grad_norm": 7.258157253265381,
      "learning_rate": 2.8198411357106642e-05,
      "loss": 0.5679,
      "step": 10820
    },
    {
      "epoch": 1.7924528301886793,
      "grad_norm": 6.686504364013672,
      "learning_rate": 2.8177285786716244e-05,
      "loss": 0.7579,
      "step": 10830
    },
    {
      "epoch": 1.7941079112876532,
      "grad_norm": 10.540482521057129,
      "learning_rate": 2.8156160216325846e-05,
      "loss": 0.4968,
      "step": 10840
    },
    {
      "epoch": 1.795762992386627,
      "grad_norm": 9.53823184967041,
      "learning_rate": 2.8135034645935444e-05,
      "loss": 0.582,
      "step": 10850
    },
    {
      "epoch": 1.7974180734856007,
      "grad_norm": 4.284958839416504,
      "learning_rate": 2.8113909075545042e-05,
      "loss": 0.6028,
      "step": 10860
    },
    {
      "epoch": 1.7990731545845746,
      "grad_norm": 9.711718559265137,
      "learning_rate": 2.809278350515464e-05,
      "loss": 0.7532,
      "step": 10870
    },
    {
      "epoch": 1.8007282356835486,
      "grad_norm": 6.902862548828125,
      "learning_rate": 2.807165793476424e-05,
      "loss": 0.809,
      "step": 10880
    },
    {
      "epoch": 1.8023833167825223,
      "grad_norm": 8.179754257202148,
      "learning_rate": 2.8050532364373837e-05,
      "loss": 0.6302,
      "step": 10890
    },
    {
      "epoch": 1.8040383978814962,
      "grad_norm": 9.080558776855469,
      "learning_rate": 2.8029406793983436e-05,
      "loss": 0.7332,
      "step": 10900
    },
    {
      "epoch": 1.8056934789804702,
      "grad_norm": 12.572257995605469,
      "learning_rate": 2.8008281223593034e-05,
      "loss": 1.3787,
      "step": 10910
    },
    {
      "epoch": 1.8073485600794439,
      "grad_norm": 0.821594774723053,
      "learning_rate": 2.798715565320264e-05,
      "loss": 0.8835,
      "step": 10920
    },
    {
      "epoch": 1.8090036411784176,
      "grad_norm": 3.8074162006378174,
      "learning_rate": 2.7966030082812238e-05,
      "loss": 0.7603,
      "step": 10930
    },
    {
      "epoch": 1.8106587222773916,
      "grad_norm": 2.790959596633911,
      "learning_rate": 2.794490451242184e-05,
      "loss": 0.6041,
      "step": 10940
    },
    {
      "epoch": 1.8123138033763655,
      "grad_norm": 6.163257122039795,
      "learning_rate": 2.7923778942031438e-05,
      "loss": 0.8702,
      "step": 10950
    },
    {
      "epoch": 1.8139688844753392,
      "grad_norm": 7.015653610229492,
      "learning_rate": 2.7902653371641036e-05,
      "loss": 0.6859,
      "step": 10960
    },
    {
      "epoch": 1.8156239655743132,
      "grad_norm": 8.134598731994629,
      "learning_rate": 2.7881527801250635e-05,
      "loss": 0.5471,
      "step": 10970
    },
    {
      "epoch": 1.817279046673287,
      "grad_norm": 9.133872985839844,
      "learning_rate": 2.7860402230860233e-05,
      "loss": 0.5379,
      "step": 10980
    },
    {
      "epoch": 1.8189341277722608,
      "grad_norm": 6.082035541534424,
      "learning_rate": 2.783927666046983e-05,
      "loss": 0.59,
      "step": 10990
    },
    {
      "epoch": 1.8205892088712345,
      "grad_norm": 2.086956024169922,
      "learning_rate": 2.7818151090079437e-05,
      "loss": 0.7053,
      "step": 11000
    },
    {
      "epoch": 1.8222442899702085,
      "grad_norm": 3.9870545864105225,
      "learning_rate": 2.7797025519689035e-05,
      "loss": 0.5741,
      "step": 11010
    },
    {
      "epoch": 1.8238993710691824,
      "grad_norm": 7.210365295410156,
      "learning_rate": 2.7775899949298633e-05,
      "loss": 0.5523,
      "step": 11020
    },
    {
      "epoch": 1.8255544521681561,
      "grad_norm": 5.898279190063477,
      "learning_rate": 2.7754774378908232e-05,
      "loss": 0.6738,
      "step": 11030
    },
    {
      "epoch": 1.82720953326713,
      "grad_norm": 3.7487051486968994,
      "learning_rate": 2.773364880851783e-05,
      "loss": 0.5874,
      "step": 11040
    },
    {
      "epoch": 1.828864614366104,
      "grad_norm": 6.380067348480225,
      "learning_rate": 2.7712523238127432e-05,
      "loss": 0.6087,
      "step": 11050
    },
    {
      "epoch": 1.8305196954650778,
      "grad_norm": 6.079387187957764,
      "learning_rate": 2.769139766773703e-05,
      "loss": 0.6415,
      "step": 11060
    },
    {
      "epoch": 1.8321747765640515,
      "grad_norm": 7.479349613189697,
      "learning_rate": 2.767027209734663e-05,
      "loss": 0.7015,
      "step": 11070
    },
    {
      "epoch": 1.8338298576630256,
      "grad_norm": 8.856660842895508,
      "learning_rate": 2.7649146526956227e-05,
      "loss": 0.6407,
      "step": 11080
    },
    {
      "epoch": 1.8354849387619994,
      "grad_norm": 5.084643363952637,
      "learning_rate": 2.7628020956565832e-05,
      "loss": 0.5616,
      "step": 11090
    },
    {
      "epoch": 1.837140019860973,
      "grad_norm": 10.033227920532227,
      "learning_rate": 2.760689538617543e-05,
      "loss": 0.5345,
      "step": 11100
    },
    {
      "epoch": 1.838795100959947,
      "grad_norm": 7.844731330871582,
      "learning_rate": 2.758576981578503e-05,
      "loss": 0.5746,
      "step": 11110
    },
    {
      "epoch": 1.840450182058921,
      "grad_norm": 2.716636896133423,
      "learning_rate": 2.7564644245394627e-05,
      "loss": 0.5816,
      "step": 11120
    },
    {
      "epoch": 1.8421052631578947,
      "grad_norm": 3.7201716899871826,
      "learning_rate": 2.7543518675004226e-05,
      "loss": 0.6663,
      "step": 11130
    },
    {
      "epoch": 1.8437603442568686,
      "grad_norm": 9.72002124786377,
      "learning_rate": 2.7522393104613824e-05,
      "loss": 0.6247,
      "step": 11140
    },
    {
      "epoch": 1.8454154253558426,
      "grad_norm": 4.3175225257873535,
      "learning_rate": 2.7501267534223423e-05,
      "loss": 0.731,
      "step": 11150
    },
    {
      "epoch": 1.8470705064548163,
      "grad_norm": 5.636064052581787,
      "learning_rate": 2.7480141963833024e-05,
      "loss": 0.797,
      "step": 11160
    },
    {
      "epoch": 1.84872558755379,
      "grad_norm": 7.097209453582764,
      "learning_rate": 2.7459016393442626e-05,
      "loss": 0.8246,
      "step": 11170
    },
    {
      "epoch": 1.850380668652764,
      "grad_norm": 2.3647985458374023,
      "learning_rate": 2.7437890823052224e-05,
      "loss": 0.5222,
      "step": 11180
    },
    {
      "epoch": 1.852035749751738,
      "grad_norm": 7.887512683868408,
      "learning_rate": 2.7416765252661826e-05,
      "loss": 0.6683,
      "step": 11190
    },
    {
      "epoch": 1.8536908308507116,
      "grad_norm": 4.718008518218994,
      "learning_rate": 2.7395639682271425e-05,
      "loss": 0.423,
      "step": 11200
    },
    {
      "epoch": 1.8553459119496856,
      "grad_norm": 4.46136474609375,
      "learning_rate": 2.7374514111881023e-05,
      "loss": 0.5828,
      "step": 11210
    },
    {
      "epoch": 1.8570009930486595,
      "grad_norm": 9.49305248260498,
      "learning_rate": 2.735338854149062e-05,
      "loss": 0.4998,
      "step": 11220
    },
    {
      "epoch": 1.8586560741476332,
      "grad_norm": 12.336441993713379,
      "learning_rate": 2.733226297110022e-05,
      "loss": 0.872,
      "step": 11230
    },
    {
      "epoch": 1.860311155246607,
      "grad_norm": 16.26667022705078,
      "learning_rate": 2.7311137400709818e-05,
      "loss": 0.8469,
      "step": 11240
    },
    {
      "epoch": 1.861966236345581,
      "grad_norm": 6.5281267166137695,
      "learning_rate": 2.7290011830319416e-05,
      "loss": 0.6596,
      "step": 11250
    },
    {
      "epoch": 1.8636213174445548,
      "grad_norm": 3.4119770526885986,
      "learning_rate": 2.726888625992902e-05,
      "loss": 0.5131,
      "step": 11260
    },
    {
      "epoch": 1.8652763985435286,
      "grad_norm": 6.9927077293396,
      "learning_rate": 2.724776068953862e-05,
      "loss": 0.459,
      "step": 11270
    },
    {
      "epoch": 1.8669314796425025,
      "grad_norm": 4.523588180541992,
      "learning_rate": 2.722663511914822e-05,
      "loss": 0.5144,
      "step": 11280
    },
    {
      "epoch": 1.8685865607414764,
      "grad_norm": 6.561376571655273,
      "learning_rate": 2.7205509548757817e-05,
      "loss": 0.9042,
      "step": 11290
    },
    {
      "epoch": 1.8702416418404502,
      "grad_norm": 1.4205429553985596,
      "learning_rate": 2.718438397836742e-05,
      "loss": 0.697,
      "step": 11300
    },
    {
      "epoch": 1.8718967229394239,
      "grad_norm": 3.553602933883667,
      "learning_rate": 2.7163258407977017e-05,
      "loss": 0.8697,
      "step": 11310
    },
    {
      "epoch": 1.8735518040383978,
      "grad_norm": 7.370333671569824,
      "learning_rate": 2.7142132837586615e-05,
      "loss": 0.4877,
      "step": 11320
    },
    {
      "epoch": 1.8752068851373718,
      "grad_norm": 12.96839714050293,
      "learning_rate": 2.7121007267196214e-05,
      "loss": 0.7989,
      "step": 11330
    },
    {
      "epoch": 1.8768619662363455,
      "grad_norm": 7.851128578186035,
      "learning_rate": 2.709988169680582e-05,
      "loss": 0.6133,
      "step": 11340
    },
    {
      "epoch": 1.8785170473353194,
      "grad_norm": 3.7234365940093994,
      "learning_rate": 2.7078756126415417e-05,
      "loss": 0.8014,
      "step": 11350
    },
    {
      "epoch": 1.8801721284342934,
      "grad_norm": 7.6572136878967285,
      "learning_rate": 2.7057630556025016e-05,
      "loss": 1.1146,
      "step": 11360
    },
    {
      "epoch": 1.881827209533267,
      "grad_norm": 15.97938060760498,
      "learning_rate": 2.7036504985634614e-05,
      "loss": 0.6419,
      "step": 11370
    },
    {
      "epoch": 1.8834822906322408,
      "grad_norm": 5.039325714111328,
      "learning_rate": 2.7015379415244212e-05,
      "loss": 0.6463,
      "step": 11380
    },
    {
      "epoch": 1.885137371731215,
      "grad_norm": 3.1931204795837402,
      "learning_rate": 2.699425384485381e-05,
      "loss": 0.6616,
      "step": 11390
    },
    {
      "epoch": 1.8867924528301887,
      "grad_norm": 15.371889114379883,
      "learning_rate": 2.697312827446341e-05,
      "loss": 0.7662,
      "step": 11400
    },
    {
      "epoch": 1.8884475339291624,
      "grad_norm": 4.939863681793213,
      "learning_rate": 2.695200270407301e-05,
      "loss": 0.6993,
      "step": 11410
    },
    {
      "epoch": 1.8901026150281364,
      "grad_norm": 9.487849235534668,
      "learning_rate": 2.693087713368261e-05,
      "loss": 0.5206,
      "step": 11420
    },
    {
      "epoch": 1.8917576961271103,
      "grad_norm": 7.005796909332275,
      "learning_rate": 2.690975156329221e-05,
      "loss": 0.6504,
      "step": 11430
    },
    {
      "epoch": 1.893412777226084,
      "grad_norm": 8.260921478271484,
      "learning_rate": 2.6888625992901813e-05,
      "loss": 0.4978,
      "step": 11440
    },
    {
      "epoch": 1.895067858325058,
      "grad_norm": 7.263406753540039,
      "learning_rate": 2.686750042251141e-05,
      "loss": 0.6031,
      "step": 11450
    },
    {
      "epoch": 1.896722939424032,
      "grad_norm": 7.041758060455322,
      "learning_rate": 2.684637485212101e-05,
      "loss": 0.7448,
      "step": 11460
    },
    {
      "epoch": 1.8983780205230056,
      "grad_norm": 8.920317649841309,
      "learning_rate": 2.6825249281730608e-05,
      "loss": 0.6412,
      "step": 11470
    },
    {
      "epoch": 1.9000331016219794,
      "grad_norm": 7.251124382019043,
      "learning_rate": 2.6804123711340206e-05,
      "loss": 0.6119,
      "step": 11480
    },
    {
      "epoch": 1.9016881827209533,
      "grad_norm": 9.376360893249512,
      "learning_rate": 2.6782998140949805e-05,
      "loss": 0.5737,
      "step": 11490
    },
    {
      "epoch": 1.9033432638199272,
      "grad_norm": 11.813860893249512,
      "learning_rate": 2.6761872570559403e-05,
      "loss": 0.7317,
      "step": 11500
    },
    {
      "epoch": 1.904998344918901,
      "grad_norm": 3.818513870239258,
      "learning_rate": 2.674074700016901e-05,
      "loss": 0.6721,
      "step": 11510
    },
    {
      "epoch": 1.906653426017875,
      "grad_norm": 9.76960563659668,
      "learning_rate": 2.6719621429778607e-05,
      "loss": 0.9387,
      "step": 11520
    },
    {
      "epoch": 1.9083085071168489,
      "grad_norm": 7.79152774810791,
      "learning_rate": 2.6698495859388205e-05,
      "loss": 0.7683,
      "step": 11530
    },
    {
      "epoch": 1.9099635882158226,
      "grad_norm": 9.979121208190918,
      "learning_rate": 2.6677370288997803e-05,
      "loss": 0.7389,
      "step": 11540
    },
    {
      "epoch": 1.9116186693147963,
      "grad_norm": 4.865985870361328,
      "learning_rate": 2.6656244718607405e-05,
      "loss": 0.968,
      "step": 11550
    },
    {
      "epoch": 1.9132737504137702,
      "grad_norm": 7.992015361785889,
      "learning_rate": 2.6635119148217004e-05,
      "loss": 0.8791,
      "step": 11560
    },
    {
      "epoch": 1.9149288315127442,
      "grad_norm": 6.297119617462158,
      "learning_rate": 2.6613993577826602e-05,
      "loss": 0.7482,
      "step": 11570
    },
    {
      "epoch": 1.916583912611718,
      "grad_norm": 2.8887088298797607,
      "learning_rate": 2.65928680074362e-05,
      "loss": 0.5483,
      "step": 11580
    },
    {
      "epoch": 1.9182389937106918,
      "grad_norm": 10.49492359161377,
      "learning_rate": 2.65717424370458e-05,
      "loss": 0.5371,
      "step": 11590
    },
    {
      "epoch": 1.9198940748096658,
      "grad_norm": 6.460445880889893,
      "learning_rate": 2.6550616866655404e-05,
      "loss": 0.7356,
      "step": 11600
    },
    {
      "epoch": 1.9215491559086395,
      "grad_norm": 9.367919921875,
      "learning_rate": 2.6529491296265002e-05,
      "loss": 0.7875,
      "step": 11610
    },
    {
      "epoch": 1.9232042370076132,
      "grad_norm": 6.753652572631836,
      "learning_rate": 2.65083657258746e-05,
      "loss": 0.6686,
      "step": 11620
    },
    {
      "epoch": 1.9248593181065872,
      "grad_norm": 10.88228702545166,
      "learning_rate": 2.64872401554842e-05,
      "loss": 0.8549,
      "step": 11630
    },
    {
      "epoch": 1.9265143992055611,
      "grad_norm": 6.009291648864746,
      "learning_rate": 2.6466114585093797e-05,
      "loss": 0.8383,
      "step": 11640
    },
    {
      "epoch": 1.9281694803045348,
      "grad_norm": 5.272893905639648,
      "learning_rate": 2.6444989014703396e-05,
      "loss": 0.6214,
      "step": 11650
    },
    {
      "epoch": 1.9298245614035088,
      "grad_norm": 9.108641624450684,
      "learning_rate": 2.6423863444312998e-05,
      "loss": 0.6079,
      "step": 11660
    },
    {
      "epoch": 1.9314796425024827,
      "grad_norm": 4.889091491699219,
      "learning_rate": 2.6402737873922596e-05,
      "loss": 0.6088,
      "step": 11670
    },
    {
      "epoch": 1.9331347236014564,
      "grad_norm": 5.582423686981201,
      "learning_rate": 2.6381612303532198e-05,
      "loss": 0.4524,
      "step": 11680
    },
    {
      "epoch": 1.9347898047004302,
      "grad_norm": 11.567883491516113,
      "learning_rate": 2.63604867331418e-05,
      "loss": 0.6744,
      "step": 11690
    },
    {
      "epoch": 1.9364448857994043,
      "grad_norm": 4.840481758117676,
      "learning_rate": 2.6339361162751398e-05,
      "loss": 0.5944,
      "step": 11700
    },
    {
      "epoch": 1.938099966898378,
      "grad_norm": 5.520872592926025,
      "learning_rate": 2.6318235592360996e-05,
      "loss": 0.9857,
      "step": 11710
    },
    {
      "epoch": 1.9397550479973518,
      "grad_norm": 10.392516136169434,
      "learning_rate": 2.6297110021970595e-05,
      "loss": 0.6162,
      "step": 11720
    },
    {
      "epoch": 1.9414101290963257,
      "grad_norm": 4.256656169891357,
      "learning_rate": 2.6275984451580193e-05,
      "loss": 0.782,
      "step": 11730
    },
    {
      "epoch": 1.9430652101952997,
      "grad_norm": 3.5730466842651367,
      "learning_rate": 2.625485888118979e-05,
      "loss": 0.4999,
      "step": 11740
    },
    {
      "epoch": 1.9447202912942734,
      "grad_norm": 5.9669623374938965,
      "learning_rate": 2.623373331079939e-05,
      "loss": 0.7186,
      "step": 11750
    },
    {
      "epoch": 1.9463753723932473,
      "grad_norm": 4.916635513305664,
      "learning_rate": 2.6212607740408988e-05,
      "loss": 0.8722,
      "step": 11760
    },
    {
      "epoch": 1.9480304534922213,
      "grad_norm": 2.7786076068878174,
      "learning_rate": 2.6191482170018593e-05,
      "loss": 0.811,
      "step": 11770
    },
    {
      "epoch": 1.949685534591195,
      "grad_norm": 8.172378540039062,
      "learning_rate": 2.6170356599628192e-05,
      "loss": 0.783,
      "step": 11780
    },
    {
      "epoch": 1.9513406156901687,
      "grad_norm": 3.4159371852874756,
      "learning_rate": 2.614923102923779e-05,
      "loss": 0.6533,
      "step": 11790
    },
    {
      "epoch": 1.9529956967891426,
      "grad_norm": 1.6024651527404785,
      "learning_rate": 2.6128105458847392e-05,
      "loss": 0.9008,
      "step": 11800
    },
    {
      "epoch": 1.9546507778881166,
      "grad_norm": 6.659937381744385,
      "learning_rate": 2.610697988845699e-05,
      "loss": 0.401,
      "step": 11810
    },
    {
      "epoch": 1.9563058589870903,
      "grad_norm": 5.116189479827881,
      "learning_rate": 2.608585431806659e-05,
      "loss": 0.692,
      "step": 11820
    },
    {
      "epoch": 1.9579609400860642,
      "grad_norm": 12.277504920959473,
      "learning_rate": 2.6064728747676187e-05,
      "loss": 0.6657,
      "step": 11830
    },
    {
      "epoch": 1.9596160211850382,
      "grad_norm": 10.450069427490234,
      "learning_rate": 2.6043603177285785e-05,
      "loss": 0.7935,
      "step": 11840
    },
    {
      "epoch": 1.961271102284012,
      "grad_norm": 6.333822250366211,
      "learning_rate": 2.602247760689539e-05,
      "loss": 0.7396,
      "step": 11850
    },
    {
      "epoch": 1.9629261833829856,
      "grad_norm": 9.002862930297852,
      "learning_rate": 2.600135203650499e-05,
      "loss": 0.4987,
      "step": 11860
    },
    {
      "epoch": 1.9645812644819596,
      "grad_norm": 4.263606071472168,
      "learning_rate": 2.5980226466114587e-05,
      "loss": 0.7053,
      "step": 11870
    },
    {
      "epoch": 1.9662363455809335,
      "grad_norm": 6.319537162780762,
      "learning_rate": 2.5959100895724186e-05,
      "loss": 0.6639,
      "step": 11880
    },
    {
      "epoch": 1.9678914266799072,
      "grad_norm": 6.470550537109375,
      "learning_rate": 2.5937975325333784e-05,
      "loss": 0.653,
      "step": 11890
    },
    {
      "epoch": 1.9695465077788812,
      "grad_norm": 17.440753936767578,
      "learning_rate": 2.5916849754943383e-05,
      "loss": 1.0208,
      "step": 11900
    },
    {
      "epoch": 1.9712015888778551,
      "grad_norm": 7.6418232917785645,
      "learning_rate": 2.5895724184552984e-05,
      "loss": 0.6385,
      "step": 11910
    },
    {
      "epoch": 1.9728566699768288,
      "grad_norm": 8.591548919677734,
      "learning_rate": 2.5874598614162583e-05,
      "loss": 0.5961,
      "step": 11920
    },
    {
      "epoch": 1.9745117510758026,
      "grad_norm": 4.126323223114014,
      "learning_rate": 2.585347304377218e-05,
      "loss": 0.4409,
      "step": 11930
    },
    {
      "epoch": 1.9761668321747765,
      "grad_norm": 1.949657917022705,
      "learning_rate": 2.5832347473381786e-05,
      "loss": 0.704,
      "step": 11940
    },
    {
      "epoch": 1.9778219132737505,
      "grad_norm": 8.87740707397461,
      "learning_rate": 2.5811221902991385e-05,
      "loss": 0.7217,
      "step": 11950
    },
    {
      "epoch": 1.9794769943727242,
      "grad_norm": 5.372537612915039,
      "learning_rate": 2.5790096332600983e-05,
      "loss": 0.5814,
      "step": 11960
    },
    {
      "epoch": 1.9811320754716981,
      "grad_norm": 3.392500162124634,
      "learning_rate": 2.576897076221058e-05,
      "loss": 0.52,
      "step": 11970
    },
    {
      "epoch": 1.982787156570672,
      "grad_norm": 5.720313549041748,
      "learning_rate": 2.574784519182018e-05,
      "loss": 0.6135,
      "step": 11980
    },
    {
      "epoch": 1.9844422376696458,
      "grad_norm": 15.157927513122559,
      "learning_rate": 2.5726719621429778e-05,
      "loss": 0.9472,
      "step": 11990
    },
    {
      "epoch": 1.9860973187686195,
      "grad_norm": 7.303770542144775,
      "learning_rate": 2.5705594051039377e-05,
      "loss": 0.8612,
      "step": 12000
    },
    {
      "epoch": 1.9877523998675937,
      "grad_norm": 9.365641593933105,
      "learning_rate": 2.5684468480648975e-05,
      "loss": 1.095,
      "step": 12010
    },
    {
      "epoch": 1.9894074809665674,
      "grad_norm": 10.799443244934082,
      "learning_rate": 2.566334291025858e-05,
      "loss": 0.6967,
      "step": 12020
    },
    {
      "epoch": 1.991062562065541,
      "grad_norm": 4.058383464813232,
      "learning_rate": 2.564221733986818e-05,
      "loss": 0.6169,
      "step": 12030
    },
    {
      "epoch": 1.992717643164515,
      "grad_norm": 5.2603373527526855,
      "learning_rate": 2.5621091769477777e-05,
      "loss": 0.4979,
      "step": 12040
    },
    {
      "epoch": 1.994372724263489,
      "grad_norm": 4.266921520233154,
      "learning_rate": 2.559996619908738e-05,
      "loss": 0.5869,
      "step": 12050
    },
    {
      "epoch": 1.9960278053624627,
      "grad_norm": 7.452713966369629,
      "learning_rate": 2.5578840628696977e-05,
      "loss": 0.5867,
      "step": 12060
    },
    {
      "epoch": 1.9976828864614367,
      "grad_norm": 9.311662673950195,
      "learning_rate": 2.5557715058306575e-05,
      "loss": 0.7754,
      "step": 12070
    },
    {
      "epoch": 1.9993379675604106,
      "grad_norm": 6.503630638122559,
      "learning_rate": 2.5536589487916174e-05,
      "loss": 0.4524,
      "step": 12080
    },
    {
      "epoch": 2.0009930486593843,
      "grad_norm": 5.143138408660889,
      "learning_rate": 2.5515463917525772e-05,
      "loss": 0.9485,
      "step": 12090
    },
    {
      "epoch": 2.002648129758358,
      "grad_norm": 12.57167911529541,
      "learning_rate": 2.549433834713537e-05,
      "loss": 0.6995,
      "step": 12100
    },
    {
      "epoch": 2.004303210857332,
      "grad_norm": 6.110803604125977,
      "learning_rate": 2.5473212776744976e-05,
      "loss": 0.5488,
      "step": 12110
    },
    {
      "epoch": 2.005958291956306,
      "grad_norm": 2.2789018154144287,
      "learning_rate": 2.5452087206354574e-05,
      "loss": 0.4726,
      "step": 12120
    },
    {
      "epoch": 2.0076133730552796,
      "grad_norm": 5.685675144195557,
      "learning_rate": 2.5430961635964172e-05,
      "loss": 0.8198,
      "step": 12130
    },
    {
      "epoch": 2.0092684541542534,
      "grad_norm": 13.751742362976074,
      "learning_rate": 2.540983606557377e-05,
      "loss": 0.5648,
      "step": 12140
    },
    {
      "epoch": 2.0109235352532275,
      "grad_norm": 4.847718238830566,
      "learning_rate": 2.5388710495183373e-05,
      "loss": 0.55,
      "step": 12150
    },
    {
      "epoch": 2.0125786163522013,
      "grad_norm": 3.4423885345458984,
      "learning_rate": 2.536758492479297e-05,
      "loss": 0.5455,
      "step": 12160
    },
    {
      "epoch": 2.014233697451175,
      "grad_norm": 14.159363746643066,
      "learning_rate": 2.534645935440257e-05,
      "loss": 1.0336,
      "step": 12170
    },
    {
      "epoch": 2.015888778550149,
      "grad_norm": 11.10091781616211,
      "learning_rate": 2.5325333784012168e-05,
      "loss": 0.7534,
      "step": 12180
    },
    {
      "epoch": 2.017543859649123,
      "grad_norm": 1.812164545059204,
      "learning_rate": 2.5304208213621773e-05,
      "loss": 0.7432,
      "step": 12190
    },
    {
      "epoch": 2.0191989407480966,
      "grad_norm": 9.428847312927246,
      "learning_rate": 2.528308264323137e-05,
      "loss": 0.9476,
      "step": 12200
    },
    {
      "epoch": 2.0208540218470703,
      "grad_norm": 5.482785224914551,
      "learning_rate": 2.526195707284097e-05,
      "loss": 0.6576,
      "step": 12210
    },
    {
      "epoch": 2.0225091029460445,
      "grad_norm": 5.117173194885254,
      "learning_rate": 2.5240831502450568e-05,
      "loss": 0.6559,
      "step": 12220
    },
    {
      "epoch": 2.024164184045018,
      "grad_norm": 5.9809489250183105,
      "learning_rate": 2.5219705932060166e-05,
      "loss": 0.5725,
      "step": 12230
    },
    {
      "epoch": 2.025819265143992,
      "grad_norm": 6.737792015075684,
      "learning_rate": 2.5198580361669765e-05,
      "loss": 0.9227,
      "step": 12240
    },
    {
      "epoch": 2.027474346242966,
      "grad_norm": 12.14630126953125,
      "learning_rate": 2.5177454791279363e-05,
      "loss": 0.7601,
      "step": 12250
    },
    {
      "epoch": 2.02912942734194,
      "grad_norm": 3.288093090057373,
      "learning_rate": 2.5156329220888965e-05,
      "loss": 0.4459,
      "step": 12260
    },
    {
      "epoch": 2.0307845084409135,
      "grad_norm": 9.579849243164062,
      "learning_rate": 2.5135203650498563e-05,
      "loss": 0.7549,
      "step": 12270
    },
    {
      "epoch": 2.0324395895398872,
      "grad_norm": 6.380425453186035,
      "learning_rate": 2.5114078080108165e-05,
      "loss": 0.6142,
      "step": 12280
    },
    {
      "epoch": 2.0340946706388614,
      "grad_norm": 6.776975631713867,
      "learning_rate": 2.5092952509717767e-05,
      "loss": 0.4758,
      "step": 12290
    },
    {
      "epoch": 2.035749751737835,
      "grad_norm": 2.6169750690460205,
      "learning_rate": 2.5071826939327365e-05,
      "loss": 0.4746,
      "step": 12300
    },
    {
      "epoch": 2.037404832836809,
      "grad_norm": 11.645594596862793,
      "learning_rate": 2.5050701368936964e-05,
      "loss": 0.7003,
      "step": 12310
    },
    {
      "epoch": 2.039059913935783,
      "grad_norm": 1.6118006706237793,
      "learning_rate": 2.5029575798546562e-05,
      "loss": 0.6515,
      "step": 12320
    },
    {
      "epoch": 2.0407149950347567,
      "grad_norm": 8.22458553314209,
      "learning_rate": 2.500845022815616e-05,
      "loss": 0.5659,
      "step": 12330
    },
    {
      "epoch": 2.0423700761337304,
      "grad_norm": 6.070284843444824,
      "learning_rate": 2.4987324657765762e-05,
      "loss": 0.5566,
      "step": 12340
    },
    {
      "epoch": 2.0440251572327046,
      "grad_norm": 7.893292427062988,
      "learning_rate": 2.496619908737536e-05,
      "loss": 0.8031,
      "step": 12350
    },
    {
      "epoch": 2.0456802383316783,
      "grad_norm": 9.852980613708496,
      "learning_rate": 2.494507351698496e-05,
      "loss": 0.4343,
      "step": 12360
    },
    {
      "epoch": 2.047335319430652,
      "grad_norm": 13.492615699768066,
      "learning_rate": 2.4923947946594557e-05,
      "loss": 0.6567,
      "step": 12370
    },
    {
      "epoch": 2.0489904005296258,
      "grad_norm": 19.66620635986328,
      "learning_rate": 2.4902822376204156e-05,
      "loss": 0.8166,
      "step": 12380
    },
    {
      "epoch": 2.0506454816286,
      "grad_norm": 8.559783935546875,
      "learning_rate": 2.4881696805813758e-05,
      "loss": 0.5352,
      "step": 12390
    },
    {
      "epoch": 2.0523005627275737,
      "grad_norm": 1.5116767883300781,
      "learning_rate": 2.486057123542336e-05,
      "loss": 0.5582,
      "step": 12400
    },
    {
      "epoch": 2.0539556438265474,
      "grad_norm": 9.787927627563477,
      "learning_rate": 2.4839445665032958e-05,
      "loss": 0.6791,
      "step": 12410
    },
    {
      "epoch": 2.0556107249255215,
      "grad_norm": 5.483076095581055,
      "learning_rate": 2.4818320094642556e-05,
      "loss": 0.5165,
      "step": 12420
    },
    {
      "epoch": 2.0572658060244953,
      "grad_norm": 9.593470573425293,
      "learning_rate": 2.4797194524252158e-05,
      "loss": 0.5415,
      "step": 12430
    },
    {
      "epoch": 2.058920887123469,
      "grad_norm": 10.82944107055664,
      "learning_rate": 2.4776068953861756e-05,
      "loss": 0.6853,
      "step": 12440
    },
    {
      "epoch": 2.0605759682224427,
      "grad_norm": 8.793251037597656,
      "learning_rate": 2.4754943383471355e-05,
      "loss": 0.7958,
      "step": 12450
    },
    {
      "epoch": 2.062231049321417,
      "grad_norm": 15.514778137207031,
      "learning_rate": 2.4733817813080953e-05,
      "loss": 0.8556,
      "step": 12460
    },
    {
      "epoch": 2.0638861304203906,
      "grad_norm": 9.100138664245605,
      "learning_rate": 2.4712692242690555e-05,
      "loss": 0.7183,
      "step": 12470
    },
    {
      "epoch": 2.0655412115193643,
      "grad_norm": 1.601211428642273,
      "learning_rate": 2.4691566672300153e-05,
      "loss": 0.4878,
      "step": 12480
    },
    {
      "epoch": 2.0671962926183385,
      "grad_norm": 11.017544746398926,
      "learning_rate": 2.467044110190975e-05,
      "loss": 0.5721,
      "step": 12490
    },
    {
      "epoch": 2.068851373717312,
      "grad_norm": 7.533577919006348,
      "learning_rate": 2.464931553151935e-05,
      "loss": 0.5369,
      "step": 12500
    },
    {
      "epoch": 2.070506454816286,
      "grad_norm": 9.263910293579102,
      "learning_rate": 2.4628189961128952e-05,
      "loss": 0.5733,
      "step": 12510
    },
    {
      "epoch": 2.0721615359152596,
      "grad_norm": 5.43499231338501,
      "learning_rate": 2.460706439073855e-05,
      "loss": 0.5601,
      "step": 12520
    },
    {
      "epoch": 2.073816617014234,
      "grad_norm": 9.444048881530762,
      "learning_rate": 2.4585938820348152e-05,
      "loss": 0.5857,
      "step": 12530
    },
    {
      "epoch": 2.0754716981132075,
      "grad_norm": 6.828461170196533,
      "learning_rate": 2.456481324995775e-05,
      "loss": 0.641,
      "step": 12540
    },
    {
      "epoch": 2.0771267792121813,
      "grad_norm": 8.050933837890625,
      "learning_rate": 2.454368767956735e-05,
      "loss": 0.6539,
      "step": 12550
    },
    {
      "epoch": 2.0787818603111554,
      "grad_norm": 11.32827091217041,
      "learning_rate": 2.452256210917695e-05,
      "loss": 0.698,
      "step": 12560
    },
    {
      "epoch": 2.080436941410129,
      "grad_norm": 4.64754581451416,
      "learning_rate": 2.450143653878655e-05,
      "loss": 0.6328,
      "step": 12570
    },
    {
      "epoch": 2.082092022509103,
      "grad_norm": 2.866670608520508,
      "learning_rate": 2.4480310968396147e-05,
      "loss": 0.6017,
      "step": 12580
    },
    {
      "epoch": 2.0837471036080766,
      "grad_norm": 8.213902473449707,
      "learning_rate": 2.4459185398005746e-05,
      "loss": 0.4507,
      "step": 12590
    },
    {
      "epoch": 2.0854021847070507,
      "grad_norm": 12.46671199798584,
      "learning_rate": 2.4438059827615347e-05,
      "loss": 0.5089,
      "step": 12600
    },
    {
      "epoch": 2.0870572658060245,
      "grad_norm": 6.603592872619629,
      "learning_rate": 2.4416934257224946e-05,
      "loss": 0.7173,
      "step": 12610
    },
    {
      "epoch": 2.088712346904998,
      "grad_norm": 7.276727199554443,
      "learning_rate": 2.4395808686834544e-05,
      "loss": 0.5643,
      "step": 12620
    },
    {
      "epoch": 2.0903674280039723,
      "grad_norm": 0.8849647641181946,
      "learning_rate": 2.4374683116444142e-05,
      "loss": 0.8599,
      "step": 12630
    },
    {
      "epoch": 2.092022509102946,
      "grad_norm": 6.37057638168335,
      "learning_rate": 2.4353557546053744e-05,
      "loss": 0.7528,
      "step": 12640
    },
    {
      "epoch": 2.09367759020192,
      "grad_norm": 5.3119120597839355,
      "learning_rate": 2.4332431975663346e-05,
      "loss": 0.738,
      "step": 12650
    },
    {
      "epoch": 2.095332671300894,
      "grad_norm": 8.213275909423828,
      "learning_rate": 2.4311306405272944e-05,
      "loss": 1.0757,
      "step": 12660
    },
    {
      "epoch": 2.0969877523998677,
      "grad_norm": 4.72123384475708,
      "learning_rate": 2.4290180834882543e-05,
      "loss": 0.4509,
      "step": 12670
    },
    {
      "epoch": 2.0986428334988414,
      "grad_norm": 8.459196090698242,
      "learning_rate": 2.4269055264492145e-05,
      "loss": 0.8699,
      "step": 12680
    },
    {
      "epoch": 2.100297914597815,
      "grad_norm": 6.703444480895996,
      "learning_rate": 2.4247929694101743e-05,
      "loss": 0.4957,
      "step": 12690
    },
    {
      "epoch": 2.1019529956967893,
      "grad_norm": 6.101469993591309,
      "learning_rate": 2.422680412371134e-05,
      "loss": 0.6574,
      "step": 12700
    },
    {
      "epoch": 2.103608076795763,
      "grad_norm": 5.333622932434082,
      "learning_rate": 2.420567855332094e-05,
      "loss": 0.5378,
      "step": 12710
    },
    {
      "epoch": 2.1052631578947367,
      "grad_norm": 19.068145751953125,
      "learning_rate": 2.4184552982930538e-05,
      "loss": 0.48,
      "step": 12720
    },
    {
      "epoch": 2.106918238993711,
      "grad_norm": 11.487529754638672,
      "learning_rate": 2.416342741254014e-05,
      "loss": 0.4406,
      "step": 12730
    },
    {
      "epoch": 2.1085733200926846,
      "grad_norm": 7.915150165557861,
      "learning_rate": 2.4142301842149738e-05,
      "loss": 0.6731,
      "step": 12740
    },
    {
      "epoch": 2.1102284011916583,
      "grad_norm": 17.524168014526367,
      "learning_rate": 2.4121176271759337e-05,
      "loss": 0.7173,
      "step": 12750
    },
    {
      "epoch": 2.111883482290632,
      "grad_norm": 4.899764537811279,
      "learning_rate": 2.410005070136894e-05,
      "loss": 0.5679,
      "step": 12760
    },
    {
      "epoch": 2.113538563389606,
      "grad_norm": 3.07222056388855,
      "learning_rate": 2.407892513097854e-05,
      "loss": 0.8013,
      "step": 12770
    },
    {
      "epoch": 2.11519364448858,
      "grad_norm": 9.451594352722168,
      "learning_rate": 2.405779956058814e-05,
      "loss": 0.8526,
      "step": 12780
    },
    {
      "epoch": 2.1168487255875537,
      "grad_norm": 9.054977416992188,
      "learning_rate": 2.4036673990197737e-05,
      "loss": 0.7764,
      "step": 12790
    },
    {
      "epoch": 2.118503806686528,
      "grad_norm": 10.639378547668457,
      "learning_rate": 2.4015548419807335e-05,
      "loss": 0.5784,
      "step": 12800
    },
    {
      "epoch": 2.1201588877855015,
      "grad_norm": 16.33058738708496,
      "learning_rate": 2.3994422849416937e-05,
      "loss": 0.8155,
      "step": 12810
    },
    {
      "epoch": 2.1218139688844753,
      "grad_norm": 7.6870598793029785,
      "learning_rate": 2.3973297279026535e-05,
      "loss": 0.7089,
      "step": 12820
    },
    {
      "epoch": 2.123469049983449,
      "grad_norm": 5.305695056915283,
      "learning_rate": 2.3952171708636134e-05,
      "loss": 0.6047,
      "step": 12830
    },
    {
      "epoch": 2.125124131082423,
      "grad_norm": 2.975097179412842,
      "learning_rate": 2.3931046138245732e-05,
      "loss": 0.4882,
      "step": 12840
    },
    {
      "epoch": 2.126779212181397,
      "grad_norm": 7.396853446960449,
      "learning_rate": 2.3909920567855334e-05,
      "loss": 0.5802,
      "step": 12850
    },
    {
      "epoch": 2.1284342932803706,
      "grad_norm": 7.184856414794922,
      "learning_rate": 2.3888794997464932e-05,
      "loss": 0.5902,
      "step": 12860
    },
    {
      "epoch": 2.1300893743793448,
      "grad_norm": 13.981529235839844,
      "learning_rate": 2.386766942707453e-05,
      "loss": 0.8044,
      "step": 12870
    },
    {
      "epoch": 2.1317444554783185,
      "grad_norm": 7.011593818664551,
      "learning_rate": 2.3846543856684133e-05,
      "loss": 0.3445,
      "step": 12880
    },
    {
      "epoch": 2.133399536577292,
      "grad_norm": 9.505497932434082,
      "learning_rate": 2.382541828629373e-05,
      "loss": 0.7144,
      "step": 12890
    },
    {
      "epoch": 2.135054617676266,
      "grad_norm": 2.257108449935913,
      "learning_rate": 2.3804292715903333e-05,
      "loss": 0.4945,
      "step": 12900
    },
    {
      "epoch": 2.13670969877524,
      "grad_norm": 3.6385114192962646,
      "learning_rate": 2.378316714551293e-05,
      "loss": 0.6557,
      "step": 12910
    },
    {
      "epoch": 2.138364779874214,
      "grad_norm": 7.175348281860352,
      "learning_rate": 2.376204157512253e-05,
      "loss": 0.6809,
      "step": 12920
    },
    {
      "epoch": 2.1400198609731875,
      "grad_norm": 7.542967319488525,
      "learning_rate": 2.3740916004732128e-05,
      "loss": 0.6302,
      "step": 12930
    },
    {
      "epoch": 2.1416749420721617,
      "grad_norm": 8.54224681854248,
      "learning_rate": 2.371979043434173e-05,
      "loss": 0.904,
      "step": 12940
    },
    {
      "epoch": 2.1433300231711354,
      "grad_norm": 1.4422818422317505,
      "learning_rate": 2.3698664863951328e-05,
      "loss": 0.655,
      "step": 12950
    },
    {
      "epoch": 2.144985104270109,
      "grad_norm": 5.229928493499756,
      "learning_rate": 2.3677539293560926e-05,
      "loss": 0.5897,
      "step": 12960
    },
    {
      "epoch": 2.1466401853690833,
      "grad_norm": 9.104884147644043,
      "learning_rate": 2.3656413723170525e-05,
      "loss": 0.5498,
      "step": 12970
    },
    {
      "epoch": 2.148295266468057,
      "grad_norm": 10.995546340942383,
      "learning_rate": 2.3635288152780127e-05,
      "loss": 0.845,
      "step": 12980
    },
    {
      "epoch": 2.1499503475670307,
      "grad_norm": 6.361529350280762,
      "learning_rate": 2.3614162582389725e-05,
      "loss": 0.6733,
      "step": 12990
    },
    {
      "epoch": 2.1516054286660045,
      "grad_norm": 0.38223451375961304,
      "learning_rate": 2.3593037011999323e-05,
      "loss": 0.6755,
      "step": 13000
    },
    {
      "epoch": 2.1532605097649786,
      "grad_norm": 9.588419914245605,
      "learning_rate": 2.3571911441608925e-05,
      "loss": 0.677,
      "step": 13010
    },
    {
      "epoch": 2.1549155908639523,
      "grad_norm": 4.898079872131348,
      "learning_rate": 2.3550785871218527e-05,
      "loss": 0.5209,
      "step": 13020
    },
    {
      "epoch": 2.156570671962926,
      "grad_norm": 4.295017719268799,
      "learning_rate": 2.3529660300828125e-05,
      "loss": 0.3492,
      "step": 13030
    },
    {
      "epoch": 2.1582257530619002,
      "grad_norm": 11.922882080078125,
      "learning_rate": 2.3508534730437724e-05,
      "loss": 0.6769,
      "step": 13040
    },
    {
      "epoch": 2.159880834160874,
      "grad_norm": 7.1274614334106445,
      "learning_rate": 2.3487409160047322e-05,
      "loss": 0.6076,
      "step": 13050
    },
    {
      "epoch": 2.1615359152598477,
      "grad_norm": 9.482973098754883,
      "learning_rate": 2.346628358965692e-05,
      "loss": 0.6062,
      "step": 13060
    },
    {
      "epoch": 2.1631909963588214,
      "grad_norm": 2.1896820068359375,
      "learning_rate": 2.3445158019266522e-05,
      "loss": 0.5286,
      "step": 13070
    },
    {
      "epoch": 2.1648460774577956,
      "grad_norm": 6.167243480682373,
      "learning_rate": 2.342403244887612e-05,
      "loss": 0.9114,
      "step": 13080
    },
    {
      "epoch": 2.1665011585567693,
      "grad_norm": 4.880818843841553,
      "learning_rate": 2.340290687848572e-05,
      "loss": 0.3147,
      "step": 13090
    },
    {
      "epoch": 2.168156239655743,
      "grad_norm": 9.225013732910156,
      "learning_rate": 2.3381781308095317e-05,
      "loss": 0.7916,
      "step": 13100
    },
    {
      "epoch": 2.169811320754717,
      "grad_norm": 7.458749771118164,
      "learning_rate": 2.336065573770492e-05,
      "loss": 0.6615,
      "step": 13110
    },
    {
      "epoch": 2.171466401853691,
      "grad_norm": 9.527663230895996,
      "learning_rate": 2.3339530167314517e-05,
      "loss": 0.6756,
      "step": 13120
    },
    {
      "epoch": 2.1731214829526646,
      "grad_norm": 11.071285247802734,
      "learning_rate": 2.331840459692412e-05,
      "loss": 0.8219,
      "step": 13130
    },
    {
      "epoch": 2.1747765640516383,
      "grad_norm": 6.970534324645996,
      "learning_rate": 2.3297279026533718e-05,
      "loss": 0.5271,
      "step": 13140
    },
    {
      "epoch": 2.1764316451506125,
      "grad_norm": 9.485676765441895,
      "learning_rate": 2.327615345614332e-05,
      "loss": 0.7407,
      "step": 13150
    },
    {
      "epoch": 2.178086726249586,
      "grad_norm": 3.9959166049957275,
      "learning_rate": 2.3255027885752918e-05,
      "loss": 0.5515,
      "step": 13160
    },
    {
      "epoch": 2.17974180734856,
      "grad_norm": 3.066304922103882,
      "learning_rate": 2.3233902315362516e-05,
      "loss": 0.3869,
      "step": 13170
    },
    {
      "epoch": 2.181396888447534,
      "grad_norm": 5.823259353637695,
      "learning_rate": 2.3212776744972115e-05,
      "loss": 0.3909,
      "step": 13180
    },
    {
      "epoch": 2.183051969546508,
      "grad_norm": 8.340502738952637,
      "learning_rate": 2.3191651174581716e-05,
      "loss": 0.4774,
      "step": 13190
    },
    {
      "epoch": 2.1847070506454815,
      "grad_norm": 7.913177967071533,
      "learning_rate": 2.3170525604191315e-05,
      "loss": 0.4581,
      "step": 13200
    },
    {
      "epoch": 2.1863621317444553,
      "grad_norm": 4.392423152923584,
      "learning_rate": 2.3149400033800913e-05,
      "loss": 0.5632,
      "step": 13210
    },
    {
      "epoch": 2.1880172128434294,
      "grad_norm": 5.3960771560668945,
      "learning_rate": 2.312827446341051e-05,
      "loss": 0.4453,
      "step": 13220
    },
    {
      "epoch": 2.189672293942403,
      "grad_norm": 4.433848857879639,
      "learning_rate": 2.310714889302011e-05,
      "loss": 0.7487,
      "step": 13230
    },
    {
      "epoch": 2.191327375041377,
      "grad_norm": 1.108404278755188,
      "learning_rate": 2.308602332262971e-05,
      "loss": 0.5732,
      "step": 13240
    },
    {
      "epoch": 2.192982456140351,
      "grad_norm": 13.03085994720459,
      "learning_rate": 2.306489775223931e-05,
      "loss": 0.4058,
      "step": 13250
    },
    {
      "epoch": 2.1946375372393248,
      "grad_norm": 7.7609663009643555,
      "learning_rate": 2.3043772181848912e-05,
      "loss": 0.4648,
      "step": 13260
    },
    {
      "epoch": 2.1962926183382985,
      "grad_norm": 6.320751190185547,
      "learning_rate": 2.302264661145851e-05,
      "loss": 0.8297,
      "step": 13270
    },
    {
      "epoch": 2.1979476994372726,
      "grad_norm": 7.078385353088379,
      "learning_rate": 2.3001521041068112e-05,
      "loss": 0.5302,
      "step": 13280
    },
    {
      "epoch": 2.1996027805362464,
      "grad_norm": 1.2695754766464233,
      "learning_rate": 2.298039547067771e-05,
      "loss": 0.7415,
      "step": 13290
    },
    {
      "epoch": 2.20125786163522,
      "grad_norm": 7.02971076965332,
      "learning_rate": 2.295926990028731e-05,
      "loss": 0.7157,
      "step": 13300
    },
    {
      "epoch": 2.202912942734194,
      "grad_norm": 9.628299713134766,
      "learning_rate": 2.2938144329896907e-05,
      "loss": 0.5877,
      "step": 13310
    },
    {
      "epoch": 2.204568023833168,
      "grad_norm": 10.41122817993164,
      "learning_rate": 2.291701875950651e-05,
      "loss": 0.788,
      "step": 13320
    },
    {
      "epoch": 2.2062231049321417,
      "grad_norm": 9.158013343811035,
      "learning_rate": 2.2895893189116107e-05,
      "loss": 0.5389,
      "step": 13330
    },
    {
      "epoch": 2.2078781860311154,
      "grad_norm": 13.282702445983887,
      "learning_rate": 2.2874767618725706e-05,
      "loss": 0.6921,
      "step": 13340
    },
    {
      "epoch": 2.2095332671300896,
      "grad_norm": 2.025907039642334,
      "learning_rate": 2.2853642048335304e-05,
      "loss": 0.438,
      "step": 13350
    },
    {
      "epoch": 2.2111883482290633,
      "grad_norm": 12.74975872039795,
      "learning_rate": 2.2832516477944906e-05,
      "loss": 0.6234,
      "step": 13360
    },
    {
      "epoch": 2.212843429328037,
      "grad_norm": 6.582385063171387,
      "learning_rate": 2.2811390907554504e-05,
      "loss": 0.5551,
      "step": 13370
    },
    {
      "epoch": 2.2144985104270107,
      "grad_norm": 8.639120101928711,
      "learning_rate": 2.2790265337164106e-05,
      "loss": 0.8059,
      "step": 13380
    },
    {
      "epoch": 2.216153591525985,
      "grad_norm": 4.3446807861328125,
      "learning_rate": 2.2769139766773704e-05,
      "loss": 0.6399,
      "step": 13390
    },
    {
      "epoch": 2.2178086726249586,
      "grad_norm": 3.619426965713501,
      "learning_rate": 2.2748014196383303e-05,
      "loss": 0.5011,
      "step": 13400
    },
    {
      "epoch": 2.2194637537239323,
      "grad_norm": 6.72926664352417,
      "learning_rate": 2.2726888625992904e-05,
      "loss": 0.4498,
      "step": 13410
    },
    {
      "epoch": 2.2211188348229065,
      "grad_norm": 8.792291641235352,
      "learning_rate": 2.2705763055602503e-05,
      "loss": 0.6661,
      "step": 13420
    },
    {
      "epoch": 2.2227739159218802,
      "grad_norm": 1.8787288665771484,
      "learning_rate": 2.26846374852121e-05,
      "loss": 0.5008,
      "step": 13430
    },
    {
      "epoch": 2.224428997020854,
      "grad_norm": 8.74829387664795,
      "learning_rate": 2.26635119148217e-05,
      "loss": 0.5652,
      "step": 13440
    },
    {
      "epoch": 2.2260840781198277,
      "grad_norm": 11.251057624816895,
      "learning_rate": 2.26423863444313e-05,
      "loss": 0.676,
      "step": 13450
    },
    {
      "epoch": 2.227739159218802,
      "grad_norm": 7.536021709442139,
      "learning_rate": 2.26212607740409e-05,
      "loss": 0.6098,
      "step": 13460
    },
    {
      "epoch": 2.2293942403177756,
      "grad_norm": 9.606626510620117,
      "learning_rate": 2.2600135203650498e-05,
      "loss": 0.7046,
      "step": 13470
    },
    {
      "epoch": 2.2310493214167493,
      "grad_norm": 10.4229097366333,
      "learning_rate": 2.2579009633260097e-05,
      "loss": 0.7989,
      "step": 13480
    },
    {
      "epoch": 2.2327044025157234,
      "grad_norm": 4.638187885284424,
      "learning_rate": 2.25578840628697e-05,
      "loss": 0.5837,
      "step": 13490
    },
    {
      "epoch": 2.234359483614697,
      "grad_norm": 10.353434562683105,
      "learning_rate": 2.25367584924793e-05,
      "loss": 0.4938,
      "step": 13500
    },
    {
      "epoch": 2.236014564713671,
      "grad_norm": 0.3662639856338501,
      "learning_rate": 2.25156329220889e-05,
      "loss": 0.633,
      "step": 13510
    },
    {
      "epoch": 2.2376696458126446,
      "grad_norm": 4.5210795402526855,
      "learning_rate": 2.2494507351698497e-05,
      "loss": 0.4336,
      "step": 13520
    },
    {
      "epoch": 2.2393247269116188,
      "grad_norm": 7.731559753417969,
      "learning_rate": 2.24733817813081e-05,
      "loss": 0.5727,
      "step": 13530
    },
    {
      "epoch": 2.2409798080105925,
      "grad_norm": 16.090728759765625,
      "learning_rate": 2.2452256210917697e-05,
      "loss": 0.6484,
      "step": 13540
    },
    {
      "epoch": 2.242634889109566,
      "grad_norm": 8.082454681396484,
      "learning_rate": 2.2431130640527295e-05,
      "loss": 0.9649,
      "step": 13550
    },
    {
      "epoch": 2.2442899702085404,
      "grad_norm": 12.144664764404297,
      "learning_rate": 2.2410005070136894e-05,
      "loss": 0.4418,
      "step": 13560
    },
    {
      "epoch": 2.245945051307514,
      "grad_norm": 9.5991849899292,
      "learning_rate": 2.2388879499746492e-05,
      "loss": 0.5569,
      "step": 13570
    },
    {
      "epoch": 2.247600132406488,
      "grad_norm": 3.5887773036956787,
      "learning_rate": 2.2367753929356094e-05,
      "loss": 0.6636,
      "step": 13580
    },
    {
      "epoch": 2.249255213505462,
      "grad_norm": 5.042080879211426,
      "learning_rate": 2.2346628358965692e-05,
      "loss": 0.6122,
      "step": 13590
    },
    {
      "epoch": 2.2509102946044357,
      "grad_norm": 7.918135166168213,
      "learning_rate": 2.232550278857529e-05,
      "loss": 0.6689,
      "step": 13600
    },
    {
      "epoch": 2.2525653757034094,
      "grad_norm": 8.002357482910156,
      "learning_rate": 2.2304377218184892e-05,
      "loss": 0.4207,
      "step": 13610
    },
    {
      "epoch": 2.254220456802383,
      "grad_norm": 6.4409308433532715,
      "learning_rate": 2.228325164779449e-05,
      "loss": 0.3744,
      "step": 13620
    },
    {
      "epoch": 2.2558755379013573,
      "grad_norm": 6.8521623611450195,
      "learning_rate": 2.2262126077404093e-05,
      "loss": 0.609,
      "step": 13630
    },
    {
      "epoch": 2.257530619000331,
      "grad_norm": 13.002952575683594,
      "learning_rate": 2.224100050701369e-05,
      "loss": 0.5582,
      "step": 13640
    },
    {
      "epoch": 2.2591857000993047,
      "grad_norm": 1.8713867664337158,
      "learning_rate": 2.221987493662329e-05,
      "loss": 0.5789,
      "step": 13650
    },
    {
      "epoch": 2.2608407811982785,
      "grad_norm": 14.237205505371094,
      "learning_rate": 2.219874936623289e-05,
      "loss": 0.6738,
      "step": 13660
    },
    {
      "epoch": 2.2624958622972526,
      "grad_norm": 15.801751136779785,
      "learning_rate": 2.217762379584249e-05,
      "loss": 0.7351,
      "step": 13670
    },
    {
      "epoch": 2.2641509433962264,
      "grad_norm": 8.614109992980957,
      "learning_rate": 2.2156498225452088e-05,
      "loss": 0.5271,
      "step": 13680
    },
    {
      "epoch": 2.2658060244952,
      "grad_norm": 7.915506362915039,
      "learning_rate": 2.2135372655061686e-05,
      "loss": 0.4634,
      "step": 13690
    },
    {
      "epoch": 2.2674611055941742,
      "grad_norm": 8.758493423461914,
      "learning_rate": 2.2114247084671288e-05,
      "loss": 0.781,
      "step": 13700
    },
    {
      "epoch": 2.269116186693148,
      "grad_norm": 5.591545104980469,
      "learning_rate": 2.2093121514280886e-05,
      "loss": 0.5078,
      "step": 13710
    },
    {
      "epoch": 2.2707712677921217,
      "grad_norm": 4.436402320861816,
      "learning_rate": 2.2071995943890485e-05,
      "loss": 0.7415,
      "step": 13720
    },
    {
      "epoch": 2.272426348891096,
      "grad_norm": 8.306211471557617,
      "learning_rate": 2.2050870373500083e-05,
      "loss": 0.8071,
      "step": 13730
    },
    {
      "epoch": 2.2740814299900696,
      "grad_norm": 10.801403999328613,
      "learning_rate": 2.2029744803109685e-05,
      "loss": 0.5877,
      "step": 13740
    },
    {
      "epoch": 2.2757365110890433,
      "grad_norm": 10.663542747497559,
      "learning_rate": 2.2008619232719287e-05,
      "loss": 0.5529,
      "step": 13750
    },
    {
      "epoch": 2.2773915921880175,
      "grad_norm": 9.147966384887695,
      "learning_rate": 2.1987493662328885e-05,
      "loss": 0.7592,
      "step": 13760
    },
    {
      "epoch": 2.279046673286991,
      "grad_norm": 10.519007682800293,
      "learning_rate": 2.1966368091938484e-05,
      "loss": 0.6765,
      "step": 13770
    },
    {
      "epoch": 2.280701754385965,
      "grad_norm": 9.309673309326172,
      "learning_rate": 2.1945242521548082e-05,
      "loss": 0.7776,
      "step": 13780
    },
    {
      "epoch": 2.2823568354849386,
      "grad_norm": 5.826881408691406,
      "learning_rate": 2.1924116951157684e-05,
      "loss": 0.6498,
      "step": 13790
    },
    {
      "epoch": 2.284011916583913,
      "grad_norm": 4.7942118644714355,
      "learning_rate": 2.1902991380767282e-05,
      "loss": 0.5363,
      "step": 13800
    },
    {
      "epoch": 2.2856669976828865,
      "grad_norm": 15.153264045715332,
      "learning_rate": 2.188186581037688e-05,
      "loss": 0.8684,
      "step": 13810
    },
    {
      "epoch": 2.2873220787818602,
      "grad_norm": 8.63984489440918,
      "learning_rate": 2.186074023998648e-05,
      "loss": 0.6676,
      "step": 13820
    },
    {
      "epoch": 2.288977159880834,
      "grad_norm": 11.197067260742188,
      "learning_rate": 2.183961466959608e-05,
      "loss": 0.5107,
      "step": 13830
    },
    {
      "epoch": 2.290632240979808,
      "grad_norm": 7.896108150482178,
      "learning_rate": 2.181848909920568e-05,
      "loss": 0.4993,
      "step": 13840
    },
    {
      "epoch": 2.292287322078782,
      "grad_norm": 13.843343734741211,
      "learning_rate": 2.1797363528815277e-05,
      "loss": 0.7243,
      "step": 13850
    },
    {
      "epoch": 2.2939424031777556,
      "grad_norm": 8.772977828979492,
      "learning_rate": 2.177623795842488e-05,
      "loss": 0.7388,
      "step": 13860
    },
    {
      "epoch": 2.2955974842767297,
      "grad_norm": 7.105792999267578,
      "learning_rate": 2.175511238803448e-05,
      "loss": 0.4938,
      "step": 13870
    },
    {
      "epoch": 2.2972525653757034,
      "grad_norm": 4.582803726196289,
      "learning_rate": 2.173398681764408e-05,
      "loss": 0.6496,
      "step": 13880
    },
    {
      "epoch": 2.298907646474677,
      "grad_norm": 3.72663950920105,
      "learning_rate": 2.1712861247253678e-05,
      "loss": 0.821,
      "step": 13890
    },
    {
      "epoch": 2.3005627275736513,
      "grad_norm": 4.0633978843688965,
      "learning_rate": 2.1691735676863276e-05,
      "loss": 0.6937,
      "step": 13900
    },
    {
      "epoch": 2.302217808672625,
      "grad_norm": 1.6835578680038452,
      "learning_rate": 2.1670610106472874e-05,
      "loss": 0.968,
      "step": 13910
    },
    {
      "epoch": 2.3038728897715988,
      "grad_norm": 8.172380447387695,
      "learning_rate": 2.1649484536082476e-05,
      "loss": 0.5874,
      "step": 13920
    },
    {
      "epoch": 2.3055279708705725,
      "grad_norm": 5.603165626525879,
      "learning_rate": 2.1628358965692075e-05,
      "loss": 0.8133,
      "step": 13930
    },
    {
      "epoch": 2.3071830519695466,
      "grad_norm": 2.296903610229492,
      "learning_rate": 2.1607233395301673e-05,
      "loss": 0.5117,
      "step": 13940
    },
    {
      "epoch": 2.3088381330685204,
      "grad_norm": 3.433790922164917,
      "learning_rate": 2.158610782491127e-05,
      "loss": 0.7219,
      "step": 13950
    },
    {
      "epoch": 2.310493214167494,
      "grad_norm": 6.168792247772217,
      "learning_rate": 2.1564982254520873e-05,
      "loss": 0.5958,
      "step": 13960
    },
    {
      "epoch": 2.312148295266468,
      "grad_norm": 14.452911376953125,
      "learning_rate": 2.154385668413047e-05,
      "loss": 0.6363,
      "step": 13970
    },
    {
      "epoch": 2.313803376365442,
      "grad_norm": 6.123717784881592,
      "learning_rate": 2.1522731113740073e-05,
      "loss": 0.5493,
      "step": 13980
    },
    {
      "epoch": 2.3154584574644157,
      "grad_norm": 5.903880596160889,
      "learning_rate": 2.1501605543349672e-05,
      "loss": 0.7145,
      "step": 13990
    },
    {
      "epoch": 2.3171135385633894,
      "grad_norm": 5.8749680519104,
      "learning_rate": 2.1480479972959273e-05,
      "loss": 0.5426,
      "step": 14000
    },
    {
      "epoch": 2.3187686196623636,
      "grad_norm": 7.532459735870361,
      "learning_rate": 2.1459354402568872e-05,
      "loss": 0.6161,
      "step": 14010
    },
    {
      "epoch": 2.3204237007613373,
      "grad_norm": 5.321092128753662,
      "learning_rate": 2.143822883217847e-05,
      "loss": 0.5104,
      "step": 14020
    },
    {
      "epoch": 2.322078781860311,
      "grad_norm": 11.370227813720703,
      "learning_rate": 2.141710326178807e-05,
      "loss": 0.8395,
      "step": 14030
    },
    {
      "epoch": 2.323733862959285,
      "grad_norm": 0.7402486205101013,
      "learning_rate": 2.139597769139767e-05,
      "loss": 0.4725,
      "step": 14040
    },
    {
      "epoch": 2.325388944058259,
      "grad_norm": 4.379803657531738,
      "learning_rate": 2.137485212100727e-05,
      "loss": 0.8716,
      "step": 14050
    },
    {
      "epoch": 2.3270440251572326,
      "grad_norm": 1.7059239149093628,
      "learning_rate": 2.1353726550616867e-05,
      "loss": 0.3366,
      "step": 14060
    },
    {
      "epoch": 2.328699106256207,
      "grad_norm": 7.963507175445557,
      "learning_rate": 2.1332600980226466e-05,
      "loss": 0.8131,
      "step": 14070
    },
    {
      "epoch": 2.3303541873551805,
      "grad_norm": 10.179487228393555,
      "learning_rate": 2.1311475409836064e-05,
      "loss": 0.5779,
      "step": 14080
    },
    {
      "epoch": 2.3320092684541542,
      "grad_norm": 3.0391411781311035,
      "learning_rate": 2.1290349839445666e-05,
      "loss": 0.7338,
      "step": 14090
    },
    {
      "epoch": 2.333664349553128,
      "grad_norm": 6.557188034057617,
      "learning_rate": 2.1269224269055264e-05,
      "loss": 0.5208,
      "step": 14100
    },
    {
      "epoch": 2.335319430652102,
      "grad_norm": 3.2459499835968018,
      "learning_rate": 2.1248098698664866e-05,
      "loss": 0.9565,
      "step": 14110
    },
    {
      "epoch": 2.336974511751076,
      "grad_norm": 6.293309211730957,
      "learning_rate": 2.1226973128274464e-05,
      "loss": 0.731,
      "step": 14120
    },
    {
      "epoch": 2.3386295928500496,
      "grad_norm": 10.665974617004395,
      "learning_rate": 2.1205847557884066e-05,
      "loss": 0.7704,
      "step": 14130
    },
    {
      "epoch": 2.3402846739490233,
      "grad_norm": 5.423940658569336,
      "learning_rate": 2.1184721987493664e-05,
      "loss": 0.5436,
      "step": 14140
    },
    {
      "epoch": 2.3419397550479975,
      "grad_norm": 8.765881538391113,
      "learning_rate": 2.1163596417103263e-05,
      "loss": 0.7135,
      "step": 14150
    },
    {
      "epoch": 2.343594836146971,
      "grad_norm": 2.277557611465454,
      "learning_rate": 2.114247084671286e-05,
      "loss": 0.4462,
      "step": 14160
    },
    {
      "epoch": 2.345249917245945,
      "grad_norm": 6.194435119628906,
      "learning_rate": 2.1121345276322463e-05,
      "loss": 0.632,
      "step": 14170
    },
    {
      "epoch": 2.346904998344919,
      "grad_norm": 7.072089195251465,
      "learning_rate": 2.110021970593206e-05,
      "loss": 0.3701,
      "step": 14180
    },
    {
      "epoch": 2.348560079443893,
      "grad_norm": 12.563509941101074,
      "learning_rate": 2.107909413554166e-05,
      "loss": 0.6144,
      "step": 14190
    },
    {
      "epoch": 2.3502151605428665,
      "grad_norm": 5.652981758117676,
      "learning_rate": 2.1057968565151258e-05,
      "loss": 0.7027,
      "step": 14200
    },
    {
      "epoch": 2.3518702416418407,
      "grad_norm": 5.572032928466797,
      "learning_rate": 2.103684299476086e-05,
      "loss": 0.6688,
      "step": 14210
    },
    {
      "epoch": 2.3535253227408144,
      "grad_norm": 9.274613380432129,
      "learning_rate": 2.1015717424370458e-05,
      "loss": 0.5486,
      "step": 14220
    },
    {
      "epoch": 2.355180403839788,
      "grad_norm": 10.010333061218262,
      "learning_rate": 2.099459185398006e-05,
      "loss": 0.5205,
      "step": 14230
    },
    {
      "epoch": 2.356835484938762,
      "grad_norm": 13.318350791931152,
      "learning_rate": 2.097346628358966e-05,
      "loss": 0.6547,
      "step": 14240
    },
    {
      "epoch": 2.358490566037736,
      "grad_norm": 4.498316764831543,
      "learning_rate": 2.0952340713199257e-05,
      "loss": 0.3663,
      "step": 14250
    },
    {
      "epoch": 2.3601456471367097,
      "grad_norm": 7.748896598815918,
      "learning_rate": 2.093121514280886e-05,
      "loss": 0.5204,
      "step": 14260
    },
    {
      "epoch": 2.3618007282356834,
      "grad_norm": 7.990764141082764,
      "learning_rate": 2.0910089572418457e-05,
      "loss": 0.5611,
      "step": 14270
    },
    {
      "epoch": 2.363455809334657,
      "grad_norm": 7.809503555297852,
      "learning_rate": 2.0888964002028055e-05,
      "loss": 0.5752,
      "step": 14280
    },
    {
      "epoch": 2.3651108904336313,
      "grad_norm": 4.948361396789551,
      "learning_rate": 2.0867838431637654e-05,
      "loss": 0.7291,
      "step": 14290
    },
    {
      "epoch": 2.366765971532605,
      "grad_norm": 0.7698342800140381,
      "learning_rate": 2.0846712861247255e-05,
      "loss": 0.3881,
      "step": 14300
    },
    {
      "epoch": 2.3684210526315788,
      "grad_norm": 5.419276237487793,
      "learning_rate": 2.0825587290856854e-05,
      "loss": 0.4729,
      "step": 14310
    },
    {
      "epoch": 2.370076133730553,
      "grad_norm": 20.576087951660156,
      "learning_rate": 2.0804461720466452e-05,
      "loss": 0.7596,
      "step": 14320
    },
    {
      "epoch": 2.3717312148295266,
      "grad_norm": 5.749523162841797,
      "learning_rate": 2.078333615007605e-05,
      "loss": 0.7438,
      "step": 14330
    },
    {
      "epoch": 2.3733862959285004,
      "grad_norm": 8.38940715789795,
      "learning_rate": 2.0762210579685652e-05,
      "loss": 0.5347,
      "step": 14340
    },
    {
      "epoch": 2.3750413770274745,
      "grad_norm": 12.02057933807373,
      "learning_rate": 2.074108500929525e-05,
      "loss": 0.7013,
      "step": 14350
    },
    {
      "epoch": 2.3766964581264483,
      "grad_norm": 5.198694705963135,
      "learning_rate": 2.0719959438904853e-05,
      "loss": 0.8944,
      "step": 14360
    },
    {
      "epoch": 2.378351539225422,
      "grad_norm": 8.762221336364746,
      "learning_rate": 2.069883386851445e-05,
      "loss": 0.6271,
      "step": 14370
    },
    {
      "epoch": 2.380006620324396,
      "grad_norm": 4.581204414367676,
      "learning_rate": 2.0677708298124053e-05,
      "loss": 0.6009,
      "step": 14380
    },
    {
      "epoch": 2.38166170142337,
      "grad_norm": 1.78107488155365,
      "learning_rate": 2.065658272773365e-05,
      "loss": 0.8006,
      "step": 14390
    },
    {
      "epoch": 2.3833167825223436,
      "grad_norm": 18.377304077148438,
      "learning_rate": 2.063545715734325e-05,
      "loss": 0.7916,
      "step": 14400
    },
    {
      "epoch": 2.3849718636213173,
      "grad_norm": 8.341451644897461,
      "learning_rate": 2.0614331586952848e-05,
      "loss": 0.7456,
      "step": 14410
    },
    {
      "epoch": 2.3866269447202915,
      "grad_norm": 15.370765686035156,
      "learning_rate": 2.0593206016562446e-05,
      "loss": 0.574,
      "step": 14420
    },
    {
      "epoch": 2.388282025819265,
      "grad_norm": 9.58170223236084,
      "learning_rate": 2.0572080446172048e-05,
      "loss": 0.8919,
      "step": 14430
    },
    {
      "epoch": 2.389937106918239,
      "grad_norm": 3.734553337097168,
      "learning_rate": 2.0550954875781646e-05,
      "loss": 0.6042,
      "step": 14440
    },
    {
      "epoch": 2.3915921880172126,
      "grad_norm": 6.296032428741455,
      "learning_rate": 2.0529829305391245e-05,
      "loss": 0.5494,
      "step": 14450
    },
    {
      "epoch": 2.393247269116187,
      "grad_norm": 5.003880500793457,
      "learning_rate": 2.0508703735000847e-05,
      "loss": 0.7609,
      "step": 14460
    },
    {
      "epoch": 2.3949023502151605,
      "grad_norm": 2.7977471351623535,
      "learning_rate": 2.0487578164610445e-05,
      "loss": 0.4949,
      "step": 14470
    },
    {
      "epoch": 2.3965574313141342,
      "grad_norm": 4.802281379699707,
      "learning_rate": 2.0466452594220047e-05,
      "loss": 0.5808,
      "step": 14480
    },
    {
      "epoch": 2.3982125124131084,
      "grad_norm": 6.903770446777344,
      "learning_rate": 2.0445327023829645e-05,
      "loss": 0.4562,
      "step": 14490
    },
    {
      "epoch": 2.399867593512082,
      "grad_norm": 1.8155772686004639,
      "learning_rate": 2.0424201453439243e-05,
      "loss": 0.7469,
      "step": 14500
    },
    {
      "epoch": 2.401522674611056,
      "grad_norm": 5.454338550567627,
      "learning_rate": 2.0403075883048845e-05,
      "loss": 0.6154,
      "step": 14510
    },
    {
      "epoch": 2.40317775571003,
      "grad_norm": 5.3687567710876465,
      "learning_rate": 2.0381950312658444e-05,
      "loss": 0.744,
      "step": 14520
    },
    {
      "epoch": 2.4048328368090037,
      "grad_norm": 11.567329406738281,
      "learning_rate": 2.0360824742268042e-05,
      "loss": 0.6361,
      "step": 14530
    },
    {
      "epoch": 2.4064879179079774,
      "grad_norm": 7.726376056671143,
      "learning_rate": 2.033969917187764e-05,
      "loss": 0.7146,
      "step": 14540
    },
    {
      "epoch": 2.408142999006951,
      "grad_norm": 5.459855079650879,
      "learning_rate": 2.0318573601487242e-05,
      "loss": 0.6149,
      "step": 14550
    },
    {
      "epoch": 2.4097980801059253,
      "grad_norm": 2.5761210918426514,
      "learning_rate": 2.029744803109684e-05,
      "loss": 0.567,
      "step": 14560
    },
    {
      "epoch": 2.411453161204899,
      "grad_norm": 9.995842933654785,
      "learning_rate": 2.027632246070644e-05,
      "loss": 0.6637,
      "step": 14570
    },
    {
      "epoch": 2.4131082423038728,
      "grad_norm": 3.8366122245788574,
      "learning_rate": 2.0255196890316037e-05,
      "loss": 0.7524,
      "step": 14580
    },
    {
      "epoch": 2.4147633234028465,
      "grad_norm": 11.721359252929688,
      "learning_rate": 2.023407131992564e-05,
      "loss": 0.7027,
      "step": 14590
    },
    {
      "epoch": 2.4164184045018207,
      "grad_norm": 4.641366004943848,
      "learning_rate": 2.021294574953524e-05,
      "loss": 0.5238,
      "step": 14600
    },
    {
      "epoch": 2.4180734856007944,
      "grad_norm": 2.6282424926757812,
      "learning_rate": 2.019182017914484e-05,
      "loss": 0.4181,
      "step": 14610
    },
    {
      "epoch": 2.419728566699768,
      "grad_norm": 5.230597972869873,
      "learning_rate": 2.0170694608754438e-05,
      "loss": 0.529,
      "step": 14620
    },
    {
      "epoch": 2.4213836477987423,
      "grad_norm": 2.464907646179199,
      "learning_rate": 2.0149569038364036e-05,
      "loss": 0.5524,
      "step": 14630
    },
    {
      "epoch": 2.423038728897716,
      "grad_norm": 11.58574390411377,
      "learning_rate": 2.0128443467973638e-05,
      "loss": 0.8645,
      "step": 14640
    },
    {
      "epoch": 2.4246938099966897,
      "grad_norm": 4.163921356201172,
      "learning_rate": 2.0107317897583236e-05,
      "loss": 0.4344,
      "step": 14650
    },
    {
      "epoch": 2.426348891095664,
      "grad_norm": 5.43701171875,
      "learning_rate": 2.0086192327192835e-05,
      "loss": 0.5958,
      "step": 14660
    },
    {
      "epoch": 2.4280039721946376,
      "grad_norm": 7.465475559234619,
      "learning_rate": 2.0065066756802433e-05,
      "loss": 0.4389,
      "step": 14670
    },
    {
      "epoch": 2.4296590532936113,
      "grad_norm": 0.6396211385726929,
      "learning_rate": 2.0043941186412035e-05,
      "loss": 0.6526,
      "step": 14680
    },
    {
      "epoch": 2.4313141343925855,
      "grad_norm": 2.8412747383117676,
      "learning_rate": 2.0022815616021633e-05,
      "loss": 0.4705,
      "step": 14690
    },
    {
      "epoch": 2.432969215491559,
      "grad_norm": 9.341231346130371,
      "learning_rate": 2.000169004563123e-05,
      "loss": 0.9505,
      "step": 14700
    },
    {
      "epoch": 2.434624296590533,
      "grad_norm": 16.553709030151367,
      "learning_rate": 1.9980564475240833e-05,
      "loss": 0.6933,
      "step": 14710
    },
    {
      "epoch": 2.4362793776895066,
      "grad_norm": 11.924160957336426,
      "learning_rate": 1.995943890485043e-05,
      "loss": 0.8227,
      "step": 14720
    },
    {
      "epoch": 2.437934458788481,
      "grad_norm": 7.808882236480713,
      "learning_rate": 1.9938313334460033e-05,
      "loss": 0.6021,
      "step": 14730
    },
    {
      "epoch": 2.4395895398874545,
      "grad_norm": 1.6572191715240479,
      "learning_rate": 1.9917187764069632e-05,
      "loss": 0.7343,
      "step": 14740
    },
    {
      "epoch": 2.4412446209864282,
      "grad_norm": 6.1221022605896,
      "learning_rate": 1.989606219367923e-05,
      "loss": 0.6603,
      "step": 14750
    },
    {
      "epoch": 2.442899702085402,
      "grad_norm": 9.979453086853027,
      "learning_rate": 1.987493662328883e-05,
      "loss": 0.6246,
      "step": 14760
    },
    {
      "epoch": 2.444554783184376,
      "grad_norm": 10.235153198242188,
      "learning_rate": 1.985381105289843e-05,
      "loss": 0.4584,
      "step": 14770
    },
    {
      "epoch": 2.44620986428335,
      "grad_norm": 14.682839393615723,
      "learning_rate": 1.983268548250803e-05,
      "loss": 0.8014,
      "step": 14780
    },
    {
      "epoch": 2.4478649453823236,
      "grad_norm": 11.148695945739746,
      "learning_rate": 1.9811559912117627e-05,
      "loss": 0.7703,
      "step": 14790
    },
    {
      "epoch": 2.4495200264812977,
      "grad_norm": 5.001650333404541,
      "learning_rate": 1.9790434341727225e-05,
      "loss": 0.6313,
      "step": 14800
    },
    {
      "epoch": 2.4511751075802715,
      "grad_norm": 7.203594207763672,
      "learning_rate": 1.9769308771336827e-05,
      "loss": 0.6508,
      "step": 14810
    },
    {
      "epoch": 2.452830188679245,
      "grad_norm": 3.329256057739258,
      "learning_rate": 1.9748183200946426e-05,
      "loss": 0.623,
      "step": 14820
    },
    {
      "epoch": 2.4544852697782193,
      "grad_norm": 11.002965927124023,
      "learning_rate": 1.9727057630556024e-05,
      "loss": 0.3792,
      "step": 14830
    },
    {
      "epoch": 2.456140350877193,
      "grad_norm": 2.717021942138672,
      "learning_rate": 1.9705932060165626e-05,
      "loss": 0.4843,
      "step": 14840
    },
    {
      "epoch": 2.457795431976167,
      "grad_norm": 1.5955579280853271,
      "learning_rate": 1.9684806489775228e-05,
      "loss": 0.6092,
      "step": 14850
    },
    {
      "epoch": 2.4594505130751405,
      "grad_norm": 10.038949012756348,
      "learning_rate": 1.9663680919384826e-05,
      "loss": 1.14,
      "step": 14860
    },
    {
      "epoch": 2.4611055941741147,
      "grad_norm": 4.956606864929199,
      "learning_rate": 1.9642555348994424e-05,
      "loss": 0.6497,
      "step": 14870
    },
    {
      "epoch": 2.4627606752730884,
      "grad_norm": 5.887897491455078,
      "learning_rate": 1.9621429778604023e-05,
      "loss": 0.656,
      "step": 14880
    },
    {
      "epoch": 2.464415756372062,
      "grad_norm": 7.044051170349121,
      "learning_rate": 1.9600304208213624e-05,
      "loss": 0.8531,
      "step": 14890
    },
    {
      "epoch": 2.466070837471036,
      "grad_norm": 10.51335334777832,
      "learning_rate": 1.9579178637823223e-05,
      "loss": 0.397,
      "step": 14900
    },
    {
      "epoch": 2.46772591857001,
      "grad_norm": 10.842195510864258,
      "learning_rate": 1.955805306743282e-05,
      "loss": 0.8326,
      "step": 14910
    },
    {
      "epoch": 2.4693809996689837,
      "grad_norm": 5.660043239593506,
      "learning_rate": 1.953692749704242e-05,
      "loss": 0.5508,
      "step": 14920
    },
    {
      "epoch": 2.4710360807679574,
      "grad_norm": 7.693474292755127,
      "learning_rate": 1.9515801926652018e-05,
      "loss": 0.5438,
      "step": 14930
    },
    {
      "epoch": 2.4726911618669316,
      "grad_norm": 9.204378128051758,
      "learning_rate": 1.949467635626162e-05,
      "loss": 0.5032,
      "step": 14940
    },
    {
      "epoch": 2.4743462429659053,
      "grad_norm": 4.090540409088135,
      "learning_rate": 1.9473550785871218e-05,
      "loss": 0.5313,
      "step": 14950
    },
    {
      "epoch": 2.476001324064879,
      "grad_norm": 8.767327308654785,
      "learning_rate": 1.945242521548082e-05,
      "loss": 0.7153,
      "step": 14960
    },
    {
      "epoch": 2.477656405163853,
      "grad_norm": 6.275547504425049,
      "learning_rate": 1.943129964509042e-05,
      "loss": 0.424,
      "step": 14970
    },
    {
      "epoch": 2.479311486262827,
      "grad_norm": 5.346283435821533,
      "learning_rate": 1.941017407470002e-05,
      "loss": 0.4629,
      "step": 14980
    },
    {
      "epoch": 2.4809665673618007,
      "grad_norm": 13.458003997802734,
      "learning_rate": 1.938904850430962e-05,
      "loss": 0.7999,
      "step": 14990
    },
    {
      "epoch": 2.482621648460775,
      "grad_norm": 4.801461219787598,
      "learning_rate": 1.9367922933919217e-05,
      "loss": 0.4803,
      "step": 15000
    },
    {
      "epoch": 2.4842767295597485,
      "grad_norm": 11.096657752990723,
      "learning_rate": 1.9346797363528815e-05,
      "loss": 0.8035,
      "step": 15010
    },
    {
      "epoch": 2.4859318106587223,
      "grad_norm": 5.088523864746094,
      "learning_rate": 1.9325671793138417e-05,
      "loss": 0.5285,
      "step": 15020
    },
    {
      "epoch": 2.487586891757696,
      "grad_norm": 18.14032745361328,
      "learning_rate": 1.9304546222748015e-05,
      "loss": 0.7729,
      "step": 15030
    },
    {
      "epoch": 2.48924197285667,
      "grad_norm": 5.128427028656006,
      "learning_rate": 1.9283420652357614e-05,
      "loss": 0.728,
      "step": 15040
    },
    {
      "epoch": 2.490897053955644,
      "grad_norm": 11.047918319702148,
      "learning_rate": 1.9262295081967212e-05,
      "loss": 0.7412,
      "step": 15050
    },
    {
      "epoch": 2.4925521350546176,
      "grad_norm": 9.074233055114746,
      "learning_rate": 1.9241169511576814e-05,
      "loss": 0.8169,
      "step": 15060
    },
    {
      "epoch": 2.4942072161535913,
      "grad_norm": 4.958708763122559,
      "learning_rate": 1.9220043941186412e-05,
      "loss": 0.9797,
      "step": 15070
    },
    {
      "epoch": 2.4958622972525655,
      "grad_norm": 11.435872077941895,
      "learning_rate": 1.9198918370796014e-05,
      "loss": 0.5304,
      "step": 15080
    },
    {
      "epoch": 2.497517378351539,
      "grad_norm": 12.357901573181152,
      "learning_rate": 1.9177792800405612e-05,
      "loss": 0.5304,
      "step": 15090
    },
    {
      "epoch": 2.499172459450513,
      "grad_norm": 2.394948959350586,
      "learning_rate": 1.915666723001521e-05,
      "loss": 0.5218,
      "step": 15100
    },
    {
      "epoch": 2.500827540549487,
      "grad_norm": 11.325176239013672,
      "learning_rate": 1.9135541659624813e-05,
      "loss": 0.4218,
      "step": 15110
    },
    {
      "epoch": 2.502482621648461,
      "grad_norm": 6.060939788818359,
      "learning_rate": 1.911441608923441e-05,
      "loss": 0.409,
      "step": 15120
    },
    {
      "epoch": 2.5041377027474345,
      "grad_norm": 7.649308681488037,
      "learning_rate": 1.909329051884401e-05,
      "loss": 0.7698,
      "step": 15130
    },
    {
      "epoch": 2.5057927838464087,
      "grad_norm": 6.910034656524658,
      "learning_rate": 1.9072164948453608e-05,
      "loss": 0.5647,
      "step": 15140
    },
    {
      "epoch": 2.5074478649453824,
      "grad_norm": 6.365167617797852,
      "learning_rate": 1.905103937806321e-05,
      "loss": 0.7575,
      "step": 15150
    },
    {
      "epoch": 2.509102946044356,
      "grad_norm": 2.791717052459717,
      "learning_rate": 1.9029913807672808e-05,
      "loss": 0.4375,
      "step": 15160
    },
    {
      "epoch": 2.5107580271433303,
      "grad_norm": 8.50652027130127,
      "learning_rate": 1.9008788237282406e-05,
      "loss": 0.5152,
      "step": 15170
    },
    {
      "epoch": 2.512413108242304,
      "grad_norm": 5.484718322753906,
      "learning_rate": 1.8987662666892005e-05,
      "loss": 0.4352,
      "step": 15180
    },
    {
      "epoch": 2.5140681893412777,
      "grad_norm": 0.6904693841934204,
      "learning_rate": 1.8966537096501606e-05,
      "loss": 0.6031,
      "step": 15190
    },
    {
      "epoch": 2.5157232704402515,
      "grad_norm": 9.548791885375977,
      "learning_rate": 1.8945411526111205e-05,
      "loss": 0.6682,
      "step": 15200
    },
    {
      "epoch": 2.517378351539225,
      "grad_norm": 11.928031921386719,
      "learning_rate": 1.8924285955720807e-05,
      "loss": 0.9627,
      "step": 15210
    },
    {
      "epoch": 2.5190334326381993,
      "grad_norm": 5.151234149932861,
      "learning_rate": 1.8903160385330405e-05,
      "loss": 0.5001,
      "step": 15220
    },
    {
      "epoch": 2.520688513737173,
      "grad_norm": 11.175167083740234,
      "learning_rate": 1.8882034814940007e-05,
      "loss": 0.6865,
      "step": 15230
    },
    {
      "epoch": 2.522343594836147,
      "grad_norm": 11.528264045715332,
      "learning_rate": 1.8860909244549605e-05,
      "loss": 0.7655,
      "step": 15240
    },
    {
      "epoch": 2.523998675935121,
      "grad_norm": 8.900710105895996,
      "learning_rate": 1.8839783674159204e-05,
      "loss": 0.2976,
      "step": 15250
    },
    {
      "epoch": 2.5256537570340947,
      "grad_norm": 10.109274864196777,
      "learning_rate": 1.8818658103768802e-05,
      "loss": 0.6511,
      "step": 15260
    },
    {
      "epoch": 2.5273088381330684,
      "grad_norm": 7.906128406524658,
      "learning_rate": 1.87975325333784e-05,
      "loss": 0.4194,
      "step": 15270
    },
    {
      "epoch": 2.5289639192320426,
      "grad_norm": 3.7542829513549805,
      "learning_rate": 1.8776406962988002e-05,
      "loss": 0.693,
      "step": 15280
    },
    {
      "epoch": 2.5306190003310163,
      "grad_norm": 8.398608207702637,
      "learning_rate": 1.87552813925976e-05,
      "loss": 0.5915,
      "step": 15290
    },
    {
      "epoch": 2.53227408142999,
      "grad_norm": 7.158956527709961,
      "learning_rate": 1.87341558222072e-05,
      "loss": 0.6931,
      "step": 15300
    },
    {
      "epoch": 2.533929162528964,
      "grad_norm": 6.377035617828369,
      "learning_rate": 1.8713030251816797e-05,
      "loss": 0.7857,
      "step": 15310
    },
    {
      "epoch": 2.535584243627938,
      "grad_norm": 8.207581520080566,
      "learning_rate": 1.86919046814264e-05,
      "loss": 0.4762,
      "step": 15320
    },
    {
      "epoch": 2.5372393247269116,
      "grad_norm": 6.845165729522705,
      "learning_rate": 1.8670779111036e-05,
      "loss": 0.3776,
      "step": 15330
    },
    {
      "epoch": 2.5388944058258853,
      "grad_norm": 5.820088863372803,
      "learning_rate": 1.86496535406456e-05,
      "loss": 0.5175,
      "step": 15340
    },
    {
      "epoch": 2.540549486924859,
      "grad_norm": 3.7447540760040283,
      "learning_rate": 1.8628527970255198e-05,
      "loss": 0.6812,
      "step": 15350
    },
    {
      "epoch": 2.542204568023833,
      "grad_norm": 3.554814577102661,
      "learning_rate": 1.86074023998648e-05,
      "loss": 0.5828,
      "step": 15360
    },
    {
      "epoch": 2.543859649122807,
      "grad_norm": 1.724340558052063,
      "learning_rate": 1.8586276829474398e-05,
      "loss": 0.5002,
      "step": 15370
    },
    {
      "epoch": 2.5455147302217807,
      "grad_norm": 12.844998359680176,
      "learning_rate": 1.8565151259083996e-05,
      "loss": 0.8331,
      "step": 15380
    },
    {
      "epoch": 2.547169811320755,
      "grad_norm": 5.030555248260498,
      "learning_rate": 1.8544025688693594e-05,
      "loss": 0.5769,
      "step": 15390
    },
    {
      "epoch": 2.5488248924197285,
      "grad_norm": 2.4122560024261475,
      "learning_rate": 1.8522900118303196e-05,
      "loss": 0.6028,
      "step": 15400
    },
    {
      "epoch": 2.5504799735187023,
      "grad_norm": 15.579612731933594,
      "learning_rate": 1.8501774547912795e-05,
      "loss": 0.6414,
      "step": 15410
    },
    {
      "epoch": 2.5521350546176764,
      "grad_norm": 10.8717041015625,
      "learning_rate": 1.8480648977522393e-05,
      "loss": 0.697,
      "step": 15420
    },
    {
      "epoch": 2.55379013571665,
      "grad_norm": 2.9087231159210205,
      "learning_rate": 1.845952340713199e-05,
      "loss": 0.5627,
      "step": 15430
    },
    {
      "epoch": 2.555445216815624,
      "grad_norm": 4.542511463165283,
      "learning_rate": 1.8438397836741593e-05,
      "loss": 0.7564,
      "step": 15440
    },
    {
      "epoch": 2.557100297914598,
      "grad_norm": 9.265904426574707,
      "learning_rate": 1.841727226635119e-05,
      "loss": 0.8122,
      "step": 15450
    },
    {
      "epoch": 2.5587553790135718,
      "grad_norm": 8.7076416015625,
      "learning_rate": 1.8396146695960793e-05,
      "loss": 0.5536,
      "step": 15460
    },
    {
      "epoch": 2.5604104601125455,
      "grad_norm": 8.91969108581543,
      "learning_rate": 1.837502112557039e-05,
      "loss": 0.6309,
      "step": 15470
    },
    {
      "epoch": 2.5620655412115196,
      "grad_norm": 10.873576164245605,
      "learning_rate": 1.835389555517999e-05,
      "loss": 0.9669,
      "step": 15480
    },
    {
      "epoch": 2.5637206223104934,
      "grad_norm": 11.163580894470215,
      "learning_rate": 1.8332769984789592e-05,
      "loss": 0.5279,
      "step": 15490
    },
    {
      "epoch": 2.565375703409467,
      "grad_norm": 10.603894233703613,
      "learning_rate": 1.831164441439919e-05,
      "loss": 0.6351,
      "step": 15500
    },
    {
      "epoch": 2.567030784508441,
      "grad_norm": 12.930049896240234,
      "learning_rate": 1.829051884400879e-05,
      "loss": 0.5301,
      "step": 15510
    },
    {
      "epoch": 2.5686858656074145,
      "grad_norm": 5.157965183258057,
      "learning_rate": 1.8269393273618387e-05,
      "loss": 0.6972,
      "step": 15520
    },
    {
      "epoch": 2.5703409467063887,
      "grad_norm": 8.978963851928711,
      "learning_rate": 1.824826770322799e-05,
      "loss": 0.7997,
      "step": 15530
    },
    {
      "epoch": 2.5719960278053624,
      "grad_norm": 4.736737251281738,
      "learning_rate": 1.8227142132837587e-05,
      "loss": 0.6215,
      "step": 15540
    },
    {
      "epoch": 2.573651108904336,
      "grad_norm": 9.42545223236084,
      "learning_rate": 1.8206016562447186e-05,
      "loss": 0.8168,
      "step": 15550
    },
    {
      "epoch": 2.5753061900033103,
      "grad_norm": 9.161725997924805,
      "learning_rate": 1.8184890992056784e-05,
      "loss": 0.8867,
      "step": 15560
    },
    {
      "epoch": 2.576961271102284,
      "grad_norm": 4.409026145935059,
      "learning_rate": 1.8163765421666386e-05,
      "loss": 0.5612,
      "step": 15570
    },
    {
      "epoch": 2.5786163522012577,
      "grad_norm": 7.7632269859313965,
      "learning_rate": 1.8142639851275987e-05,
      "loss": 0.377,
      "step": 15580
    },
    {
      "epoch": 2.580271433300232,
      "grad_norm": 2.259730815887451,
      "learning_rate": 1.8121514280885586e-05,
      "loss": 0.5095,
      "step": 15590
    },
    {
      "epoch": 2.5819265143992056,
      "grad_norm": 6.2025909423828125,
      "learning_rate": 1.8100388710495184e-05,
      "loss": 0.5307,
      "step": 15600
    },
    {
      "epoch": 2.5835815954981793,
      "grad_norm": 5.725159645080566,
      "learning_rate": 1.8079263140104783e-05,
      "loss": 0.56,
      "step": 15610
    },
    {
      "epoch": 2.5852366765971535,
      "grad_norm": 14.16188907623291,
      "learning_rate": 1.8058137569714384e-05,
      "loss": 0.6804,
      "step": 15620
    },
    {
      "epoch": 2.5868917576961272,
      "grad_norm": 9.264693260192871,
      "learning_rate": 1.8037011999323983e-05,
      "loss": 0.4299,
      "step": 15630
    },
    {
      "epoch": 2.588546838795101,
      "grad_norm": 5.811919212341309,
      "learning_rate": 1.801588642893358e-05,
      "loss": 0.5627,
      "step": 15640
    },
    {
      "epoch": 2.5902019198940747,
      "grad_norm": 13.091760635375977,
      "learning_rate": 1.799476085854318e-05,
      "loss": 0.6573,
      "step": 15650
    },
    {
      "epoch": 2.5918570009930484,
      "grad_norm": 2.5606677532196045,
      "learning_rate": 1.797363528815278e-05,
      "loss": 0.4456,
      "step": 15660
    },
    {
      "epoch": 2.5935120820920226,
      "grad_norm": 9.779330253601074,
      "learning_rate": 1.795250971776238e-05,
      "loss": 0.7034,
      "step": 15670
    },
    {
      "epoch": 2.5951671631909963,
      "grad_norm": 19.380329132080078,
      "learning_rate": 1.7931384147371978e-05,
      "loss": 0.7068,
      "step": 15680
    },
    {
      "epoch": 2.59682224428997,
      "grad_norm": 6.458541393280029,
      "learning_rate": 1.791025857698158e-05,
      "loss": 0.7034,
      "step": 15690
    },
    {
      "epoch": 2.598477325388944,
      "grad_norm": 7.918004512786865,
      "learning_rate": 1.788913300659118e-05,
      "loss": 0.428,
      "step": 15700
    },
    {
      "epoch": 2.600132406487918,
      "grad_norm": 7.709639072418213,
      "learning_rate": 1.786800743620078e-05,
      "loss": 0.8119,
      "step": 15710
    },
    {
      "epoch": 2.6017874875868916,
      "grad_norm": 5.523321151733398,
      "learning_rate": 1.784688186581038e-05,
      "loss": 0.6533,
      "step": 15720
    },
    {
      "epoch": 2.6034425686858658,
      "grad_norm": 5.9688215255737305,
      "learning_rate": 1.7825756295419977e-05,
      "loss": 0.9126,
      "step": 15730
    },
    {
      "epoch": 2.6050976497848395,
      "grad_norm": 4.5986104011535645,
      "learning_rate": 1.780463072502958e-05,
      "loss": 0.4207,
      "step": 15740
    },
    {
      "epoch": 2.606752730883813,
      "grad_norm": 7.683123588562012,
      "learning_rate": 1.7783505154639177e-05,
      "loss": 0.5348,
      "step": 15750
    },
    {
      "epoch": 2.6084078119827874,
      "grad_norm": 4.7581095695495605,
      "learning_rate": 1.7762379584248775e-05,
      "loss": 0.6954,
      "step": 15760
    },
    {
      "epoch": 2.610062893081761,
      "grad_norm": 12.659771919250488,
      "learning_rate": 1.7741254013858374e-05,
      "loss": 0.5751,
      "step": 15770
    },
    {
      "epoch": 2.611717974180735,
      "grad_norm": 7.679503917694092,
      "learning_rate": 1.7720128443467972e-05,
      "loss": 1.0135,
      "step": 15780
    },
    {
      "epoch": 2.613373055279709,
      "grad_norm": 8.653302192687988,
      "learning_rate": 1.7699002873077574e-05,
      "loss": 0.8994,
      "step": 15790
    },
    {
      "epoch": 2.6150281363786827,
      "grad_norm": 6.466132164001465,
      "learning_rate": 1.7677877302687172e-05,
      "loss": 0.6031,
      "step": 15800
    },
    {
      "epoch": 2.6166832174776564,
      "grad_norm": 10.271648406982422,
      "learning_rate": 1.7656751732296774e-05,
      "loss": 0.6754,
      "step": 15810
    },
    {
      "epoch": 2.61833829857663,
      "grad_norm": 10.730630874633789,
      "learning_rate": 1.7635626161906372e-05,
      "loss": 0.7476,
      "step": 15820
    },
    {
      "epoch": 2.619993379675604,
      "grad_norm": 6.236824989318848,
      "learning_rate": 1.7614500591515974e-05,
      "loss": 0.6295,
      "step": 15830
    },
    {
      "epoch": 2.621648460774578,
      "grad_norm": 6.695950984954834,
      "learning_rate": 1.7593375021125573e-05,
      "loss": 0.6163,
      "step": 15840
    },
    {
      "epoch": 2.6233035418735517,
      "grad_norm": 4.311526775360107,
      "learning_rate": 1.757224945073517e-05,
      "loss": 0.9134,
      "step": 15850
    },
    {
      "epoch": 2.6249586229725255,
      "grad_norm": 6.170975208282471,
      "learning_rate": 1.755112388034477e-05,
      "loss": 0.6304,
      "step": 15860
    },
    {
      "epoch": 2.6266137040714996,
      "grad_norm": 4.991878986358643,
      "learning_rate": 1.752999830995437e-05,
      "loss": 0.6527,
      "step": 15870
    },
    {
      "epoch": 2.6282687851704734,
      "grad_norm": 6.553131103515625,
      "learning_rate": 1.750887273956397e-05,
      "loss": 0.5393,
      "step": 15880
    },
    {
      "epoch": 2.629923866269447,
      "grad_norm": 3.7837154865264893,
      "learning_rate": 1.7487747169173568e-05,
      "loss": 0.5431,
      "step": 15890
    },
    {
      "epoch": 2.6315789473684212,
      "grad_norm": 8.569952011108398,
      "learning_rate": 1.7466621598783166e-05,
      "loss": 0.6091,
      "step": 15900
    },
    {
      "epoch": 2.633234028467395,
      "grad_norm": 8.83755111694336,
      "learning_rate": 1.7445496028392768e-05,
      "loss": 0.5138,
      "step": 15910
    },
    {
      "epoch": 2.6348891095663687,
      "grad_norm": 21.535463333129883,
      "learning_rate": 1.7424370458002366e-05,
      "loss": 0.6865,
      "step": 15920
    },
    {
      "epoch": 2.636544190665343,
      "grad_norm": 4.498911380767822,
      "learning_rate": 1.7403244887611965e-05,
      "loss": 0.7631,
      "step": 15930
    },
    {
      "epoch": 2.6381992717643166,
      "grad_norm": 8.735265731811523,
      "learning_rate": 1.7382119317221567e-05,
      "loss": 0.7716,
      "step": 15940
    },
    {
      "epoch": 2.6398543528632903,
      "grad_norm": 6.654677391052246,
      "learning_rate": 1.7360993746831165e-05,
      "loss": 0.7212,
      "step": 15950
    },
    {
      "epoch": 2.641509433962264,
      "grad_norm": 2.0146877765655518,
      "learning_rate": 1.7339868176440767e-05,
      "loss": 0.6207,
      "step": 15960
    },
    {
      "epoch": 2.6431645150612377,
      "grad_norm": 9.342792510986328,
      "learning_rate": 1.7318742606050365e-05,
      "loss": 0.9611,
      "step": 15970
    },
    {
      "epoch": 2.644819596160212,
      "grad_norm": 7.789965629577637,
      "learning_rate": 1.7297617035659963e-05,
      "loss": 0.5837,
      "step": 15980
    },
    {
      "epoch": 2.6464746772591856,
      "grad_norm": 11.969477653503418,
      "learning_rate": 1.7276491465269562e-05,
      "loss": 0.4418,
      "step": 15990
    },
    {
      "epoch": 2.6481297583581593,
      "grad_norm": 6.152204990386963,
      "learning_rate": 1.7255365894879164e-05,
      "loss": 0.8461,
      "step": 16000
    },
    {
      "epoch": 2.6497848394571335,
      "grad_norm": 4.717044353485107,
      "learning_rate": 1.7234240324488762e-05,
      "loss": 0.5762,
      "step": 16010
    },
    {
      "epoch": 2.651439920556107,
      "grad_norm": 7.388810634613037,
      "learning_rate": 1.721311475409836e-05,
      "loss": 0.6116,
      "step": 16020
    },
    {
      "epoch": 2.653095001655081,
      "grad_norm": 2.9310455322265625,
      "learning_rate": 1.719198918370796e-05,
      "loss": 0.6392,
      "step": 16030
    },
    {
      "epoch": 2.654750082754055,
      "grad_norm": 6.373434543609619,
      "learning_rate": 1.717086361331756e-05,
      "loss": 0.5338,
      "step": 16040
    },
    {
      "epoch": 2.656405163853029,
      "grad_norm": 2.4389750957489014,
      "learning_rate": 1.714973804292716e-05,
      "loss": 0.8149,
      "step": 16050
    },
    {
      "epoch": 2.6580602449520025,
      "grad_norm": 3.4231247901916504,
      "learning_rate": 1.712861247253676e-05,
      "loss": 0.4164,
      "step": 16060
    },
    {
      "epoch": 2.6597153260509767,
      "grad_norm": 8.535202026367188,
      "learning_rate": 1.710748690214636e-05,
      "loss": 0.8102,
      "step": 16070
    },
    {
      "epoch": 2.6613704071499504,
      "grad_norm": 8.774346351623535,
      "learning_rate": 1.708636133175596e-05,
      "loss": 0.514,
      "step": 16080
    },
    {
      "epoch": 2.663025488248924,
      "grad_norm": 6.711238384246826,
      "learning_rate": 1.706523576136556e-05,
      "loss": 0.5187,
      "step": 16090
    },
    {
      "epoch": 2.6646805693478983,
      "grad_norm": 6.3299736976623535,
      "learning_rate": 1.7044110190975158e-05,
      "loss": 0.7933,
      "step": 16100
    },
    {
      "epoch": 2.666335650446872,
      "grad_norm": 7.748512268066406,
      "learning_rate": 1.7022984620584756e-05,
      "loss": 0.7462,
      "step": 16110
    },
    {
      "epoch": 2.6679907315458458,
      "grad_norm": 3.3701131343841553,
      "learning_rate": 1.7001859050194354e-05,
      "loss": 0.6479,
      "step": 16120
    },
    {
      "epoch": 2.6696458126448195,
      "grad_norm": 8.705230712890625,
      "learning_rate": 1.6980733479803956e-05,
      "loss": 0.4007,
      "step": 16130
    },
    {
      "epoch": 2.671300893743793,
      "grad_norm": 8.053678512573242,
      "learning_rate": 1.6959607909413555e-05,
      "loss": 0.4912,
      "step": 16140
    },
    {
      "epoch": 2.6729559748427674,
      "grad_norm": 8.317292213439941,
      "learning_rate": 1.6938482339023153e-05,
      "loss": 0.5235,
      "step": 16150
    },
    {
      "epoch": 2.674611055941741,
      "grad_norm": 10.811957359313965,
      "learning_rate": 1.691735676863275e-05,
      "loss": 0.7182,
      "step": 16160
    },
    {
      "epoch": 2.676266137040715,
      "grad_norm": 9.436575889587402,
      "learning_rate": 1.6896231198242353e-05,
      "loss": 0.5708,
      "step": 16170
    },
    {
      "epoch": 2.677921218139689,
      "grad_norm": 7.640363693237305,
      "learning_rate": 1.6875105627851955e-05,
      "loss": 0.5877,
      "step": 16180
    },
    {
      "epoch": 2.6795762992386627,
      "grad_norm": 11.24476432800293,
      "learning_rate": 1.6853980057461553e-05,
      "loss": 0.3752,
      "step": 16190
    },
    {
      "epoch": 2.6812313803376364,
      "grad_norm": 4.044723987579346,
      "learning_rate": 1.683285448707115e-05,
      "loss": 0.6152,
      "step": 16200
    },
    {
      "epoch": 2.6828864614366106,
      "grad_norm": 2.2350778579711914,
      "learning_rate": 1.6811728916680753e-05,
      "loss": 0.5148,
      "step": 16210
    },
    {
      "epoch": 2.6845415425355843,
      "grad_norm": 10.440828323364258,
      "learning_rate": 1.6790603346290352e-05,
      "loss": 0.5189,
      "step": 16220
    },
    {
      "epoch": 2.686196623634558,
      "grad_norm": 10.288664817810059,
      "learning_rate": 1.676947777589995e-05,
      "loss": 0.6424,
      "step": 16230
    },
    {
      "epoch": 2.687851704733532,
      "grad_norm": 9.964888572692871,
      "learning_rate": 1.674835220550955e-05,
      "loss": 0.6681,
      "step": 16240
    },
    {
      "epoch": 2.689506785832506,
      "grad_norm": 9.679462432861328,
      "learning_rate": 1.672722663511915e-05,
      "loss": 0.5648,
      "step": 16250
    },
    {
      "epoch": 2.6911618669314796,
      "grad_norm": 6.001791000366211,
      "learning_rate": 1.670610106472875e-05,
      "loss": 0.5942,
      "step": 16260
    },
    {
      "epoch": 2.6928169480304534,
      "grad_norm": 21.7435359954834,
      "learning_rate": 1.6684975494338347e-05,
      "loss": 0.8744,
      "step": 16270
    },
    {
      "epoch": 2.694472029129427,
      "grad_norm": 6.639042377471924,
      "learning_rate": 1.6663849923947945e-05,
      "loss": 0.5921,
      "step": 16280
    },
    {
      "epoch": 2.6961271102284012,
      "grad_norm": 8.099863052368164,
      "learning_rate": 1.6642724353557547e-05,
      "loss": 0.5798,
      "step": 16290
    },
    {
      "epoch": 2.697782191327375,
      "grad_norm": 2.085700750350952,
      "learning_rate": 1.6621598783167146e-05,
      "loss": 0.7226,
      "step": 16300
    },
    {
      "epoch": 2.6994372724263487,
      "grad_norm": 5.839237689971924,
      "learning_rate": 1.6600473212776747e-05,
      "loss": 0.5467,
      "step": 16310
    },
    {
      "epoch": 2.701092353525323,
      "grad_norm": 6.420608043670654,
      "learning_rate": 1.6579347642386346e-05,
      "loss": 0.8518,
      "step": 16320
    },
    {
      "epoch": 2.7027474346242966,
      "grad_norm": 1.6690844297409058,
      "learning_rate": 1.6558222071995944e-05,
      "loss": 0.5839,
      "step": 16330
    },
    {
      "epoch": 2.7044025157232703,
      "grad_norm": 2.512324333190918,
      "learning_rate": 1.6537096501605546e-05,
      "loss": 0.6074,
      "step": 16340
    },
    {
      "epoch": 2.7060575968222444,
      "grad_norm": 4.026632785797119,
      "learning_rate": 1.6515970931215144e-05,
      "loss": 0.8405,
      "step": 16350
    },
    {
      "epoch": 2.707712677921218,
      "grad_norm": 6.316347122192383,
      "learning_rate": 1.6494845360824743e-05,
      "loss": 0.5192,
      "step": 16360
    },
    {
      "epoch": 2.709367759020192,
      "grad_norm": 3.0573971271514893,
      "learning_rate": 1.647371979043434e-05,
      "loss": 0.678,
      "step": 16370
    },
    {
      "epoch": 2.711022840119166,
      "grad_norm": 10.017127990722656,
      "learning_rate": 1.6452594220043943e-05,
      "loss": 0.8295,
      "step": 16380
    },
    {
      "epoch": 2.7126779212181398,
      "grad_norm": 6.83493185043335,
      "learning_rate": 1.643146864965354e-05,
      "loss": 0.5164,
      "step": 16390
    },
    {
      "epoch": 2.7143330023171135,
      "grad_norm": 8.717233657836914,
      "learning_rate": 1.641034307926314e-05,
      "loss": 0.7591,
      "step": 16400
    },
    {
      "epoch": 2.7159880834160877,
      "grad_norm": 11.617888450622559,
      "learning_rate": 1.6389217508872738e-05,
      "loss": 0.7908,
      "step": 16410
    },
    {
      "epoch": 2.7176431645150614,
      "grad_norm": 3.2987499237060547,
      "learning_rate": 1.636809193848234e-05,
      "loss": 0.4373,
      "step": 16420
    },
    {
      "epoch": 2.719298245614035,
      "grad_norm": 8.018780708312988,
      "learning_rate": 1.634696636809194e-05,
      "loss": 0.4721,
      "step": 16430
    },
    {
      "epoch": 2.720953326713009,
      "grad_norm": 7.916794776916504,
      "learning_rate": 1.632584079770154e-05,
      "loss": 0.6353,
      "step": 16440
    },
    {
      "epoch": 2.7226084078119825,
      "grad_norm": 2.605475425720215,
      "learning_rate": 1.6304715227311138e-05,
      "loss": 0.5763,
      "step": 16450
    },
    {
      "epoch": 2.7242634889109567,
      "grad_norm": 4.477676868438721,
      "learning_rate": 1.6283589656920737e-05,
      "loss": 0.6804,
      "step": 16460
    },
    {
      "epoch": 2.7259185700099304,
      "grad_norm": 5.26466178894043,
      "learning_rate": 1.626246408653034e-05,
      "loss": 0.4958,
      "step": 16470
    },
    {
      "epoch": 2.727573651108904,
      "grad_norm": 7.381441593170166,
      "learning_rate": 1.6241338516139937e-05,
      "loss": 0.6021,
      "step": 16480
    },
    {
      "epoch": 2.7292287322078783,
      "grad_norm": 14.564352989196777,
      "learning_rate": 1.6220212945749535e-05,
      "loss": 0.8403,
      "step": 16490
    },
    {
      "epoch": 2.730883813306852,
      "grad_norm": 5.3941969871521,
      "learning_rate": 1.6199087375359134e-05,
      "loss": 0.6406,
      "step": 16500
    },
    {
      "epoch": 2.7325388944058258,
      "grad_norm": 3.4143919944763184,
      "learning_rate": 1.6177961804968735e-05,
      "loss": 0.8943,
      "step": 16510
    },
    {
      "epoch": 2.7341939755048,
      "grad_norm": 6.496552467346191,
      "learning_rate": 1.6156836234578334e-05,
      "loss": 0.4764,
      "step": 16520
    },
    {
      "epoch": 2.7358490566037736,
      "grad_norm": 10.192441940307617,
      "learning_rate": 1.6135710664187932e-05,
      "loss": 0.6977,
      "step": 16530
    },
    {
      "epoch": 2.7375041377027474,
      "grad_norm": 5.847233295440674,
      "learning_rate": 1.6114585093797534e-05,
      "loss": 0.4451,
      "step": 16540
    },
    {
      "epoch": 2.7391592188017215,
      "grad_norm": 16.394474029541016,
      "learning_rate": 1.6093459523407132e-05,
      "loss": 0.6928,
      "step": 16550
    },
    {
      "epoch": 2.7408142999006953,
      "grad_norm": 1.7417253255844116,
      "learning_rate": 1.6072333953016734e-05,
      "loss": 0.4794,
      "step": 16560
    },
    {
      "epoch": 2.742469380999669,
      "grad_norm": 3.5740103721618652,
      "learning_rate": 1.6051208382626332e-05,
      "loss": 0.5604,
      "step": 16570
    },
    {
      "epoch": 2.7441244620986427,
      "grad_norm": 10.215116500854492,
      "learning_rate": 1.603008281223593e-05,
      "loss": 0.5553,
      "step": 16580
    },
    {
      "epoch": 2.7457795431976164,
      "grad_norm": 1.9092365503311157,
      "learning_rate": 1.6008957241845533e-05,
      "loss": 0.8541,
      "step": 16590
    },
    {
      "epoch": 2.7474346242965906,
      "grad_norm": 13.487693786621094,
      "learning_rate": 1.598783167145513e-05,
      "loss": 0.6711,
      "step": 16600
    },
    {
      "epoch": 2.7490897053955643,
      "grad_norm": 15.90569019317627,
      "learning_rate": 1.596670610106473e-05,
      "loss": 0.6179,
      "step": 16610
    },
    {
      "epoch": 2.750744786494538,
      "grad_norm": 11.78601360321045,
      "learning_rate": 1.5945580530674328e-05,
      "loss": 0.8847,
      "step": 16620
    },
    {
      "epoch": 2.752399867593512,
      "grad_norm": 5.7218828201293945,
      "learning_rate": 1.5924454960283926e-05,
      "loss": 0.477,
      "step": 16630
    },
    {
      "epoch": 2.754054948692486,
      "grad_norm": 0.47530582547187805,
      "learning_rate": 1.5903329389893528e-05,
      "loss": 0.671,
      "step": 16640
    },
    {
      "epoch": 2.7557100297914596,
      "grad_norm": 11.42142105102539,
      "learning_rate": 1.5882203819503126e-05,
      "loss": 0.6125,
      "step": 16650
    },
    {
      "epoch": 2.757365110890434,
      "grad_norm": 6.9611992835998535,
      "learning_rate": 1.5861078249112725e-05,
      "loss": 0.7025,
      "step": 16660
    },
    {
      "epoch": 2.7590201919894075,
      "grad_norm": 5.666558265686035,
      "learning_rate": 1.5839952678722326e-05,
      "loss": 0.4082,
      "step": 16670
    },
    {
      "epoch": 2.7606752730883812,
      "grad_norm": 4.895297527313232,
      "learning_rate": 1.5818827108331928e-05,
      "loss": 0.612,
      "step": 16680
    },
    {
      "epoch": 2.7623303541873554,
      "grad_norm": 12.665104866027832,
      "learning_rate": 1.5797701537941527e-05,
      "loss": 0.7358,
      "step": 16690
    },
    {
      "epoch": 2.763985435286329,
      "grad_norm": 8.389798164367676,
      "learning_rate": 1.5776575967551125e-05,
      "loss": 0.6603,
      "step": 16700
    },
    {
      "epoch": 2.765640516385303,
      "grad_norm": 8.219243049621582,
      "learning_rate": 1.5755450397160723e-05,
      "loss": 0.3833,
      "step": 16710
    },
    {
      "epoch": 2.767295597484277,
      "grad_norm": 10.5895357131958,
      "learning_rate": 1.5734324826770325e-05,
      "loss": 0.8409,
      "step": 16720
    },
    {
      "epoch": 2.7689506785832507,
      "grad_norm": 9.835973739624023,
      "learning_rate": 1.5713199256379924e-05,
      "loss": 0.5367,
      "step": 16730
    },
    {
      "epoch": 2.7706057596822244,
      "grad_norm": 4.673189640045166,
      "learning_rate": 1.5692073685989522e-05,
      "loss": 0.4743,
      "step": 16740
    },
    {
      "epoch": 2.772260840781198,
      "grad_norm": 2.2398340702056885,
      "learning_rate": 1.567094811559912e-05,
      "loss": 0.5633,
      "step": 16750
    },
    {
      "epoch": 2.773915921880172,
      "grad_norm": 9.73769474029541,
      "learning_rate": 1.5649822545208722e-05,
      "loss": 0.5844,
      "step": 16760
    },
    {
      "epoch": 2.775571002979146,
      "grad_norm": 2.549445390701294,
      "learning_rate": 1.562869697481832e-05,
      "loss": 0.5239,
      "step": 16770
    },
    {
      "epoch": 2.7772260840781198,
      "grad_norm": 6.49201774597168,
      "learning_rate": 1.560757140442792e-05,
      "loss": 0.7872,
      "step": 16780
    },
    {
      "epoch": 2.7788811651770935,
      "grad_norm": 11.474983215332031,
      "learning_rate": 1.558644583403752e-05,
      "loss": 0.5429,
      "step": 16790
    },
    {
      "epoch": 2.7805362462760677,
      "grad_norm": 6.424015045166016,
      "learning_rate": 1.556532026364712e-05,
      "loss": 0.8001,
      "step": 16800
    },
    {
      "epoch": 2.7821913273750414,
      "grad_norm": 4.751643657684326,
      "learning_rate": 1.554419469325672e-05,
      "loss": 0.5331,
      "step": 16810
    },
    {
      "epoch": 2.783846408474015,
      "grad_norm": 7.4174933433532715,
      "learning_rate": 1.552306912286632e-05,
      "loss": 0.6439,
      "step": 16820
    },
    {
      "epoch": 2.7855014895729893,
      "grad_norm": 11.60888385772705,
      "learning_rate": 1.5501943552475918e-05,
      "loss": 0.6208,
      "step": 16830
    },
    {
      "epoch": 2.787156570671963,
      "grad_norm": 5.686887741088867,
      "learning_rate": 1.5480817982085516e-05,
      "loss": 0.5908,
      "step": 16840
    },
    {
      "epoch": 2.7888116517709367,
      "grad_norm": 3.0817127227783203,
      "learning_rate": 1.5459692411695118e-05,
      "loss": 0.4828,
      "step": 16850
    },
    {
      "epoch": 2.790466732869911,
      "grad_norm": 7.0249104499816895,
      "learning_rate": 1.5438566841304716e-05,
      "loss": 0.6273,
      "step": 16860
    },
    {
      "epoch": 2.7921218139688846,
      "grad_norm": 8.272521018981934,
      "learning_rate": 1.5417441270914314e-05,
      "loss": 0.9316,
      "step": 16870
    },
    {
      "epoch": 2.7937768950678583,
      "grad_norm": 13.4315824508667,
      "learning_rate": 1.5396315700523913e-05,
      "loss": 0.617,
      "step": 16880
    },
    {
      "epoch": 2.795431976166832,
      "grad_norm": 5.958446502685547,
      "learning_rate": 1.5375190130133515e-05,
      "loss": 0.6818,
      "step": 16890
    },
    {
      "epoch": 2.7970870572658058,
      "grad_norm": 4.704529762268066,
      "learning_rate": 1.5354064559743113e-05,
      "loss": 0.5228,
      "step": 16900
    },
    {
      "epoch": 2.79874213836478,
      "grad_norm": 5.2595086097717285,
      "learning_rate": 1.5332938989352715e-05,
      "loss": 0.4157,
      "step": 16910
    },
    {
      "epoch": 2.8003972194637536,
      "grad_norm": 12.992048263549805,
      "learning_rate": 1.5311813418962313e-05,
      "loss": 0.9761,
      "step": 16920
    },
    {
      "epoch": 2.8020523005627274,
      "grad_norm": 19.267963409423828,
      "learning_rate": 1.5290687848571915e-05,
      "loss": 0.5691,
      "step": 16930
    },
    {
      "epoch": 2.8037073816617015,
      "grad_norm": 3.422745943069458,
      "learning_rate": 1.5269562278181513e-05,
      "loss": 0.6347,
      "step": 16940
    },
    {
      "epoch": 2.8053624627606752,
      "grad_norm": 4.106106281280518,
      "learning_rate": 1.5248436707791112e-05,
      "loss": 0.6246,
      "step": 16950
    },
    {
      "epoch": 2.807017543859649,
      "grad_norm": 8.882885932922363,
      "learning_rate": 1.522731113740071e-05,
      "loss": 0.5133,
      "step": 16960
    },
    {
      "epoch": 2.808672624958623,
      "grad_norm": 3.771024703979492,
      "learning_rate": 1.5206185567010308e-05,
      "loss": 0.3743,
      "step": 16970
    },
    {
      "epoch": 2.810327706057597,
      "grad_norm": 5.024911403656006,
      "learning_rate": 1.518505999661991e-05,
      "loss": 0.6177,
      "step": 16980
    },
    {
      "epoch": 2.8119827871565706,
      "grad_norm": 7.259607791900635,
      "learning_rate": 1.5163934426229509e-05,
      "loss": 0.7201,
      "step": 16990
    },
    {
      "epoch": 2.8136378682555447,
      "grad_norm": 4.761568069458008,
      "learning_rate": 1.5142808855839109e-05,
      "loss": 0.4266,
      "step": 17000
    },
    {
      "epoch": 2.8152929493545185,
      "grad_norm": 10.2174072265625,
      "learning_rate": 1.5121683285448707e-05,
      "loss": 0.8489,
      "step": 17010
    },
    {
      "epoch": 2.816948030453492,
      "grad_norm": 4.1530442237854,
      "learning_rate": 1.5100557715058309e-05,
      "loss": 0.3915,
      "step": 17020
    },
    {
      "epoch": 2.8186031115524663,
      "grad_norm": 7.028632164001465,
      "learning_rate": 1.5079432144667907e-05,
      "loss": 0.736,
      "step": 17030
    },
    {
      "epoch": 2.82025819265144,
      "grad_norm": 3.938800811767578,
      "learning_rate": 1.5058306574277506e-05,
      "loss": 0.5599,
      "step": 17040
    },
    {
      "epoch": 2.821913273750414,
      "grad_norm": 9.803556442260742,
      "learning_rate": 1.5037181003887104e-05,
      "loss": 0.6955,
      "step": 17050
    },
    {
      "epoch": 2.8235683548493875,
      "grad_norm": 9.909597396850586,
      "learning_rate": 1.5016055433496706e-05,
      "loss": 0.8232,
      "step": 17060
    },
    {
      "epoch": 2.8252234359483612,
      "grad_norm": 8.645647048950195,
      "learning_rate": 1.4994929863106306e-05,
      "loss": 0.5999,
      "step": 17070
    },
    {
      "epoch": 2.8268785170473354,
      "grad_norm": 7.49737548828125,
      "learning_rate": 1.4973804292715904e-05,
      "loss": 0.5105,
      "step": 17080
    },
    {
      "epoch": 2.828533598146309,
      "grad_norm": 2.1718831062316895,
      "learning_rate": 1.4952678722325503e-05,
      "loss": 0.7017,
      "step": 17090
    },
    {
      "epoch": 2.830188679245283,
      "grad_norm": 4.671913146972656,
      "learning_rate": 1.4931553151935104e-05,
      "loss": 0.4551,
      "step": 17100
    },
    {
      "epoch": 2.831843760344257,
      "grad_norm": 4.1305012702941895,
      "learning_rate": 1.4910427581544703e-05,
      "loss": 0.3764,
      "step": 17110
    },
    {
      "epoch": 2.8334988414432307,
      "grad_norm": 6.469846725463867,
      "learning_rate": 1.4889302011154301e-05,
      "loss": 0.5015,
      "step": 17120
    },
    {
      "epoch": 2.8351539225422044,
      "grad_norm": 6.402920246124268,
      "learning_rate": 1.4868176440763901e-05,
      "loss": 0.4984,
      "step": 17130
    },
    {
      "epoch": 2.8368090036411786,
      "grad_norm": 6.771716117858887,
      "learning_rate": 1.48470508703735e-05,
      "loss": 0.6706,
      "step": 17140
    },
    {
      "epoch": 2.8384640847401523,
      "grad_norm": 4.651158332824707,
      "learning_rate": 1.4825925299983101e-05,
      "loss": 0.4182,
      "step": 17150
    },
    {
      "epoch": 2.840119165839126,
      "grad_norm": 9.649710655212402,
      "learning_rate": 1.48047997295927e-05,
      "loss": 0.5581,
      "step": 17160
    },
    {
      "epoch": 2.8417742469381,
      "grad_norm": 11.32800579071045,
      "learning_rate": 1.4783674159202298e-05,
      "loss": 0.7315,
      "step": 17170
    },
    {
      "epoch": 2.843429328037074,
      "grad_norm": 8.450060844421387,
      "learning_rate": 1.4762548588811898e-05,
      "loss": 0.5681,
      "step": 17180
    },
    {
      "epoch": 2.8450844091360477,
      "grad_norm": 3.3775594234466553,
      "learning_rate": 1.4741423018421498e-05,
      "loss": 0.8011,
      "step": 17190
    },
    {
      "epoch": 2.8467394902350214,
      "grad_norm": 3.6714744567871094,
      "learning_rate": 1.4720297448031098e-05,
      "loss": 0.4645,
      "step": 17200
    },
    {
      "epoch": 2.848394571333995,
      "grad_norm": 5.308077335357666,
      "learning_rate": 1.4699171877640697e-05,
      "loss": 0.5576,
      "step": 17210
    },
    {
      "epoch": 2.8500496524329693,
      "grad_norm": 6.998898983001709,
      "learning_rate": 1.4678046307250295e-05,
      "loss": 0.4307,
      "step": 17220
    },
    {
      "epoch": 2.851704733531943,
      "grad_norm": 18.084840774536133,
      "learning_rate": 1.4656920736859897e-05,
      "loss": 0.6369,
      "step": 17230
    },
    {
      "epoch": 2.8533598146309167,
      "grad_norm": 8.155932426452637,
      "learning_rate": 1.4635795166469495e-05,
      "loss": 0.6524,
      "step": 17240
    },
    {
      "epoch": 2.855014895729891,
      "grad_norm": 5.286746978759766,
      "learning_rate": 1.4614669596079095e-05,
      "loss": 0.5306,
      "step": 17250
    },
    {
      "epoch": 2.8566699768288646,
      "grad_norm": 5.178205490112305,
      "learning_rate": 1.4593544025688694e-05,
      "loss": 0.6379,
      "step": 17260
    },
    {
      "epoch": 2.8583250579278383,
      "grad_norm": 3.9355063438415527,
      "learning_rate": 1.4572418455298296e-05,
      "loss": 0.3449,
      "step": 17270
    },
    {
      "epoch": 2.8599801390268125,
      "grad_norm": 2.869802236557007,
      "learning_rate": 1.4551292884907894e-05,
      "loss": 0.6753,
      "step": 17280
    },
    {
      "epoch": 2.861635220125786,
      "grad_norm": 12.687870025634766,
      "learning_rate": 1.4530167314517492e-05,
      "loss": 0.5611,
      "step": 17290
    },
    {
      "epoch": 2.86329030122476,
      "grad_norm": 6.2936015129089355,
      "learning_rate": 1.450904174412709e-05,
      "loss": 0.4187,
      "step": 17300
    },
    {
      "epoch": 2.864945382323734,
      "grad_norm": 4.247826099395752,
      "learning_rate": 1.448791617373669e-05,
      "loss": 0.6027,
      "step": 17310
    },
    {
      "epoch": 2.866600463422708,
      "grad_norm": 7.235470771789551,
      "learning_rate": 1.4466790603346293e-05,
      "loss": 0.5559,
      "step": 17320
    },
    {
      "epoch": 2.8682555445216815,
      "grad_norm": 3.377692699432373,
      "learning_rate": 1.4445665032955891e-05,
      "loss": 0.4414,
      "step": 17330
    },
    {
      "epoch": 2.8699106256206557,
      "grad_norm": 13.617721557617188,
      "learning_rate": 1.442453946256549e-05,
      "loss": 0.5044,
      "step": 17340
    },
    {
      "epoch": 2.8715657067196294,
      "grad_norm": 6.562067031860352,
      "learning_rate": 1.4403413892175088e-05,
      "loss": 0.7206,
      "step": 17350
    },
    {
      "epoch": 2.873220787818603,
      "grad_norm": 3.051948070526123,
      "learning_rate": 1.438228832178469e-05,
      "loss": 0.6761,
      "step": 17360
    },
    {
      "epoch": 2.874875868917577,
      "grad_norm": 2.264538526535034,
      "learning_rate": 1.436116275139429e-05,
      "loss": 0.4543,
      "step": 17370
    },
    {
      "epoch": 2.8765309500165506,
      "grad_norm": 10.290918350219727,
      "learning_rate": 1.4340037181003888e-05,
      "loss": 0.51,
      "step": 17380
    },
    {
      "epoch": 2.8781860311155247,
      "grad_norm": 4.547457695007324,
      "learning_rate": 1.4318911610613486e-05,
      "loss": 0.9171,
      "step": 17390
    },
    {
      "epoch": 2.8798411122144985,
      "grad_norm": 4.098553657531738,
      "learning_rate": 1.4297786040223088e-05,
      "loss": 0.6666,
      "step": 17400
    },
    {
      "epoch": 2.881496193313472,
      "grad_norm": 7.774106025695801,
      "learning_rate": 1.4276660469832686e-05,
      "loss": 0.6088,
      "step": 17410
    },
    {
      "epoch": 2.8831512744124463,
      "grad_norm": 2.2411856651306152,
      "learning_rate": 1.4255534899442285e-05,
      "loss": 0.5024,
      "step": 17420
    },
    {
      "epoch": 2.88480635551142,
      "grad_norm": 5.1504669189453125,
      "learning_rate": 1.4234409329051885e-05,
      "loss": 0.4948,
      "step": 17430
    },
    {
      "epoch": 2.886461436610394,
      "grad_norm": 12.908953666687012,
      "learning_rate": 1.4213283758661487e-05,
      "loss": 0.9542,
      "step": 17440
    },
    {
      "epoch": 2.888116517709368,
      "grad_norm": 8.402814865112305,
      "learning_rate": 1.4192158188271085e-05,
      "loss": 0.5271,
      "step": 17450
    },
    {
      "epoch": 2.8897715988083417,
      "grad_norm": 13.086644172668457,
      "learning_rate": 1.4171032617880683e-05,
      "loss": 0.5924,
      "step": 17460
    },
    {
      "epoch": 2.8914266799073154,
      "grad_norm": 4.338310241699219,
      "learning_rate": 1.4149907047490282e-05,
      "loss": 0.5365,
      "step": 17470
    },
    {
      "epoch": 2.8930817610062896,
      "grad_norm": 14.35244369506836,
      "learning_rate": 1.4128781477099882e-05,
      "loss": 0.8958,
      "step": 17480
    },
    {
      "epoch": 2.8947368421052633,
      "grad_norm": 13.837552070617676,
      "learning_rate": 1.4107655906709482e-05,
      "loss": 0.6631,
      "step": 17490
    },
    {
      "epoch": 2.896391923204237,
      "grad_norm": 4.842273235321045,
      "learning_rate": 1.4086530336319082e-05,
      "loss": 0.5385,
      "step": 17500
    },
    {
      "epoch": 2.8980470043032107,
      "grad_norm": 7.189507961273193,
      "learning_rate": 1.406540476592868e-05,
      "loss": 0.7046,
      "step": 17510
    },
    {
      "epoch": 2.8997020854021844,
      "grad_norm": 4.97965669631958,
      "learning_rate": 1.4044279195538279e-05,
      "loss": 0.3169,
      "step": 17520
    },
    {
      "epoch": 2.9013571665011586,
      "grad_norm": 17.94260597229004,
      "learning_rate": 1.402315362514788e-05,
      "loss": 0.3898,
      "step": 17530
    },
    {
      "epoch": 2.9030122476001323,
      "grad_norm": 9.313467025756836,
      "learning_rate": 1.4002028054757479e-05,
      "loss": 0.6755,
      "step": 17540
    },
    {
      "epoch": 2.904667328699106,
      "grad_norm": 13.83727741241455,
      "learning_rate": 1.3980902484367079e-05,
      "loss": 0.5602,
      "step": 17550
    },
    {
      "epoch": 2.90632240979808,
      "grad_norm": 9.589516639709473,
      "learning_rate": 1.3959776913976677e-05,
      "loss": 0.6381,
      "step": 17560
    },
    {
      "epoch": 2.907977490897054,
      "grad_norm": 7.589547634124756,
      "learning_rate": 1.393865134358628e-05,
      "loss": 0.4646,
      "step": 17570
    },
    {
      "epoch": 2.9096325719960277,
      "grad_norm": 9.021241188049316,
      "learning_rate": 1.3917525773195878e-05,
      "loss": 0.8473,
      "step": 17580
    },
    {
      "epoch": 2.911287653095002,
      "grad_norm": 13.913453102111816,
      "learning_rate": 1.3896400202805476e-05,
      "loss": 0.8002,
      "step": 17590
    },
    {
      "epoch": 2.9129427341939755,
      "grad_norm": 5.968593597412109,
      "learning_rate": 1.3875274632415074e-05,
      "loss": 0.6813,
      "step": 17600
    },
    {
      "epoch": 2.9145978152929493,
      "grad_norm": 6.540733814239502,
      "learning_rate": 1.3854149062024676e-05,
      "loss": 0.5245,
      "step": 17610
    },
    {
      "epoch": 2.9162528963919234,
      "grad_norm": 4.8636698722839355,
      "learning_rate": 1.3833023491634276e-05,
      "loss": 0.4456,
      "step": 17620
    },
    {
      "epoch": 2.917907977490897,
      "grad_norm": 4.935339450836182,
      "learning_rate": 1.3811897921243875e-05,
      "loss": 0.7168,
      "step": 17630
    },
    {
      "epoch": 2.919563058589871,
      "grad_norm": 5.949192047119141,
      "learning_rate": 1.3790772350853473e-05,
      "loss": 0.5655,
      "step": 17640
    },
    {
      "epoch": 2.921218139688845,
      "grad_norm": 5.206032752990723,
      "learning_rate": 1.3769646780463071e-05,
      "loss": 0.5769,
      "step": 17650
    },
    {
      "epoch": 2.9228732207878187,
      "grad_norm": 4.923304557800293,
      "learning_rate": 1.3748521210072673e-05,
      "loss": 0.4811,
      "step": 17660
    },
    {
      "epoch": 2.9245283018867925,
      "grad_norm": 2.197390079498291,
      "learning_rate": 1.3727395639682272e-05,
      "loss": 0.4986,
      "step": 17670
    },
    {
      "epoch": 2.926183382985766,
      "grad_norm": 4.7792439460754395,
      "learning_rate": 1.3706270069291872e-05,
      "loss": 0.5026,
      "step": 17680
    },
    {
      "epoch": 2.92783846408474,
      "grad_norm": 6.622406005859375,
      "learning_rate": 1.368514449890147e-05,
      "loss": 0.7249,
      "step": 17690
    },
    {
      "epoch": 2.929493545183714,
      "grad_norm": 10.401774406433105,
      "learning_rate": 1.3664018928511072e-05,
      "loss": 0.6769,
      "step": 17700
    },
    {
      "epoch": 2.931148626282688,
      "grad_norm": 7.16597843170166,
      "learning_rate": 1.364289335812067e-05,
      "loss": 0.7146,
      "step": 17710
    },
    {
      "epoch": 2.9328037073816615,
      "grad_norm": 4.860462188720703,
      "learning_rate": 1.3621767787730269e-05,
      "loss": 0.8051,
      "step": 17720
    },
    {
      "epoch": 2.9344587884806357,
      "grad_norm": 4.051560878753662,
      "learning_rate": 1.3600642217339869e-05,
      "loss": 0.4289,
      "step": 17730
    },
    {
      "epoch": 2.9361138695796094,
      "grad_norm": 6.464430332183838,
      "learning_rate": 1.3579516646949469e-05,
      "loss": 0.7518,
      "step": 17740
    },
    {
      "epoch": 2.937768950678583,
      "grad_norm": 10.996299743652344,
      "learning_rate": 1.3558391076559069e-05,
      "loss": 0.8066,
      "step": 17750
    },
    {
      "epoch": 2.9394240317775573,
      "grad_norm": 4.9151153564453125,
      "learning_rate": 1.3537265506168667e-05,
      "loss": 0.8896,
      "step": 17760
    },
    {
      "epoch": 2.941079112876531,
      "grad_norm": 3.4187123775482178,
      "learning_rate": 1.3516139935778266e-05,
      "loss": 0.597,
      "step": 17770
    },
    {
      "epoch": 2.9427341939755047,
      "grad_norm": 5.938857555389404,
      "learning_rate": 1.3495014365387867e-05,
      "loss": 0.5885,
      "step": 17780
    },
    {
      "epoch": 2.944389275074479,
      "grad_norm": 11.689529418945312,
      "learning_rate": 1.3473888794997466e-05,
      "loss": 0.5377,
      "step": 17790
    },
    {
      "epoch": 2.9460443561734526,
      "grad_norm": 3.8553647994995117,
      "learning_rate": 1.3452763224607066e-05,
      "loss": 0.7628,
      "step": 17800
    },
    {
      "epoch": 2.9476994372724263,
      "grad_norm": 6.93792200088501,
      "learning_rate": 1.3431637654216664e-05,
      "loss": 0.4854,
      "step": 17810
    },
    {
      "epoch": 2.9493545183714,
      "grad_norm": 8.420942306518555,
      "learning_rate": 1.3410512083826263e-05,
      "loss": 0.5342,
      "step": 17820
    },
    {
      "epoch": 2.951009599470374,
      "grad_norm": 5.260020732879639,
      "learning_rate": 1.3389386513435864e-05,
      "loss": 0.6239,
      "step": 17830
    },
    {
      "epoch": 2.952664680569348,
      "grad_norm": 12.136760711669922,
      "learning_rate": 1.3368260943045463e-05,
      "loss": 0.4768,
      "step": 17840
    },
    {
      "epoch": 2.9543197616683217,
      "grad_norm": 10.508031845092773,
      "learning_rate": 1.3347135372655061e-05,
      "loss": 0.8794,
      "step": 17850
    },
    {
      "epoch": 2.9559748427672954,
      "grad_norm": 5.008006572723389,
      "learning_rate": 1.3326009802264661e-05,
      "loss": 0.4963,
      "step": 17860
    },
    {
      "epoch": 2.9576299238662696,
      "grad_norm": 9.402606010437012,
      "learning_rate": 1.3304884231874263e-05,
      "loss": 0.6491,
      "step": 17870
    },
    {
      "epoch": 2.9592850049652433,
      "grad_norm": 3.882638692855835,
      "learning_rate": 1.3283758661483861e-05,
      "loss": 0.9376,
      "step": 17880
    },
    {
      "epoch": 2.960940086064217,
      "grad_norm": 5.366298675537109,
      "learning_rate": 1.326263309109346e-05,
      "loss": 0.5175,
      "step": 17890
    },
    {
      "epoch": 2.962595167163191,
      "grad_norm": 10.51119613647461,
      "learning_rate": 1.3241507520703058e-05,
      "loss": 0.5297,
      "step": 17900
    },
    {
      "epoch": 2.964250248262165,
      "grad_norm": 3.2816100120544434,
      "learning_rate": 1.322038195031266e-05,
      "loss": 0.4429,
      "step": 17910
    },
    {
      "epoch": 2.9659053293611386,
      "grad_norm": 6.599557876586914,
      "learning_rate": 1.319925637992226e-05,
      "loss": 0.4624,
      "step": 17920
    },
    {
      "epoch": 2.9675604104601128,
      "grad_norm": 15.393393516540527,
      "learning_rate": 1.3178130809531858e-05,
      "loss": 0.569,
      "step": 17930
    },
    {
      "epoch": 2.9692154915590865,
      "grad_norm": 9.438179969787598,
      "learning_rate": 1.3157005239141457e-05,
      "loss": 0.8496,
      "step": 17940
    },
    {
      "epoch": 2.97087057265806,
      "grad_norm": 4.650087833404541,
      "learning_rate": 1.3135879668751058e-05,
      "loss": 0.6265,
      "step": 17950
    },
    {
      "epoch": 2.9725256537570344,
      "grad_norm": 3.430633544921875,
      "learning_rate": 1.3114754098360657e-05,
      "loss": 0.6974,
      "step": 17960
    },
    {
      "epoch": 2.974180734856008,
      "grad_norm": 7.693291187286377,
      "learning_rate": 1.3093628527970255e-05,
      "loss": 0.4088,
      "step": 17970
    },
    {
      "epoch": 2.975835815954982,
      "grad_norm": 1.8986414670944214,
      "learning_rate": 1.3072502957579855e-05,
      "loss": 0.5026,
      "step": 17980
    },
    {
      "epoch": 2.9774908970539555,
      "grad_norm": 10.414414405822754,
      "learning_rate": 1.3051377387189454e-05,
      "loss": 0.6951,
      "step": 17990
    },
    {
      "epoch": 2.9791459781529293,
      "grad_norm": 8.457942962646484,
      "learning_rate": 1.3030251816799055e-05,
      "loss": 0.8649,
      "step": 18000
    },
    {
      "epoch": 2.9808010592519034,
      "grad_norm": 5.515058517456055,
      "learning_rate": 1.3009126246408654e-05,
      "loss": 0.4436,
      "step": 18010
    },
    {
      "epoch": 2.982456140350877,
      "grad_norm": 7.64990758895874,
      "learning_rate": 1.2988000676018252e-05,
      "loss": 0.5761,
      "step": 18020
    },
    {
      "epoch": 2.984111221449851,
      "grad_norm": 10.861041069030762,
      "learning_rate": 1.2966875105627852e-05,
      "loss": 0.2714,
      "step": 18030
    },
    {
      "epoch": 2.985766302548825,
      "grad_norm": 6.967355728149414,
      "learning_rate": 1.2945749535237452e-05,
      "loss": 0.4961,
      "step": 18040
    },
    {
      "epoch": 2.9874213836477987,
      "grad_norm": 8.732219696044922,
      "learning_rate": 1.2924623964847052e-05,
      "loss": 0.7317,
      "step": 18050
    },
    {
      "epoch": 2.9890764647467725,
      "grad_norm": 9.937785148620605,
      "learning_rate": 1.290349839445665e-05,
      "loss": 0.5418,
      "step": 18060
    },
    {
      "epoch": 2.9907315458457466,
      "grad_norm": 13.13371467590332,
      "learning_rate": 1.288237282406625e-05,
      "loss": 0.6796,
      "step": 18070
    },
    {
      "epoch": 2.9923866269447204,
      "grad_norm": 3.9478628635406494,
      "learning_rate": 1.2861247253675851e-05,
      "loss": 0.6808,
      "step": 18080
    },
    {
      "epoch": 2.994041708043694,
      "grad_norm": 8.117042541503906,
      "learning_rate": 1.284012168328545e-05,
      "loss": 0.4556,
      "step": 18090
    },
    {
      "epoch": 2.9956967891426682,
      "grad_norm": 8.547276496887207,
      "learning_rate": 1.281899611289505e-05,
      "loss": 0.8324,
      "step": 18100
    },
    {
      "epoch": 2.997351870241642,
      "grad_norm": 5.640383243560791,
      "learning_rate": 1.2797870542504648e-05,
      "loss": 0.7837,
      "step": 18110
    },
    {
      "epoch": 2.9990069513406157,
      "grad_norm": 10.07026481628418,
      "learning_rate": 1.277674497211425e-05,
      "loss": 0.7155,
      "step": 18120
    },
    {
      "epoch": 3.0006620324395894,
      "grad_norm": 14.859984397888184,
      "learning_rate": 1.2755619401723848e-05,
      "loss": 0.6331,
      "step": 18130
    },
    {
      "epoch": 3.0023171135385636,
      "grad_norm": 10.048537254333496,
      "learning_rate": 1.2734493831333446e-05,
      "loss": 0.5204,
      "step": 18140
    },
    {
      "epoch": 3.0039721946375373,
      "grad_norm": 5.660069465637207,
      "learning_rate": 1.2713368260943045e-05,
      "loss": 0.3727,
      "step": 18150
    },
    {
      "epoch": 3.005627275736511,
      "grad_norm": 6.531924724578857,
      "learning_rate": 1.2692242690552645e-05,
      "loss": 0.5919,
      "step": 18160
    },
    {
      "epoch": 3.0072823568354847,
      "grad_norm": 14.942354202270508,
      "learning_rate": 1.2671117120162247e-05,
      "loss": 0.6463,
      "step": 18170
    },
    {
      "epoch": 3.008937437934459,
      "grad_norm": 13.427783966064453,
      "learning_rate": 1.2649991549771845e-05,
      "loss": 0.6802,
      "step": 18180
    },
    {
      "epoch": 3.0105925190334326,
      "grad_norm": 10.87596321105957,
      "learning_rate": 1.2628865979381443e-05,
      "loss": 0.6359,
      "step": 18190
    },
    {
      "epoch": 3.0122476001324063,
      "grad_norm": 6.882893085479736,
      "learning_rate": 1.2607740408991042e-05,
      "loss": 0.4766,
      "step": 18200
    },
    {
      "epoch": 3.0139026812313805,
      "grad_norm": 1.4444200992584229,
      "learning_rate": 1.2586614838600644e-05,
      "loss": 0.4633,
      "step": 18210
    },
    {
      "epoch": 3.015557762330354,
      "grad_norm": 8.243524551391602,
      "learning_rate": 1.2565489268210242e-05,
      "loss": 0.7191,
      "step": 18220
    },
    {
      "epoch": 3.017212843429328,
      "grad_norm": 4.46786642074585,
      "learning_rate": 1.2544363697819842e-05,
      "loss": 0.4941,
      "step": 18230
    },
    {
      "epoch": 3.018867924528302,
      "grad_norm": 6.232215404510498,
      "learning_rate": 1.252323812742944e-05,
      "loss": 0.5978,
      "step": 18240
    },
    {
      "epoch": 3.020523005627276,
      "grad_norm": 7.431605339050293,
      "learning_rate": 1.2502112557039042e-05,
      "loss": 0.7081,
      "step": 18250
    },
    {
      "epoch": 3.0221780867262495,
      "grad_norm": 12.513487815856934,
      "learning_rate": 1.248098698664864e-05,
      "loss": 0.689,
      "step": 18260
    },
    {
      "epoch": 3.0238331678252233,
      "grad_norm": 3.600874423980713,
      "learning_rate": 1.2459861416258239e-05,
      "loss": 0.4513,
      "step": 18270
    },
    {
      "epoch": 3.0254882489241974,
      "grad_norm": 0.9117813110351562,
      "learning_rate": 1.2438735845867839e-05,
      "loss": 0.3991,
      "step": 18280
    },
    {
      "epoch": 3.027143330023171,
      "grad_norm": 9.915124893188477,
      "learning_rate": 1.2417610275477439e-05,
      "loss": 0.7596,
      "step": 18290
    },
    {
      "epoch": 3.028798411122145,
      "grad_norm": 6.210230350494385,
      "learning_rate": 1.2396484705087039e-05,
      "loss": 0.6106,
      "step": 18300
    },
    {
      "epoch": 3.030453492221119,
      "grad_norm": 9.071185111999512,
      "learning_rate": 1.2375359134696637e-05,
      "loss": 0.5546,
      "step": 18310
    },
    {
      "epoch": 3.0321085733200928,
      "grad_norm": 9.141032218933105,
      "learning_rate": 1.2354233564306236e-05,
      "loss": 0.4761,
      "step": 18320
    },
    {
      "epoch": 3.0337636544190665,
      "grad_norm": 3.683849811553955,
      "learning_rate": 1.2333107993915836e-05,
      "loss": 0.6128,
      "step": 18330
    },
    {
      "epoch": 3.03541873551804,
      "grad_norm": 3.7176241874694824,
      "learning_rate": 1.2311982423525436e-05,
      "loss": 0.5605,
      "step": 18340
    },
    {
      "epoch": 3.0370738166170144,
      "grad_norm": 6.863282203674316,
      "learning_rate": 1.2290856853135036e-05,
      "loss": 0.5098,
      "step": 18350
    },
    {
      "epoch": 3.038728897715988,
      "grad_norm": 6.193714618682861,
      "learning_rate": 1.2269731282744634e-05,
      "loss": 0.4105,
      "step": 18360
    },
    {
      "epoch": 3.040383978814962,
      "grad_norm": 8.297868728637695,
      "learning_rate": 1.2248605712354235e-05,
      "loss": 0.5606,
      "step": 18370
    },
    {
      "epoch": 3.042039059913936,
      "grad_norm": 5.58281135559082,
      "learning_rate": 1.2227480141963833e-05,
      "loss": 0.5453,
      "step": 18380
    },
    {
      "epoch": 3.0436941410129097,
      "grad_norm": 3.2200894355773926,
      "learning_rate": 1.2206354571573433e-05,
      "loss": 0.5924,
      "step": 18390
    },
    {
      "epoch": 3.0453492221118834,
      "grad_norm": 11.31961441040039,
      "learning_rate": 1.2185229001183031e-05,
      "loss": 0.7456,
      "step": 18400
    },
    {
      "epoch": 3.047004303210857,
      "grad_norm": 4.350691795349121,
      "learning_rate": 1.2164103430792633e-05,
      "loss": 0.5185,
      "step": 18410
    },
    {
      "epoch": 3.0486593843098313,
      "grad_norm": 6.40102481842041,
      "learning_rate": 1.2142977860402232e-05,
      "loss": 0.4303,
      "step": 18420
    },
    {
      "epoch": 3.050314465408805,
      "grad_norm": 9.720788955688477,
      "learning_rate": 1.2121852290011832e-05,
      "loss": 0.5724,
      "step": 18430
    },
    {
      "epoch": 3.0519695465077787,
      "grad_norm": 11.256665229797363,
      "learning_rate": 1.210072671962143e-05,
      "loss": 0.732,
      "step": 18440
    },
    {
      "epoch": 3.053624627606753,
      "grad_norm": 5.515401363372803,
      "learning_rate": 1.207960114923103e-05,
      "loss": 0.6518,
      "step": 18450
    },
    {
      "epoch": 3.0552797087057266,
      "grad_norm": 12.55871295928955,
      "learning_rate": 1.2058475578840628e-05,
      "loss": 0.6651,
      "step": 18460
    },
    {
      "epoch": 3.0569347898047003,
      "grad_norm": 11.651174545288086,
      "learning_rate": 1.203735000845023e-05,
      "loss": 0.6581,
      "step": 18470
    },
    {
      "epoch": 3.058589870903674,
      "grad_norm": 8.70408821105957,
      "learning_rate": 1.2016224438059829e-05,
      "loss": 0.3229,
      "step": 18480
    },
    {
      "epoch": 3.0602449520026482,
      "grad_norm": 9.86813735961914,
      "learning_rate": 1.1995098867669427e-05,
      "loss": 0.7924,
      "step": 18490
    },
    {
      "epoch": 3.061900033101622,
      "grad_norm": 4.198465347290039,
      "learning_rate": 1.1973973297279027e-05,
      "loss": 0.6694,
      "step": 18500
    },
    {
      "epoch": 3.0635551142005957,
      "grad_norm": 5.2534589767456055,
      "learning_rate": 1.1952847726888625e-05,
      "loss": 0.6536,
      "step": 18510
    },
    {
      "epoch": 3.06521019529957,
      "grad_norm": 4.051943302154541,
      "learning_rate": 1.1931722156498226e-05,
      "loss": 0.4619,
      "step": 18520
    },
    {
      "epoch": 3.0668652763985436,
      "grad_norm": 7.02269172668457,
      "learning_rate": 1.1910596586107826e-05,
      "loss": 0.7621,
      "step": 18530
    },
    {
      "epoch": 3.0685203574975173,
      "grad_norm": 3.760266065597534,
      "learning_rate": 1.1889471015717426e-05,
      "loss": 0.5283,
      "step": 18540
    },
    {
      "epoch": 3.0701754385964914,
      "grad_norm": 1.5719529390335083,
      "learning_rate": 1.1868345445327024e-05,
      "loss": 0.4441,
      "step": 18550
    },
    {
      "epoch": 3.071830519695465,
      "grad_norm": 4.671748161315918,
      "learning_rate": 1.1847219874936624e-05,
      "loss": 0.455,
      "step": 18560
    },
    {
      "epoch": 3.073485600794439,
      "grad_norm": 6.584219932556152,
      "learning_rate": 1.1826094304546223e-05,
      "loss": 0.541,
      "step": 18570
    },
    {
      "epoch": 3.0751406818934126,
      "grad_norm": 7.882304668426514,
      "learning_rate": 1.1804968734155823e-05,
      "loss": 0.5215,
      "step": 18580
    },
    {
      "epoch": 3.0767957629923868,
      "grad_norm": 5.058819770812988,
      "learning_rate": 1.1783843163765423e-05,
      "loss": 0.7977,
      "step": 18590
    },
    {
      "epoch": 3.0784508440913605,
      "grad_norm": 7.846408843994141,
      "learning_rate": 1.1762717593375023e-05,
      "loss": 0.5647,
      "step": 18600
    },
    {
      "epoch": 3.080105925190334,
      "grad_norm": 3.0160601139068604,
      "learning_rate": 1.1741592022984621e-05,
      "loss": 0.6732,
      "step": 18610
    },
    {
      "epoch": 3.0817610062893084,
      "grad_norm": 14.536540031433105,
      "learning_rate": 1.1720466452594221e-05,
      "loss": 0.6428,
      "step": 18620
    },
    {
      "epoch": 3.083416087388282,
      "grad_norm": 11.375365257263184,
      "learning_rate": 1.169934088220382e-05,
      "loss": 0.6209,
      "step": 18630
    },
    {
      "epoch": 3.085071168487256,
      "grad_norm": 6.077273845672607,
      "learning_rate": 1.167821531181342e-05,
      "loss": 0.8561,
      "step": 18640
    },
    {
      "epoch": 3.0867262495862295,
      "grad_norm": 12.985930442810059,
      "learning_rate": 1.165708974142302e-05,
      "loss": 0.7972,
      "step": 18650
    },
    {
      "epoch": 3.0883813306852037,
      "grad_norm": 4.556812763214111,
      "learning_rate": 1.1635964171032618e-05,
      "loss": 0.5687,
      "step": 18660
    },
    {
      "epoch": 3.0900364117841774,
      "grad_norm": 2.249073028564453,
      "learning_rate": 1.1614838600642218e-05,
      "loss": 0.7251,
      "step": 18670
    },
    {
      "epoch": 3.091691492883151,
      "grad_norm": 13.209188461303711,
      "learning_rate": 1.1593713030251817e-05,
      "loss": 0.916,
      "step": 18680
    },
    {
      "epoch": 3.0933465739821253,
      "grad_norm": 0.6501002907752991,
      "learning_rate": 1.1572587459861417e-05,
      "loss": 0.7259,
      "step": 18690
    },
    {
      "epoch": 3.095001655081099,
      "grad_norm": 6.909213066101074,
      "learning_rate": 1.1551461889471015e-05,
      "loss": 0.6059,
      "step": 18700
    },
    {
      "epoch": 3.0966567361800728,
      "grad_norm": 12.572071075439453,
      "learning_rate": 1.1530336319080615e-05,
      "loss": 0.4563,
      "step": 18710
    },
    {
      "epoch": 3.0983118172790465,
      "grad_norm": 7.339498996734619,
      "learning_rate": 1.1509210748690215e-05,
      "loss": 0.4338,
      "step": 18720
    },
    {
      "epoch": 3.0999668983780206,
      "grad_norm": 13.23297119140625,
      "learning_rate": 1.1488085178299815e-05,
      "loss": 0.7858,
      "step": 18730
    },
    {
      "epoch": 3.1016219794769944,
      "grad_norm": 9.619379997253418,
      "learning_rate": 1.1466959607909414e-05,
      "loss": 0.7743,
      "step": 18740
    },
    {
      "epoch": 3.103277060575968,
      "grad_norm": 12.079302787780762,
      "learning_rate": 1.1445834037519014e-05,
      "loss": 0.4257,
      "step": 18750
    },
    {
      "epoch": 3.1049321416749422,
      "grad_norm": 9.065779685974121,
      "learning_rate": 1.1424708467128612e-05,
      "loss": 0.5398,
      "step": 18760
    },
    {
      "epoch": 3.106587222773916,
      "grad_norm": 6.488968372344971,
      "learning_rate": 1.1403582896738212e-05,
      "loss": 0.6156,
      "step": 18770
    },
    {
      "epoch": 3.1082423038728897,
      "grad_norm": 8.283015251159668,
      "learning_rate": 1.1382457326347812e-05,
      "loss": 0.8224,
      "step": 18780
    },
    {
      "epoch": 3.1098973849718634,
      "grad_norm": 7.38850212097168,
      "learning_rate": 1.1361331755957412e-05,
      "loss": 0.375,
      "step": 18790
    },
    {
      "epoch": 3.1115524660708376,
      "grad_norm": 8.848690032958984,
      "learning_rate": 1.134020618556701e-05,
      "loss": 0.6519,
      "step": 18800
    },
    {
      "epoch": 3.1132075471698113,
      "grad_norm": 3.5506718158721924,
      "learning_rate": 1.1319080615176611e-05,
      "loss": 0.7848,
      "step": 18810
    },
    {
      "epoch": 3.114862628268785,
      "grad_norm": 10.730677604675293,
      "learning_rate": 1.129795504478621e-05,
      "loss": 0.6205,
      "step": 18820
    },
    {
      "epoch": 3.116517709367759,
      "grad_norm": 3.6007230281829834,
      "learning_rate": 1.127682947439581e-05,
      "loss": 0.5529,
      "step": 18830
    },
    {
      "epoch": 3.118172790466733,
      "grad_norm": 5.2450432777404785,
      "learning_rate": 1.125570390400541e-05,
      "loss": 0.6836,
      "step": 18840
    },
    {
      "epoch": 3.1198278715657066,
      "grad_norm": 14.437126159667969,
      "learning_rate": 1.1234578333615008e-05,
      "loss": 0.3977,
      "step": 18850
    },
    {
      "epoch": 3.121482952664681,
      "grad_norm": 5.637230396270752,
      "learning_rate": 1.1213452763224608e-05,
      "loss": 0.6756,
      "step": 18860
    },
    {
      "epoch": 3.1231380337636545,
      "grad_norm": 3.1200664043426514,
      "learning_rate": 1.1192327192834206e-05,
      "loss": 0.6189,
      "step": 18870
    },
    {
      "epoch": 3.1247931148626282,
      "grad_norm": 15.184192657470703,
      "learning_rate": 1.1171201622443806e-05,
      "loss": 0.7703,
      "step": 18880
    },
    {
      "epoch": 3.126448195961602,
      "grad_norm": 7.077007293701172,
      "learning_rate": 1.1150076052053406e-05,
      "loss": 0.6245,
      "step": 18890
    },
    {
      "epoch": 3.128103277060576,
      "grad_norm": 1.612727165222168,
      "learning_rate": 1.1128950481663006e-05,
      "loss": 0.3777,
      "step": 18900
    },
    {
      "epoch": 3.12975835815955,
      "grad_norm": 7.051661014556885,
      "learning_rate": 1.1107824911272605e-05,
      "loss": 0.3712,
      "step": 18910
    },
    {
      "epoch": 3.1314134392585236,
      "grad_norm": 11.638951301574707,
      "learning_rate": 1.1086699340882205e-05,
      "loss": 0.7742,
      "step": 18920
    },
    {
      "epoch": 3.1330685203574977,
      "grad_norm": 9.782147407531738,
      "learning_rate": 1.1065573770491803e-05,
      "loss": 0.6012,
      "step": 18930
    },
    {
      "epoch": 3.1347236014564714,
      "grad_norm": 5.997650146484375,
      "learning_rate": 1.1044448200101403e-05,
      "loss": 0.6334,
      "step": 18940
    },
    {
      "epoch": 3.136378682555445,
      "grad_norm": 13.910616874694824,
      "learning_rate": 1.1023322629711002e-05,
      "loss": 0.6394,
      "step": 18950
    },
    {
      "epoch": 3.138033763654419,
      "grad_norm": 4.29095458984375,
      "learning_rate": 1.1002197059320604e-05,
      "loss": 0.5083,
      "step": 18960
    },
    {
      "epoch": 3.139688844753393,
      "grad_norm": 11.419568061828613,
      "learning_rate": 1.0981071488930202e-05,
      "loss": 0.7263,
      "step": 18970
    },
    {
      "epoch": 3.1413439258523668,
      "grad_norm": 2.2471861839294434,
      "learning_rate": 1.0959945918539802e-05,
      "loss": 0.3668,
      "step": 18980
    },
    {
      "epoch": 3.1429990069513405,
      "grad_norm": 8.860939025878906,
      "learning_rate": 1.09388203481494e-05,
      "loss": 0.632,
      "step": 18990
    },
    {
      "epoch": 3.1446540880503147,
      "grad_norm": 16.50090789794922,
      "learning_rate": 1.0917694777758999e-05,
      "loss": 0.5079,
      "step": 19000
    },
    {
      "epoch": 3.1463091691492884,
      "grad_norm": 9.035608291625977,
      "learning_rate": 1.0896569207368599e-05,
      "loss": 0.9282,
      "step": 19010
    },
    {
      "epoch": 3.147964250248262,
      "grad_norm": 8.36648941040039,
      "learning_rate": 1.0875443636978199e-05,
      "loss": 0.4984,
      "step": 19020
    },
    {
      "epoch": 3.149619331347236,
      "grad_norm": 4.511916160583496,
      "learning_rate": 1.0854318066587799e-05,
      "loss": 0.4513,
      "step": 19030
    },
    {
      "epoch": 3.15127441244621,
      "grad_norm": 3.4084243774414062,
      "learning_rate": 1.0833192496197397e-05,
      "loss": 0.6463,
      "step": 19040
    },
    {
      "epoch": 3.1529294935451837,
      "grad_norm": 8.489219665527344,
      "learning_rate": 1.0812066925806997e-05,
      "loss": 0.6113,
      "step": 19050
    },
    {
      "epoch": 3.1545845746441574,
      "grad_norm": 16.279155731201172,
      "learning_rate": 1.0790941355416596e-05,
      "loss": 0.5893,
      "step": 19060
    },
    {
      "epoch": 3.1562396557431316,
      "grad_norm": 7.679905414581299,
      "learning_rate": 1.0769815785026196e-05,
      "loss": 0.4785,
      "step": 19070
    },
    {
      "epoch": 3.1578947368421053,
      "grad_norm": 9.176420211791992,
      "learning_rate": 1.0748690214635796e-05,
      "loss": 0.5804,
      "step": 19080
    },
    {
      "epoch": 3.159549817941079,
      "grad_norm": 9.517733573913574,
      "learning_rate": 1.0727564644245396e-05,
      "loss": 0.7257,
      "step": 19090
    },
    {
      "epoch": 3.1612048990400528,
      "grad_norm": 6.382286548614502,
      "learning_rate": 1.0706439073854994e-05,
      "loss": 0.4714,
      "step": 19100
    },
    {
      "epoch": 3.162859980139027,
      "grad_norm": 6.712212562561035,
      "learning_rate": 1.0685313503464595e-05,
      "loss": 0.6443,
      "step": 19110
    },
    {
      "epoch": 3.1645150612380006,
      "grad_norm": 7.0193281173706055,
      "learning_rate": 1.0664187933074193e-05,
      "loss": 0.6299,
      "step": 19120
    },
    {
      "epoch": 3.1661701423369744,
      "grad_norm": 10.192673683166504,
      "learning_rate": 1.0643062362683793e-05,
      "loss": 0.586,
      "step": 19130
    },
    {
      "epoch": 3.1678252234359485,
      "grad_norm": 3.9018373489379883,
      "learning_rate": 1.0621936792293393e-05,
      "loss": 0.5301,
      "step": 19140
    },
    {
      "epoch": 3.1694803045349222,
      "grad_norm": 3.4676249027252197,
      "learning_rate": 1.0600811221902991e-05,
      "loss": 0.4521,
      "step": 19150
    },
    {
      "epoch": 3.171135385633896,
      "grad_norm": 7.5163445472717285,
      "learning_rate": 1.0579685651512592e-05,
      "loss": 0.737,
      "step": 19160
    },
    {
      "epoch": 3.17279046673287,
      "grad_norm": 3.618809938430786,
      "learning_rate": 1.055856008112219e-05,
      "loss": 0.6325,
      "step": 19170
    },
    {
      "epoch": 3.174445547831844,
      "grad_norm": 9.184440612792969,
      "learning_rate": 1.053743451073179e-05,
      "loss": 0.4444,
      "step": 19180
    },
    {
      "epoch": 3.1761006289308176,
      "grad_norm": 4.22422981262207,
      "learning_rate": 1.0516308940341388e-05,
      "loss": 0.5541,
      "step": 19190
    },
    {
      "epoch": 3.1777557100297913,
      "grad_norm": 9.66331672668457,
      "learning_rate": 1.049518336995099e-05,
      "loss": 0.8359,
      "step": 19200
    },
    {
      "epoch": 3.1794107911287655,
      "grad_norm": 3.909930944442749,
      "learning_rate": 1.0474057799560589e-05,
      "loss": 0.5836,
      "step": 19210
    },
    {
      "epoch": 3.181065872227739,
      "grad_norm": 14.456399917602539,
      "learning_rate": 1.0452932229170189e-05,
      "loss": 0.4482,
      "step": 19220
    },
    {
      "epoch": 3.182720953326713,
      "grad_norm": 4.023258209228516,
      "learning_rate": 1.0431806658779787e-05,
      "loss": 0.5383,
      "step": 19230
    },
    {
      "epoch": 3.184376034425687,
      "grad_norm": 19.5091552734375,
      "learning_rate": 1.0410681088389387e-05,
      "loss": 0.481,
      "step": 19240
    },
    {
      "epoch": 3.186031115524661,
      "grad_norm": 6.510165691375732,
      "learning_rate": 1.0389555517998985e-05,
      "loss": 0.6947,
      "step": 19250
    },
    {
      "epoch": 3.1876861966236345,
      "grad_norm": 7.7587175369262695,
      "learning_rate": 1.0368429947608586e-05,
      "loss": 0.8252,
      "step": 19260
    },
    {
      "epoch": 3.1893412777226082,
      "grad_norm": 6.535053253173828,
      "learning_rate": 1.0347304377218186e-05,
      "loss": 0.509,
      "step": 19270
    },
    {
      "epoch": 3.1909963588215824,
      "grad_norm": 2.018087387084961,
      "learning_rate": 1.0326178806827786e-05,
      "loss": 0.4839,
      "step": 19280
    },
    {
      "epoch": 3.192651439920556,
      "grad_norm": 9.005183219909668,
      "learning_rate": 1.0305053236437384e-05,
      "loss": 0.6313,
      "step": 19290
    },
    {
      "epoch": 3.19430652101953,
      "grad_norm": 2.466295003890991,
      "learning_rate": 1.0283927666046984e-05,
      "loss": 0.363,
      "step": 19300
    },
    {
      "epoch": 3.195961602118504,
      "grad_norm": 10.102276802062988,
      "learning_rate": 1.0262802095656583e-05,
      "loss": 0.7351,
      "step": 19310
    },
    {
      "epoch": 3.1976166832174777,
      "grad_norm": 4.598213195800781,
      "learning_rate": 1.0241676525266183e-05,
      "loss": 0.7274,
      "step": 19320
    },
    {
      "epoch": 3.1992717643164514,
      "grad_norm": 17.13178253173828,
      "learning_rate": 1.0220550954875783e-05,
      "loss": 0.4895,
      "step": 19330
    },
    {
      "epoch": 3.200926845415425,
      "grad_norm": 7.221676349639893,
      "learning_rate": 1.0199425384485381e-05,
      "loss": 0.544,
      "step": 19340
    },
    {
      "epoch": 3.2025819265143993,
      "grad_norm": 17.407075881958008,
      "learning_rate": 1.0178299814094981e-05,
      "loss": 0.7561,
      "step": 19350
    },
    {
      "epoch": 3.204237007613373,
      "grad_norm": 13.726116180419922,
      "learning_rate": 1.015717424370458e-05,
      "loss": 0.7373,
      "step": 19360
    },
    {
      "epoch": 3.2058920887123468,
      "grad_norm": 4.748294830322266,
      "learning_rate": 1.013604867331418e-05,
      "loss": 0.3904,
      "step": 19370
    },
    {
      "epoch": 3.207547169811321,
      "grad_norm": 3.191178321838379,
      "learning_rate": 1.011492310292378e-05,
      "loss": 0.5004,
      "step": 19380
    },
    {
      "epoch": 3.2092022509102947,
      "grad_norm": 1.5439904928207397,
      "learning_rate": 1.009379753253338e-05,
      "loss": 0.3574,
      "step": 19390
    },
    {
      "epoch": 3.2108573320092684,
      "grad_norm": 0.8142011761665344,
      "learning_rate": 1.0072671962142978e-05,
      "loss": 0.637,
      "step": 19400
    },
    {
      "epoch": 3.212512413108242,
      "grad_norm": 9.564130783081055,
      "learning_rate": 1.0051546391752578e-05,
      "loss": 0.5635,
      "step": 19410
    },
    {
      "epoch": 3.2141674942072163,
      "grad_norm": 5.706026554107666,
      "learning_rate": 1.0030420821362177e-05,
      "loss": 0.5643,
      "step": 19420
    },
    {
      "epoch": 3.21582257530619,
      "grad_norm": 7.914167404174805,
      "learning_rate": 1.0009295250971777e-05,
      "loss": 0.4064,
      "step": 19430
    },
    {
      "epoch": 3.2174776564051637,
      "grad_norm": 7.128412246704102,
      "learning_rate": 9.988169680581377e-06,
      "loss": 0.7413,
      "step": 19440
    },
    {
      "epoch": 3.219132737504138,
      "grad_norm": 9.852913856506348,
      "learning_rate": 9.967044110190977e-06,
      "loss": 0.5447,
      "step": 19450
    },
    {
      "epoch": 3.2207878186031116,
      "grad_norm": 7.920624256134033,
      "learning_rate": 9.945918539800575e-06,
      "loss": 0.5853,
      "step": 19460
    },
    {
      "epoch": 3.2224428997020853,
      "grad_norm": 4.507227897644043,
      "learning_rate": 9.924792969410175e-06,
      "loss": 0.5014,
      "step": 19470
    },
    {
      "epoch": 3.2240979808010595,
      "grad_norm": 11.758011817932129,
      "learning_rate": 9.903667399019774e-06,
      "loss": 0.5197,
      "step": 19480
    },
    {
      "epoch": 3.225753061900033,
      "grad_norm": 13.390951156616211,
      "learning_rate": 9.882541828629372e-06,
      "loss": 0.579,
      "step": 19490
    },
    {
      "epoch": 3.227408142999007,
      "grad_norm": 7.964306831359863,
      "learning_rate": 9.861416258238972e-06,
      "loss": 0.5781,
      "step": 19500
    },
    {
      "epoch": 3.2290632240979806,
      "grad_norm": 7.249353885650635,
      "learning_rate": 9.840290687848572e-06,
      "loss": 0.3612,
      "step": 19510
    },
    {
      "epoch": 3.230718305196955,
      "grad_norm": 5.860542297363281,
      "learning_rate": 9.819165117458172e-06,
      "loss": 0.3924,
      "step": 19520
    },
    {
      "epoch": 3.2323733862959285,
      "grad_norm": 1.9138540029525757,
      "learning_rate": 9.79803954706777e-06,
      "loss": 0.7119,
      "step": 19530
    },
    {
      "epoch": 3.2340284673949022,
      "grad_norm": 9.581950187683105,
      "learning_rate": 9.77691397667737e-06,
      "loss": 0.6475,
      "step": 19540
    },
    {
      "epoch": 3.2356835484938764,
      "grad_norm": 6.797519683837891,
      "learning_rate": 9.75578840628697e-06,
      "loss": 0.7145,
      "step": 19550
    },
    {
      "epoch": 3.23733862959285,
      "grad_norm": 5.718178749084473,
      "learning_rate": 9.73466283589657e-06,
      "loss": 0.5399,
      "step": 19560
    },
    {
      "epoch": 3.238993710691824,
      "grad_norm": 5.6392903327941895,
      "learning_rate": 9.71353726550617e-06,
      "loss": 0.4227,
      "step": 19570
    },
    {
      "epoch": 3.2406487917907976,
      "grad_norm": 14.512079238891602,
      "learning_rate": 9.69241169511577e-06,
      "loss": 0.594,
      "step": 19580
    },
    {
      "epoch": 3.2423038728897717,
      "grad_norm": 11.52888011932373,
      "learning_rate": 9.671286124725368e-06,
      "loss": 0.4267,
      "step": 19590
    },
    {
      "epoch": 3.2439589539887455,
      "grad_norm": 13.823573112487793,
      "learning_rate": 9.650160554334968e-06,
      "loss": 0.5101,
      "step": 19600
    },
    {
      "epoch": 3.245614035087719,
      "grad_norm": 13.843592643737793,
      "learning_rate": 9.629034983944566e-06,
      "loss": 0.7587,
      "step": 19610
    },
    {
      "epoch": 3.2472691161866933,
      "grad_norm": 3.0400848388671875,
      "learning_rate": 9.607909413554166e-06,
      "loss": 0.5656,
      "step": 19620
    },
    {
      "epoch": 3.248924197285667,
      "grad_norm": 11.05147647857666,
      "learning_rate": 9.586783843163766e-06,
      "loss": 0.8327,
      "step": 19630
    },
    {
      "epoch": 3.250579278384641,
      "grad_norm": 6.550338268280029,
      "learning_rate": 9.565658272773366e-06,
      "loss": 0.6785,
      "step": 19640
    },
    {
      "epoch": 3.252234359483615,
      "grad_norm": 13.40648078918457,
      "learning_rate": 9.544532702382965e-06,
      "loss": 0.9156,
      "step": 19650
    },
    {
      "epoch": 3.2538894405825887,
      "grad_norm": 10.27076530456543,
      "learning_rate": 9.523407131992563e-06,
      "loss": 0.4886,
      "step": 19660
    },
    {
      "epoch": 3.2555445216815624,
      "grad_norm": 4.484545707702637,
      "learning_rate": 9.502281561602163e-06,
      "loss": 0.6753,
      "step": 19670
    },
    {
      "epoch": 3.257199602780536,
      "grad_norm": 6.648712158203125,
      "learning_rate": 9.481155991211763e-06,
      "loss": 0.6138,
      "step": 19680
    },
    {
      "epoch": 3.2588546838795103,
      "grad_norm": 9.544949531555176,
      "learning_rate": 9.460030420821363e-06,
      "loss": 0.6221,
      "step": 19690
    },
    {
      "epoch": 3.260509764978484,
      "grad_norm": 10.494659423828125,
      "learning_rate": 9.438904850430962e-06,
      "loss": 0.5722,
      "step": 19700
    },
    {
      "epoch": 3.2621648460774577,
      "grad_norm": 2.522632122039795,
      "learning_rate": 9.417779280040562e-06,
      "loss": 0.4902,
      "step": 19710
    },
    {
      "epoch": 3.2638199271764314,
      "grad_norm": 1.5148251056671143,
      "learning_rate": 9.39665370965016e-06,
      "loss": 0.5438,
      "step": 19720
    },
    {
      "epoch": 3.2654750082754056,
      "grad_norm": 3.5323643684387207,
      "learning_rate": 9.37552813925976e-06,
      "loss": 0.6897,
      "step": 19730
    },
    {
      "epoch": 3.2671300893743793,
      "grad_norm": 7.02012825012207,
      "learning_rate": 9.354402568869359e-06,
      "loss": 0.4028,
      "step": 19740
    },
    {
      "epoch": 3.268785170473353,
      "grad_norm": 3.263869524002075,
      "learning_rate": 9.33327699847896e-06,
      "loss": 0.3911,
      "step": 19750
    },
    {
      "epoch": 3.270440251572327,
      "grad_norm": 7.484622478485107,
      "learning_rate": 9.312151428088559e-06,
      "loss": 0.7506,
      "step": 19760
    },
    {
      "epoch": 3.272095332671301,
      "grad_norm": 8.309547424316406,
      "learning_rate": 9.291025857698159e-06,
      "loss": 0.6793,
      "step": 19770
    },
    {
      "epoch": 3.2737504137702746,
      "grad_norm": 11.825336456298828,
      "learning_rate": 9.269900287307757e-06,
      "loss": 0.5868,
      "step": 19780
    },
    {
      "epoch": 3.275405494869249,
      "grad_norm": 3.9188435077667236,
      "learning_rate": 9.248774716917357e-06,
      "loss": 0.5421,
      "step": 19790
    },
    {
      "epoch": 3.2770605759682225,
      "grad_norm": 4.631619453430176,
      "learning_rate": 9.227649146526956e-06,
      "loss": 0.5183,
      "step": 19800
    },
    {
      "epoch": 3.2787156570671963,
      "grad_norm": 6.910606861114502,
      "learning_rate": 9.206523576136556e-06,
      "loss": 0.8671,
      "step": 19810
    },
    {
      "epoch": 3.28037073816617,
      "grad_norm": 1.3257712125778198,
      "learning_rate": 9.185398005746156e-06,
      "loss": 0.7748,
      "step": 19820
    },
    {
      "epoch": 3.282025819265144,
      "grad_norm": 4.972503662109375,
      "learning_rate": 9.164272435355754e-06,
      "loss": 0.5683,
      "step": 19830
    },
    {
      "epoch": 3.283680900364118,
      "grad_norm": 10.157594680786133,
      "learning_rate": 9.143146864965354e-06,
      "loss": 0.7054,
      "step": 19840
    },
    {
      "epoch": 3.2853359814630916,
      "grad_norm": 6.376071453094482,
      "learning_rate": 9.122021294574953e-06,
      "loss": 0.5337,
      "step": 19850
    },
    {
      "epoch": 3.2869910625620653,
      "grad_norm": 9.51389217376709,
      "learning_rate": 9.100895724184553e-06,
      "loss": 0.8264,
      "step": 19860
    },
    {
      "epoch": 3.2886461436610395,
      "grad_norm": 6.961690425872803,
      "learning_rate": 9.079770153794153e-06,
      "loss": 0.6109,
      "step": 19870
    },
    {
      "epoch": 3.290301224760013,
      "grad_norm": 9.228675842285156,
      "learning_rate": 9.058644583403753e-06,
      "loss": 0.4964,
      "step": 19880
    },
    {
      "epoch": 3.291956305858987,
      "grad_norm": 7.266816139221191,
      "learning_rate": 9.037519013013351e-06,
      "loss": 0.4405,
      "step": 19890
    },
    {
      "epoch": 3.293611386957961,
      "grad_norm": 9.309627532958984,
      "learning_rate": 9.016393442622952e-06,
      "loss": 0.54,
      "step": 19900
    },
    {
      "epoch": 3.295266468056935,
      "grad_norm": 6.609226703643799,
      "learning_rate": 8.99526787223255e-06,
      "loss": 0.6602,
      "step": 19910
    },
    {
      "epoch": 3.2969215491559085,
      "grad_norm": 5.9418439865112305,
      "learning_rate": 8.97414230184215e-06,
      "loss": 0.4408,
      "step": 19920
    },
    {
      "epoch": 3.2985766302548827,
      "grad_norm": 6.517536640167236,
      "learning_rate": 8.95301673145175e-06,
      "loss": 0.496,
      "step": 19930
    },
    {
      "epoch": 3.3002317113538564,
      "grad_norm": 1.222118616104126,
      "learning_rate": 8.93189116106135e-06,
      "loss": 0.474,
      "step": 19940
    },
    {
      "epoch": 3.30188679245283,
      "grad_norm": 5.843421459197998,
      "learning_rate": 8.910765590670949e-06,
      "loss": 0.6811,
      "step": 19950
    },
    {
      "epoch": 3.3035418735518043,
      "grad_norm": 10.010007858276367,
      "learning_rate": 8.889640020280549e-06,
      "loss": 0.6359,
      "step": 19960
    },
    {
      "epoch": 3.305196954650778,
      "grad_norm": 6.827680587768555,
      "learning_rate": 8.868514449890147e-06,
      "loss": 0.7589,
      "step": 19970
    },
    {
      "epoch": 3.3068520357497517,
      "grad_norm": 7.0927934646606445,
      "learning_rate": 8.847388879499747e-06,
      "loss": 0.5915,
      "step": 19980
    },
    {
      "epoch": 3.3085071168487254,
      "grad_norm": 6.098482131958008,
      "learning_rate": 8.826263309109347e-06,
      "loss": 0.4042,
      "step": 19990
    },
    {
      "epoch": 3.3101621979476996,
      "grad_norm": 10.695394515991211,
      "learning_rate": 8.805137738718946e-06,
      "loss": 0.4901,
      "step": 20000
    },
    {
      "epoch": 3.3118172790466733,
      "grad_norm": 2.8023226261138916,
      "learning_rate": 8.784012168328546e-06,
      "loss": 0.4248,
      "step": 20010
    },
    {
      "epoch": 3.313472360145647,
      "grad_norm": 3.3528356552124023,
      "learning_rate": 8.762886597938144e-06,
      "loss": 0.4582,
      "step": 20020
    },
    {
      "epoch": 3.3151274412446208,
      "grad_norm": 4.3637895584106445,
      "learning_rate": 8.741761027547744e-06,
      "loss": 0.5159,
      "step": 20030
    },
    {
      "epoch": 3.316782522343595,
      "grad_norm": 2.5478615760803223,
      "learning_rate": 8.720635457157342e-06,
      "loss": 0.6475,
      "step": 20040
    },
    {
      "epoch": 3.3184376034425687,
      "grad_norm": 5.964216232299805,
      "learning_rate": 8.699509886766943e-06,
      "loss": 0.8105,
      "step": 20050
    },
    {
      "epoch": 3.3200926845415424,
      "grad_norm": 7.279385566711426,
      "learning_rate": 8.678384316376543e-06,
      "loss": 0.5925,
      "step": 20060
    },
    {
      "epoch": 3.3217477656405165,
      "grad_norm": 4.5999016761779785,
      "learning_rate": 8.657258745986143e-06,
      "loss": 0.3458,
      "step": 20070
    },
    {
      "epoch": 3.3234028467394903,
      "grad_norm": 14.186579704284668,
      "learning_rate": 8.636133175595741e-06,
      "loss": 0.6462,
      "step": 20080
    },
    {
      "epoch": 3.325057927838464,
      "grad_norm": 10.960090637207031,
      "learning_rate": 8.615007605205341e-06,
      "loss": 0.4726,
      "step": 20090
    },
    {
      "epoch": 3.326713008937438,
      "grad_norm": 5.800335884094238,
      "learning_rate": 8.59388203481494e-06,
      "loss": 0.5522,
      "step": 20100
    },
    {
      "epoch": 3.328368090036412,
      "grad_norm": 6.251192092895508,
      "learning_rate": 8.57275646442454e-06,
      "loss": 0.5371,
      "step": 20110
    },
    {
      "epoch": 3.3300231711353856,
      "grad_norm": 12.862053871154785,
      "learning_rate": 8.55163089403414e-06,
      "loss": 0.6224,
      "step": 20120
    },
    {
      "epoch": 3.3316782522343593,
      "grad_norm": 7.950098514556885,
      "learning_rate": 8.53050532364374e-06,
      "loss": 0.5475,
      "step": 20130
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 14.594592094421387,
      "learning_rate": 8.509379753253338e-06,
      "loss": 0.6776,
      "step": 20140
    },
    {
      "epoch": 3.334988414432307,
      "grad_norm": 6.0594658851623535,
      "learning_rate": 8.488254182862938e-06,
      "loss": 0.6272,
      "step": 20150
    },
    {
      "epoch": 3.336643495531281,
      "grad_norm": 6.695760726928711,
      "learning_rate": 8.467128612472537e-06,
      "loss": 0.4752,
      "step": 20160
    },
    {
      "epoch": 3.3382985766302546,
      "grad_norm": 4.906540393829346,
      "learning_rate": 8.446003042082137e-06,
      "loss": 0.5797,
      "step": 20170
    },
    {
      "epoch": 3.339953657729229,
      "grad_norm": 13.175236701965332,
      "learning_rate": 8.424877471691737e-06,
      "loss": 0.4327,
      "step": 20180
    },
    {
      "epoch": 3.3416087388282025,
      "grad_norm": 12.51700210571289,
      "learning_rate": 8.403751901301335e-06,
      "loss": 0.6338,
      "step": 20190
    },
    {
      "epoch": 3.3432638199271763,
      "grad_norm": 11.988213539123535,
      "learning_rate": 8.382626330910935e-06,
      "loss": 0.5182,
      "step": 20200
    },
    {
      "epoch": 3.3449189010261504,
      "grad_norm": 18.36306381225586,
      "learning_rate": 8.361500760520534e-06,
      "loss": 0.791,
      "step": 20210
    },
    {
      "epoch": 3.346573982125124,
      "grad_norm": 6.944839000701904,
      "learning_rate": 8.340375190130134e-06,
      "loss": 0.6473,
      "step": 20220
    },
    {
      "epoch": 3.348229063224098,
      "grad_norm": 10.254260063171387,
      "learning_rate": 8.319249619739732e-06,
      "loss": 0.4388,
      "step": 20230
    },
    {
      "epoch": 3.349884144323072,
      "grad_norm": 16.25437355041504,
      "learning_rate": 8.298124049349334e-06,
      "loss": 0.4122,
      "step": 20240
    },
    {
      "epoch": 3.3515392254220457,
      "grad_norm": 3.493657112121582,
      "learning_rate": 8.276998478958932e-06,
      "loss": 0.5309,
      "step": 20250
    },
    {
      "epoch": 3.3531943065210195,
      "grad_norm": 2.0458390712738037,
      "learning_rate": 8.255872908568532e-06,
      "loss": 0.5125,
      "step": 20260
    },
    {
      "epoch": 3.3548493876199936,
      "grad_norm": 6.757201671600342,
      "learning_rate": 8.23474733817813e-06,
      "loss": 0.5837,
      "step": 20270
    },
    {
      "epoch": 3.3565044687189673,
      "grad_norm": 7.059043884277344,
      "learning_rate": 8.21362176778773e-06,
      "loss": 0.7302,
      "step": 20280
    },
    {
      "epoch": 3.358159549817941,
      "grad_norm": 9.32634162902832,
      "learning_rate": 8.19249619739733e-06,
      "loss": 0.8592,
      "step": 20290
    },
    {
      "epoch": 3.359814630916915,
      "grad_norm": 11.111029624938965,
      "learning_rate": 8.171370627006931e-06,
      "loss": 0.5312,
      "step": 20300
    },
    {
      "epoch": 3.361469712015889,
      "grad_norm": 9.771405220031738,
      "learning_rate": 8.15024505661653e-06,
      "loss": 0.4347,
      "step": 20310
    },
    {
      "epoch": 3.3631247931148627,
      "grad_norm": 14.663296699523926,
      "learning_rate": 8.12911948622613e-06,
      "loss": 0.7214,
      "step": 20320
    },
    {
      "epoch": 3.3647798742138364,
      "grad_norm": 7.854523658752441,
      "learning_rate": 8.107993915835728e-06,
      "loss": 0.5483,
      "step": 20330
    },
    {
      "epoch": 3.36643495531281,
      "grad_norm": 17.508098602294922,
      "learning_rate": 8.086868345445326e-06,
      "loss": 0.7625,
      "step": 20340
    },
    {
      "epoch": 3.3680900364117843,
      "grad_norm": 8.84481430053711,
      "learning_rate": 8.065742775054926e-06,
      "loss": 0.6021,
      "step": 20350
    },
    {
      "epoch": 3.369745117510758,
      "grad_norm": 5.538722038269043,
      "learning_rate": 8.044617204664526e-06,
      "loss": 0.4278,
      "step": 20360
    },
    {
      "epoch": 3.3714001986097317,
      "grad_norm": 4.18977689743042,
      "learning_rate": 8.023491634274126e-06,
      "loss": 0.5555,
      "step": 20370
    },
    {
      "epoch": 3.373055279708706,
      "grad_norm": 6.1576080322265625,
      "learning_rate": 8.002366063883725e-06,
      "loss": 0.5148,
      "step": 20380
    },
    {
      "epoch": 3.3747103608076796,
      "grad_norm": 9.334477424621582,
      "learning_rate": 7.981240493493325e-06,
      "loss": 0.4511,
      "step": 20390
    },
    {
      "epoch": 3.3763654419066533,
      "grad_norm": 10.927391052246094,
      "learning_rate": 7.960114923102923e-06,
      "loss": 0.4693,
      "step": 20400
    },
    {
      "epoch": 3.3780205230056275,
      "grad_norm": 9.953035354614258,
      "learning_rate": 7.938989352712523e-06,
      "loss": 0.6867,
      "step": 20410
    },
    {
      "epoch": 3.379675604104601,
      "grad_norm": 2.0232441425323486,
      "learning_rate": 7.917863782322123e-06,
      "loss": 0.7243,
      "step": 20420
    },
    {
      "epoch": 3.381330685203575,
      "grad_norm": 13.126084327697754,
      "learning_rate": 7.896738211931723e-06,
      "loss": 0.7069,
      "step": 20430
    },
    {
      "epoch": 3.3829857663025487,
      "grad_norm": 5.656723499298096,
      "learning_rate": 7.875612641541322e-06,
      "loss": 0.6954,
      "step": 20440
    },
    {
      "epoch": 3.384640847401523,
      "grad_norm": 6.848257541656494,
      "learning_rate": 7.854487071150922e-06,
      "loss": 0.5244,
      "step": 20450
    },
    {
      "epoch": 3.3862959285004965,
      "grad_norm": 7.287803649902344,
      "learning_rate": 7.83336150076052e-06,
      "loss": 0.566,
      "step": 20460
    },
    {
      "epoch": 3.3879510095994703,
      "grad_norm": 8.522003173828125,
      "learning_rate": 7.81223593037012e-06,
      "loss": 0.6411,
      "step": 20470
    },
    {
      "epoch": 3.389606090698444,
      "grad_norm": 12.45159912109375,
      "learning_rate": 7.79111035997972e-06,
      "loss": 0.4272,
      "step": 20480
    },
    {
      "epoch": 3.391261171797418,
      "grad_norm": 2.841639757156372,
      "learning_rate": 7.76998478958932e-06,
      "loss": 0.6606,
      "step": 20490
    },
    {
      "epoch": 3.392916252896392,
      "grad_norm": 5.528858184814453,
      "learning_rate": 7.748859219198919e-06,
      "loss": 0.5605,
      "step": 20500
    },
    {
      "epoch": 3.3945713339953656,
      "grad_norm": 10.584486961364746,
      "learning_rate": 7.727733648808517e-06,
      "loss": 0.4982,
      "step": 20510
    },
    {
      "epoch": 3.3962264150943398,
      "grad_norm": 12.22732162475586,
      "learning_rate": 7.706608078418117e-06,
      "loss": 0.607,
      "step": 20520
    },
    {
      "epoch": 3.3978814961933135,
      "grad_norm": 3.0948596000671387,
      "learning_rate": 7.685482508027716e-06,
      "loss": 0.7769,
      "step": 20530
    },
    {
      "epoch": 3.399536577292287,
      "grad_norm": 9.36003589630127,
      "learning_rate": 7.664356937637318e-06,
      "loss": 0.7147,
      "step": 20540
    },
    {
      "epoch": 3.4011916583912614,
      "grad_norm": 2.576993703842163,
      "learning_rate": 7.643231367246916e-06,
      "loss": 0.4181,
      "step": 20550
    },
    {
      "epoch": 3.402846739490235,
      "grad_norm": 7.498482704162598,
      "learning_rate": 7.622105796856516e-06,
      "loss": 0.4353,
      "step": 20560
    },
    {
      "epoch": 3.404501820589209,
      "grad_norm": 7.430896759033203,
      "learning_rate": 7.600980226466114e-06,
      "loss": 0.5043,
      "step": 20570
    },
    {
      "epoch": 3.406156901688183,
      "grad_norm": 3.3639328479766846,
      "learning_rate": 7.5798546560757145e-06,
      "loss": 0.5213,
      "step": 20580
    },
    {
      "epoch": 3.4078119827871567,
      "grad_norm": 10.36118221282959,
      "learning_rate": 7.558729085685314e-06,
      "loss": 0.6336,
      "step": 20590
    },
    {
      "epoch": 3.4094670638861304,
      "grad_norm": 1.5241156816482544,
      "learning_rate": 7.537603515294914e-06,
      "loss": 0.4748,
      "step": 20600
    },
    {
      "epoch": 3.411122144985104,
      "grad_norm": 8.247313499450684,
      "learning_rate": 7.516477944904512e-06,
      "loss": 0.5317,
      "step": 20610
    },
    {
      "epoch": 3.4127772260840783,
      "grad_norm": 8.252578735351562,
      "learning_rate": 7.495352374514113e-06,
      "loss": 0.716,
      "step": 20620
    },
    {
      "epoch": 3.414432307183052,
      "grad_norm": 7.078047275543213,
      "learning_rate": 7.4742268041237115e-06,
      "loss": 0.6179,
      "step": 20630
    },
    {
      "epoch": 3.4160873882820257,
      "grad_norm": 10.315231323242188,
      "learning_rate": 7.4531012337333115e-06,
      "loss": 0.4579,
      "step": 20640
    },
    {
      "epoch": 3.4177424693809995,
      "grad_norm": 4.55654239654541,
      "learning_rate": 7.431975663342911e-06,
      "loss": 0.3885,
      "step": 20650
    },
    {
      "epoch": 3.4193975504799736,
      "grad_norm": 8.021623611450195,
      "learning_rate": 7.410850092952511e-06,
      "loss": 0.7405,
      "step": 20660
    },
    {
      "epoch": 3.4210526315789473,
      "grad_norm": 3.601386785507202,
      "learning_rate": 7.389724522562109e-06,
      "loss": 0.365,
      "step": 20670
    },
    {
      "epoch": 3.422707712677921,
      "grad_norm": 3.416002035140991,
      "learning_rate": 7.3685989521717085e-06,
      "loss": 0.4989,
      "step": 20680
    },
    {
      "epoch": 3.4243627937768952,
      "grad_norm": 3.412463426589966,
      "learning_rate": 7.3474733817813085e-06,
      "loss": 0.5505,
      "step": 20690
    },
    {
      "epoch": 3.426017874875869,
      "grad_norm": 8.595657348632812,
      "learning_rate": 7.326347811390907e-06,
      "loss": 0.7651,
      "step": 20700
    },
    {
      "epoch": 3.4276729559748427,
      "grad_norm": 9.356941223144531,
      "learning_rate": 7.305222241000508e-06,
      "loss": 0.5691,
      "step": 20710
    },
    {
      "epoch": 3.429328037073817,
      "grad_norm": 2.521035671234131,
      "learning_rate": 7.284096670610106e-06,
      "loss": 0.4205,
      "step": 20720
    },
    {
      "epoch": 3.4309831181727906,
      "grad_norm": 8.450325965881348,
      "learning_rate": 7.262971100219706e-06,
      "loss": 0.5267,
      "step": 20730
    },
    {
      "epoch": 3.4326381992717643,
      "grad_norm": 6.210724353790283,
      "learning_rate": 7.2418455298293055e-06,
      "loss": 0.5579,
      "step": 20740
    },
    {
      "epoch": 3.434293280370738,
      "grad_norm": 9.090806007385254,
      "learning_rate": 7.220719959438906e-06,
      "loss": 0.7784,
      "step": 20750
    },
    {
      "epoch": 3.435948361469712,
      "grad_norm": 2.8324151039123535,
      "learning_rate": 7.199594389048504e-06,
      "loss": 0.3592,
      "step": 20760
    },
    {
      "epoch": 3.437603442568686,
      "grad_norm": 5.232076644897461,
      "learning_rate": 7.178468818658104e-06,
      "loss": 0.6357,
      "step": 20770
    },
    {
      "epoch": 3.4392585236676596,
      "grad_norm": 9.984319686889648,
      "learning_rate": 7.157343248267703e-06,
      "loss": 0.6431,
      "step": 20780
    },
    {
      "epoch": 3.4409136047666333,
      "grad_norm": 4.523266315460205,
      "learning_rate": 7.136217677877303e-06,
      "loss": 0.5722,
      "step": 20790
    },
    {
      "epoch": 3.4425686858656075,
      "grad_norm": 5.300229549407959,
      "learning_rate": 7.115092107486903e-06,
      "loss": 0.6122,
      "step": 20800
    },
    {
      "epoch": 3.444223766964581,
      "grad_norm": 12.684614181518555,
      "learning_rate": 7.093966537096503e-06,
      "loss": 0.5272,
      "step": 20810
    },
    {
      "epoch": 3.445878848063555,
      "grad_norm": 11.734672546386719,
      "learning_rate": 7.072840966706101e-06,
      "loss": 0.4306,
      "step": 20820
    },
    {
      "epoch": 3.447533929162529,
      "grad_norm": 14.211931228637695,
      "learning_rate": 7.051715396315701e-06,
      "loss": 0.7818,
      "step": 20830
    },
    {
      "epoch": 3.449189010261503,
      "grad_norm": 8.176591873168945,
      "learning_rate": 7.0305898259253e-06,
      "loss": 0.445,
      "step": 20840
    },
    {
      "epoch": 3.4508440913604765,
      "grad_norm": 1.8648899793624878,
      "learning_rate": 7.009464255534899e-06,
      "loss": 0.5767,
      "step": 20850
    },
    {
      "epoch": 3.4524991724594507,
      "grad_norm": 12.517316818237305,
      "learning_rate": 6.9883386851445e-06,
      "loss": 0.5863,
      "step": 20860
    },
    {
      "epoch": 3.4541542535584244,
      "grad_norm": 5.778703212738037,
      "learning_rate": 6.967213114754098e-06,
      "loss": 0.4378,
      "step": 20870
    },
    {
      "epoch": 3.455809334657398,
      "grad_norm": 6.579868316650391,
      "learning_rate": 6.946087544363698e-06,
      "loss": 0.679,
      "step": 20880
    },
    {
      "epoch": 3.4574644157563723,
      "grad_norm": 7.976960182189941,
      "learning_rate": 6.924961973973297e-06,
      "loss": 0.4317,
      "step": 20890
    },
    {
      "epoch": 3.459119496855346,
      "grad_norm": 9.76223373413086,
      "learning_rate": 6.9038364035828975e-06,
      "loss": 0.5416,
      "step": 20900
    },
    {
      "epoch": 3.4607745779543198,
      "grad_norm": 6.31391716003418,
      "learning_rate": 6.882710833192496e-06,
      "loss": 0.571,
      "step": 20910
    },
    {
      "epoch": 3.4624296590532935,
      "grad_norm": 7.9885759353637695,
      "learning_rate": 6.861585262802096e-06,
      "loss": 0.6017,
      "step": 20920
    },
    {
      "epoch": 3.4640847401522676,
      "grad_norm": 11.728462219238281,
      "learning_rate": 6.840459692411695e-06,
      "loss": 0.4308,
      "step": 20930
    },
    {
      "epoch": 3.4657398212512414,
      "grad_norm": 9.778373718261719,
      "learning_rate": 6.819334122021295e-06,
      "loss": 0.5461,
      "step": 20940
    },
    {
      "epoch": 3.467394902350215,
      "grad_norm": 2.5802698135375977,
      "learning_rate": 6.7982085516308945e-06,
      "loss": 0.5684,
      "step": 20950
    },
    {
      "epoch": 3.469049983449189,
      "grad_norm": 7.9510087966918945,
      "learning_rate": 6.7770829812404945e-06,
      "loss": 0.6972,
      "step": 20960
    },
    {
      "epoch": 3.470705064548163,
      "grad_norm": 14.483747482299805,
      "learning_rate": 6.755957410850093e-06,
      "loss": 0.8774,
      "step": 20970
    },
    {
      "epoch": 3.4723601456471367,
      "grad_norm": 3.2706639766693115,
      "learning_rate": 6.734831840459693e-06,
      "loss": 0.5057,
      "step": 20980
    },
    {
      "epoch": 3.4740152267461104,
      "grad_norm": 6.0010833740234375,
      "learning_rate": 6.713706270069292e-06,
      "loss": 0.5464,
      "step": 20990
    },
    {
      "epoch": 3.4756703078450846,
      "grad_norm": 4.9865264892578125,
      "learning_rate": 6.692580699678892e-06,
      "loss": 0.5882,
      "step": 21000
    },
    {
      "epoch": 3.4773253889440583,
      "grad_norm": 5.053715229034424,
      "learning_rate": 6.671455129288491e-06,
      "loss": 0.6634,
      "step": 21010
    },
    {
      "epoch": 3.478980470043032,
      "grad_norm": 8.57636833190918,
      "learning_rate": 6.65032955889809e-06,
      "loss": 0.5946,
      "step": 21020
    },
    {
      "epoch": 3.480635551142006,
      "grad_norm": 8.041935920715332,
      "learning_rate": 6.62920398850769e-06,
      "loss": 0.6081,
      "step": 21030
    },
    {
      "epoch": 3.48229063224098,
      "grad_norm": 4.877477169036865,
      "learning_rate": 6.608078418117289e-06,
      "loss": 0.4124,
      "step": 21040
    },
    {
      "epoch": 3.4839457133399536,
      "grad_norm": 3.565035581588745,
      "learning_rate": 6.586952847726889e-06,
      "loss": 0.4818,
      "step": 21050
    },
    {
      "epoch": 3.4856007944389273,
      "grad_norm": 2.0129880905151367,
      "learning_rate": 6.565827277336488e-06,
      "loss": 0.5317,
      "step": 21060
    },
    {
      "epoch": 3.4872558755379015,
      "grad_norm": 9.929262161254883,
      "learning_rate": 6.544701706946088e-06,
      "loss": 0.6942,
      "step": 21070
    },
    {
      "epoch": 3.4889109566368752,
      "grad_norm": 6.231513500213623,
      "learning_rate": 6.523576136555687e-06,
      "loss": 0.4729,
      "step": 21080
    },
    {
      "epoch": 3.490566037735849,
      "grad_norm": 2.208660364151001,
      "learning_rate": 6.502450566165287e-06,
      "loss": 0.3092,
      "step": 21090
    },
    {
      "epoch": 3.4922211188348227,
      "grad_norm": 12.25134563446045,
      "learning_rate": 6.4813249957748855e-06,
      "loss": 0.4734,
      "step": 21100
    },
    {
      "epoch": 3.493876199933797,
      "grad_norm": 8.303017616271973,
      "learning_rate": 6.460199425384486e-06,
      "loss": 0.4641,
      "step": 21110
    },
    {
      "epoch": 3.4955312810327706,
      "grad_norm": 20.801549911499023,
      "learning_rate": 6.439073854994085e-06,
      "loss": 0.6671,
      "step": 21120
    },
    {
      "epoch": 3.4971863621317443,
      "grad_norm": 7.214042663574219,
      "learning_rate": 6.417948284603685e-06,
      "loss": 0.4083,
      "step": 21130
    },
    {
      "epoch": 3.4988414432307184,
      "grad_norm": 8.71277141571045,
      "learning_rate": 6.396822714213284e-06,
      "loss": 0.4172,
      "step": 21140
    },
    {
      "epoch": 3.500496524329692,
      "grad_norm": 12.806310653686523,
      "learning_rate": 6.375697143822884e-06,
      "loss": 0.6825,
      "step": 21150
    },
    {
      "epoch": 3.502151605428666,
      "grad_norm": 3.3203048706054688,
      "learning_rate": 6.3545715734324825e-06,
      "loss": 0.8077,
      "step": 21160
    },
    {
      "epoch": 3.50380668652764,
      "grad_norm": 6.582108020782471,
      "learning_rate": 6.3334460030420835e-06,
      "loss": 0.527,
      "step": 21170
    },
    {
      "epoch": 3.5054617676266138,
      "grad_norm": 11.926016807556152,
      "learning_rate": 6.312320432651682e-06,
      "loss": 0.5092,
      "step": 21180
    },
    {
      "epoch": 3.5071168487255875,
      "grad_norm": 6.390162944793701,
      "learning_rate": 6.291194862261281e-06,
      "loss": 0.4619,
      "step": 21190
    },
    {
      "epoch": 3.5087719298245617,
      "grad_norm": 13.93960952758789,
      "learning_rate": 6.270069291870881e-06,
      "loss": 0.4217,
      "step": 21200
    },
    {
      "epoch": 3.5104270109235354,
      "grad_norm": 16.846538543701172,
      "learning_rate": 6.24894372148048e-06,
      "loss": 0.6158,
      "step": 21210
    },
    {
      "epoch": 3.512082092022509,
      "grad_norm": 4.395644187927246,
      "learning_rate": 6.22781815109008e-06,
      "loss": 0.7483,
      "step": 21220
    },
    {
      "epoch": 3.513737173121483,
      "grad_norm": 12.365630149841309,
      "learning_rate": 6.20669258069968e-06,
      "loss": 0.6275,
      "step": 21230
    },
    {
      "epoch": 3.5153922542204565,
      "grad_norm": 3.3644931316375732,
      "learning_rate": 6.185567010309279e-06,
      "loss": 0.7292,
      "step": 21240
    },
    {
      "epoch": 3.5170473353194307,
      "grad_norm": 10.206817626953125,
      "learning_rate": 6.164441439918878e-06,
      "loss": 0.3401,
      "step": 21250
    },
    {
      "epoch": 3.5187024164184044,
      "grad_norm": 19.30539321899414,
      "learning_rate": 6.143315869528478e-06,
      "loss": 0.5433,
      "step": 21260
    },
    {
      "epoch": 3.520357497517378,
      "grad_norm": 5.096323490142822,
      "learning_rate": 6.122190299138077e-06,
      "loss": 0.7717,
      "step": 21270
    },
    {
      "epoch": 3.5220125786163523,
      "grad_norm": 4.600900173187256,
      "learning_rate": 6.101064728747676e-06,
      "loss": 0.5727,
      "step": 21280
    },
    {
      "epoch": 3.523667659715326,
      "grad_norm": 8.556601524353027,
      "learning_rate": 6.079939158357276e-06,
      "loss": 0.7469,
      "step": 21290
    },
    {
      "epoch": 3.5253227408142997,
      "grad_norm": 0.9331560134887695,
      "learning_rate": 6.058813587966875e-06,
      "loss": 0.4171,
      "step": 21300
    },
    {
      "epoch": 3.526977821913274,
      "grad_norm": 7.352385997772217,
      "learning_rate": 6.037688017576474e-06,
      "loss": 0.5044,
      "step": 21310
    },
    {
      "epoch": 3.5286329030122476,
      "grad_norm": 8.00589370727539,
      "learning_rate": 6.0165624471860745e-06,
      "loss": 0.4807,
      "step": 21320
    },
    {
      "epoch": 3.5302879841112214,
      "grad_norm": 9.381062507629395,
      "learning_rate": 5.995436876795674e-06,
      "loss": 0.7134,
      "step": 21330
    },
    {
      "epoch": 3.5319430652101955,
      "grad_norm": 5.912435531616211,
      "learning_rate": 5.974311306405273e-06,
      "loss": 0.4004,
      "step": 21340
    },
    {
      "epoch": 3.5335981463091692,
      "grad_norm": 7.796051979064941,
      "learning_rate": 5.953185736014873e-06,
      "loss": 0.477,
      "step": 21350
    },
    {
      "epoch": 3.535253227408143,
      "grad_norm": 10.46708869934082,
      "learning_rate": 5.932060165624472e-06,
      "loss": 0.7164,
      "step": 21360
    },
    {
      "epoch": 3.5369083085071167,
      "grad_norm": 8.121723175048828,
      "learning_rate": 5.9109345952340715e-06,
      "loss": 0.7534,
      "step": 21370
    },
    {
      "epoch": 3.538563389606091,
      "grad_norm": 6.9732890129089355,
      "learning_rate": 5.8898090248436715e-06,
      "loss": 0.4225,
      "step": 21380
    },
    {
      "epoch": 3.5402184707050646,
      "grad_norm": 5.639476299285889,
      "learning_rate": 5.868683454453271e-06,
      "loss": 0.4548,
      "step": 21390
    },
    {
      "epoch": 3.5418735518040383,
      "grad_norm": 12.439802169799805,
      "learning_rate": 5.84755788406287e-06,
      "loss": 0.5502,
      "step": 21400
    },
    {
      "epoch": 3.543528632903012,
      "grad_norm": 7.735401630401611,
      "learning_rate": 5.82643231367247e-06,
      "loss": 0.8619,
      "step": 21410
    },
    {
      "epoch": 3.545183714001986,
      "grad_norm": 10.11503791809082,
      "learning_rate": 5.805306743282069e-06,
      "loss": 0.7151,
      "step": 21420
    },
    {
      "epoch": 3.54683879510096,
      "grad_norm": 10.75956916809082,
      "learning_rate": 5.7841811728916685e-06,
      "loss": 0.5734,
      "step": 21430
    },
    {
      "epoch": 3.5484938761999336,
      "grad_norm": 2.0683200359344482,
      "learning_rate": 5.763055602501268e-06,
      "loss": 0.4143,
      "step": 21440
    },
    {
      "epoch": 3.550148957298908,
      "grad_norm": 6.7578887939453125,
      "learning_rate": 5.741930032110867e-06,
      "loss": 0.6737,
      "step": 21450
    },
    {
      "epoch": 3.5518040383978815,
      "grad_norm": 8.7205171585083,
      "learning_rate": 5.720804461720466e-06,
      "loss": 0.7203,
      "step": 21460
    },
    {
      "epoch": 3.5534591194968552,
      "grad_norm": 7.292201519012451,
      "learning_rate": 5.699678891330066e-06,
      "loss": 0.6717,
      "step": 21470
    },
    {
      "epoch": 3.5551142005958294,
      "grad_norm": 2.1260793209075928,
      "learning_rate": 5.6785533209396655e-06,
      "loss": 0.5924,
      "step": 21480
    },
    {
      "epoch": 3.556769281694803,
      "grad_norm": 4.953850746154785,
      "learning_rate": 5.657427750549265e-06,
      "loss": 0.4052,
      "step": 21490
    },
    {
      "epoch": 3.558424362793777,
      "grad_norm": 9.49803638458252,
      "learning_rate": 5.636302180158865e-06,
      "loss": 0.9542,
      "step": 21500
    },
    {
      "epoch": 3.560079443892751,
      "grad_norm": 4.359382152557373,
      "learning_rate": 5.615176609768464e-06,
      "loss": 0.6803,
      "step": 21510
    },
    {
      "epoch": 3.5617345249917247,
      "grad_norm": 12.340859413146973,
      "learning_rate": 5.594051039378063e-06,
      "loss": 0.7344,
      "step": 21520
    },
    {
      "epoch": 3.5633896060906984,
      "grad_norm": 9.89897346496582,
      "learning_rate": 5.572925468987663e-06,
      "loss": 0.9563,
      "step": 21530
    },
    {
      "epoch": 3.565044687189672,
      "grad_norm": 3.2102208137512207,
      "learning_rate": 5.551799898597263e-06,
      "loss": 0.6239,
      "step": 21540
    },
    {
      "epoch": 3.566699768288646,
      "grad_norm": 4.969236850738525,
      "learning_rate": 5.530674328206862e-06,
      "loss": 0.6146,
      "step": 21550
    },
    {
      "epoch": 3.56835484938762,
      "grad_norm": 7.487651824951172,
      "learning_rate": 5.509548757816461e-06,
      "loss": 0.6145,
      "step": 21560
    },
    {
      "epoch": 3.5700099304865938,
      "grad_norm": 10.815552711486816,
      "learning_rate": 5.488423187426061e-06,
      "loss": 0.6799,
      "step": 21570
    },
    {
      "epoch": 3.5716650115855675,
      "grad_norm": 3.263076066970825,
      "learning_rate": 5.46729761703566e-06,
      "loss": 0.4306,
      "step": 21580
    },
    {
      "epoch": 3.5733200926845416,
      "grad_norm": 10.863839149475098,
      "learning_rate": 5.44617204664526e-06,
      "loss": 0.5133,
      "step": 21590
    },
    {
      "epoch": 3.5749751737835154,
      "grad_norm": 8.341663360595703,
      "learning_rate": 5.42504647625486e-06,
      "loss": 0.4332,
      "step": 21600
    },
    {
      "epoch": 3.576630254882489,
      "grad_norm": 10.832067489624023,
      "learning_rate": 5.403920905864458e-06,
      "loss": 0.8059,
      "step": 21610
    },
    {
      "epoch": 3.5782853359814633,
      "grad_norm": 3.00124454498291,
      "learning_rate": 5.382795335474058e-06,
      "loss": 0.5548,
      "step": 21620
    },
    {
      "epoch": 3.579940417080437,
      "grad_norm": 18.23126983642578,
      "learning_rate": 5.361669765083657e-06,
      "loss": 0.5744,
      "step": 21630
    },
    {
      "epoch": 3.5815954981794107,
      "grad_norm": 11.334783554077148,
      "learning_rate": 5.340544194693257e-06,
      "loss": 0.5493,
      "step": 21640
    },
    {
      "epoch": 3.583250579278385,
      "grad_norm": 2.617307186126709,
      "learning_rate": 5.319418624302856e-06,
      "loss": 0.4755,
      "step": 21650
    },
    {
      "epoch": 3.5849056603773586,
      "grad_norm": 16.964265823364258,
      "learning_rate": 5.298293053912456e-06,
      "loss": 0.6528,
      "step": 21660
    },
    {
      "epoch": 3.5865607414763323,
      "grad_norm": 7.008820056915283,
      "learning_rate": 5.277167483522055e-06,
      "loss": 0.717,
      "step": 21670
    },
    {
      "epoch": 3.588215822575306,
      "grad_norm": 4.280428886413574,
      "learning_rate": 5.256041913131654e-06,
      "loss": 0.5849,
      "step": 21680
    },
    {
      "epoch": 3.58987090367428,
      "grad_norm": 10.16082763671875,
      "learning_rate": 5.2349163427412545e-06,
      "loss": 0.4803,
      "step": 21690
    },
    {
      "epoch": 3.591525984773254,
      "grad_norm": 10.390951156616211,
      "learning_rate": 5.213790772350854e-06,
      "loss": 0.5966,
      "step": 21700
    },
    {
      "epoch": 3.5931810658722276,
      "grad_norm": 2.0411555767059326,
      "learning_rate": 5.192665201960453e-06,
      "loss": 0.5798,
      "step": 21710
    },
    {
      "epoch": 3.5948361469712014,
      "grad_norm": 9.222814559936523,
      "learning_rate": 5.171539631570053e-06,
      "loss": 0.6287,
      "step": 21720
    },
    {
      "epoch": 3.5964912280701755,
      "grad_norm": 7.6453857421875,
      "learning_rate": 5.150414061179652e-06,
      "loss": 0.6198,
      "step": 21730
    },
    {
      "epoch": 3.5981463091691492,
      "grad_norm": 5.80342435836792,
      "learning_rate": 5.1292884907892515e-06,
      "loss": 0.4098,
      "step": 21740
    },
    {
      "epoch": 3.599801390268123,
      "grad_norm": 9.957162857055664,
      "learning_rate": 5.1081629203988515e-06,
      "loss": 0.4438,
      "step": 21750
    },
    {
      "epoch": 3.601456471367097,
      "grad_norm": 7.456904888153076,
      "learning_rate": 5.087037350008451e-06,
      "loss": 0.5936,
      "step": 21760
    },
    {
      "epoch": 3.603111552466071,
      "grad_norm": 6.166233539581299,
      "learning_rate": 5.06591177961805e-06,
      "loss": 0.6364,
      "step": 21770
    },
    {
      "epoch": 3.6047666335650446,
      "grad_norm": 1.5898628234863281,
      "learning_rate": 5.044786209227649e-06,
      "loss": 0.8721,
      "step": 21780
    },
    {
      "epoch": 3.6064217146640187,
      "grad_norm": 1.950026035308838,
      "learning_rate": 5.0236606388372485e-06,
      "loss": 0.4021,
      "step": 21790
    },
    {
      "epoch": 3.6080767957629925,
      "grad_norm": 2.787059783935547,
      "learning_rate": 5.002535068446848e-06,
      "loss": 0.6712,
      "step": 21800
    },
    {
      "epoch": 3.609731876861966,
      "grad_norm": 3.4297006130218506,
      "learning_rate": 4.981409498056448e-06,
      "loss": 0.5391,
      "step": 21810
    },
    {
      "epoch": 3.6113869579609403,
      "grad_norm": 5.161849498748779,
      "learning_rate": 4.960283927666047e-06,
      "loss": 0.7148,
      "step": 21820
    },
    {
      "epoch": 3.613042039059914,
      "grad_norm": 8.180127143859863,
      "learning_rate": 4.939158357275646e-06,
      "loss": 0.4979,
      "step": 21830
    },
    {
      "epoch": 3.6146971201588878,
      "grad_norm": 3.333158254623413,
      "learning_rate": 4.918032786885246e-06,
      "loss": 0.6324,
      "step": 21840
    },
    {
      "epoch": 3.6163522012578615,
      "grad_norm": 2.810330390930176,
      "learning_rate": 4.8969072164948455e-06,
      "loss": 0.3874,
      "step": 21850
    },
    {
      "epoch": 3.618007282356835,
      "grad_norm": 4.3945770263671875,
      "learning_rate": 4.875781646104445e-06,
      "loss": 0.453,
      "step": 21860
    },
    {
      "epoch": 3.6196623634558094,
      "grad_norm": 8.007221221923828,
      "learning_rate": 4.854656075714045e-06,
      "loss": 0.7173,
      "step": 21870
    },
    {
      "epoch": 3.621317444554783,
      "grad_norm": 1.8695076704025269,
      "learning_rate": 4.833530505323644e-06,
      "loss": 0.7257,
      "step": 21880
    },
    {
      "epoch": 3.622972525653757,
      "grad_norm": 3.4621753692626953,
      "learning_rate": 4.812404934933243e-06,
      "loss": 0.5692,
      "step": 21890
    },
    {
      "epoch": 3.624627606752731,
      "grad_norm": 13.428905487060547,
      "learning_rate": 4.791279364542843e-06,
      "loss": 0.637,
      "step": 21900
    },
    {
      "epoch": 3.6262826878517047,
      "grad_norm": 2.1371936798095703,
      "learning_rate": 4.770153794152443e-06,
      "loss": 0.4601,
      "step": 21910
    },
    {
      "epoch": 3.6279377689506784,
      "grad_norm": 1.1977479457855225,
      "learning_rate": 4.749028223762042e-06,
      "loss": 0.3632,
      "step": 21920
    },
    {
      "epoch": 3.6295928500496526,
      "grad_norm": 9.834086418151855,
      "learning_rate": 4.727902653371642e-06,
      "loss": 0.6918,
      "step": 21930
    },
    {
      "epoch": 3.6312479311486263,
      "grad_norm": 3.271462917327881,
      "learning_rate": 4.706777082981241e-06,
      "loss": 0.5871,
      "step": 21940
    },
    {
      "epoch": 3.6329030122476,
      "grad_norm": 6.416780948638916,
      "learning_rate": 4.6856515125908395e-06,
      "loss": 0.5212,
      "step": 21950
    },
    {
      "epoch": 3.634558093346574,
      "grad_norm": 27.57560157775879,
      "learning_rate": 4.66452594220044e-06,
      "loss": 0.6791,
      "step": 21960
    },
    {
      "epoch": 3.636213174445548,
      "grad_norm": 11.923681259155273,
      "learning_rate": 4.643400371810039e-06,
      "loss": 0.6879,
      "step": 21970
    },
    {
      "epoch": 3.6378682555445216,
      "grad_norm": 9.411735534667969,
      "learning_rate": 4.622274801419638e-06,
      "loss": 0.3162,
      "step": 21980
    },
    {
      "epoch": 3.6395233366434954,
      "grad_norm": 11.525458335876465,
      "learning_rate": 4.601149231029238e-06,
      "loss": 0.591,
      "step": 21990
    },
    {
      "epoch": 3.6411784177424695,
      "grad_norm": 7.35546350479126,
      "learning_rate": 4.580023660638837e-06,
      "loss": 0.4166,
      "step": 22000
    },
    {
      "epoch": 3.6428334988414433,
      "grad_norm": 8.22558879852295,
      "learning_rate": 4.558898090248437e-06,
      "loss": 0.5498,
      "step": 22010
    },
    {
      "epoch": 3.644488579940417,
      "grad_norm": 10.506159782409668,
      "learning_rate": 4.537772519858037e-06,
      "loss": 0.8632,
      "step": 22020
    },
    {
      "epoch": 3.6461436610393907,
      "grad_norm": 13.979070663452148,
      "learning_rate": 4.516646949467636e-06,
      "loss": 0.4358,
      "step": 22030
    },
    {
      "epoch": 3.647798742138365,
      "grad_norm": 12.495862007141113,
      "learning_rate": 4.495521379077235e-06,
      "loss": 0.6531,
      "step": 22040
    },
    {
      "epoch": 3.6494538232373386,
      "grad_norm": 8.89136791229248,
      "learning_rate": 4.474395808686835e-06,
      "loss": 0.4996,
      "step": 22050
    },
    {
      "epoch": 3.6511089043363123,
      "grad_norm": 10.849014282226562,
      "learning_rate": 4.4532702382964345e-06,
      "loss": 0.6164,
      "step": 22060
    },
    {
      "epoch": 3.6527639854352865,
      "grad_norm": 5.75811243057251,
      "learning_rate": 4.432144667906034e-06,
      "loss": 0.7265,
      "step": 22070
    },
    {
      "epoch": 3.65441906653426,
      "grad_norm": 4.6277337074279785,
      "learning_rate": 4.411019097515633e-06,
      "loss": 0.4702,
      "step": 22080
    },
    {
      "epoch": 3.656074147633234,
      "grad_norm": 6.612766265869141,
      "learning_rate": 4.389893527125233e-06,
      "loss": 0.6272,
      "step": 22090
    },
    {
      "epoch": 3.657729228732208,
      "grad_norm": 5.619239330291748,
      "learning_rate": 4.368767956734832e-06,
      "loss": 0.5048,
      "step": 22100
    },
    {
      "epoch": 3.659384309831182,
      "grad_norm": 14.877222061157227,
      "learning_rate": 4.3476423863444315e-06,
      "loss": 0.7515,
      "step": 22110
    },
    {
      "epoch": 3.6610393909301555,
      "grad_norm": 9.916908264160156,
      "learning_rate": 4.326516815954031e-06,
      "loss": 0.4384,
      "step": 22120
    },
    {
      "epoch": 3.6626944720291297,
      "grad_norm": 5.720104694366455,
      "learning_rate": 4.30539124556363e-06,
      "loss": 0.4092,
      "step": 22130
    },
    {
      "epoch": 3.6643495531281034,
      "grad_norm": 8.916966438293457,
      "learning_rate": 4.28426567517323e-06,
      "loss": 0.548,
      "step": 22140
    },
    {
      "epoch": 3.666004634227077,
      "grad_norm": 11.689653396606445,
      "learning_rate": 4.263140104782829e-06,
      "loss": 0.9467,
      "step": 22150
    },
    {
      "epoch": 3.667659715326051,
      "grad_norm": 6.033030986785889,
      "learning_rate": 4.2420145343924285e-06,
      "loss": 0.6276,
      "step": 22160
    },
    {
      "epoch": 3.6693147964250246,
      "grad_norm": 2.2534584999084473,
      "learning_rate": 4.2208889640020285e-06,
      "loss": 0.6281,
      "step": 22170
    },
    {
      "epoch": 3.6709698775239987,
      "grad_norm": 2.8192923069000244,
      "learning_rate": 4.199763393611628e-06,
      "loss": 0.5575,
      "step": 22180
    },
    {
      "epoch": 3.6726249586229724,
      "grad_norm": 5.700973987579346,
      "learning_rate": 4.178637823221227e-06,
      "loss": 0.3972,
      "step": 22190
    },
    {
      "epoch": 3.674280039721946,
      "grad_norm": 8.864646911621094,
      "learning_rate": 4.157512252830826e-06,
      "loss": 0.5996,
      "step": 22200
    },
    {
      "epoch": 3.6759351208209203,
      "grad_norm": 5.124478340148926,
      "learning_rate": 4.136386682440426e-06,
      "loss": 0.5948,
      "step": 22210
    },
    {
      "epoch": 3.677590201919894,
      "grad_norm": 4.889939785003662,
      "learning_rate": 4.1152611120500255e-06,
      "loss": 0.5131,
      "step": 22220
    },
    {
      "epoch": 3.6792452830188678,
      "grad_norm": 8.400643348693848,
      "learning_rate": 4.094135541659625e-06,
      "loss": 0.5503,
      "step": 22230
    },
    {
      "epoch": 3.680900364117842,
      "grad_norm": 3.4394192695617676,
      "learning_rate": 4.073009971269225e-06,
      "loss": 0.501,
      "step": 22240
    },
    {
      "epoch": 3.6825554452168157,
      "grad_norm": 12.984783172607422,
      "learning_rate": 4.051884400878824e-06,
      "loss": 0.6217,
      "step": 22250
    },
    {
      "epoch": 3.6842105263157894,
      "grad_norm": 6.732165336608887,
      "learning_rate": 4.030758830488423e-06,
      "loss": 0.3963,
      "step": 22260
    },
    {
      "epoch": 3.6858656074147635,
      "grad_norm": 10.267647743225098,
      "learning_rate": 4.009633260098023e-06,
      "loss": 0.7209,
      "step": 22270
    },
    {
      "epoch": 3.6875206885137373,
      "grad_norm": 13.943804740905762,
      "learning_rate": 3.988507689707623e-06,
      "loss": 0.6941,
      "step": 22280
    },
    {
      "epoch": 3.689175769612711,
      "grad_norm": 4.713579177856445,
      "learning_rate": 3.967382119317222e-06,
      "loss": 0.763,
      "step": 22290
    },
    {
      "epoch": 3.6908308507116847,
      "grad_norm": 4.191120624542236,
      "learning_rate": 3.946256548926821e-06,
      "loss": 0.5301,
      "step": 22300
    },
    {
      "epoch": 3.692485931810659,
      "grad_norm": 10.265851974487305,
      "learning_rate": 3.92513097853642e-06,
      "loss": 0.5503,
      "step": 22310
    },
    {
      "epoch": 3.6941410129096326,
      "grad_norm": 6.446580410003662,
      "learning_rate": 3.9040054081460195e-06,
      "loss": 0.4886,
      "step": 22320
    },
    {
      "epoch": 3.6957960940086063,
      "grad_norm": 5.135663986206055,
      "learning_rate": 3.88287983775562e-06,
      "loss": 0.7175,
      "step": 22330
    },
    {
      "epoch": 3.69745117510758,
      "grad_norm": 7.986839771270752,
      "learning_rate": 3.861754267365219e-06,
      "loss": 0.7765,
      "step": 22340
    },
    {
      "epoch": 3.699106256206554,
      "grad_norm": 7.191687107086182,
      "learning_rate": 3.840628696974818e-06,
      "loss": 0.6506,
      "step": 22350
    },
    {
      "epoch": 3.700761337305528,
      "grad_norm": 1.0929197072982788,
      "learning_rate": 3.819503126584418e-06,
      "loss": 0.6267,
      "step": 22360
    },
    {
      "epoch": 3.7024164184045016,
      "grad_norm": 9.613122940063477,
      "learning_rate": 3.7983775561940174e-06,
      "loss": 0.6779,
      "step": 22370
    },
    {
      "epoch": 3.704071499503476,
      "grad_norm": 9.94726276397705,
      "learning_rate": 3.777251985803617e-06,
      "loss": 0.5676,
      "step": 22380
    },
    {
      "epoch": 3.7057265806024495,
      "grad_norm": 7.573821544647217,
      "learning_rate": 3.7561264154132163e-06,
      "loss": 0.5256,
      "step": 22390
    },
    {
      "epoch": 3.7073816617014232,
      "grad_norm": 7.898770809173584,
      "learning_rate": 3.735000845022816e-06,
      "loss": 0.3864,
      "step": 22400
    },
    {
      "epoch": 3.7090367428003974,
      "grad_norm": 14.456692695617676,
      "learning_rate": 3.7138752746324156e-06,
      "loss": 0.5556,
      "step": 22410
    },
    {
      "epoch": 3.710691823899371,
      "grad_norm": 12.830733299255371,
      "learning_rate": 3.692749704242015e-06,
      "loss": 0.8307,
      "step": 22420
    },
    {
      "epoch": 3.712346904998345,
      "grad_norm": 12.324469566345215,
      "learning_rate": 3.6716241338516145e-06,
      "loss": 0.597,
      "step": 22430
    },
    {
      "epoch": 3.714001986097319,
      "grad_norm": 4.242330551147461,
      "learning_rate": 3.650498563461214e-06,
      "loss": 0.5828,
      "step": 22440
    },
    {
      "epoch": 3.7156570671962927,
      "grad_norm": 4.495916366577148,
      "learning_rate": 3.6293729930708133e-06,
      "loss": 0.3712,
      "step": 22450
    },
    {
      "epoch": 3.7173121482952665,
      "grad_norm": 11.27193546295166,
      "learning_rate": 3.608247422680412e-06,
      "loss": 0.6269,
      "step": 22460
    },
    {
      "epoch": 3.71896722939424,
      "grad_norm": 4.613974571228027,
      "learning_rate": 3.587121852290012e-06,
      "loss": 0.5522,
      "step": 22470
    },
    {
      "epoch": 3.720622310493214,
      "grad_norm": 1.4860588312149048,
      "learning_rate": 3.565996281899611e-06,
      "loss": 0.4541,
      "step": 22480
    },
    {
      "epoch": 3.722277391592188,
      "grad_norm": 5.490604877471924,
      "learning_rate": 3.5448707115092107e-06,
      "loss": 0.7306,
      "step": 22490
    },
    {
      "epoch": 3.723932472691162,
      "grad_norm": 6.67794132232666,
      "learning_rate": 3.5237451411188103e-06,
      "loss": 0.7064,
      "step": 22500
    },
    {
      "epoch": 3.7255875537901355,
      "grad_norm": 6.189249038696289,
      "learning_rate": 3.5026195707284096e-06,
      "loss": 0.5695,
      "step": 22510
    },
    {
      "epoch": 3.7272426348891097,
      "grad_norm": 4.2993059158325195,
      "learning_rate": 3.4814940003380092e-06,
      "loss": 0.4353,
      "step": 22520
    },
    {
      "epoch": 3.7288977159880834,
      "grad_norm": 2.6532320976257324,
      "learning_rate": 3.460368429947609e-06,
      "loss": 0.6806,
      "step": 22530
    },
    {
      "epoch": 3.730552797087057,
      "grad_norm": 5.332549095153809,
      "learning_rate": 3.439242859557208e-06,
      "loss": 0.5721,
      "step": 22540
    },
    {
      "epoch": 3.7322078781860313,
      "grad_norm": 6.6692938804626465,
      "learning_rate": 3.4181172891668078e-06,
      "loss": 0.5632,
      "step": 22550
    },
    {
      "epoch": 3.733862959285005,
      "grad_norm": 11.048778533935547,
      "learning_rate": 3.3969917187764074e-06,
      "loss": 0.3965,
      "step": 22560
    },
    {
      "epoch": 3.7355180403839787,
      "grad_norm": 7.150579452514648,
      "learning_rate": 3.3758661483860066e-06,
      "loss": 0.8188,
      "step": 22570
    },
    {
      "epoch": 3.737173121482953,
      "grad_norm": 13.929088592529297,
      "learning_rate": 3.3547405779956063e-06,
      "loss": 0.6529,
      "step": 22580
    },
    {
      "epoch": 3.7388282025819266,
      "grad_norm": 11.02455997467041,
      "learning_rate": 3.3336150076052055e-06,
      "loss": 0.5695,
      "step": 22590
    },
    {
      "epoch": 3.7404832836809003,
      "grad_norm": 6.826391696929932,
      "learning_rate": 3.312489437214805e-06,
      "loss": 0.8182,
      "step": 22600
    },
    {
      "epoch": 3.742138364779874,
      "grad_norm": 6.132340908050537,
      "learning_rate": 3.291363866824405e-06,
      "loss": 0.6773,
      "step": 22610
    },
    {
      "epoch": 3.743793445878848,
      "grad_norm": 12.3432035446167,
      "learning_rate": 3.270238296434004e-06,
      "loss": 0.3927,
      "step": 22620
    },
    {
      "epoch": 3.745448526977822,
      "grad_norm": 3.4967002868652344,
      "learning_rate": 3.249112726043603e-06,
      "loss": 0.564,
      "step": 22630
    },
    {
      "epoch": 3.7471036080767957,
      "grad_norm": 4.718992233276367,
      "learning_rate": 3.2279871556532025e-06,
      "loss": 0.5034,
      "step": 22640
    },
    {
      "epoch": 3.7487586891757694,
      "grad_norm": 5.125853538513184,
      "learning_rate": 3.206861585262802e-06,
      "loss": 0.6141,
      "step": 22650
    },
    {
      "epoch": 3.7504137702747435,
      "grad_norm": 5.338390350341797,
      "learning_rate": 3.1857360148724014e-06,
      "loss": 0.759,
      "step": 22660
    },
    {
      "epoch": 3.7520688513737173,
      "grad_norm": 5.164187908172607,
      "learning_rate": 3.164610444482001e-06,
      "loss": 0.6209,
      "step": 22670
    },
    {
      "epoch": 3.753723932472691,
      "grad_norm": 2.80462908744812,
      "learning_rate": 3.1434848740916003e-06,
      "loss": 0.4548,
      "step": 22680
    },
    {
      "epoch": 3.755379013571665,
      "grad_norm": 9.578734397888184,
      "learning_rate": 3.1223593037012e-06,
      "loss": 0.4978,
      "step": 22690
    },
    {
      "epoch": 3.757034094670639,
      "grad_norm": 6.416243076324463,
      "learning_rate": 3.1012337333107996e-06,
      "loss": 0.7172,
      "step": 22700
    },
    {
      "epoch": 3.7586891757696126,
      "grad_norm": 7.599975109100342,
      "learning_rate": 3.080108162920399e-06,
      "loss": 0.6089,
      "step": 22710
    },
    {
      "epoch": 3.7603442568685868,
      "grad_norm": 12.478181838989258,
      "learning_rate": 3.0589825925299985e-06,
      "loss": 0.3947,
      "step": 22720
    },
    {
      "epoch": 3.7619993379675605,
      "grad_norm": 13.26384449005127,
      "learning_rate": 3.037857022139598e-06,
      "loss": 0.5417,
      "step": 22730
    },
    {
      "epoch": 3.763654419066534,
      "grad_norm": 4.504035472869873,
      "learning_rate": 3.0167314517491974e-06,
      "loss": 0.5623,
      "step": 22740
    },
    {
      "epoch": 3.7653095001655084,
      "grad_norm": 8.44609546661377,
      "learning_rate": 2.995605881358797e-06,
      "loss": 0.4541,
      "step": 22750
    },
    {
      "epoch": 3.766964581264482,
      "grad_norm": 6.163570880889893,
      "learning_rate": 2.9744803109683963e-06,
      "loss": 0.7193,
      "step": 22760
    },
    {
      "epoch": 3.768619662363456,
      "grad_norm": 3.2853446006774902,
      "learning_rate": 2.9533547405779955e-06,
      "loss": 0.3433,
      "step": 22770
    },
    {
      "epoch": 3.7702747434624295,
      "grad_norm": 4.013418197631836,
      "learning_rate": 2.932229170187595e-06,
      "loss": 0.5942,
      "step": 22780
    },
    {
      "epoch": 3.7719298245614032,
      "grad_norm": 2.920304298400879,
      "learning_rate": 2.911103599797195e-06,
      "loss": 0.4557,
      "step": 22790
    },
    {
      "epoch": 3.7735849056603774,
      "grad_norm": 2.2720932960510254,
      "learning_rate": 2.889978029406794e-06,
      "loss": 0.5331,
      "step": 22800
    },
    {
      "epoch": 3.775239986759351,
      "grad_norm": 9.360526084899902,
      "learning_rate": 2.8688524590163937e-06,
      "loss": 0.4678,
      "step": 22810
    },
    {
      "epoch": 3.776895067858325,
      "grad_norm": 4.262598991394043,
      "learning_rate": 2.8477268886259933e-06,
      "loss": 0.6453,
      "step": 22820
    },
    {
      "epoch": 3.778550148957299,
      "grad_norm": 5.511570930480957,
      "learning_rate": 2.8266013182355926e-06,
      "loss": 0.5938,
      "step": 22830
    },
    {
      "epoch": 3.7802052300562727,
      "grad_norm": 2.3325512409210205,
      "learning_rate": 2.805475747845192e-06,
      "loss": 0.4548,
      "step": 22840
    },
    {
      "epoch": 3.7818603111552465,
      "grad_norm": 8.62809944152832,
      "learning_rate": 2.7843501774547914e-06,
      "loss": 0.6428,
      "step": 22850
    },
    {
      "epoch": 3.7835153922542206,
      "grad_norm": 11.40123462677002,
      "learning_rate": 2.7632246070643907e-06,
      "loss": 0.6284,
      "step": 22860
    },
    {
      "epoch": 3.7851704733531943,
      "grad_norm": 5.09042501449585,
      "learning_rate": 2.7420990366739903e-06,
      "loss": 0.5074,
      "step": 22870
    },
    {
      "epoch": 3.786825554452168,
      "grad_norm": 10.753046035766602,
      "learning_rate": 2.72097346628359e-06,
      "loss": 0.4541,
      "step": 22880
    },
    {
      "epoch": 3.7884806355511422,
      "grad_norm": 11.352522850036621,
      "learning_rate": 2.6998478958931892e-06,
      "loss": 0.4254,
      "step": 22890
    },
    {
      "epoch": 3.790135716650116,
      "grad_norm": 16.097597122192383,
      "learning_rate": 2.678722325502789e-06,
      "loss": 0.6165,
      "step": 22900
    },
    {
      "epoch": 3.7917907977490897,
      "grad_norm": 3.0254228115081787,
      "learning_rate": 2.657596755112388e-06,
      "loss": 0.5102,
      "step": 22910
    },
    {
      "epoch": 3.7934458788480634,
      "grad_norm": 10.041312217712402,
      "learning_rate": 2.6364711847219878e-06,
      "loss": 0.5531,
      "step": 22920
    },
    {
      "epoch": 3.7951009599470376,
      "grad_norm": 7.757381916046143,
      "learning_rate": 2.615345614331587e-06,
      "loss": 0.3893,
      "step": 22930
    },
    {
      "epoch": 3.7967560410460113,
      "grad_norm": 6.070517539978027,
      "learning_rate": 2.5942200439411866e-06,
      "loss": 0.7317,
      "step": 22940
    },
    {
      "epoch": 3.798411122144985,
      "grad_norm": 11.308534622192383,
      "learning_rate": 2.573094473550786e-06,
      "loss": 0.5279,
      "step": 22950
    },
    {
      "epoch": 3.8000662032439587,
      "grad_norm": 13.5304536819458,
      "learning_rate": 2.5519689031603855e-06,
      "loss": 0.5306,
      "step": 22960
    },
    {
      "epoch": 3.801721284342933,
      "grad_norm": 11.20168685913086,
      "learning_rate": 2.5308433327699848e-06,
      "loss": 0.6121,
      "step": 22970
    },
    {
      "epoch": 3.8033763654419066,
      "grad_norm": 3.770498037338257,
      "learning_rate": 2.5097177623795844e-06,
      "loss": 0.4501,
      "step": 22980
    },
    {
      "epoch": 3.8050314465408803,
      "grad_norm": 7.839200019836426,
      "learning_rate": 2.488592191989184e-06,
      "loss": 0.3687,
      "step": 22990
    },
    {
      "epoch": 3.8066865276398545,
      "grad_norm": 1.6560808420181274,
      "learning_rate": 2.4674666215987833e-06,
      "loss": 0.386,
      "step": 23000
    },
    {
      "epoch": 3.808341608738828,
      "grad_norm": 6.596641540527344,
      "learning_rate": 2.4463410512083825e-06,
      "loss": 0.7071,
      "step": 23010
    },
    {
      "epoch": 3.809996689837802,
      "grad_norm": 7.863429069519043,
      "learning_rate": 2.425215480817982e-06,
      "loss": 0.4821,
      "step": 23020
    },
    {
      "epoch": 3.811651770936776,
      "grad_norm": 15.29848861694336,
      "learning_rate": 2.4040899104275814e-06,
      "loss": 0.5357,
      "step": 23030
    },
    {
      "epoch": 3.81330685203575,
      "grad_norm": 1.5031355619430542,
      "learning_rate": 2.382964340037181e-06,
      "loss": 0.4293,
      "step": 23040
    },
    {
      "epoch": 3.8149619331347235,
      "grad_norm": 5.534980773925781,
      "learning_rate": 2.3618387696467807e-06,
      "loss": 0.4125,
      "step": 23050
    },
    {
      "epoch": 3.8166170142336977,
      "grad_norm": 8.777741432189941,
      "learning_rate": 2.34071319925638e-06,
      "loss": 0.6011,
      "step": 23060
    },
    {
      "epoch": 3.8182720953326714,
      "grad_norm": 6.985212326049805,
      "learning_rate": 2.3195876288659796e-06,
      "loss": 0.6169,
      "step": 23070
    },
    {
      "epoch": 3.819927176431645,
      "grad_norm": 3.468431234359741,
      "learning_rate": 2.2984620584755793e-06,
      "loss": 0.5297,
      "step": 23080
    },
    {
      "epoch": 3.821582257530619,
      "grad_norm": 7.1241607666015625,
      "learning_rate": 2.2773364880851785e-06,
      "loss": 0.5431,
      "step": 23090
    },
    {
      "epoch": 3.8232373386295926,
      "grad_norm": 10.208841323852539,
      "learning_rate": 2.2562109176947777e-06,
      "loss": 0.7703,
      "step": 23100
    },
    {
      "epoch": 3.8248924197285668,
      "grad_norm": 9.991290092468262,
      "learning_rate": 2.2350853473043774e-06,
      "loss": 0.7424,
      "step": 23110
    },
    {
      "epoch": 3.8265475008275405,
      "grad_norm": 4.327629566192627,
      "learning_rate": 2.2139597769139766e-06,
      "loss": 0.6522,
      "step": 23120
    },
    {
      "epoch": 3.828202581926514,
      "grad_norm": 12.810473442077637,
      "learning_rate": 2.1928342065235763e-06,
      "loss": 0.6381,
      "step": 23130
    },
    {
      "epoch": 3.8298576630254884,
      "grad_norm": 6.9418463706970215,
      "learning_rate": 2.171708636133176e-06,
      "loss": 0.5163,
      "step": 23140
    },
    {
      "epoch": 3.831512744124462,
      "grad_norm": 10.530851364135742,
      "learning_rate": 2.150583065742775e-06,
      "loss": 0.6255,
      "step": 23150
    },
    {
      "epoch": 3.833167825223436,
      "grad_norm": 11.722837448120117,
      "learning_rate": 2.129457495352375e-06,
      "loss": 0.4289,
      "step": 23160
    },
    {
      "epoch": 3.83482290632241,
      "grad_norm": 5.001682758331299,
      "learning_rate": 2.108331924961974e-06,
      "loss": 0.5589,
      "step": 23170
    },
    {
      "epoch": 3.8364779874213837,
      "grad_norm": 5.152156829833984,
      "learning_rate": 2.0872063545715732e-06,
      "loss": 0.8829,
      "step": 23180
    },
    {
      "epoch": 3.8381330685203574,
      "grad_norm": 10.35811710357666,
      "learning_rate": 2.066080784181173e-06,
      "loss": 0.5972,
      "step": 23190
    },
    {
      "epoch": 3.8397881496193316,
      "grad_norm": 1.8243687152862549,
      "learning_rate": 2.0449552137907726e-06,
      "loss": 0.7527,
      "step": 23200
    },
    {
      "epoch": 3.8414432307183053,
      "grad_norm": 16.923095703125,
      "learning_rate": 2.0238296434003718e-06,
      "loss": 0.6232,
      "step": 23210
    },
    {
      "epoch": 3.843098311817279,
      "grad_norm": 8.383341789245605,
      "learning_rate": 2.0027040730099714e-06,
      "loss": 0.3805,
      "step": 23220
    },
    {
      "epoch": 3.8447533929162527,
      "grad_norm": 5.4789958000183105,
      "learning_rate": 1.9815785026195707e-06,
      "loss": 0.5813,
      "step": 23230
    },
    {
      "epoch": 3.846408474015227,
      "grad_norm": 4.473122596740723,
      "learning_rate": 1.9604529322291703e-06,
      "loss": 0.49,
      "step": 23240
    },
    {
      "epoch": 3.8480635551142006,
      "grad_norm": 10.368941307067871,
      "learning_rate": 1.93932736183877e-06,
      "loss": 0.6727,
      "step": 23250
    },
    {
      "epoch": 3.8497186362131743,
      "grad_norm": 6.909063816070557,
      "learning_rate": 1.918201791448369e-06,
      "loss": 0.5183,
      "step": 23260
    },
    {
      "epoch": 3.851373717312148,
      "grad_norm": 15.222658157348633,
      "learning_rate": 1.8970762210579684e-06,
      "loss": 0.6142,
      "step": 23270
    },
    {
      "epoch": 3.8530287984111222,
      "grad_norm": 4.856114387512207,
      "learning_rate": 1.875950650667568e-06,
      "loss": 0.7438,
      "step": 23280
    },
    {
      "epoch": 3.854683879510096,
      "grad_norm": 2.8435184955596924,
      "learning_rate": 1.8548250802771675e-06,
      "loss": 0.6345,
      "step": 23290
    },
    {
      "epoch": 3.8563389606090697,
      "grad_norm": 2.447441577911377,
      "learning_rate": 1.833699509886767e-06,
      "loss": 0.6178,
      "step": 23300
    },
    {
      "epoch": 3.857994041708044,
      "grad_norm": 2.617866277694702,
      "learning_rate": 1.8125739394963664e-06,
      "loss": 0.6608,
      "step": 23310
    },
    {
      "epoch": 3.8596491228070176,
      "grad_norm": 6.449437618255615,
      "learning_rate": 1.791448369105966e-06,
      "loss": 0.3335,
      "step": 23320
    },
    {
      "epoch": 3.8613042039059913,
      "grad_norm": 7.958863735198975,
      "learning_rate": 1.7703227987155655e-06,
      "loss": 0.4494,
      "step": 23330
    },
    {
      "epoch": 3.8629592850049654,
      "grad_norm": 20.071653366088867,
      "learning_rate": 1.749197228325165e-06,
      "loss": 0.5542,
      "step": 23340
    },
    {
      "epoch": 3.864614366103939,
      "grad_norm": 9.853260040283203,
      "learning_rate": 1.7280716579347642e-06,
      "loss": 0.5285,
      "step": 23350
    },
    {
      "epoch": 3.866269447202913,
      "grad_norm": 8.845576286315918,
      "learning_rate": 1.7069460875443636e-06,
      "loss": 0.3022,
      "step": 23360
    },
    {
      "epoch": 3.867924528301887,
      "grad_norm": 12.902731895446777,
      "learning_rate": 1.685820517153963e-06,
      "loss": 0.56,
      "step": 23370
    },
    {
      "epoch": 3.8695796094008608,
      "grad_norm": 2.452481985092163,
      "learning_rate": 1.6646949467635627e-06,
      "loss": 0.4182,
      "step": 23380
    },
    {
      "epoch": 3.8712346904998345,
      "grad_norm": 9.124794006347656,
      "learning_rate": 1.6435693763731622e-06,
      "loss": 0.3691,
      "step": 23390
    },
    {
      "epoch": 3.872889771598808,
      "grad_norm": 13.668585777282715,
      "learning_rate": 1.6224438059827616e-06,
      "loss": 0.6114,
      "step": 23400
    },
    {
      "epoch": 3.874544852697782,
      "grad_norm": 5.08274507522583,
      "learning_rate": 1.601318235592361e-06,
      "loss": 0.4019,
      "step": 23410
    },
    {
      "epoch": 3.876199933796756,
      "grad_norm": 11.110843658447266,
      "learning_rate": 1.5801926652019607e-06,
      "loss": 0.5051,
      "step": 23420
    },
    {
      "epoch": 3.87785501489573,
      "grad_norm": 13.806998252868652,
      "learning_rate": 1.55906709481156e-06,
      "loss": 0.753,
      "step": 23430
    },
    {
      "epoch": 3.8795100959947035,
      "grad_norm": 3.539543628692627,
      "learning_rate": 1.5379415244211594e-06,
      "loss": 0.7216,
      "step": 23440
    },
    {
      "epoch": 3.8811651770936777,
      "grad_norm": 5.3425374031066895,
      "learning_rate": 1.516815954030759e-06,
      "loss": 0.4935,
      "step": 23450
    },
    {
      "epoch": 3.8828202581926514,
      "grad_norm": 5.674301624298096,
      "learning_rate": 1.4956903836403583e-06,
      "loss": 0.5625,
      "step": 23460
    },
    {
      "epoch": 3.884475339291625,
      "grad_norm": 7.194077491760254,
      "learning_rate": 1.4745648132499577e-06,
      "loss": 0.6484,
      "step": 23470
    },
    {
      "epoch": 3.8861304203905993,
      "grad_norm": 2.0296318531036377,
      "learning_rate": 1.4534392428595574e-06,
      "loss": 0.4413,
      "step": 23480
    },
    {
      "epoch": 3.887785501489573,
      "grad_norm": 10.983795166015625,
      "learning_rate": 1.4323136724691568e-06,
      "loss": 0.6393,
      "step": 23490
    },
    {
      "epoch": 3.8894405825885467,
      "grad_norm": 19.954618453979492,
      "learning_rate": 1.411188102078756e-06,
      "loss": 0.5562,
      "step": 23500
    },
    {
      "epoch": 3.891095663687521,
      "grad_norm": 10.294180870056152,
      "learning_rate": 1.3900625316883557e-06,
      "loss": 0.4896,
      "step": 23510
    },
    {
      "epoch": 3.8927507447864946,
      "grad_norm": 8.884443283081055,
      "learning_rate": 1.3689369612979551e-06,
      "loss": 0.4385,
      "step": 23520
    },
    {
      "epoch": 3.8944058258854684,
      "grad_norm": 6.896376609802246,
      "learning_rate": 1.3478113909075546e-06,
      "loss": 0.8669,
      "step": 23530
    },
    {
      "epoch": 3.896060906984442,
      "grad_norm": 6.830526828765869,
      "learning_rate": 1.326685820517154e-06,
      "loss": 0.8096,
      "step": 23540
    },
    {
      "epoch": 3.8977159880834162,
      "grad_norm": 4.428904056549072,
      "learning_rate": 1.3055602501267535e-06,
      "loss": 0.4867,
      "step": 23550
    },
    {
      "epoch": 3.89937106918239,
      "grad_norm": 6.271547794342041,
      "learning_rate": 1.284434679736353e-06,
      "loss": 0.5439,
      "step": 23560
    },
    {
      "epoch": 3.9010261502813637,
      "grad_norm": 10.739569664001465,
      "learning_rate": 1.2633091093459526e-06,
      "loss": 0.4259,
      "step": 23570
    },
    {
      "epoch": 3.9026812313803374,
      "grad_norm": 8.022424697875977,
      "learning_rate": 1.2421835389555518e-06,
      "loss": 0.7112,
      "step": 23580
    },
    {
      "epoch": 3.9043363124793116,
      "grad_norm": 14.72719669342041,
      "learning_rate": 1.2210579685651512e-06,
      "loss": 0.7639,
      "step": 23590
    },
    {
      "epoch": 3.9059913935782853,
      "grad_norm": 6.100985527038574,
      "learning_rate": 1.1999323981747509e-06,
      "loss": 0.4278,
      "step": 23600
    },
    {
      "epoch": 3.907646474677259,
      "grad_norm": 6.241672515869141,
      "learning_rate": 1.1788068277843503e-06,
      "loss": 0.5922,
      "step": 23610
    },
    {
      "epoch": 3.909301555776233,
      "grad_norm": 8.799107551574707,
      "learning_rate": 1.1576812573939498e-06,
      "loss": 0.5573,
      "step": 23620
    },
    {
      "epoch": 3.910956636875207,
      "grad_norm": 7.164446830749512,
      "learning_rate": 1.136555687003549e-06,
      "loss": 0.4517,
      "step": 23630
    },
    {
      "epoch": 3.9126117179741806,
      "grad_norm": 8.480664253234863,
      "learning_rate": 1.1154301166131486e-06,
      "loss": 0.7085,
      "step": 23640
    },
    {
      "epoch": 3.914266799073155,
      "grad_norm": 10.075387954711914,
      "learning_rate": 1.094304546222748e-06,
      "loss": 0.7275,
      "step": 23650
    },
    {
      "epoch": 3.9159218801721285,
      "grad_norm": 7.342435359954834,
      "learning_rate": 1.0731789758323475e-06,
      "loss": 0.6322,
      "step": 23660
    },
    {
      "epoch": 3.917576961271102,
      "grad_norm": 1.8004320859909058,
      "learning_rate": 1.052053405441947e-06,
      "loss": 0.2564,
      "step": 23670
    },
    {
      "epoch": 3.9192320423700764,
      "grad_norm": 7.492271423339844,
      "learning_rate": 1.0309278350515464e-06,
      "loss": 0.5053,
      "step": 23680
    },
    {
      "epoch": 3.92088712346905,
      "grad_norm": 9.781768798828125,
      "learning_rate": 1.0098022646611459e-06,
      "loss": 0.7517,
      "step": 23690
    },
    {
      "epoch": 3.922542204568024,
      "grad_norm": 5.569870948791504,
      "learning_rate": 9.886766942707455e-07,
      "loss": 0.5145,
      "step": 23700
    },
    {
      "epoch": 3.9241972856669975,
      "grad_norm": 4.725621700286865,
      "learning_rate": 9.675511238803447e-07,
      "loss": 0.325,
      "step": 23710
    },
    {
      "epoch": 3.9258523667659713,
      "grad_norm": 7.687855243682861,
      "learning_rate": 9.464255534899443e-07,
      "loss": 0.4187,
      "step": 23720
    },
    {
      "epoch": 3.9275074478649454,
      "grad_norm": 5.558547496795654,
      "learning_rate": 9.252999830995437e-07,
      "loss": 0.3654,
      "step": 23730
    },
    {
      "epoch": 3.929162528963919,
      "grad_norm": 2.8047354221343994,
      "learning_rate": 9.041744127091433e-07,
      "loss": 0.5054,
      "step": 23740
    },
    {
      "epoch": 3.930817610062893,
      "grad_norm": 9.226170539855957,
      "learning_rate": 8.830488423187426e-07,
      "loss": 0.6572,
      "step": 23750
    },
    {
      "epoch": 3.932472691161867,
      "grad_norm": 10.385080337524414,
      "learning_rate": 8.619232719283421e-07,
      "loss": 0.5826,
      "step": 23760
    },
    {
      "epoch": 3.9341277722608408,
      "grad_norm": 11.234435081481934,
      "learning_rate": 8.407977015379416e-07,
      "loss": 0.6965,
      "step": 23770
    },
    {
      "epoch": 3.9357828533598145,
      "grad_norm": 8.193721771240234,
      "learning_rate": 8.19672131147541e-07,
      "loss": 0.4965,
      "step": 23780
    },
    {
      "epoch": 3.9374379344587886,
      "grad_norm": 15.358000755310059,
      "learning_rate": 7.985465607571406e-07,
      "loss": 0.5785,
      "step": 23790
    },
    {
      "epoch": 3.9390930155577624,
      "grad_norm": 26.02456283569336,
      "learning_rate": 7.774209903667399e-07,
      "loss": 0.5161,
      "step": 23800
    },
    {
      "epoch": 3.940748096656736,
      "grad_norm": 9.973167419433594,
      "learning_rate": 7.562954199763394e-07,
      "loss": 0.4757,
      "step": 23810
    },
    {
      "epoch": 3.9424031777557103,
      "grad_norm": 8.993938446044922,
      "learning_rate": 7.351698495859389e-07,
      "loss": 0.5627,
      "step": 23820
    },
    {
      "epoch": 3.944058258854684,
      "grad_norm": 3.739119291305542,
      "learning_rate": 7.140442791955383e-07,
      "loss": 0.6348,
      "step": 23830
    },
    {
      "epoch": 3.9457133399536577,
      "grad_norm": 12.98507022857666,
      "learning_rate": 6.929187088051378e-07,
      "loss": 0.547,
      "step": 23840
    },
    {
      "epoch": 3.9473684210526314,
      "grad_norm": 11.471195220947266,
      "learning_rate": 6.717931384147373e-07,
      "loss": 0.4345,
      "step": 23850
    },
    {
      "epoch": 3.9490235021516056,
      "grad_norm": 14.081214904785156,
      "learning_rate": 6.506675680243367e-07,
      "loss": 0.7173,
      "step": 23860
    },
    {
      "epoch": 3.9506785832505793,
      "grad_norm": 10.567951202392578,
      "learning_rate": 6.295419976339361e-07,
      "loss": 0.7412,
      "step": 23870
    },
    {
      "epoch": 3.952333664349553,
      "grad_norm": 6.795701503753662,
      "learning_rate": 6.084164272435356e-07,
      "loss": 0.6025,
      "step": 23880
    },
    {
      "epoch": 3.9539887454485267,
      "grad_norm": 1.4938005208969116,
      "learning_rate": 5.87290856853135e-07,
      "loss": 0.5,
      "step": 23890
    },
    {
      "epoch": 3.955643826547501,
      "grad_norm": 6.539125919342041,
      "learning_rate": 5.661652864627346e-07,
      "loss": 0.4335,
      "step": 23900
    },
    {
      "epoch": 3.9572989076464746,
      "grad_norm": 2.0038435459136963,
      "learning_rate": 5.450397160723339e-07,
      "loss": 0.5775,
      "step": 23910
    },
    {
      "epoch": 3.9589539887454483,
      "grad_norm": 11.52605152130127,
      "learning_rate": 5.239141456819335e-07,
      "loss": 0.5133,
      "step": 23920
    },
    {
      "epoch": 3.9606090698444225,
      "grad_norm": 5.010540962219238,
      "learning_rate": 5.027885752915329e-07,
      "loss": 0.5758,
      "step": 23930
    },
    {
      "epoch": 3.9622641509433962,
      "grad_norm": 1.3130388259887695,
      "learning_rate": 4.816630049011323e-07,
      "loss": 0.6143,
      "step": 23940
    },
    {
      "epoch": 3.96391923204237,
      "grad_norm": 2.653916835784912,
      "learning_rate": 4.605374345107318e-07,
      "loss": 0.4994,
      "step": 23950
    },
    {
      "epoch": 3.965574313141344,
      "grad_norm": 6.347496509552002,
      "learning_rate": 4.394118641203313e-07,
      "loss": 0.516,
      "step": 23960
    },
    {
      "epoch": 3.967229394240318,
      "grad_norm": 5.922764301300049,
      "learning_rate": 4.1828629372993077e-07,
      "loss": 0.5396,
      "step": 23970
    },
    {
      "epoch": 3.9688844753392916,
      "grad_norm": 14.3089017868042,
      "learning_rate": 3.9716072333953016e-07,
      "loss": 0.9372,
      "step": 23980
    },
    {
      "epoch": 3.9705395564382657,
      "grad_norm": 14.857633590698242,
      "learning_rate": 3.7603515294912965e-07,
      "loss": 0.5519,
      "step": 23990
    },
    {
      "epoch": 3.9721946375372394,
      "grad_norm": 4.580887317657471,
      "learning_rate": 3.549095825587291e-07,
      "loss": 0.7063,
      "step": 24000
    },
    {
      "epoch": 3.973849718636213,
      "grad_norm": 8.34363842010498,
      "learning_rate": 3.3378401216832854e-07,
      "loss": 0.494,
      "step": 24010
    },
    {
      "epoch": 3.975504799735187,
      "grad_norm": 7.58985710144043,
      "learning_rate": 3.12658441777928e-07,
      "loss": 0.337,
      "step": 24020
    },
    {
      "epoch": 3.9771598808341606,
      "grad_norm": 8.942164421081543,
      "learning_rate": 2.915328713875275e-07,
      "loss": 0.8099,
      "step": 24030
    },
    {
      "epoch": 3.9788149619331348,
      "grad_norm": 6.12766170501709,
      "learning_rate": 2.704073009971269e-07,
      "loss": 0.6573,
      "step": 24040
    },
    {
      "epoch": 3.9804700430321085,
      "grad_norm": 9.101509094238281,
      "learning_rate": 2.492817306067264e-07,
      "loss": 0.3579,
      "step": 24050
    },
    {
      "epoch": 3.982125124131082,
      "grad_norm": 1.230706810951233,
      "learning_rate": 2.2815616021632588e-07,
      "loss": 0.628,
      "step": 24060
    },
    {
      "epoch": 3.9837802052300564,
      "grad_norm": 3.964853048324585,
      "learning_rate": 2.0703058982592532e-07,
      "loss": 0.6824,
      "step": 24070
    },
    {
      "epoch": 3.98543528632903,
      "grad_norm": 6.576540946960449,
      "learning_rate": 1.8590501943552477e-07,
      "loss": 0.4377,
      "step": 24080
    },
    {
      "epoch": 3.987090367428004,
      "grad_norm": 6.717467784881592,
      "learning_rate": 1.6477944904512423e-07,
      "loss": 0.632,
      "step": 24090
    },
    {
      "epoch": 3.988745448526978,
      "grad_norm": 7.000547885894775,
      "learning_rate": 1.436538786547237e-07,
      "loss": 0.4856,
      "step": 24100
    },
    {
      "epoch": 3.9904005296259517,
      "grad_norm": 7.232100486755371,
      "learning_rate": 1.2252830826432315e-07,
      "loss": 0.5307,
      "step": 24110
    },
    {
      "epoch": 3.9920556107249254,
      "grad_norm": 9.747944831848145,
      "learning_rate": 1.014027378739226e-07,
      "loss": 0.6258,
      "step": 24120
    },
    {
      "epoch": 3.9937106918238996,
      "grad_norm": 10.839648246765137,
      "learning_rate": 8.027716748352206e-08,
      "loss": 0.8557,
      "step": 24130
    },
    {
      "epoch": 3.9953657729228733,
      "grad_norm": 5.259922504425049,
      "learning_rate": 5.915159709312152e-08,
      "loss": 0.6046,
      "step": 24140
    },
    {
      "epoch": 3.997020854021847,
      "grad_norm": 9.400262832641602,
      "learning_rate": 3.8026026702720974e-08,
      "loss": 0.6663,
      "step": 24150
    },
    {
      "epoch": 3.9986759351208208,
      "grad_norm": 6.701596736907959,
      "learning_rate": 1.6900456312320432e-08,
      "loss": 0.5791,
      "step": 24160
    }
  ],
  "logging_steps": 10,
  "max_steps": 24168,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 2417,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.8095000361873408e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
