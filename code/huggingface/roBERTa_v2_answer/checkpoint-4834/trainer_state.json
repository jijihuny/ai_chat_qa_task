{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.8000662032439589,
  "eval_steps": 500,
  "global_step": 4834,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016550810989738498,
      "grad_norm": 10.491287231445312,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 3.6997,
      "step": 10
    },
    {
      "epoch": 0.0033101621979476996,
      "grad_norm": 8.1528959274292,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 4.0751,
      "step": 20
    },
    {
      "epoch": 0.004965243296921549,
      "grad_norm": 11.886052131652832,
      "learning_rate": 3e-06,
      "loss": 3.9032,
      "step": 30
    },
    {
      "epoch": 0.006620324395895399,
      "grad_norm": 5.415994167327881,
      "learning_rate": 4.000000000000001e-06,
      "loss": 3.4249,
      "step": 40
    },
    {
      "epoch": 0.00827540549486925,
      "grad_norm": 12.988325119018555,
      "learning_rate": 5e-06,
      "loss": 3.5436,
      "step": 50
    },
    {
      "epoch": 0.009930486593843098,
      "grad_norm": 11.426213264465332,
      "learning_rate": 6e-06,
      "loss": 4.8924,
      "step": 60
    },
    {
      "epoch": 0.011585567692816948,
      "grad_norm": 9.009767532348633,
      "learning_rate": 7.000000000000001e-06,
      "loss": 4.0708,
      "step": 70
    },
    {
      "epoch": 0.013240648791790799,
      "grad_norm": 5.40336275100708,
      "learning_rate": 8.000000000000001e-06,
      "loss": 3.6271,
      "step": 80
    },
    {
      "epoch": 0.014895729890764648,
      "grad_norm": 14.442776679992676,
      "learning_rate": 9e-06,
      "loss": 3.5951,
      "step": 90
    },
    {
      "epoch": 0.0165508109897385,
      "grad_norm": 7.636654853820801,
      "learning_rate": 1e-05,
      "loss": 3.6199,
      "step": 100
    },
    {
      "epoch": 0.018205892088712348,
      "grad_norm": 7.5129075050354,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 3.2084,
      "step": 110
    },
    {
      "epoch": 0.019860973187686197,
      "grad_norm": 10.015198707580566,
      "learning_rate": 1.2e-05,
      "loss": 3.9251,
      "step": 120
    },
    {
      "epoch": 0.021516054286660046,
      "grad_norm": 13.5173978805542,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 4.1535,
      "step": 130
    },
    {
      "epoch": 0.023171135385633895,
      "grad_norm": 8.180093765258789,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 3.5428,
      "step": 140
    },
    {
      "epoch": 0.024826216484607744,
      "grad_norm": 2.851651906967163,
      "learning_rate": 1.5e-05,
      "loss": 2.7841,
      "step": 150
    },
    {
      "epoch": 0.026481297583581597,
      "grad_norm": 11.961166381835938,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 2.3978,
      "step": 160
    },
    {
      "epoch": 0.028136378682555446,
      "grad_norm": 8.412672996520996,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 2.4668,
      "step": 170
    },
    {
      "epoch": 0.029791459781529295,
      "grad_norm": 14.415621757507324,
      "learning_rate": 1.8e-05,
      "loss": 2.8025,
      "step": 180
    },
    {
      "epoch": 0.031446540880503145,
      "grad_norm": 12.371260643005371,
      "learning_rate": 1.9e-05,
      "loss": 2.2191,
      "step": 190
    },
    {
      "epoch": 0.033101621979477,
      "grad_norm": 15.451358795166016,
      "learning_rate": 2e-05,
      "loss": 2.3608,
      "step": 200
    },
    {
      "epoch": 0.03475670307845084,
      "grad_norm": 7.22649621963501,
      "learning_rate": 2.1e-05,
      "loss": 1.8181,
      "step": 210
    },
    {
      "epoch": 0.036411784177424696,
      "grad_norm": 4.971599578857422,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.7533,
      "step": 220
    },
    {
      "epoch": 0.03806686527639854,
      "grad_norm": 4.988760948181152,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 1.8505,
      "step": 230
    },
    {
      "epoch": 0.039721946375372394,
      "grad_norm": 3.332304000854492,
      "learning_rate": 2.4e-05,
      "loss": 1.8816,
      "step": 240
    },
    {
      "epoch": 0.04137702747434624,
      "grad_norm": 7.057027339935303,
      "learning_rate": 2.5e-05,
      "loss": 1.6548,
      "step": 250
    },
    {
      "epoch": 0.04303210857332009,
      "grad_norm": 7.131735324859619,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.9387,
      "step": 260
    },
    {
      "epoch": 0.044687189672293945,
      "grad_norm": 8.290486335754395,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.8058,
      "step": 270
    },
    {
      "epoch": 0.04634227077126779,
      "grad_norm": 5.917972087860107,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.8249,
      "step": 280
    },
    {
      "epoch": 0.04799735187024164,
      "grad_norm": 6.318717002868652,
      "learning_rate": 2.9e-05,
      "loss": 1.2006,
      "step": 290
    },
    {
      "epoch": 0.04965243296921549,
      "grad_norm": 5.791667938232422,
      "learning_rate": 3e-05,
      "loss": 2.1717,
      "step": 300
    },
    {
      "epoch": 0.05130751406818934,
      "grad_norm": 5.035686492919922,
      "learning_rate": 3.1e-05,
      "loss": 1.9212,
      "step": 310
    },
    {
      "epoch": 0.052962595167163194,
      "grad_norm": 7.047187805175781,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.7182,
      "step": 320
    },
    {
      "epoch": 0.05461767626613704,
      "grad_norm": 5.455808639526367,
      "learning_rate": 3.3e-05,
      "loss": 1.352,
      "step": 330
    },
    {
      "epoch": 0.05627275736511089,
      "grad_norm": 7.1284308433532715,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.2526,
      "step": 340
    },
    {
      "epoch": 0.05792783846408474,
      "grad_norm": 2.6936628818511963,
      "learning_rate": 3.5e-05,
      "loss": 1.4086,
      "step": 350
    },
    {
      "epoch": 0.05958291956305859,
      "grad_norm": 6.265352249145508,
      "learning_rate": 3.6e-05,
      "loss": 1.4256,
      "step": 360
    },
    {
      "epoch": 0.06123800066203244,
      "grad_norm": 6.356632709503174,
      "learning_rate": 3.7e-05,
      "loss": 1.6221,
      "step": 370
    },
    {
      "epoch": 0.06289308176100629,
      "grad_norm": 4.151512145996094,
      "learning_rate": 3.8e-05,
      "loss": 1.2793,
      "step": 380
    },
    {
      "epoch": 0.06454816285998013,
      "grad_norm": 5.9260993003845215,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.6107,
      "step": 390
    },
    {
      "epoch": 0.066203243958954,
      "grad_norm": 4.526922225952148,
      "learning_rate": 4e-05,
      "loss": 1.5396,
      "step": 400
    },
    {
      "epoch": 0.06785832505792784,
      "grad_norm": 5.142249584197998,
      "learning_rate": 4.1e-05,
      "loss": 1.5912,
      "step": 410
    },
    {
      "epoch": 0.06951340615690169,
      "grad_norm": 5.746337413787842,
      "learning_rate": 4.2e-05,
      "loss": 1.4797,
      "step": 420
    },
    {
      "epoch": 0.07116848725587553,
      "grad_norm": 2.304018497467041,
      "learning_rate": 4.3e-05,
      "loss": 0.9609,
      "step": 430
    },
    {
      "epoch": 0.07282356835484939,
      "grad_norm": 6.13914155960083,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.1815,
      "step": 440
    },
    {
      "epoch": 0.07447864945382324,
      "grad_norm": 6.120832443237305,
      "learning_rate": 4.5e-05,
      "loss": 1.6981,
      "step": 450
    },
    {
      "epoch": 0.07613373055279708,
      "grad_norm": 3.058312177658081,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.3897,
      "step": 460
    },
    {
      "epoch": 0.07778881165177094,
      "grad_norm": 7.008513927459717,
      "learning_rate": 4.7e-05,
      "loss": 0.8884,
      "step": 470
    },
    {
      "epoch": 0.07944389275074479,
      "grad_norm": 8.14610481262207,
      "learning_rate": 4.8e-05,
      "loss": 1.8417,
      "step": 480
    },
    {
      "epoch": 0.08109897384971863,
      "grad_norm": 9.467963218688965,
      "learning_rate": 4.9e-05,
      "loss": 1.4363,
      "step": 490
    },
    {
      "epoch": 0.08275405494869248,
      "grad_norm": 6.290310859680176,
      "learning_rate": 5e-05,
      "loss": 1.2156,
      "step": 500
    },
    {
      "epoch": 0.08440913604766634,
      "grad_norm": 10.386557579040527,
      "learning_rate": 4.99788744296096e-05,
      "loss": 1.0915,
      "step": 510
    },
    {
      "epoch": 0.08606421714664018,
      "grad_norm": 4.732298851013184,
      "learning_rate": 4.99577488592192e-05,
      "loss": 1.0337,
      "step": 520
    },
    {
      "epoch": 0.08771929824561403,
      "grad_norm": 7.166783809661865,
      "learning_rate": 4.99366232888288e-05,
      "loss": 1.3134,
      "step": 530
    },
    {
      "epoch": 0.08937437934458789,
      "grad_norm": 8.70981502532959,
      "learning_rate": 4.9915497718438396e-05,
      "loss": 1.4855,
      "step": 540
    },
    {
      "epoch": 0.09102946044356174,
      "grad_norm": 9.945058822631836,
      "learning_rate": 4.9894372148048e-05,
      "loss": 1.2601,
      "step": 550
    },
    {
      "epoch": 0.09268454154253558,
      "grad_norm": 3.8860156536102295,
      "learning_rate": 4.98732465776576e-05,
      "loss": 0.9946,
      "step": 560
    },
    {
      "epoch": 0.09433962264150944,
      "grad_norm": 5.446142673492432,
      "learning_rate": 4.98521210072672e-05,
      "loss": 1.2979,
      "step": 570
    },
    {
      "epoch": 0.09599470374048329,
      "grad_norm": 13.52455997467041,
      "learning_rate": 4.9830995436876796e-05,
      "loss": 1.1829,
      "step": 580
    },
    {
      "epoch": 0.09764978483945713,
      "grad_norm": 6.01290225982666,
      "learning_rate": 4.9809869866486395e-05,
      "loss": 1.4549,
      "step": 590
    },
    {
      "epoch": 0.09930486593843098,
      "grad_norm": 6.11596155166626,
      "learning_rate": 4.978874429609599e-05,
      "loss": 1.1775,
      "step": 600
    },
    {
      "epoch": 0.10095994703740484,
      "grad_norm": 7.768533706665039,
      "learning_rate": 4.97676187257056e-05,
      "loss": 1.5284,
      "step": 610
    },
    {
      "epoch": 0.10261502813637868,
      "grad_norm": 4.7729973793029785,
      "learning_rate": 4.9746493155315197e-05,
      "loss": 0.9855,
      "step": 620
    },
    {
      "epoch": 0.10427010923535253,
      "grad_norm": 6.364742755889893,
      "learning_rate": 4.9725367584924795e-05,
      "loss": 1.0399,
      "step": 630
    },
    {
      "epoch": 0.10592519033432639,
      "grad_norm": 9.551055908203125,
      "learning_rate": 4.97042420145344e-05,
      "loss": 1.2766,
      "step": 640
    },
    {
      "epoch": 0.10758027143330023,
      "grad_norm": 5.833217144012451,
      "learning_rate": 4.9683116444144e-05,
      "loss": 1.3197,
      "step": 650
    },
    {
      "epoch": 0.10923535253227408,
      "grad_norm": 9.327737808227539,
      "learning_rate": 4.96619908737536e-05,
      "loss": 1.2004,
      "step": 660
    },
    {
      "epoch": 0.11089043363124793,
      "grad_norm": 6.029761791229248,
      "learning_rate": 4.9640865303363195e-05,
      "loss": 1.1285,
      "step": 670
    },
    {
      "epoch": 0.11254551473022179,
      "grad_norm": 7.959981918334961,
      "learning_rate": 4.9619739732972794e-05,
      "loss": 1.3077,
      "step": 680
    },
    {
      "epoch": 0.11420059582919563,
      "grad_norm": 7.7021260261535645,
      "learning_rate": 4.959861416258239e-05,
      "loss": 1.2503,
      "step": 690
    },
    {
      "epoch": 0.11585567692816948,
      "grad_norm": 4.773941516876221,
      "learning_rate": 4.957748859219199e-05,
      "loss": 1.2603,
      "step": 700
    },
    {
      "epoch": 0.11751075802714334,
      "grad_norm": 11.288993835449219,
      "learning_rate": 4.955636302180159e-05,
      "loss": 0.8977,
      "step": 710
    },
    {
      "epoch": 0.11916583912611718,
      "grad_norm": 9.984114646911621,
      "learning_rate": 4.9535237451411194e-05,
      "loss": 0.9651,
      "step": 720
    },
    {
      "epoch": 0.12082092022509103,
      "grad_norm": 4.741395473480225,
      "learning_rate": 4.951411188102079e-05,
      "loss": 1.508,
      "step": 730
    },
    {
      "epoch": 0.12247600132406487,
      "grad_norm": 6.255035400390625,
      "learning_rate": 4.949298631063039e-05,
      "loss": 0.9833,
      "step": 740
    },
    {
      "epoch": 0.12413108242303873,
      "grad_norm": 4.81515645980835,
      "learning_rate": 4.947186074023999e-05,
      "loss": 1.0659,
      "step": 750
    },
    {
      "epoch": 0.12578616352201258,
      "grad_norm": 8.93841552734375,
      "learning_rate": 4.945073516984959e-05,
      "loss": 1.0845,
      "step": 760
    },
    {
      "epoch": 0.12744124462098644,
      "grad_norm": 11.186384201049805,
      "learning_rate": 4.9429609599459186e-05,
      "loss": 1.0993,
      "step": 770
    },
    {
      "epoch": 0.12909632571996027,
      "grad_norm": 4.88622522354126,
      "learning_rate": 4.9408484029068784e-05,
      "loss": 1.2958,
      "step": 780
    },
    {
      "epoch": 0.13075140681893413,
      "grad_norm": 8.505717277526855,
      "learning_rate": 4.938735845867838e-05,
      "loss": 1.3057,
      "step": 790
    },
    {
      "epoch": 0.132406487917908,
      "grad_norm": 7.886797904968262,
      "learning_rate": 4.936623288828799e-05,
      "loss": 1.2592,
      "step": 800
    },
    {
      "epoch": 0.13406156901688182,
      "grad_norm": 5.561014652252197,
      "learning_rate": 4.9345107317897586e-05,
      "loss": 1.1112,
      "step": 810
    },
    {
      "epoch": 0.13571665011585568,
      "grad_norm": 7.795823574066162,
      "learning_rate": 4.9323981747507185e-05,
      "loss": 1.1049,
      "step": 820
    },
    {
      "epoch": 0.13737173121482954,
      "grad_norm": 4.678593158721924,
      "learning_rate": 4.930285617711678e-05,
      "loss": 0.9318,
      "step": 830
    },
    {
      "epoch": 0.13902681231380337,
      "grad_norm": 6.312258720397949,
      "learning_rate": 4.928173060672638e-05,
      "loss": 0.8961,
      "step": 840
    },
    {
      "epoch": 0.14068189341277723,
      "grad_norm": 5.5256123542785645,
      "learning_rate": 4.926060503633598e-05,
      "loss": 1.058,
      "step": 850
    },
    {
      "epoch": 0.14233697451175106,
      "grad_norm": 9.251810073852539,
      "learning_rate": 4.9239479465945585e-05,
      "loss": 1.0689,
      "step": 860
    },
    {
      "epoch": 0.14399205561072492,
      "grad_norm": 10.534529685974121,
      "learning_rate": 4.921835389555518e-05,
      "loss": 1.2571,
      "step": 870
    },
    {
      "epoch": 0.14564713670969878,
      "grad_norm": 8.464441299438477,
      "learning_rate": 4.919722832516478e-05,
      "loss": 0.7042,
      "step": 880
    },
    {
      "epoch": 0.14730221780867261,
      "grad_norm": 6.656115531921387,
      "learning_rate": 4.917610275477439e-05,
      "loss": 1.1147,
      "step": 890
    },
    {
      "epoch": 0.14895729890764647,
      "grad_norm": 5.315909385681152,
      "learning_rate": 4.9154977184383985e-05,
      "loss": 1.085,
      "step": 900
    },
    {
      "epoch": 0.15061238000662033,
      "grad_norm": 6.3923444747924805,
      "learning_rate": 4.9133851613993584e-05,
      "loss": 0.8607,
      "step": 910
    },
    {
      "epoch": 0.15226746110559417,
      "grad_norm": 1.0524730682373047,
      "learning_rate": 4.911272604360318e-05,
      "loss": 1.4105,
      "step": 920
    },
    {
      "epoch": 0.15392254220456802,
      "grad_norm": 8.541412353515625,
      "learning_rate": 4.909160047321278e-05,
      "loss": 1.2395,
      "step": 930
    },
    {
      "epoch": 0.15557762330354188,
      "grad_norm": 8.274399757385254,
      "learning_rate": 4.907047490282238e-05,
      "loss": 1.2217,
      "step": 940
    },
    {
      "epoch": 0.15723270440251572,
      "grad_norm": 12.976698875427246,
      "learning_rate": 4.904934933243198e-05,
      "loss": 0.9044,
      "step": 950
    },
    {
      "epoch": 0.15888778550148958,
      "grad_norm": 9.225919723510742,
      "learning_rate": 4.9028223762041575e-05,
      "loss": 1.6052,
      "step": 960
    },
    {
      "epoch": 0.16054286660046344,
      "grad_norm": 10.446148872375488,
      "learning_rate": 4.900709819165118e-05,
      "loss": 1.1844,
      "step": 970
    },
    {
      "epoch": 0.16219794769943727,
      "grad_norm": 7.481561660766602,
      "learning_rate": 4.898597262126078e-05,
      "loss": 0.9881,
      "step": 980
    },
    {
      "epoch": 0.16385302879841113,
      "grad_norm": 6.5786919593811035,
      "learning_rate": 4.896484705087038e-05,
      "loss": 1.2164,
      "step": 990
    },
    {
      "epoch": 0.16550810989738496,
      "grad_norm": 7.9727582931518555,
      "learning_rate": 4.8943721480479976e-05,
      "loss": 0.9195,
      "step": 1000
    },
    {
      "epoch": 0.16716319099635882,
      "grad_norm": 5.710793972015381,
      "learning_rate": 4.8922595910089574e-05,
      "loss": 1.2506,
      "step": 1010
    },
    {
      "epoch": 0.16881827209533268,
      "grad_norm": 5.972925662994385,
      "learning_rate": 4.890147033969917e-05,
      "loss": 0.9573,
      "step": 1020
    },
    {
      "epoch": 0.1704733531943065,
      "grad_norm": 4.672503471374512,
      "learning_rate": 4.888034476930877e-05,
      "loss": 1.076,
      "step": 1030
    },
    {
      "epoch": 0.17212843429328037,
      "grad_norm": 8.415233612060547,
      "learning_rate": 4.885921919891837e-05,
      "loss": 0.8532,
      "step": 1040
    },
    {
      "epoch": 0.17378351539225423,
      "grad_norm": 5.87423038482666,
      "learning_rate": 4.883809362852797e-05,
      "loss": 1.0938,
      "step": 1050
    },
    {
      "epoch": 0.17543859649122806,
      "grad_norm": 7.390071868896484,
      "learning_rate": 4.881696805813757e-05,
      "loss": 0.9239,
      "step": 1060
    },
    {
      "epoch": 0.17709367759020192,
      "grad_norm": 7.279072284698486,
      "learning_rate": 4.879584248774717e-05,
      "loss": 1.4614,
      "step": 1070
    },
    {
      "epoch": 0.17874875868917578,
      "grad_norm": 7.712642669677734,
      "learning_rate": 4.877471691735677e-05,
      "loss": 1.2552,
      "step": 1080
    },
    {
      "epoch": 0.1804038397881496,
      "grad_norm": 5.452831745147705,
      "learning_rate": 4.875359134696637e-05,
      "loss": 1.0082,
      "step": 1090
    },
    {
      "epoch": 0.18205892088712347,
      "grad_norm": 3.5975940227508545,
      "learning_rate": 4.8732465776575966e-05,
      "loss": 1.0337,
      "step": 1100
    },
    {
      "epoch": 0.18371400198609733,
      "grad_norm": 5.293908596038818,
      "learning_rate": 4.871134020618557e-05,
      "loss": 1.1785,
      "step": 1110
    },
    {
      "epoch": 0.18536908308507116,
      "grad_norm": 6.499624252319336,
      "learning_rate": 4.869021463579517e-05,
      "loss": 0.9932,
      "step": 1120
    },
    {
      "epoch": 0.18702416418404502,
      "grad_norm": 6.528020858764648,
      "learning_rate": 4.866908906540477e-05,
      "loss": 1.0532,
      "step": 1130
    },
    {
      "epoch": 0.18867924528301888,
      "grad_norm": 6.886061668395996,
      "learning_rate": 4.8647963495014373e-05,
      "loss": 1.2608,
      "step": 1140
    },
    {
      "epoch": 0.1903343263819927,
      "grad_norm": 1.0990080833435059,
      "learning_rate": 4.862683792462397e-05,
      "loss": 0.6045,
      "step": 1150
    },
    {
      "epoch": 0.19198940748096657,
      "grad_norm": 9.182982444763184,
      "learning_rate": 4.860571235423357e-05,
      "loss": 1.1212,
      "step": 1160
    },
    {
      "epoch": 0.1936444885799404,
      "grad_norm": 12.97887134552002,
      "learning_rate": 4.858458678384317e-05,
      "loss": 1.2021,
      "step": 1170
    },
    {
      "epoch": 0.19529956967891426,
      "grad_norm": 10.343672752380371,
      "learning_rate": 4.856346121345277e-05,
      "loss": 0.9037,
      "step": 1180
    },
    {
      "epoch": 0.19695465077788812,
      "grad_norm": 5.573896884918213,
      "learning_rate": 4.8542335643062365e-05,
      "loss": 1.181,
      "step": 1190
    },
    {
      "epoch": 0.19860973187686196,
      "grad_norm": 4.694684982299805,
      "learning_rate": 4.8521210072671964e-05,
      "loss": 1.2865,
      "step": 1200
    },
    {
      "epoch": 0.20026481297583582,
      "grad_norm": 6.90242862701416,
      "learning_rate": 4.850008450228156e-05,
      "loss": 1.0798,
      "step": 1210
    },
    {
      "epoch": 0.20191989407480967,
      "grad_norm": 6.095139980316162,
      "learning_rate": 4.847895893189116e-05,
      "loss": 1.0511,
      "step": 1220
    },
    {
      "epoch": 0.2035749751737835,
      "grad_norm": 7.662085056304932,
      "learning_rate": 4.8457833361500766e-05,
      "loss": 1.1618,
      "step": 1230
    },
    {
      "epoch": 0.20523005627275737,
      "grad_norm": 4.982565879821777,
      "learning_rate": 4.8436707791110364e-05,
      "loss": 0.8293,
      "step": 1240
    },
    {
      "epoch": 0.20688513737173123,
      "grad_norm": 11.322824478149414,
      "learning_rate": 4.841558222071996e-05,
      "loss": 0.8927,
      "step": 1250
    },
    {
      "epoch": 0.20854021847070506,
      "grad_norm": 4.266307353973389,
      "learning_rate": 4.839445665032956e-05,
      "loss": 0.7087,
      "step": 1260
    },
    {
      "epoch": 0.21019529956967892,
      "grad_norm": 9.22838306427002,
      "learning_rate": 4.837333107993916e-05,
      "loss": 1.0479,
      "step": 1270
    },
    {
      "epoch": 0.21185038066865278,
      "grad_norm": 14.849045753479004,
      "learning_rate": 4.835220550954876e-05,
      "loss": 0.9063,
      "step": 1280
    },
    {
      "epoch": 0.2135054617676266,
      "grad_norm": 12.852745056152344,
      "learning_rate": 4.8331079939158356e-05,
      "loss": 0.9111,
      "step": 1290
    },
    {
      "epoch": 0.21516054286660047,
      "grad_norm": 10.388618469238281,
      "learning_rate": 4.8309954368767954e-05,
      "loss": 0.8381,
      "step": 1300
    },
    {
      "epoch": 0.2168156239655743,
      "grad_norm": 8.935135841369629,
      "learning_rate": 4.828882879837756e-05,
      "loss": 0.793,
      "step": 1310
    },
    {
      "epoch": 0.21847070506454816,
      "grad_norm": 9.99417495727539,
      "learning_rate": 4.826770322798716e-05,
      "loss": 1.4309,
      "step": 1320
    },
    {
      "epoch": 0.22012578616352202,
      "grad_norm": 9.451557159423828,
      "learning_rate": 4.8246577657596756e-05,
      "loss": 1.148,
      "step": 1330
    },
    {
      "epoch": 0.22178086726249585,
      "grad_norm": 5.033353328704834,
      "learning_rate": 4.8225452087206355e-05,
      "loss": 0.7394,
      "step": 1340
    },
    {
      "epoch": 0.2234359483614697,
      "grad_norm": 7.628906726837158,
      "learning_rate": 4.820432651681595e-05,
      "loss": 0.7979,
      "step": 1350
    },
    {
      "epoch": 0.22509102946044357,
      "grad_norm": 4.09216833114624,
      "learning_rate": 4.818320094642556e-05,
      "loss": 0.9801,
      "step": 1360
    },
    {
      "epoch": 0.2267461105594174,
      "grad_norm": 9.092968940734863,
      "learning_rate": 4.816207537603516e-05,
      "loss": 1.5788,
      "step": 1370
    },
    {
      "epoch": 0.22840119165839126,
      "grad_norm": 8.166152000427246,
      "learning_rate": 4.8140949805644755e-05,
      "loss": 0.7949,
      "step": 1380
    },
    {
      "epoch": 0.23005627275736512,
      "grad_norm": 4.641820907592773,
      "learning_rate": 4.811982423525435e-05,
      "loss": 1.2087,
      "step": 1390
    },
    {
      "epoch": 0.23171135385633895,
      "grad_norm": 7.541827201843262,
      "learning_rate": 4.809869866486396e-05,
      "loss": 1.0402,
      "step": 1400
    },
    {
      "epoch": 0.2333664349553128,
      "grad_norm": 4.966800689697266,
      "learning_rate": 4.807757309447356e-05,
      "loss": 1.2821,
      "step": 1410
    },
    {
      "epoch": 0.23502151605428667,
      "grad_norm": 4.65952730178833,
      "learning_rate": 4.8056447524083155e-05,
      "loss": 1.1426,
      "step": 1420
    },
    {
      "epoch": 0.2366765971532605,
      "grad_norm": 7.216237545013428,
      "learning_rate": 4.8035321953692754e-05,
      "loss": 0.898,
      "step": 1430
    },
    {
      "epoch": 0.23833167825223436,
      "grad_norm": 9.235370635986328,
      "learning_rate": 4.801419638330235e-05,
      "loss": 0.9671,
      "step": 1440
    },
    {
      "epoch": 0.2399867593512082,
      "grad_norm": 4.024963855743408,
      "learning_rate": 4.799307081291195e-05,
      "loss": 0.6373,
      "step": 1450
    },
    {
      "epoch": 0.24164184045018205,
      "grad_norm": 6.676187038421631,
      "learning_rate": 4.797194524252155e-05,
      "loss": 1.0034,
      "step": 1460
    },
    {
      "epoch": 0.24329692154915591,
      "grad_norm": 6.9953413009643555,
      "learning_rate": 4.795081967213115e-05,
      "loss": 1.2088,
      "step": 1470
    },
    {
      "epoch": 0.24495200264812975,
      "grad_norm": 2.9078562259674072,
      "learning_rate": 4.792969410174075e-05,
      "loss": 0.8398,
      "step": 1480
    },
    {
      "epoch": 0.2466070837471036,
      "grad_norm": 6.16707706451416,
      "learning_rate": 4.790856853135035e-05,
      "loss": 1.1536,
      "step": 1490
    },
    {
      "epoch": 0.24826216484607747,
      "grad_norm": 4.324840545654297,
      "learning_rate": 4.788744296095995e-05,
      "loss": 1.0408,
      "step": 1500
    },
    {
      "epoch": 0.2499172459450513,
      "grad_norm": 15.712027549743652,
      "learning_rate": 4.786631739056955e-05,
      "loss": 0.9953,
      "step": 1510
    },
    {
      "epoch": 0.25157232704402516,
      "grad_norm": 9.069661140441895,
      "learning_rate": 4.7845191820179146e-05,
      "loss": 1.2565,
      "step": 1520
    },
    {
      "epoch": 0.253227408142999,
      "grad_norm": 12.16580581665039,
      "learning_rate": 4.7824066249788744e-05,
      "loss": 0.9488,
      "step": 1530
    },
    {
      "epoch": 0.2548824892419729,
      "grad_norm": 13.393353462219238,
      "learning_rate": 4.780294067939834e-05,
      "loss": 0.733,
      "step": 1540
    },
    {
      "epoch": 0.2565375703409467,
      "grad_norm": 3.2282142639160156,
      "learning_rate": 4.778181510900794e-05,
      "loss": 0.852,
      "step": 1550
    },
    {
      "epoch": 0.25819265143992054,
      "grad_norm": 6.335540294647217,
      "learning_rate": 4.776068953861754e-05,
      "loss": 1.0715,
      "step": 1560
    },
    {
      "epoch": 0.2598477325388944,
      "grad_norm": 9.485562324523926,
      "learning_rate": 4.7739563968227145e-05,
      "loss": 0.7638,
      "step": 1570
    },
    {
      "epoch": 0.26150281363786826,
      "grad_norm": 5.468998908996582,
      "learning_rate": 4.771843839783674e-05,
      "loss": 0.796,
      "step": 1580
    },
    {
      "epoch": 0.2631578947368421,
      "grad_norm": 4.316401481628418,
      "learning_rate": 4.769731282744634e-05,
      "loss": 0.9601,
      "step": 1590
    },
    {
      "epoch": 0.264812975835816,
      "grad_norm": 8.861205101013184,
      "learning_rate": 4.767618725705594e-05,
      "loss": 0.824,
      "step": 1600
    },
    {
      "epoch": 0.2664680569347898,
      "grad_norm": 5.394019603729248,
      "learning_rate": 4.7655061686665545e-05,
      "loss": 0.8,
      "step": 1610
    },
    {
      "epoch": 0.26812313803376364,
      "grad_norm": 6.36236047744751,
      "learning_rate": 4.763393611627514e-05,
      "loss": 1.3162,
      "step": 1620
    },
    {
      "epoch": 0.26977821913273753,
      "grad_norm": 5.691561222076416,
      "learning_rate": 4.761281054588474e-05,
      "loss": 0.7874,
      "step": 1630
    },
    {
      "epoch": 0.27143330023171136,
      "grad_norm": 11.217209815979004,
      "learning_rate": 4.759168497549434e-05,
      "loss": 1.2582,
      "step": 1640
    },
    {
      "epoch": 0.2730883813306852,
      "grad_norm": 11.502766609191895,
      "learning_rate": 4.7570559405103945e-05,
      "loss": 0.9681,
      "step": 1650
    },
    {
      "epoch": 0.2747434624296591,
      "grad_norm": 5.960086822509766,
      "learning_rate": 4.7549433834713544e-05,
      "loss": 0.9101,
      "step": 1660
    },
    {
      "epoch": 0.2763985435286329,
      "grad_norm": 4.755646228790283,
      "learning_rate": 4.752830826432314e-05,
      "loss": 1.1065,
      "step": 1670
    },
    {
      "epoch": 0.27805362462760674,
      "grad_norm": 5.744128227233887,
      "learning_rate": 4.750718269393274e-05,
      "loss": 0.7898,
      "step": 1680
    },
    {
      "epoch": 0.2797087057265806,
      "grad_norm": 6.398251056671143,
      "learning_rate": 4.748605712354234e-05,
      "loss": 0.881,
      "step": 1690
    },
    {
      "epoch": 0.28136378682555446,
      "grad_norm": 11.929609298706055,
      "learning_rate": 4.746493155315194e-05,
      "loss": 1.1886,
      "step": 1700
    },
    {
      "epoch": 0.2830188679245283,
      "grad_norm": 11.165188789367676,
      "learning_rate": 4.7443805982761536e-05,
      "loss": 1.3147,
      "step": 1710
    },
    {
      "epoch": 0.2846739490235021,
      "grad_norm": 7.1000471115112305,
      "learning_rate": 4.7422680412371134e-05,
      "loss": 1.0023,
      "step": 1720
    },
    {
      "epoch": 0.286329030122476,
      "grad_norm": 8.47039794921875,
      "learning_rate": 4.740155484198073e-05,
      "loss": 0.9653,
      "step": 1730
    },
    {
      "epoch": 0.28798411122144985,
      "grad_norm": 6.560060024261475,
      "learning_rate": 4.738042927159034e-05,
      "loss": 1.1021,
      "step": 1740
    },
    {
      "epoch": 0.2896391923204237,
      "grad_norm": 13.052764892578125,
      "learning_rate": 4.7359303701199936e-05,
      "loss": 1.2055,
      "step": 1750
    },
    {
      "epoch": 0.29129427341939756,
      "grad_norm": 3.893538475036621,
      "learning_rate": 4.7338178130809534e-05,
      "loss": 1.0219,
      "step": 1760
    },
    {
      "epoch": 0.2929493545183714,
      "grad_norm": 4.123079776763916,
      "learning_rate": 4.731705256041913e-05,
      "loss": 1.0483,
      "step": 1770
    },
    {
      "epoch": 0.29460443561734523,
      "grad_norm": 8.050045013427734,
      "learning_rate": 4.729592699002873e-05,
      "loss": 1.1029,
      "step": 1780
    },
    {
      "epoch": 0.2962595167163191,
      "grad_norm": 5.392127513885498,
      "learning_rate": 4.727480141963833e-05,
      "loss": 0.8663,
      "step": 1790
    },
    {
      "epoch": 0.29791459781529295,
      "grad_norm": 7.382465362548828,
      "learning_rate": 4.725367584924793e-05,
      "loss": 0.6713,
      "step": 1800
    },
    {
      "epoch": 0.2995696789142668,
      "grad_norm": 8.146446228027344,
      "learning_rate": 4.7232550278857526e-05,
      "loss": 1.055,
      "step": 1810
    },
    {
      "epoch": 0.30122476001324067,
      "grad_norm": 8.0614013671875,
      "learning_rate": 4.721142470846713e-05,
      "loss": 1.318,
      "step": 1820
    },
    {
      "epoch": 0.3028798411122145,
      "grad_norm": 3.2113471031188965,
      "learning_rate": 4.719029913807673e-05,
      "loss": 0.8232,
      "step": 1830
    },
    {
      "epoch": 0.30453492221118833,
      "grad_norm": 4.718456268310547,
      "learning_rate": 4.716917356768633e-05,
      "loss": 0.8375,
      "step": 1840
    },
    {
      "epoch": 0.3061900033101622,
      "grad_norm": 5.668194770812988,
      "learning_rate": 4.714804799729593e-05,
      "loss": 0.8218,
      "step": 1850
    },
    {
      "epoch": 0.30784508440913605,
      "grad_norm": 8.703760147094727,
      "learning_rate": 4.712692242690553e-05,
      "loss": 1.3189,
      "step": 1860
    },
    {
      "epoch": 0.3095001655081099,
      "grad_norm": 6.846735954284668,
      "learning_rate": 4.710579685651513e-05,
      "loss": 1.0485,
      "step": 1870
    },
    {
      "epoch": 0.31115524660708377,
      "grad_norm": 9.82274055480957,
      "learning_rate": 4.708467128612473e-05,
      "loss": 1.0404,
      "step": 1880
    },
    {
      "epoch": 0.3128103277060576,
      "grad_norm": 5.211826801300049,
      "learning_rate": 4.706354571573433e-05,
      "loss": 1.2764,
      "step": 1890
    },
    {
      "epoch": 0.31446540880503143,
      "grad_norm": 6.238792896270752,
      "learning_rate": 4.7042420145343925e-05,
      "loss": 0.9796,
      "step": 1900
    },
    {
      "epoch": 0.3161204899040053,
      "grad_norm": 14.759909629821777,
      "learning_rate": 4.702129457495353e-05,
      "loss": 1.1424,
      "step": 1910
    },
    {
      "epoch": 0.31777557100297915,
      "grad_norm": 8.369919776916504,
      "learning_rate": 4.700016900456313e-05,
      "loss": 1.0176,
      "step": 1920
    },
    {
      "epoch": 0.319430652101953,
      "grad_norm": 11.669900894165039,
      "learning_rate": 4.697904343417273e-05,
      "loss": 0.9761,
      "step": 1930
    },
    {
      "epoch": 0.32108573320092687,
      "grad_norm": 12.827033996582031,
      "learning_rate": 4.6957917863782325e-05,
      "loss": 0.9359,
      "step": 1940
    },
    {
      "epoch": 0.3227408142999007,
      "grad_norm": 7.350112438201904,
      "learning_rate": 4.6936792293391924e-05,
      "loss": 0.8662,
      "step": 1950
    },
    {
      "epoch": 0.32439589539887453,
      "grad_norm": 9.767650604248047,
      "learning_rate": 4.691566672300152e-05,
      "loss": 0.9339,
      "step": 1960
    },
    {
      "epoch": 0.3260509764978484,
      "grad_norm": 6.722286701202393,
      "learning_rate": 4.689454115261112e-05,
      "loss": 0.8763,
      "step": 1970
    },
    {
      "epoch": 0.32770605759682225,
      "grad_norm": 5.984364986419678,
      "learning_rate": 4.687341558222072e-05,
      "loss": 0.6197,
      "step": 1980
    },
    {
      "epoch": 0.3293611386957961,
      "grad_norm": 8.830606460571289,
      "learning_rate": 4.6852290011830324e-05,
      "loss": 0.7596,
      "step": 1990
    },
    {
      "epoch": 0.3310162197947699,
      "grad_norm": 10.46382999420166,
      "learning_rate": 4.683116444143992e-05,
      "loss": 0.9028,
      "step": 2000
    },
    {
      "epoch": 0.3326713008937438,
      "grad_norm": 4.29202127456665,
      "learning_rate": 4.681003887104952e-05,
      "loss": 0.9622,
      "step": 2010
    },
    {
      "epoch": 0.33432638199271764,
      "grad_norm": 5.198490619659424,
      "learning_rate": 4.678891330065912e-05,
      "loss": 1.1256,
      "step": 2020
    },
    {
      "epoch": 0.33598146309169147,
      "grad_norm": 7.314266204833984,
      "learning_rate": 4.676778773026872e-05,
      "loss": 0.7641,
      "step": 2030
    },
    {
      "epoch": 0.33763654419066536,
      "grad_norm": 7.8869853019714355,
      "learning_rate": 4.6746662159878316e-05,
      "loss": 1.215,
      "step": 2040
    },
    {
      "epoch": 0.3392916252896392,
      "grad_norm": 4.416752338409424,
      "learning_rate": 4.6725536589487914e-05,
      "loss": 0.9831,
      "step": 2050
    },
    {
      "epoch": 0.340946706388613,
      "grad_norm": 7.837658405303955,
      "learning_rate": 4.670441101909751e-05,
      "loss": 1.278,
      "step": 2060
    },
    {
      "epoch": 0.3426017874875869,
      "grad_norm": 11.287732124328613,
      "learning_rate": 4.668328544870712e-05,
      "loss": 1.1857,
      "step": 2070
    },
    {
      "epoch": 0.34425686858656074,
      "grad_norm": 6.965438365936279,
      "learning_rate": 4.6662159878316716e-05,
      "loss": 1.3615,
      "step": 2080
    },
    {
      "epoch": 0.34591194968553457,
      "grad_norm": 12.797938346862793,
      "learning_rate": 4.6641034307926315e-05,
      "loss": 1.0073,
      "step": 2090
    },
    {
      "epoch": 0.34756703078450846,
      "grad_norm": 7.017601013183594,
      "learning_rate": 4.661990873753592e-05,
      "loss": 0.9076,
      "step": 2100
    },
    {
      "epoch": 0.3492221118834823,
      "grad_norm": 15.2490234375,
      "learning_rate": 4.659878316714552e-05,
      "loss": 0.9688,
      "step": 2110
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 6.6677656173706055,
      "learning_rate": 4.657765759675512e-05,
      "loss": 0.8588,
      "step": 2120
    },
    {
      "epoch": 0.35253227408143,
      "grad_norm": 2.888070821762085,
      "learning_rate": 4.6556532026364715e-05,
      "loss": 1.1205,
      "step": 2130
    },
    {
      "epoch": 0.35418735518040384,
      "grad_norm": 4.525887489318848,
      "learning_rate": 4.6535406455974313e-05,
      "loss": 0.7531,
      "step": 2140
    },
    {
      "epoch": 0.35584243627937767,
      "grad_norm": 13.461175918579102,
      "learning_rate": 4.651428088558391e-05,
      "loss": 1.1408,
      "step": 2150
    },
    {
      "epoch": 0.35749751737835156,
      "grad_norm": 4.689554214477539,
      "learning_rate": 4.649315531519352e-05,
      "loss": 1.0282,
      "step": 2160
    },
    {
      "epoch": 0.3591525984773254,
      "grad_norm": 4.277727127075195,
      "learning_rate": 4.6472029744803115e-05,
      "loss": 1.1317,
      "step": 2170
    },
    {
      "epoch": 0.3608076795762992,
      "grad_norm": 5.21662712097168,
      "learning_rate": 4.6450904174412714e-05,
      "loss": 0.9825,
      "step": 2180
    },
    {
      "epoch": 0.3624627606752731,
      "grad_norm": 13.37752914428711,
      "learning_rate": 4.642977860402231e-05,
      "loss": 1.1966,
      "step": 2190
    },
    {
      "epoch": 0.36411784177424694,
      "grad_norm": 5.78953218460083,
      "learning_rate": 4.640865303363191e-05,
      "loss": 0.9366,
      "step": 2200
    },
    {
      "epoch": 0.3657729228732208,
      "grad_norm": 14.460609436035156,
      "learning_rate": 4.638752746324151e-05,
      "loss": 1.1944,
      "step": 2210
    },
    {
      "epoch": 0.36742800397219466,
      "grad_norm": 6.542474269866943,
      "learning_rate": 4.636640189285111e-05,
      "loss": 0.9912,
      "step": 2220
    },
    {
      "epoch": 0.3690830850711685,
      "grad_norm": 6.891678333282471,
      "learning_rate": 4.6345276322460706e-05,
      "loss": 0.761,
      "step": 2230
    },
    {
      "epoch": 0.3707381661701423,
      "grad_norm": 14.132966041564941,
      "learning_rate": 4.6324150752070304e-05,
      "loss": 1.1693,
      "step": 2240
    },
    {
      "epoch": 0.3723932472691162,
      "grad_norm": 3.1992459297180176,
      "learning_rate": 4.630302518167991e-05,
      "loss": 1.1172,
      "step": 2250
    },
    {
      "epoch": 0.37404832836809004,
      "grad_norm": 4.0023322105407715,
      "learning_rate": 4.628189961128951e-05,
      "loss": 0.7275,
      "step": 2260
    },
    {
      "epoch": 0.3757034094670639,
      "grad_norm": 10.657421112060547,
      "learning_rate": 4.6260774040899106e-05,
      "loss": 0.6986,
      "step": 2270
    },
    {
      "epoch": 0.37735849056603776,
      "grad_norm": 14.291274070739746,
      "learning_rate": 4.6239648470508704e-05,
      "loss": 0.7852,
      "step": 2280
    },
    {
      "epoch": 0.3790135716650116,
      "grad_norm": 5.616158962249756,
      "learning_rate": 4.62185229001183e-05,
      "loss": 1.0561,
      "step": 2290
    },
    {
      "epoch": 0.3806686527639854,
      "grad_norm": 9.056979179382324,
      "learning_rate": 4.61973973297279e-05,
      "loss": 0.9167,
      "step": 2300
    },
    {
      "epoch": 0.38232373386295926,
      "grad_norm": 10.916108131408691,
      "learning_rate": 4.61762717593375e-05,
      "loss": 1.1045,
      "step": 2310
    },
    {
      "epoch": 0.38397881496193315,
      "grad_norm": 7.0765581130981445,
      "learning_rate": 4.6155146188947105e-05,
      "loss": 0.7871,
      "step": 2320
    },
    {
      "epoch": 0.385633896060907,
      "grad_norm": 13.04115104675293,
      "learning_rate": 4.61340206185567e-05,
      "loss": 0.6958,
      "step": 2330
    },
    {
      "epoch": 0.3872889771598808,
      "grad_norm": 6.533575534820557,
      "learning_rate": 4.61128950481663e-05,
      "loss": 0.9052,
      "step": 2340
    },
    {
      "epoch": 0.3889440582588547,
      "grad_norm": 6.860217571258545,
      "learning_rate": 4.6091769477775907e-05,
      "loss": 0.8782,
      "step": 2350
    },
    {
      "epoch": 0.39059913935782853,
      "grad_norm": 13.65745735168457,
      "learning_rate": 4.6070643907385505e-05,
      "loss": 0.9703,
      "step": 2360
    },
    {
      "epoch": 0.39225422045680236,
      "grad_norm": 3.47294545173645,
      "learning_rate": 4.60495183369951e-05,
      "loss": 0.664,
      "step": 2370
    },
    {
      "epoch": 0.39390930155577625,
      "grad_norm": 11.261844635009766,
      "learning_rate": 4.60283927666047e-05,
      "loss": 1.1676,
      "step": 2380
    },
    {
      "epoch": 0.3955643826547501,
      "grad_norm": 11.997441291809082,
      "learning_rate": 4.60072671962143e-05,
      "loss": 1.0542,
      "step": 2390
    },
    {
      "epoch": 0.3972194637537239,
      "grad_norm": 6.871723651885986,
      "learning_rate": 4.59861416258239e-05,
      "loss": 0.9611,
      "step": 2400
    },
    {
      "epoch": 0.3988745448526978,
      "grad_norm": 5.230261325836182,
      "learning_rate": 4.59650160554335e-05,
      "loss": 0.7119,
      "step": 2410
    },
    {
      "epoch": 0.40052962595167163,
      "grad_norm": 4.8493499755859375,
      "learning_rate": 4.59438904850431e-05,
      "loss": 1.0232,
      "step": 2420
    },
    {
      "epoch": 0.40218470705064546,
      "grad_norm": 6.881191730499268,
      "learning_rate": 4.59227649146527e-05,
      "loss": 0.9283,
      "step": 2430
    },
    {
      "epoch": 0.40383978814961935,
      "grad_norm": 6.700901031494141,
      "learning_rate": 4.59016393442623e-05,
      "loss": 0.8026,
      "step": 2440
    },
    {
      "epoch": 0.4054948692485932,
      "grad_norm": 6.372035503387451,
      "learning_rate": 4.58805137738719e-05,
      "loss": 0.7256,
      "step": 2450
    },
    {
      "epoch": 0.407149950347567,
      "grad_norm": 4.700034141540527,
      "learning_rate": 4.5859388203481496e-05,
      "loss": 1.1296,
      "step": 2460
    },
    {
      "epoch": 0.4088050314465409,
      "grad_norm": 1.616067886352539,
      "learning_rate": 4.5838262633091094e-05,
      "loss": 0.6541,
      "step": 2470
    },
    {
      "epoch": 0.41046011254551473,
      "grad_norm": 9.335246086120605,
      "learning_rate": 4.581713706270069e-05,
      "loss": 0.688,
      "step": 2480
    },
    {
      "epoch": 0.41211519364448856,
      "grad_norm": 8.422526359558105,
      "learning_rate": 4.579601149231029e-05,
      "loss": 0.7278,
      "step": 2490
    },
    {
      "epoch": 0.41377027474346245,
      "grad_norm": 9.080416679382324,
      "learning_rate": 4.5774885921919896e-05,
      "loss": 0.9465,
      "step": 2500
    },
    {
      "epoch": 0.4154253558424363,
      "grad_norm": 6.632617950439453,
      "learning_rate": 4.5753760351529494e-05,
      "loss": 0.9072,
      "step": 2510
    },
    {
      "epoch": 0.4170804369414101,
      "grad_norm": 3.826263904571533,
      "learning_rate": 4.573263478113909e-05,
      "loss": 0.9312,
      "step": 2520
    },
    {
      "epoch": 0.418735518040384,
      "grad_norm": 2.985973596572876,
      "learning_rate": 4.571150921074869e-05,
      "loss": 0.7634,
      "step": 2530
    },
    {
      "epoch": 0.42039059913935783,
      "grad_norm": 9.88821029663086,
      "learning_rate": 4.569038364035829e-05,
      "loss": 0.8229,
      "step": 2540
    },
    {
      "epoch": 0.42204568023833167,
      "grad_norm": 6.639096260070801,
      "learning_rate": 4.566925806996789e-05,
      "loss": 0.7482,
      "step": 2550
    },
    {
      "epoch": 0.42370076133730555,
      "grad_norm": 8.494214057922363,
      "learning_rate": 4.5648132499577486e-05,
      "loss": 0.7668,
      "step": 2560
    },
    {
      "epoch": 0.4253558424362794,
      "grad_norm": 2.4266209602355957,
      "learning_rate": 4.562700692918709e-05,
      "loss": 0.7687,
      "step": 2570
    },
    {
      "epoch": 0.4270109235352532,
      "grad_norm": 12.757013320922852,
      "learning_rate": 4.560588135879669e-05,
      "loss": 0.8125,
      "step": 2580
    },
    {
      "epoch": 0.4286660046342271,
      "grad_norm": 8.268671989440918,
      "learning_rate": 4.558475578840629e-05,
      "loss": 0.9711,
      "step": 2590
    },
    {
      "epoch": 0.43032108573320094,
      "grad_norm": 6.398922920227051,
      "learning_rate": 4.556363021801589e-05,
      "loss": 0.7699,
      "step": 2600
    },
    {
      "epoch": 0.43197616683217477,
      "grad_norm": 6.036654949188232,
      "learning_rate": 4.554250464762549e-05,
      "loss": 0.9779,
      "step": 2610
    },
    {
      "epoch": 0.4336312479311486,
      "grad_norm": 2.2752764225006104,
      "learning_rate": 4.552137907723509e-05,
      "loss": 0.6848,
      "step": 2620
    },
    {
      "epoch": 0.4352863290301225,
      "grad_norm": 6.142755031585693,
      "learning_rate": 4.550025350684469e-05,
      "loss": 0.832,
      "step": 2630
    },
    {
      "epoch": 0.4369414101290963,
      "grad_norm": 3.3886430263519287,
      "learning_rate": 4.547912793645429e-05,
      "loss": 0.9572,
      "step": 2640
    },
    {
      "epoch": 0.43859649122807015,
      "grad_norm": 2.131927013397217,
      "learning_rate": 4.5458002366063885e-05,
      "loss": 1.1079,
      "step": 2650
    },
    {
      "epoch": 0.44025157232704404,
      "grad_norm": 10.101700782775879,
      "learning_rate": 4.5436876795673484e-05,
      "loss": 0.9348,
      "step": 2660
    },
    {
      "epoch": 0.44190665342601787,
      "grad_norm": 7.716516494750977,
      "learning_rate": 4.541575122528309e-05,
      "loss": 0.9035,
      "step": 2670
    },
    {
      "epoch": 0.4435617345249917,
      "grad_norm": 8.790989875793457,
      "learning_rate": 4.539462565489269e-05,
      "loss": 0.9221,
      "step": 2680
    },
    {
      "epoch": 0.4452168156239656,
      "grad_norm": 7.028026580810547,
      "learning_rate": 4.5373500084502286e-05,
      "loss": 0.7573,
      "step": 2690
    },
    {
      "epoch": 0.4468718967229394,
      "grad_norm": 6.49647331237793,
      "learning_rate": 4.5352374514111884e-05,
      "loss": 0.913,
      "step": 2700
    },
    {
      "epoch": 0.44852697782191325,
      "grad_norm": 8.924870491027832,
      "learning_rate": 4.533124894372148e-05,
      "loss": 1.1755,
      "step": 2710
    },
    {
      "epoch": 0.45018205892088714,
      "grad_norm": 6.205325603485107,
      "learning_rate": 4.531012337333108e-05,
      "loss": 0.7005,
      "step": 2720
    },
    {
      "epoch": 0.45183714001986097,
      "grad_norm": 12.115555763244629,
      "learning_rate": 4.528899780294068e-05,
      "loss": 0.6898,
      "step": 2730
    },
    {
      "epoch": 0.4534922211188348,
      "grad_norm": 5.578763008117676,
      "learning_rate": 4.526787223255028e-05,
      "loss": 0.8865,
      "step": 2740
    },
    {
      "epoch": 0.4551473022178087,
      "grad_norm": 9.250011444091797,
      "learning_rate": 4.5246746662159876e-05,
      "loss": 0.9077,
      "step": 2750
    },
    {
      "epoch": 0.4568023833167825,
      "grad_norm": 6.993340492248535,
      "learning_rate": 4.522562109176948e-05,
      "loss": 1.1887,
      "step": 2760
    },
    {
      "epoch": 0.45845746441575635,
      "grad_norm": 5.107805252075195,
      "learning_rate": 4.520449552137908e-05,
      "loss": 0.8029,
      "step": 2770
    },
    {
      "epoch": 0.46011254551473024,
      "grad_norm": 4.955021858215332,
      "learning_rate": 4.518336995098868e-05,
      "loss": 0.9129,
      "step": 2780
    },
    {
      "epoch": 0.4617676266137041,
      "grad_norm": 5.654029369354248,
      "learning_rate": 4.5162244380598276e-05,
      "loss": 1.0356,
      "step": 2790
    },
    {
      "epoch": 0.4634227077126779,
      "grad_norm": 7.8578104972839355,
      "learning_rate": 4.5141118810207875e-05,
      "loss": 0.9541,
      "step": 2800
    },
    {
      "epoch": 0.4650777888116518,
      "grad_norm": 2.639310836791992,
      "learning_rate": 4.511999323981747e-05,
      "loss": 0.8483,
      "step": 2810
    },
    {
      "epoch": 0.4667328699106256,
      "grad_norm": 11.16014289855957,
      "learning_rate": 4.509886766942708e-05,
      "loss": 0.603,
      "step": 2820
    },
    {
      "epoch": 0.46838795100959946,
      "grad_norm": 9.685157775878906,
      "learning_rate": 4.5077742099036676e-05,
      "loss": 0.7042,
      "step": 2830
    },
    {
      "epoch": 0.47004303210857334,
      "grad_norm": 6.205262184143066,
      "learning_rate": 4.505661652864628e-05,
      "loss": 0.8134,
      "step": 2840
    },
    {
      "epoch": 0.4716981132075472,
      "grad_norm": 6.234001159667969,
      "learning_rate": 4.503549095825588e-05,
      "loss": 0.8736,
      "step": 2850
    },
    {
      "epoch": 0.473353194306521,
      "grad_norm": 6.411564350128174,
      "learning_rate": 4.501436538786548e-05,
      "loss": 0.9473,
      "step": 2860
    },
    {
      "epoch": 0.4750082754054949,
      "grad_norm": 9.144033432006836,
      "learning_rate": 4.499323981747508e-05,
      "loss": 0.8906,
      "step": 2870
    },
    {
      "epoch": 0.4766633565044687,
      "grad_norm": 8.112288475036621,
      "learning_rate": 4.4972114247084675e-05,
      "loss": 1.0941,
      "step": 2880
    },
    {
      "epoch": 0.47831843760344256,
      "grad_norm": 3.6994872093200684,
      "learning_rate": 4.4950988676694274e-05,
      "loss": 0.9745,
      "step": 2890
    },
    {
      "epoch": 0.4799735187024164,
      "grad_norm": 6.30675745010376,
      "learning_rate": 4.492986310630387e-05,
      "loss": 0.8931,
      "step": 2900
    },
    {
      "epoch": 0.4816285998013903,
      "grad_norm": 8.197927474975586,
      "learning_rate": 4.490873753591347e-05,
      "loss": 0.6748,
      "step": 2910
    },
    {
      "epoch": 0.4832836809003641,
      "grad_norm": 8.401453018188477,
      "learning_rate": 4.488761196552307e-05,
      "loss": 1.115,
      "step": 2920
    },
    {
      "epoch": 0.48493876199933794,
      "grad_norm": 6.450730323791504,
      "learning_rate": 4.4866486395132674e-05,
      "loss": 0.9554,
      "step": 2930
    },
    {
      "epoch": 0.48659384309831183,
      "grad_norm": 4.708397388458252,
      "learning_rate": 4.484536082474227e-05,
      "loss": 0.7489,
      "step": 2940
    },
    {
      "epoch": 0.48824892419728566,
      "grad_norm": 8.569350242614746,
      "learning_rate": 4.482423525435187e-05,
      "loss": 0.7748,
      "step": 2950
    },
    {
      "epoch": 0.4899040052962595,
      "grad_norm": 4.660645484924316,
      "learning_rate": 4.480310968396147e-05,
      "loss": 0.8927,
      "step": 2960
    },
    {
      "epoch": 0.4915590863952334,
      "grad_norm": 3.760030746459961,
      "learning_rate": 4.478198411357107e-05,
      "loss": 0.9522,
      "step": 2970
    },
    {
      "epoch": 0.4932141674942072,
      "grad_norm": 7.2157769203186035,
      "learning_rate": 4.4760858543180666e-05,
      "loss": 0.7931,
      "step": 2980
    },
    {
      "epoch": 0.49486924859318104,
      "grad_norm": 5.604398250579834,
      "learning_rate": 4.4739732972790264e-05,
      "loss": 1.1067,
      "step": 2990
    },
    {
      "epoch": 0.49652432969215493,
      "grad_norm": 4.288139820098877,
      "learning_rate": 4.471860740239986e-05,
      "loss": 0.8416,
      "step": 3000
    },
    {
      "epoch": 0.49817941079112876,
      "grad_norm": 3.1991305351257324,
      "learning_rate": 4.469748183200947e-05,
      "loss": 0.7996,
      "step": 3010
    },
    {
      "epoch": 0.4998344918901026,
      "grad_norm": 7.078308582305908,
      "learning_rate": 4.4676356261619066e-05,
      "loss": 1.0293,
      "step": 3020
    },
    {
      "epoch": 0.5014895729890765,
      "grad_norm": 6.171448230743408,
      "learning_rate": 4.4655230691228664e-05,
      "loss": 1.0262,
      "step": 3030
    },
    {
      "epoch": 0.5031446540880503,
      "grad_norm": 7.430249214172363,
      "learning_rate": 4.463410512083826e-05,
      "loss": 0.7363,
      "step": 3040
    },
    {
      "epoch": 0.5047997351870241,
      "grad_norm": 6.61668062210083,
      "learning_rate": 4.461297955044786e-05,
      "loss": 0.903,
      "step": 3050
    },
    {
      "epoch": 0.506454816285998,
      "grad_norm": 7.731870174407959,
      "learning_rate": 4.4591853980057466e-05,
      "loss": 0.8504,
      "step": 3060
    },
    {
      "epoch": 0.5081098973849718,
      "grad_norm": 7.338734149932861,
      "learning_rate": 4.4570728409667065e-05,
      "loss": 0.8842,
      "step": 3070
    },
    {
      "epoch": 0.5097649784839458,
      "grad_norm": 4.170329570770264,
      "learning_rate": 4.454960283927666e-05,
      "loss": 1.0367,
      "step": 3080
    },
    {
      "epoch": 0.5114200595829196,
      "grad_norm": 11.470917701721191,
      "learning_rate": 4.452847726888626e-05,
      "loss": 0.9406,
      "step": 3090
    },
    {
      "epoch": 0.5130751406818934,
      "grad_norm": 4.3268632888793945,
      "learning_rate": 4.450735169849587e-05,
      "loss": 0.6569,
      "step": 3100
    },
    {
      "epoch": 0.5147302217808672,
      "grad_norm": 6.205569744110107,
      "learning_rate": 4.4486226128105465e-05,
      "loss": 0.7701,
      "step": 3110
    },
    {
      "epoch": 0.5163853028798411,
      "grad_norm": 9.798870086669922,
      "learning_rate": 4.4465100557715063e-05,
      "loss": 0.7788,
      "step": 3120
    },
    {
      "epoch": 0.5180403839788149,
      "grad_norm": 7.253307819366455,
      "learning_rate": 4.444397498732466e-05,
      "loss": 1.2477,
      "step": 3130
    },
    {
      "epoch": 0.5196954650777889,
      "grad_norm": 7.995439052581787,
      "learning_rate": 4.442284941693426e-05,
      "loss": 1.0995,
      "step": 3140
    },
    {
      "epoch": 0.5213505461767627,
      "grad_norm": 3.534459114074707,
      "learning_rate": 4.440172384654386e-05,
      "loss": 0.8614,
      "step": 3150
    },
    {
      "epoch": 0.5230056272757365,
      "grad_norm": 6.06298303604126,
      "learning_rate": 4.438059827615346e-05,
      "loss": 1.0413,
      "step": 3160
    },
    {
      "epoch": 0.5246607083747103,
      "grad_norm": 10.025978088378906,
      "learning_rate": 4.4359472705763055e-05,
      "loss": 0.7898,
      "step": 3170
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 6.205329418182373,
      "learning_rate": 4.433834713537266e-05,
      "loss": 0.8492,
      "step": 3180
    },
    {
      "epoch": 0.527970870572658,
      "grad_norm": 3.651817798614502,
      "learning_rate": 4.431722156498226e-05,
      "loss": 0.734,
      "step": 3190
    },
    {
      "epoch": 0.529625951671632,
      "grad_norm": 6.25150203704834,
      "learning_rate": 4.429609599459186e-05,
      "loss": 1.1334,
      "step": 3200
    },
    {
      "epoch": 0.5312810327706058,
      "grad_norm": 8.238245964050293,
      "learning_rate": 4.4274970424201456e-05,
      "loss": 0.7096,
      "step": 3210
    },
    {
      "epoch": 0.5329361138695796,
      "grad_norm": 8.685776710510254,
      "learning_rate": 4.4253844853811054e-05,
      "loss": 0.9741,
      "step": 3220
    },
    {
      "epoch": 0.5345911949685535,
      "grad_norm": 8.368990898132324,
      "learning_rate": 4.423271928342065e-05,
      "loss": 1.2481,
      "step": 3230
    },
    {
      "epoch": 0.5362462760675273,
      "grad_norm": 9.404903411865234,
      "learning_rate": 4.421159371303025e-05,
      "loss": 0.8451,
      "step": 3240
    },
    {
      "epoch": 0.5379013571665011,
      "grad_norm": 5.186917781829834,
      "learning_rate": 4.419046814263985e-05,
      "loss": 1.1256,
      "step": 3250
    },
    {
      "epoch": 0.5395564382654751,
      "grad_norm": 1.7970277070999146,
      "learning_rate": 4.416934257224945e-05,
      "loss": 0.8035,
      "step": 3260
    },
    {
      "epoch": 0.5412115193644489,
      "grad_norm": 5.78540563583374,
      "learning_rate": 4.414821700185905e-05,
      "loss": 0.9053,
      "step": 3270
    },
    {
      "epoch": 0.5428666004634227,
      "grad_norm": 4.145542621612549,
      "learning_rate": 4.412709143146865e-05,
      "loss": 0.773,
      "step": 3280
    },
    {
      "epoch": 0.5445216815623966,
      "grad_norm": 6.171741008758545,
      "learning_rate": 4.410596586107825e-05,
      "loss": 1.0764,
      "step": 3290
    },
    {
      "epoch": 0.5461767626613704,
      "grad_norm": 1.7947089672088623,
      "learning_rate": 4.408484029068785e-05,
      "loss": 0.538,
      "step": 3300
    },
    {
      "epoch": 0.5478318437603442,
      "grad_norm": 6.015474796295166,
      "learning_rate": 4.406371472029745e-05,
      "loss": 0.6116,
      "step": 3310
    },
    {
      "epoch": 0.5494869248593182,
      "grad_norm": 5.405176162719727,
      "learning_rate": 4.404258914990705e-05,
      "loss": 0.7102,
      "step": 3320
    },
    {
      "epoch": 0.551142005958292,
      "grad_norm": 8.834525108337402,
      "learning_rate": 4.402146357951665e-05,
      "loss": 0.7622,
      "step": 3330
    },
    {
      "epoch": 0.5527970870572658,
      "grad_norm": 5.83921480178833,
      "learning_rate": 4.400033800912625e-05,
      "loss": 0.938,
      "step": 3340
    },
    {
      "epoch": 0.5544521681562397,
      "grad_norm": 6.873477458953857,
      "learning_rate": 4.397921243873585e-05,
      "loss": 0.703,
      "step": 3350
    },
    {
      "epoch": 0.5561072492552135,
      "grad_norm": 6.438570499420166,
      "learning_rate": 4.395808686834545e-05,
      "loss": 0.8803,
      "step": 3360
    },
    {
      "epoch": 0.5577623303541873,
      "grad_norm": 6.389456748962402,
      "learning_rate": 4.393696129795505e-05,
      "loss": 0.9387,
      "step": 3370
    },
    {
      "epoch": 0.5594174114531612,
      "grad_norm": 8.315299987792969,
      "learning_rate": 4.391583572756465e-05,
      "loss": 0.7676,
      "step": 3380
    },
    {
      "epoch": 0.5610724925521351,
      "grad_norm": 8.758903503417969,
      "learning_rate": 4.389471015717425e-05,
      "loss": 0.7509,
      "step": 3390
    },
    {
      "epoch": 0.5627275736511089,
      "grad_norm": 2.8985164165496826,
      "learning_rate": 4.3873584586783845e-05,
      "loss": 0.5929,
      "step": 3400
    },
    {
      "epoch": 0.5643826547500828,
      "grad_norm": 11.930763244628906,
      "learning_rate": 4.3852459016393444e-05,
      "loss": 0.9522,
      "step": 3410
    },
    {
      "epoch": 0.5660377358490566,
      "grad_norm": 9.046439170837402,
      "learning_rate": 4.383133344600304e-05,
      "loss": 0.8276,
      "step": 3420
    },
    {
      "epoch": 0.5676928169480304,
      "grad_norm": 6.21189546585083,
      "learning_rate": 4.381020787561264e-05,
      "loss": 0.8181,
      "step": 3430
    },
    {
      "epoch": 0.5693478980470043,
      "grad_norm": 8.005921363830566,
      "learning_rate": 4.3789082305222246e-05,
      "loss": 0.8295,
      "step": 3440
    },
    {
      "epoch": 0.5710029791459782,
      "grad_norm": 5.560092449188232,
      "learning_rate": 4.3767956734831844e-05,
      "loss": 0.8907,
      "step": 3450
    },
    {
      "epoch": 0.572658060244952,
      "grad_norm": 5.807984828948975,
      "learning_rate": 4.374683116444144e-05,
      "loss": 0.6233,
      "step": 3460
    },
    {
      "epoch": 0.5743131413439259,
      "grad_norm": 6.971253871917725,
      "learning_rate": 4.372570559405104e-05,
      "loss": 0.9676,
      "step": 3470
    },
    {
      "epoch": 0.5759682224428997,
      "grad_norm": 9.718036651611328,
      "learning_rate": 4.370458002366064e-05,
      "loss": 0.8843,
      "step": 3480
    },
    {
      "epoch": 0.5776233035418735,
      "grad_norm": 4.739887714385986,
      "learning_rate": 4.368345445327024e-05,
      "loss": 0.6793,
      "step": 3490
    },
    {
      "epoch": 0.5792783846408474,
      "grad_norm": 6.5463690757751465,
      "learning_rate": 4.3662328882879836e-05,
      "loss": 0.8926,
      "step": 3500
    },
    {
      "epoch": 0.5809334657398213,
      "grad_norm": 8.068760871887207,
      "learning_rate": 4.3641203312489434e-05,
      "loss": 0.7175,
      "step": 3510
    },
    {
      "epoch": 0.5825885468387951,
      "grad_norm": 4.675145626068115,
      "learning_rate": 4.362007774209904e-05,
      "loss": 1.0869,
      "step": 3520
    },
    {
      "epoch": 0.584243627937769,
      "grad_norm": 7.863040924072266,
      "learning_rate": 4.359895217170864e-05,
      "loss": 0.7445,
      "step": 3530
    },
    {
      "epoch": 0.5858987090367428,
      "grad_norm": 8.852269172668457,
      "learning_rate": 4.3577826601318236e-05,
      "loss": 1.3173,
      "step": 3540
    },
    {
      "epoch": 0.5875537901357166,
      "grad_norm": 5.696547985076904,
      "learning_rate": 4.3556701030927835e-05,
      "loss": 0.9989,
      "step": 3550
    },
    {
      "epoch": 0.5892088712346905,
      "grad_norm": 10.914084434509277,
      "learning_rate": 4.353557546053744e-05,
      "loss": 0.7949,
      "step": 3560
    },
    {
      "epoch": 0.5908639523336644,
      "grad_norm": 5.762917518615723,
      "learning_rate": 4.351444989014704e-05,
      "loss": 0.754,
      "step": 3570
    },
    {
      "epoch": 0.5925190334326382,
      "grad_norm": 7.852947235107422,
      "learning_rate": 4.3493324319756637e-05,
      "loss": 0.7189,
      "step": 3580
    },
    {
      "epoch": 0.5941741145316121,
      "grad_norm": 3.6804656982421875,
      "learning_rate": 4.3472198749366235e-05,
      "loss": 0.9927,
      "step": 3590
    },
    {
      "epoch": 0.5958291956305859,
      "grad_norm": 4.671380043029785,
      "learning_rate": 4.345107317897583e-05,
      "loss": 0.7421,
      "step": 3600
    },
    {
      "epoch": 0.5974842767295597,
      "grad_norm": 4.10764217376709,
      "learning_rate": 4.342994760858544e-05,
      "loss": 0.5662,
      "step": 3610
    },
    {
      "epoch": 0.5991393578285336,
      "grad_norm": 14.370808601379395,
      "learning_rate": 4.340882203819504e-05,
      "loss": 0.8655,
      "step": 3620
    },
    {
      "epoch": 0.6007944389275075,
      "grad_norm": 9.864126205444336,
      "learning_rate": 4.3387696467804635e-05,
      "loss": 0.9969,
      "step": 3630
    },
    {
      "epoch": 0.6024495200264813,
      "grad_norm": 4.301313877105713,
      "learning_rate": 4.3366570897414234e-05,
      "loss": 0.9638,
      "step": 3640
    },
    {
      "epoch": 0.6041046011254552,
      "grad_norm": 4.02168083190918,
      "learning_rate": 4.334544532702383e-05,
      "loss": 0.5628,
      "step": 3650
    },
    {
      "epoch": 0.605759682224429,
      "grad_norm": 9.22834300994873,
      "learning_rate": 4.332431975663343e-05,
      "loss": 0.7957,
      "step": 3660
    },
    {
      "epoch": 0.6074147633234028,
      "grad_norm": 6.008138656616211,
      "learning_rate": 4.330319418624303e-05,
      "loss": 1.0057,
      "step": 3670
    },
    {
      "epoch": 0.6090698444223767,
      "grad_norm": 7.340017795562744,
      "learning_rate": 4.328206861585263e-05,
      "loss": 0.8517,
      "step": 3680
    },
    {
      "epoch": 0.6107249255213505,
      "grad_norm": 7.928074359893799,
      "learning_rate": 4.326094304546223e-05,
      "loss": 0.7449,
      "step": 3690
    },
    {
      "epoch": 0.6123800066203244,
      "grad_norm": 8.329083442687988,
      "learning_rate": 4.323981747507183e-05,
      "loss": 0.7467,
      "step": 3700
    },
    {
      "epoch": 0.6140350877192983,
      "grad_norm": 10.044578552246094,
      "learning_rate": 4.321869190468143e-05,
      "loss": 0.7991,
      "step": 3710
    },
    {
      "epoch": 0.6156901688182721,
      "grad_norm": 1.9473140239715576,
      "learning_rate": 4.319756633429103e-05,
      "loss": 0.6097,
      "step": 3720
    },
    {
      "epoch": 0.6173452499172459,
      "grad_norm": 1.8311842679977417,
      "learning_rate": 4.3176440763900626e-05,
      "loss": 0.7742,
      "step": 3730
    },
    {
      "epoch": 0.6190003310162198,
      "grad_norm": 6.810498237609863,
      "learning_rate": 4.3155315193510224e-05,
      "loss": 0.7616,
      "step": 3740
    },
    {
      "epoch": 0.6206554121151936,
      "grad_norm": 11.013484001159668,
      "learning_rate": 4.313418962311982e-05,
      "loss": 0.7351,
      "step": 3750
    },
    {
      "epoch": 0.6223104932141675,
      "grad_norm": 6.745355606079102,
      "learning_rate": 4.311306405272942e-05,
      "loss": 1.1457,
      "step": 3760
    },
    {
      "epoch": 0.6239655743131414,
      "grad_norm": 6.882542133331299,
      "learning_rate": 4.309193848233902e-05,
      "loss": 0.7859,
      "step": 3770
    },
    {
      "epoch": 0.6256206554121152,
      "grad_norm": 15.17281436920166,
      "learning_rate": 4.3070812911948625e-05,
      "loss": 0.8339,
      "step": 3780
    },
    {
      "epoch": 0.627275736511089,
      "grad_norm": 6.302368640899658,
      "learning_rate": 4.304968734155822e-05,
      "loss": 0.844,
      "step": 3790
    },
    {
      "epoch": 0.6289308176100629,
      "grad_norm": 12.103489875793457,
      "learning_rate": 4.302856177116782e-05,
      "loss": 0.8042,
      "step": 3800
    },
    {
      "epoch": 0.6305858987090367,
      "grad_norm": 9.590091705322266,
      "learning_rate": 4.3007436200777426e-05,
      "loss": 0.8798,
      "step": 3810
    },
    {
      "epoch": 0.6322409798080106,
      "grad_norm": 8.327310562133789,
      "learning_rate": 4.2986310630387025e-05,
      "loss": 0.7716,
      "step": 3820
    },
    {
      "epoch": 0.6338960609069845,
      "grad_norm": 8.551024436950684,
      "learning_rate": 4.296518505999662e-05,
      "loss": 0.8283,
      "step": 3830
    },
    {
      "epoch": 0.6355511420059583,
      "grad_norm": 8.548646926879883,
      "learning_rate": 4.294405948960622e-05,
      "loss": 0.7693,
      "step": 3840
    },
    {
      "epoch": 0.6372062231049321,
      "grad_norm": 7.225947380065918,
      "learning_rate": 4.292293391921582e-05,
      "loss": 1.1526,
      "step": 3850
    },
    {
      "epoch": 0.638861304203906,
      "grad_norm": 7.045748233795166,
      "learning_rate": 4.2901808348825425e-05,
      "loss": 0.6072,
      "step": 3860
    },
    {
      "epoch": 0.6405163853028798,
      "grad_norm": 4.800227165222168,
      "learning_rate": 4.2880682778435024e-05,
      "loss": 0.9431,
      "step": 3870
    },
    {
      "epoch": 0.6421714664018537,
      "grad_norm": 5.048583984375,
      "learning_rate": 4.285955720804462e-05,
      "loss": 0.8312,
      "step": 3880
    },
    {
      "epoch": 0.6438265475008276,
      "grad_norm": 6.927045822143555,
      "learning_rate": 4.283843163765422e-05,
      "loss": 0.685,
      "step": 3890
    },
    {
      "epoch": 0.6454816285998014,
      "grad_norm": 5.825406074523926,
      "learning_rate": 4.281730606726382e-05,
      "loss": 0.761,
      "step": 3900
    },
    {
      "epoch": 0.6471367096987752,
      "grad_norm": 4.584779262542725,
      "learning_rate": 4.279618049687342e-05,
      "loss": 0.7183,
      "step": 3910
    },
    {
      "epoch": 0.6487917907977491,
      "grad_norm": 5.699444770812988,
      "learning_rate": 4.2775054926483015e-05,
      "loss": 0.7682,
      "step": 3920
    },
    {
      "epoch": 0.6504468718967229,
      "grad_norm": 6.175193786621094,
      "learning_rate": 4.2753929356092614e-05,
      "loss": 0.7666,
      "step": 3930
    },
    {
      "epoch": 0.6521019529956968,
      "grad_norm": 6.399385929107666,
      "learning_rate": 4.273280378570221e-05,
      "loss": 1.0835,
      "step": 3940
    },
    {
      "epoch": 0.6537570340946707,
      "grad_norm": 4.850484371185303,
      "learning_rate": 4.271167821531182e-05,
      "loss": 0.9761,
      "step": 3950
    },
    {
      "epoch": 0.6554121151936445,
      "grad_norm": 7.150592803955078,
      "learning_rate": 4.2690552644921416e-05,
      "loss": 1.0394,
      "step": 3960
    },
    {
      "epoch": 0.6570671962926183,
      "grad_norm": 6.800861835479736,
      "learning_rate": 4.2669427074531014e-05,
      "loss": 0.8463,
      "step": 3970
    },
    {
      "epoch": 0.6587222773915922,
      "grad_norm": 8.591506004333496,
      "learning_rate": 4.264830150414061e-05,
      "loss": 1.0708,
      "step": 3980
    },
    {
      "epoch": 0.660377358490566,
      "grad_norm": 10.600196838378906,
      "learning_rate": 4.262717593375021e-05,
      "loss": 0.8306,
      "step": 3990
    },
    {
      "epoch": 0.6620324395895398,
      "grad_norm": 6.348047256469727,
      "learning_rate": 4.260605036335981e-05,
      "loss": 1.1668,
      "step": 4000
    },
    {
      "epoch": 0.6636875206885138,
      "grad_norm": 5.897114276885986,
      "learning_rate": 4.258492479296941e-05,
      "loss": 0.5943,
      "step": 4010
    },
    {
      "epoch": 0.6653426017874876,
      "grad_norm": 12.60709285736084,
      "learning_rate": 4.2563799222579006e-05,
      "loss": 0.7178,
      "step": 4020
    },
    {
      "epoch": 0.6669976828864614,
      "grad_norm": 7.019259452819824,
      "learning_rate": 4.254267365218861e-05,
      "loss": 0.845,
      "step": 4030
    },
    {
      "epoch": 0.6686527639854353,
      "grad_norm": 7.819041728973389,
      "learning_rate": 4.252154808179821e-05,
      "loss": 0.898,
      "step": 4040
    },
    {
      "epoch": 0.6703078450844091,
      "grad_norm": 9.637151718139648,
      "learning_rate": 4.2500422511407815e-05,
      "loss": 0.7292,
      "step": 4050
    },
    {
      "epoch": 0.6719629261833829,
      "grad_norm": 7.209911823272705,
      "learning_rate": 4.247929694101741e-05,
      "loss": 1.0762,
      "step": 4060
    },
    {
      "epoch": 0.6736180072823569,
      "grad_norm": 8.248425483703613,
      "learning_rate": 4.245817137062701e-05,
      "loss": 0.7516,
      "step": 4070
    },
    {
      "epoch": 0.6752730883813307,
      "grad_norm": 9.18157958984375,
      "learning_rate": 4.243704580023661e-05,
      "loss": 1.0679,
      "step": 4080
    },
    {
      "epoch": 0.6769281694803045,
      "grad_norm": 9.46638011932373,
      "learning_rate": 4.241592022984621e-05,
      "loss": 0.9436,
      "step": 4090
    },
    {
      "epoch": 0.6785832505792784,
      "grad_norm": 9.864697456359863,
      "learning_rate": 4.239479465945581e-05,
      "loss": 1.05,
      "step": 4100
    },
    {
      "epoch": 0.6802383316782522,
      "grad_norm": 3.716219186782837,
      "learning_rate": 4.2373669089065405e-05,
      "loss": 0.7355,
      "step": 4110
    },
    {
      "epoch": 0.681893412777226,
      "grad_norm": 5.667409896850586,
      "learning_rate": 4.235254351867501e-05,
      "loss": 0.8628,
      "step": 4120
    },
    {
      "epoch": 0.6835484938762,
      "grad_norm": 9.33585262298584,
      "learning_rate": 4.233141794828461e-05,
      "loss": 0.8877,
      "step": 4130
    },
    {
      "epoch": 0.6852035749751738,
      "grad_norm": 5.699236869812012,
      "learning_rate": 4.231029237789421e-05,
      "loss": 0.9821,
      "step": 4140
    },
    {
      "epoch": 0.6868586560741476,
      "grad_norm": 7.901264190673828,
      "learning_rate": 4.2289166807503805e-05,
      "loss": 1.0466,
      "step": 4150
    },
    {
      "epoch": 0.6885137371731215,
      "grad_norm": 9.64258861541748,
      "learning_rate": 4.2268041237113404e-05,
      "loss": 1.2131,
      "step": 4160
    },
    {
      "epoch": 0.6901688182720953,
      "grad_norm": 2.1907119750976562,
      "learning_rate": 4.2246915666723e-05,
      "loss": 0.5354,
      "step": 4170
    },
    {
      "epoch": 0.6918238993710691,
      "grad_norm": 9.380793571472168,
      "learning_rate": 4.22257900963326e-05,
      "loss": 0.8112,
      "step": 4180
    },
    {
      "epoch": 0.6934789804700431,
      "grad_norm": 7.183408260345459,
      "learning_rate": 4.22046645259422e-05,
      "loss": 0.723,
      "step": 4190
    },
    {
      "epoch": 0.6951340615690169,
      "grad_norm": 3.154494047164917,
      "learning_rate": 4.2183538955551804e-05,
      "loss": 1.0919,
      "step": 4200
    },
    {
      "epoch": 0.6967891426679907,
      "grad_norm": 7.608426570892334,
      "learning_rate": 4.21624133851614e-05,
      "loss": 0.8934,
      "step": 4210
    },
    {
      "epoch": 0.6984442237669646,
      "grad_norm": 4.74393367767334,
      "learning_rate": 4.2141287814771e-05,
      "loss": 0.8568,
      "step": 4220
    },
    {
      "epoch": 0.7000993048659384,
      "grad_norm": 2.195302724838257,
      "learning_rate": 4.21201622443806e-05,
      "loss": 0.8487,
      "step": 4230
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 11.308738708496094,
      "learning_rate": 4.20990366739902e-05,
      "loss": 0.8214,
      "step": 4240
    },
    {
      "epoch": 0.7034094670638862,
      "grad_norm": 4.068066120147705,
      "learning_rate": 4.2077911103599796e-05,
      "loss": 0.6701,
      "step": 4250
    },
    {
      "epoch": 0.70506454816286,
      "grad_norm": 11.175095558166504,
      "learning_rate": 4.2056785533209394e-05,
      "loss": 0.8972,
      "step": 4260
    },
    {
      "epoch": 0.7067196292618338,
      "grad_norm": 7.744206428527832,
      "learning_rate": 4.2035659962819e-05,
      "loss": 0.7442,
      "step": 4270
    },
    {
      "epoch": 0.7083747103608077,
      "grad_norm": 8.913176536560059,
      "learning_rate": 4.20145343924286e-05,
      "loss": 0.7186,
      "step": 4280
    },
    {
      "epoch": 0.7100297914597815,
      "grad_norm": 7.044890403747559,
      "learning_rate": 4.1993408822038196e-05,
      "loss": 0.8237,
      "step": 4290
    },
    {
      "epoch": 0.7116848725587553,
      "grad_norm": 16.207719802856445,
      "learning_rate": 4.19722832516478e-05,
      "loss": 0.9432,
      "step": 4300
    },
    {
      "epoch": 0.7133399536577292,
      "grad_norm": 9.865875244140625,
      "learning_rate": 4.19511576812574e-05,
      "loss": 0.8535,
      "step": 4310
    },
    {
      "epoch": 0.7149950347567031,
      "grad_norm": 7.980419635772705,
      "learning_rate": 4.1930032110867e-05,
      "loss": 0.9213,
      "step": 4320
    },
    {
      "epoch": 0.716650115855677,
      "grad_norm": 4.096696853637695,
      "learning_rate": 4.1908906540476597e-05,
      "loss": 0.6829,
      "step": 4330
    },
    {
      "epoch": 0.7183051969546508,
      "grad_norm": 10.01707935333252,
      "learning_rate": 4.1887780970086195e-05,
      "loss": 0.6344,
      "step": 4340
    },
    {
      "epoch": 0.7199602780536246,
      "grad_norm": 10.038146018981934,
      "learning_rate": 4.186665539969579e-05,
      "loss": 0.5451,
      "step": 4350
    },
    {
      "epoch": 0.7216153591525984,
      "grad_norm": 10.179122924804688,
      "learning_rate": 4.184552982930539e-05,
      "loss": 1.0201,
      "step": 4360
    },
    {
      "epoch": 0.7232704402515723,
      "grad_norm": 5.868716716766357,
      "learning_rate": 4.1824404258915e-05,
      "loss": 0.811,
      "step": 4370
    },
    {
      "epoch": 0.7249255213505462,
      "grad_norm": 4.631462097167969,
      "learning_rate": 4.1803278688524595e-05,
      "loss": 0.4404,
      "step": 4380
    },
    {
      "epoch": 0.72658060244952,
      "grad_norm": 5.620421886444092,
      "learning_rate": 4.1782153118134194e-05,
      "loss": 0.8852,
      "step": 4390
    },
    {
      "epoch": 0.7282356835484939,
      "grad_norm": 10.263690948486328,
      "learning_rate": 4.176102754774379e-05,
      "loss": 1.0185,
      "step": 4400
    },
    {
      "epoch": 0.7298907646474677,
      "grad_norm": 24.96410369873047,
      "learning_rate": 4.173990197735339e-05,
      "loss": 0.8385,
      "step": 4410
    },
    {
      "epoch": 0.7315458457464415,
      "grad_norm": 9.493979454040527,
      "learning_rate": 4.171877640696299e-05,
      "loss": 0.5377,
      "step": 4420
    },
    {
      "epoch": 0.7332009268454154,
      "grad_norm": 7.22935676574707,
      "learning_rate": 4.169765083657259e-05,
      "loss": 0.6193,
      "step": 4430
    },
    {
      "epoch": 0.7348560079443893,
      "grad_norm": 7.223391056060791,
      "learning_rate": 4.1676525266182186e-05,
      "loss": 0.9331,
      "step": 4440
    },
    {
      "epoch": 0.7365110890433632,
      "grad_norm": 6.107539176940918,
      "learning_rate": 4.1655399695791784e-05,
      "loss": 0.8986,
      "step": 4450
    },
    {
      "epoch": 0.738166170142337,
      "grad_norm": 3.726116180419922,
      "learning_rate": 4.163427412540139e-05,
      "loss": 0.6729,
      "step": 4460
    },
    {
      "epoch": 0.7398212512413108,
      "grad_norm": 11.27258586883545,
      "learning_rate": 4.161314855501099e-05,
      "loss": 0.8042,
      "step": 4470
    },
    {
      "epoch": 0.7414763323402846,
      "grad_norm": 6.260288238525391,
      "learning_rate": 4.1592022984620586e-05,
      "loss": 0.8616,
      "step": 4480
    },
    {
      "epoch": 0.7431314134392585,
      "grad_norm": 6.1774821281433105,
      "learning_rate": 4.1570897414230184e-05,
      "loss": 0.8522,
      "step": 4490
    },
    {
      "epoch": 0.7447864945382324,
      "grad_norm": 8.04418659210205,
      "learning_rate": 4.154977184383978e-05,
      "loss": 0.6581,
      "step": 4500
    },
    {
      "epoch": 0.7464415756372063,
      "grad_norm": 8.601615905761719,
      "learning_rate": 4.152864627344938e-05,
      "loss": 0.872,
      "step": 4510
    },
    {
      "epoch": 0.7480966567361801,
      "grad_norm": 3.2736144065856934,
      "learning_rate": 4.1507520703058986e-05,
      "loss": 0.8518,
      "step": 4520
    },
    {
      "epoch": 0.7497517378351539,
      "grad_norm": 6.955022811889648,
      "learning_rate": 4.1486395132668585e-05,
      "loss": 0.8147,
      "step": 4530
    },
    {
      "epoch": 0.7514068189341278,
      "grad_norm": 13.691156387329102,
      "learning_rate": 4.146526956227818e-05,
      "loss": 0.724,
      "step": 4540
    },
    {
      "epoch": 0.7530619000331016,
      "grad_norm": 7.433265686035156,
      "learning_rate": 4.144414399188779e-05,
      "loss": 1.1402,
      "step": 4550
    },
    {
      "epoch": 0.7547169811320755,
      "grad_norm": 3.2315666675567627,
      "learning_rate": 4.1423018421497387e-05,
      "loss": 0.7936,
      "step": 4560
    },
    {
      "epoch": 0.7563720622310494,
      "grad_norm": 7.950416564941406,
      "learning_rate": 4.1401892851106985e-05,
      "loss": 0.765,
      "step": 4570
    },
    {
      "epoch": 0.7580271433300232,
      "grad_norm": 7.682269096374512,
      "learning_rate": 4.138076728071658e-05,
      "loss": 0.791,
      "step": 4580
    },
    {
      "epoch": 0.759682224428997,
      "grad_norm": 6.168712139129639,
      "learning_rate": 4.135964171032618e-05,
      "loss": 0.6551,
      "step": 4590
    },
    {
      "epoch": 0.7613373055279709,
      "grad_norm": 13.752392768859863,
      "learning_rate": 4.133851613993578e-05,
      "loss": 1.1661,
      "step": 4600
    },
    {
      "epoch": 0.7629923866269447,
      "grad_norm": 4.9881720542907715,
      "learning_rate": 4.131739056954538e-05,
      "loss": 1.0483,
      "step": 4610
    },
    {
      "epoch": 0.7646474677259185,
      "grad_norm": 3.2760379314422607,
      "learning_rate": 4.129626499915498e-05,
      "loss": 0.6487,
      "step": 4620
    },
    {
      "epoch": 0.7663025488248925,
      "grad_norm": 5.3312764167785645,
      "learning_rate": 4.127513942876458e-05,
      "loss": 0.8175,
      "step": 4630
    },
    {
      "epoch": 0.7679576299238663,
      "grad_norm": 7.6243977546691895,
      "learning_rate": 4.125401385837418e-05,
      "loss": 0.8402,
      "step": 4640
    },
    {
      "epoch": 0.7696127110228401,
      "grad_norm": 0.8809918165206909,
      "learning_rate": 4.123288828798378e-05,
      "loss": 0.4948,
      "step": 4650
    },
    {
      "epoch": 0.771267792121814,
      "grad_norm": 12.90571117401123,
      "learning_rate": 4.121176271759338e-05,
      "loss": 0.8692,
      "step": 4660
    },
    {
      "epoch": 0.7729228732207878,
      "grad_norm": 4.1047682762146,
      "learning_rate": 4.1190637147202976e-05,
      "loss": 1.0653,
      "step": 4670
    },
    {
      "epoch": 0.7745779543197616,
      "grad_norm": 6.952812671661377,
      "learning_rate": 4.1169511576812574e-05,
      "loss": 0.8876,
      "step": 4680
    },
    {
      "epoch": 0.7762330354187356,
      "grad_norm": 3.454697847366333,
      "learning_rate": 4.114838600642217e-05,
      "loss": 0.3122,
      "step": 4690
    },
    {
      "epoch": 0.7778881165177094,
      "grad_norm": 9.471778869628906,
      "learning_rate": 4.112726043603177e-05,
      "loss": 0.8549,
      "step": 4700
    },
    {
      "epoch": 0.7795431976166832,
      "grad_norm": 3.304105520248413,
      "learning_rate": 4.1106134865641376e-05,
      "loss": 0.7305,
      "step": 4710
    },
    {
      "epoch": 0.7811982787156571,
      "grad_norm": 5.420029163360596,
      "learning_rate": 4.1085009295250974e-05,
      "loss": 1.0034,
      "step": 4720
    },
    {
      "epoch": 0.7828533598146309,
      "grad_norm": 6.436787128448486,
      "learning_rate": 4.106388372486057e-05,
      "loss": 0.7558,
      "step": 4730
    },
    {
      "epoch": 0.7845084409136047,
      "grad_norm": 9.845918655395508,
      "learning_rate": 4.104275815447017e-05,
      "loss": 0.8401,
      "step": 4740
    },
    {
      "epoch": 0.7861635220125787,
      "grad_norm": 13.204890251159668,
      "learning_rate": 4.102163258407977e-05,
      "loss": 1.0185,
      "step": 4750
    },
    {
      "epoch": 0.7878186031115525,
      "grad_norm": 9.306239128112793,
      "learning_rate": 4.100050701368937e-05,
      "loss": 0.8975,
      "step": 4760
    },
    {
      "epoch": 0.7894736842105263,
      "grad_norm": 7.704019546508789,
      "learning_rate": 4.097938144329897e-05,
      "loss": 0.7853,
      "step": 4770
    },
    {
      "epoch": 0.7911287653095002,
      "grad_norm": 6.60499382019043,
      "learning_rate": 4.095825587290857e-05,
      "loss": 1.0487,
      "step": 4780
    },
    {
      "epoch": 0.792783846408474,
      "grad_norm": 4.177285671234131,
      "learning_rate": 4.093713030251817e-05,
      "loss": 0.7792,
      "step": 4790
    },
    {
      "epoch": 0.7944389275074478,
      "grad_norm": 7.4618821144104,
      "learning_rate": 4.0916004732127775e-05,
      "loss": 0.7785,
      "step": 4800
    },
    {
      "epoch": 0.7960940086064218,
      "grad_norm": 6.4297332763671875,
      "learning_rate": 4.089487916173737e-05,
      "loss": 0.7934,
      "step": 4810
    },
    {
      "epoch": 0.7977490897053956,
      "grad_norm": 6.923448085784912,
      "learning_rate": 4.087375359134697e-05,
      "loss": 0.819,
      "step": 4820
    },
    {
      "epoch": 0.7994041708043694,
      "grad_norm": 9.160228729248047,
      "learning_rate": 4.085262802095657e-05,
      "loss": 1.0765,
      "step": 4830
    }
  ],
  "logging_steps": 10,
  "max_steps": 24168,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 2417,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.619524207034368e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
