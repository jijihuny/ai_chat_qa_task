{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.40003310162197947,
  "eval_steps": 500,
  "global_step": 2417,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016550810989738498,
      "grad_norm": 97.4415512084961,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 3.2653,
      "step": 10
    },
    {
      "epoch": 0.0033101621979476996,
      "grad_norm": 121.34355926513672,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 3.5017,
      "step": 20
    },
    {
      "epoch": 0.004965243296921549,
      "grad_norm": 48.178489685058594,
      "learning_rate": 3e-06,
      "loss": 2.8151,
      "step": 30
    },
    {
      "epoch": 0.006620324395895399,
      "grad_norm": 50.83437728881836,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.6973,
      "step": 40
    },
    {
      "epoch": 0.00827540549486925,
      "grad_norm": 39.91733169555664,
      "learning_rate": 5e-06,
      "loss": 1.5072,
      "step": 50
    },
    {
      "epoch": 0.009930486593843098,
      "grad_norm": 33.2450065612793,
      "learning_rate": 6e-06,
      "loss": 1.8742,
      "step": 60
    },
    {
      "epoch": 0.011585567692816948,
      "grad_norm": 32.60800552368164,
      "learning_rate": 7.000000000000001e-06,
      "loss": 1.6284,
      "step": 70
    },
    {
      "epoch": 0.013240648791790799,
      "grad_norm": 34.24906539916992,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.1873,
      "step": 80
    },
    {
      "epoch": 0.014895729890764648,
      "grad_norm": 50.064056396484375,
      "learning_rate": 9e-06,
      "loss": 1.1691,
      "step": 90
    },
    {
      "epoch": 0.0165508109897385,
      "grad_norm": 47.91412353515625,
      "learning_rate": 1e-05,
      "loss": 1.0641,
      "step": 100
    },
    {
      "epoch": 0.018205892088712348,
      "grad_norm": 43.95795822143555,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.0573,
      "step": 110
    },
    {
      "epoch": 0.019860973187686197,
      "grad_norm": 42.00776672363281,
      "learning_rate": 1.2e-05,
      "loss": 1.0241,
      "step": 120
    },
    {
      "epoch": 0.021516054286660046,
      "grad_norm": 28.95277214050293,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.17,
      "step": 130
    },
    {
      "epoch": 0.023171135385633895,
      "grad_norm": 17.930635452270508,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.0029,
      "step": 140
    },
    {
      "epoch": 0.024826216484607744,
      "grad_norm": 10.3660249710083,
      "learning_rate": 1.5e-05,
      "loss": 1.1087,
      "step": 150
    },
    {
      "epoch": 0.026481297583581597,
      "grad_norm": 40.463050842285156,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.9597,
      "step": 160
    },
    {
      "epoch": 0.028136378682555446,
      "grad_norm": 23.88897132873535,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 1.0799,
      "step": 170
    },
    {
      "epoch": 0.029791459781529295,
      "grad_norm": 23.142667770385742,
      "learning_rate": 1.8e-05,
      "loss": 1.0207,
      "step": 180
    },
    {
      "epoch": 0.031446540880503145,
      "grad_norm": 41.67078399658203,
      "learning_rate": 1.9e-05,
      "loss": 1.2081,
      "step": 190
    },
    {
      "epoch": 0.033101621979477,
      "grad_norm": 28.011249542236328,
      "learning_rate": 2e-05,
      "loss": 1.0128,
      "step": 200
    },
    {
      "epoch": 0.03475670307845084,
      "grad_norm": 22.017784118652344,
      "learning_rate": 2.1e-05,
      "loss": 0.9923,
      "step": 210
    },
    {
      "epoch": 0.036411784177424696,
      "grad_norm": 34.73788070678711,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.8604,
      "step": 220
    },
    {
      "epoch": 0.03806686527639854,
      "grad_norm": 27.730981826782227,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.8893,
      "step": 230
    },
    {
      "epoch": 0.039721946375372394,
      "grad_norm": 16.18294906616211,
      "learning_rate": 2.4e-05,
      "loss": 1.2433,
      "step": 240
    },
    {
      "epoch": 0.04137702747434624,
      "grad_norm": 36.50089645385742,
      "learning_rate": 2.5e-05,
      "loss": 0.9493,
      "step": 250
    },
    {
      "epoch": 0.04303210857332009,
      "grad_norm": 34.64004898071289,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.3145,
      "step": 260
    },
    {
      "epoch": 0.044687189672293945,
      "grad_norm": 35.67694854736328,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.12,
      "step": 270
    },
    {
      "epoch": 0.04634227077126779,
      "grad_norm": 14.698933601379395,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.9535,
      "step": 280
    },
    {
      "epoch": 0.04799735187024164,
      "grad_norm": 81.7050552368164,
      "learning_rate": 2.9e-05,
      "loss": 0.8433,
      "step": 290
    },
    {
      "epoch": 0.04965243296921549,
      "grad_norm": 29.671131134033203,
      "learning_rate": 3e-05,
      "loss": 1.5111,
      "step": 300
    },
    {
      "epoch": 0.05130751406818934,
      "grad_norm": 54.395599365234375,
      "learning_rate": 3.1e-05,
      "loss": 1.3558,
      "step": 310
    },
    {
      "epoch": 0.052962595167163194,
      "grad_norm": 24.62464141845703,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.4381,
      "step": 320
    },
    {
      "epoch": 0.05461767626613704,
      "grad_norm": 38.02064514160156,
      "learning_rate": 3.3e-05,
      "loss": 0.9403,
      "step": 330
    },
    {
      "epoch": 0.05627275736511089,
      "grad_norm": 44.62522506713867,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.9171,
      "step": 340
    },
    {
      "epoch": 0.05792783846408474,
      "grad_norm": 14.072800636291504,
      "learning_rate": 3.5e-05,
      "loss": 1.0232,
      "step": 350
    },
    {
      "epoch": 0.05958291956305859,
      "grad_norm": 14.406139373779297,
      "learning_rate": 3.6e-05,
      "loss": 0.7947,
      "step": 360
    },
    {
      "epoch": 0.06123800066203244,
      "grad_norm": 24.86500358581543,
      "learning_rate": 3.7e-05,
      "loss": 1.4377,
      "step": 370
    },
    {
      "epoch": 0.06289308176100629,
      "grad_norm": 18.5371036529541,
      "learning_rate": 3.8e-05,
      "loss": 0.9852,
      "step": 380
    },
    {
      "epoch": 0.06454816285998013,
      "grad_norm": 35.14235305786133,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.4349,
      "step": 390
    },
    {
      "epoch": 0.066203243958954,
      "grad_norm": 20.02771759033203,
      "learning_rate": 4e-05,
      "loss": 1.3746,
      "step": 400
    },
    {
      "epoch": 0.06785832505792784,
      "grad_norm": 28.747251510620117,
      "learning_rate": 4.1e-05,
      "loss": 1.3989,
      "step": 410
    },
    {
      "epoch": 0.06951340615690169,
      "grad_norm": 23.673721313476562,
      "learning_rate": 4.2e-05,
      "loss": 1.2095,
      "step": 420
    },
    {
      "epoch": 0.07116848725587553,
      "grad_norm": 3.3561432361602783,
      "learning_rate": 4.3e-05,
      "loss": 1.0675,
      "step": 430
    },
    {
      "epoch": 0.07282356835484939,
      "grad_norm": 51.90268325805664,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.0865,
      "step": 440
    },
    {
      "epoch": 0.07447864945382324,
      "grad_norm": 34.15875244140625,
      "learning_rate": 4.5e-05,
      "loss": 1.5736,
      "step": 450
    },
    {
      "epoch": 0.07613373055279708,
      "grad_norm": 26.975828170776367,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.2819,
      "step": 460
    },
    {
      "epoch": 0.07778881165177094,
      "grad_norm": 27.665348052978516,
      "learning_rate": 4.7e-05,
      "loss": 0.6944,
      "step": 470
    },
    {
      "epoch": 0.07944389275074479,
      "grad_norm": 29.756916046142578,
      "learning_rate": 4.8e-05,
      "loss": 1.8494,
      "step": 480
    },
    {
      "epoch": 0.08109897384971863,
      "grad_norm": 55.00706481933594,
      "learning_rate": 4.9e-05,
      "loss": 1.2432,
      "step": 490
    },
    {
      "epoch": 0.08275405494869248,
      "grad_norm": 45.52133560180664,
      "learning_rate": 5e-05,
      "loss": 1.1219,
      "step": 500
    },
    {
      "epoch": 0.08440913604766634,
      "grad_norm": 27.663555145263672,
      "learning_rate": 4.99788744296096e-05,
      "loss": 1.0477,
      "step": 510
    },
    {
      "epoch": 0.08606421714664018,
      "grad_norm": 32.310359954833984,
      "learning_rate": 4.99577488592192e-05,
      "loss": 1.1151,
      "step": 520
    },
    {
      "epoch": 0.08771929824561403,
      "grad_norm": 37.946659088134766,
      "learning_rate": 4.99366232888288e-05,
      "loss": 1.2722,
      "step": 530
    },
    {
      "epoch": 0.08937437934458789,
      "grad_norm": 27.8206729888916,
      "learning_rate": 4.9915497718438396e-05,
      "loss": 1.3977,
      "step": 540
    },
    {
      "epoch": 0.09102946044356174,
      "grad_norm": 22.341928482055664,
      "learning_rate": 4.9894372148048e-05,
      "loss": 1.1873,
      "step": 550
    },
    {
      "epoch": 0.09268454154253558,
      "grad_norm": 20.493959426879883,
      "learning_rate": 4.98732465776576e-05,
      "loss": 0.9783,
      "step": 560
    },
    {
      "epoch": 0.09433962264150944,
      "grad_norm": 36.496646881103516,
      "learning_rate": 4.98521210072672e-05,
      "loss": 1.2776,
      "step": 570
    },
    {
      "epoch": 0.09599470374048329,
      "grad_norm": 44.31249237060547,
      "learning_rate": 4.9830995436876796e-05,
      "loss": 1.2635,
      "step": 580
    },
    {
      "epoch": 0.09764978483945713,
      "grad_norm": 35.21307373046875,
      "learning_rate": 4.9809869866486395e-05,
      "loss": 1.4967,
      "step": 590
    },
    {
      "epoch": 0.09930486593843098,
      "grad_norm": 36.36589431762695,
      "learning_rate": 4.978874429609599e-05,
      "loss": 1.3594,
      "step": 600
    },
    {
      "epoch": 0.10095994703740484,
      "grad_norm": 24.680246353149414,
      "learning_rate": 4.97676187257056e-05,
      "loss": 1.499,
      "step": 610
    },
    {
      "epoch": 0.10261502813637868,
      "grad_norm": 30.076444625854492,
      "learning_rate": 4.9746493155315197e-05,
      "loss": 1.0537,
      "step": 620
    },
    {
      "epoch": 0.10427010923535253,
      "grad_norm": 18.924583435058594,
      "learning_rate": 4.9725367584924795e-05,
      "loss": 1.0623,
      "step": 630
    },
    {
      "epoch": 0.10592519033432639,
      "grad_norm": 35.29458999633789,
      "learning_rate": 4.97042420145344e-05,
      "loss": 1.3919,
      "step": 640
    },
    {
      "epoch": 0.10758027143330023,
      "grad_norm": 26.997940063476562,
      "learning_rate": 4.9683116444144e-05,
      "loss": 1.2173,
      "step": 650
    },
    {
      "epoch": 0.10923535253227408,
      "grad_norm": 23.493989944458008,
      "learning_rate": 4.96619908737536e-05,
      "loss": 1.2526,
      "step": 660
    },
    {
      "epoch": 0.11089043363124793,
      "grad_norm": 21.540573120117188,
      "learning_rate": 4.9640865303363195e-05,
      "loss": 1.2347,
      "step": 670
    },
    {
      "epoch": 0.11254551473022179,
      "grad_norm": 54.90599060058594,
      "learning_rate": 4.9619739732972794e-05,
      "loss": 1.2998,
      "step": 680
    },
    {
      "epoch": 0.11420059582919563,
      "grad_norm": 17.971019744873047,
      "learning_rate": 4.959861416258239e-05,
      "loss": 1.225,
      "step": 690
    },
    {
      "epoch": 0.11585567692816948,
      "grad_norm": 23.759201049804688,
      "learning_rate": 4.957748859219199e-05,
      "loss": 1.1704,
      "step": 700
    },
    {
      "epoch": 0.11751075802714334,
      "grad_norm": 18.30558967590332,
      "learning_rate": 4.955636302180159e-05,
      "loss": 1.1507,
      "step": 710
    },
    {
      "epoch": 0.11916583912611718,
      "grad_norm": 41.52193832397461,
      "learning_rate": 4.9535237451411194e-05,
      "loss": 1.0947,
      "step": 720
    },
    {
      "epoch": 0.12082092022509103,
      "grad_norm": 18.925830841064453,
      "learning_rate": 4.951411188102079e-05,
      "loss": 1.5544,
      "step": 730
    },
    {
      "epoch": 0.12247600132406487,
      "grad_norm": 58.37614059448242,
      "learning_rate": 4.949298631063039e-05,
      "loss": 1.3196,
      "step": 740
    },
    {
      "epoch": 0.12413108242303873,
      "grad_norm": 20.034914016723633,
      "learning_rate": 4.947186074023999e-05,
      "loss": 1.1119,
      "step": 750
    },
    {
      "epoch": 0.12578616352201258,
      "grad_norm": 42.30554962158203,
      "learning_rate": 4.945073516984959e-05,
      "loss": 0.9168,
      "step": 760
    },
    {
      "epoch": 0.12744124462098644,
      "grad_norm": 50.14617156982422,
      "learning_rate": 4.9429609599459186e-05,
      "loss": 1.0796,
      "step": 770
    },
    {
      "epoch": 0.12909632571996027,
      "grad_norm": 69.02271270751953,
      "learning_rate": 4.9408484029068784e-05,
      "loss": 1.2964,
      "step": 780
    },
    {
      "epoch": 0.13075140681893413,
      "grad_norm": 33.62277603149414,
      "learning_rate": 4.938735845867838e-05,
      "loss": 1.3134,
      "step": 790
    },
    {
      "epoch": 0.132406487917908,
      "grad_norm": 83.8985595703125,
      "learning_rate": 4.936623288828799e-05,
      "loss": 1.36,
      "step": 800
    },
    {
      "epoch": 0.13406156901688182,
      "grad_norm": 28.49913787841797,
      "learning_rate": 4.9345107317897586e-05,
      "loss": 1.2215,
      "step": 810
    },
    {
      "epoch": 0.13571665011585568,
      "grad_norm": 44.60554122924805,
      "learning_rate": 4.9323981747507185e-05,
      "loss": 1.2679,
      "step": 820
    },
    {
      "epoch": 0.13737173121482954,
      "grad_norm": 57.60272216796875,
      "learning_rate": 4.930285617711678e-05,
      "loss": 1.0022,
      "step": 830
    },
    {
      "epoch": 0.13902681231380337,
      "grad_norm": 13.666054725646973,
      "learning_rate": 4.928173060672638e-05,
      "loss": 0.9676,
      "step": 840
    },
    {
      "epoch": 0.14068189341277723,
      "grad_norm": 17.73228645324707,
      "learning_rate": 4.926060503633598e-05,
      "loss": 1.0089,
      "step": 850
    },
    {
      "epoch": 0.14233697451175106,
      "grad_norm": 38.72350311279297,
      "learning_rate": 4.9239479465945585e-05,
      "loss": 1.1762,
      "step": 860
    },
    {
      "epoch": 0.14399205561072492,
      "grad_norm": 48.70363235473633,
      "learning_rate": 4.921835389555518e-05,
      "loss": 1.4009,
      "step": 870
    },
    {
      "epoch": 0.14564713670969878,
      "grad_norm": 50.8752555847168,
      "learning_rate": 4.919722832516478e-05,
      "loss": 1.0995,
      "step": 880
    },
    {
      "epoch": 0.14730221780867261,
      "grad_norm": 44.483604431152344,
      "learning_rate": 4.917610275477439e-05,
      "loss": 1.1234,
      "step": 890
    },
    {
      "epoch": 0.14895729890764647,
      "grad_norm": 29.918901443481445,
      "learning_rate": 4.9154977184383985e-05,
      "loss": 1.0949,
      "step": 900
    },
    {
      "epoch": 0.15061238000662033,
      "grad_norm": 18.013593673706055,
      "learning_rate": 4.9133851613993584e-05,
      "loss": 1.0986,
      "step": 910
    },
    {
      "epoch": 0.15226746110559417,
      "grad_norm": 17.03639030456543,
      "learning_rate": 4.911272604360318e-05,
      "loss": 1.0965,
      "step": 920
    },
    {
      "epoch": 0.15392254220456802,
      "grad_norm": 32.880088806152344,
      "learning_rate": 4.909160047321278e-05,
      "loss": 1.1738,
      "step": 930
    },
    {
      "epoch": 0.15557762330354188,
      "grad_norm": 17.56507110595703,
      "learning_rate": 4.907047490282238e-05,
      "loss": 1.2153,
      "step": 940
    },
    {
      "epoch": 0.15723270440251572,
      "grad_norm": 26.05997085571289,
      "learning_rate": 4.904934933243198e-05,
      "loss": 0.9936,
      "step": 950
    },
    {
      "epoch": 0.15888778550148958,
      "grad_norm": 38.58293914794922,
      "learning_rate": 4.9028223762041575e-05,
      "loss": 1.39,
      "step": 960
    },
    {
      "epoch": 0.16054286660046344,
      "grad_norm": 12.222036361694336,
      "learning_rate": 4.900709819165118e-05,
      "loss": 1.0561,
      "step": 970
    },
    {
      "epoch": 0.16219794769943727,
      "grad_norm": 24.692991256713867,
      "learning_rate": 4.898597262126078e-05,
      "loss": 1.3424,
      "step": 980
    },
    {
      "epoch": 0.16385302879841113,
      "grad_norm": 67.37031555175781,
      "learning_rate": 4.896484705087038e-05,
      "loss": 1.1594,
      "step": 990
    },
    {
      "epoch": 0.16550810989738496,
      "grad_norm": 23.87232780456543,
      "learning_rate": 4.8943721480479976e-05,
      "loss": 1.191,
      "step": 1000
    },
    {
      "epoch": 0.16716319099635882,
      "grad_norm": 21.083709716796875,
      "learning_rate": 4.8922595910089574e-05,
      "loss": 1.1213,
      "step": 1010
    },
    {
      "epoch": 0.16881827209533268,
      "grad_norm": 21.680368423461914,
      "learning_rate": 4.890147033969917e-05,
      "loss": 1.0167,
      "step": 1020
    },
    {
      "epoch": 0.1704733531943065,
      "grad_norm": 18.98822593688965,
      "learning_rate": 4.888034476930877e-05,
      "loss": 1.2131,
      "step": 1030
    },
    {
      "epoch": 0.17212843429328037,
      "grad_norm": 40.81112289428711,
      "learning_rate": 4.885921919891837e-05,
      "loss": 1.0608,
      "step": 1040
    },
    {
      "epoch": 0.17378351539225423,
      "grad_norm": 27.607023239135742,
      "learning_rate": 4.883809362852797e-05,
      "loss": 1.4412,
      "step": 1050
    },
    {
      "epoch": 0.17543859649122806,
      "grad_norm": 42.07017517089844,
      "learning_rate": 4.881696805813757e-05,
      "loss": 1.1374,
      "step": 1060
    },
    {
      "epoch": 0.17709367759020192,
      "grad_norm": 35.417564392089844,
      "learning_rate": 4.879584248774717e-05,
      "loss": 1.3788,
      "step": 1070
    },
    {
      "epoch": 0.17874875868917578,
      "grad_norm": 33.0713005065918,
      "learning_rate": 4.877471691735677e-05,
      "loss": 1.1659,
      "step": 1080
    },
    {
      "epoch": 0.1804038397881496,
      "grad_norm": 32.4471549987793,
      "learning_rate": 4.875359134696637e-05,
      "loss": 1.0727,
      "step": 1090
    },
    {
      "epoch": 0.18205892088712347,
      "grad_norm": 38.011470794677734,
      "learning_rate": 4.8732465776575966e-05,
      "loss": 0.9315,
      "step": 1100
    },
    {
      "epoch": 0.18371400198609733,
      "grad_norm": 19.309675216674805,
      "learning_rate": 4.871134020618557e-05,
      "loss": 1.5556,
      "step": 1110
    },
    {
      "epoch": 0.18536908308507116,
      "grad_norm": 60.10500717163086,
      "learning_rate": 4.869021463579517e-05,
      "loss": 0.6319,
      "step": 1120
    },
    {
      "epoch": 0.18702416418404502,
      "grad_norm": 13.00697135925293,
      "learning_rate": 4.866908906540477e-05,
      "loss": 1.4658,
      "step": 1130
    },
    {
      "epoch": 0.18867924528301888,
      "grad_norm": 25.067235946655273,
      "learning_rate": 4.8647963495014373e-05,
      "loss": 1.3293,
      "step": 1140
    },
    {
      "epoch": 0.1903343263819927,
      "grad_norm": 14.16656494140625,
      "learning_rate": 4.862683792462397e-05,
      "loss": 0.7232,
      "step": 1150
    },
    {
      "epoch": 0.19198940748096657,
      "grad_norm": 31.454057693481445,
      "learning_rate": 4.860571235423357e-05,
      "loss": 1.3936,
      "step": 1160
    },
    {
      "epoch": 0.1936444885799404,
      "grad_norm": 45.0728874206543,
      "learning_rate": 4.858458678384317e-05,
      "loss": 1.3844,
      "step": 1170
    },
    {
      "epoch": 0.19529956967891426,
      "grad_norm": 31.794769287109375,
      "learning_rate": 4.856346121345277e-05,
      "loss": 1.0689,
      "step": 1180
    },
    {
      "epoch": 0.19695465077788812,
      "grad_norm": 21.14841651916504,
      "learning_rate": 4.8542335643062365e-05,
      "loss": 1.4044,
      "step": 1190
    },
    {
      "epoch": 0.19860973187686196,
      "grad_norm": 45.91816329956055,
      "learning_rate": 4.8521210072671964e-05,
      "loss": 1.1661,
      "step": 1200
    },
    {
      "epoch": 0.20026481297583582,
      "grad_norm": 18.18951988220215,
      "learning_rate": 4.850008450228156e-05,
      "loss": 1.0786,
      "step": 1210
    },
    {
      "epoch": 0.20191989407480967,
      "grad_norm": 36.60145568847656,
      "learning_rate": 4.847895893189116e-05,
      "loss": 1.443,
      "step": 1220
    },
    {
      "epoch": 0.2035749751737835,
      "grad_norm": 36.871280670166016,
      "learning_rate": 4.8457833361500766e-05,
      "loss": 1.3718,
      "step": 1230
    },
    {
      "epoch": 0.20523005627275737,
      "grad_norm": 18.16054916381836,
      "learning_rate": 4.8436707791110364e-05,
      "loss": 0.9796,
      "step": 1240
    },
    {
      "epoch": 0.20688513737173123,
      "grad_norm": 44.73019790649414,
      "learning_rate": 4.841558222071996e-05,
      "loss": 1.0275,
      "step": 1250
    },
    {
      "epoch": 0.20854021847070506,
      "grad_norm": 17.73753547668457,
      "learning_rate": 4.839445665032956e-05,
      "loss": 0.7922,
      "step": 1260
    },
    {
      "epoch": 0.21019529956967892,
      "grad_norm": 42.6441650390625,
      "learning_rate": 4.837333107993916e-05,
      "loss": 1.4442,
      "step": 1270
    },
    {
      "epoch": 0.21185038066865278,
      "grad_norm": 24.02442741394043,
      "learning_rate": 4.835220550954876e-05,
      "loss": 1.0102,
      "step": 1280
    },
    {
      "epoch": 0.2135054617676266,
      "grad_norm": 48.59553146362305,
      "learning_rate": 4.8331079939158356e-05,
      "loss": 1.2905,
      "step": 1290
    },
    {
      "epoch": 0.21516054286660047,
      "grad_norm": 17.703311920166016,
      "learning_rate": 4.8309954368767954e-05,
      "loss": 0.8522,
      "step": 1300
    },
    {
      "epoch": 0.2168156239655743,
      "grad_norm": 64.39567565917969,
      "learning_rate": 4.828882879837756e-05,
      "loss": 1.1265,
      "step": 1310
    },
    {
      "epoch": 0.21847070506454816,
      "grad_norm": 23.48064613342285,
      "learning_rate": 4.826770322798716e-05,
      "loss": 1.3915,
      "step": 1320
    },
    {
      "epoch": 0.22012578616352202,
      "grad_norm": 41.70143127441406,
      "learning_rate": 4.8246577657596756e-05,
      "loss": 1.3118,
      "step": 1330
    },
    {
      "epoch": 0.22178086726249585,
      "grad_norm": 69.24468994140625,
      "learning_rate": 4.8225452087206355e-05,
      "loss": 0.8991,
      "step": 1340
    },
    {
      "epoch": 0.2234359483614697,
      "grad_norm": 43.01826477050781,
      "learning_rate": 4.820432651681595e-05,
      "loss": 0.8805,
      "step": 1350
    },
    {
      "epoch": 0.22509102946044357,
      "grad_norm": 47.81785202026367,
      "learning_rate": 4.818320094642556e-05,
      "loss": 0.8368,
      "step": 1360
    },
    {
      "epoch": 0.2267461105594174,
      "grad_norm": 30.941341400146484,
      "learning_rate": 4.816207537603516e-05,
      "loss": 1.5406,
      "step": 1370
    },
    {
      "epoch": 0.22840119165839126,
      "grad_norm": 39.646812438964844,
      "learning_rate": 4.8140949805644755e-05,
      "loss": 0.9361,
      "step": 1380
    },
    {
      "epoch": 0.23005627275736512,
      "grad_norm": 13.443039894104004,
      "learning_rate": 4.811982423525435e-05,
      "loss": 1.2759,
      "step": 1390
    },
    {
      "epoch": 0.23171135385633895,
      "grad_norm": 26.6287899017334,
      "learning_rate": 4.809869866486396e-05,
      "loss": 1.072,
      "step": 1400
    },
    {
      "epoch": 0.2333664349553128,
      "grad_norm": 63.24628448486328,
      "learning_rate": 4.807757309447356e-05,
      "loss": 1.4455,
      "step": 1410
    },
    {
      "epoch": 0.23502151605428667,
      "grad_norm": 24.016357421875,
      "learning_rate": 4.8056447524083155e-05,
      "loss": 1.3908,
      "step": 1420
    },
    {
      "epoch": 0.2366765971532605,
      "grad_norm": 12.46517276763916,
      "learning_rate": 4.8035321953692754e-05,
      "loss": 0.7688,
      "step": 1430
    },
    {
      "epoch": 0.23833167825223436,
      "grad_norm": 10.827034950256348,
      "learning_rate": 4.801419638330235e-05,
      "loss": 1.0949,
      "step": 1440
    },
    {
      "epoch": 0.2399867593512082,
      "grad_norm": 136.52789306640625,
      "learning_rate": 4.799307081291195e-05,
      "loss": 0.7759,
      "step": 1450
    },
    {
      "epoch": 0.24164184045018205,
      "grad_norm": 35.291507720947266,
      "learning_rate": 4.797194524252155e-05,
      "loss": 1.319,
      "step": 1460
    },
    {
      "epoch": 0.24329692154915591,
      "grad_norm": 25.31490135192871,
      "learning_rate": 4.795081967213115e-05,
      "loss": 1.2102,
      "step": 1470
    },
    {
      "epoch": 0.24495200264812975,
      "grad_norm": 5.311086177825928,
      "learning_rate": 4.792969410174075e-05,
      "loss": 0.8988,
      "step": 1480
    },
    {
      "epoch": 0.2466070837471036,
      "grad_norm": 28.51626968383789,
      "learning_rate": 4.790856853135035e-05,
      "loss": 1.0968,
      "step": 1490
    },
    {
      "epoch": 0.24826216484607747,
      "grad_norm": 54.17319869995117,
      "learning_rate": 4.788744296095995e-05,
      "loss": 1.0518,
      "step": 1500
    },
    {
      "epoch": 0.2499172459450513,
      "grad_norm": 26.058807373046875,
      "learning_rate": 4.786631739056955e-05,
      "loss": 1.3213,
      "step": 1510
    },
    {
      "epoch": 0.25157232704402516,
      "grad_norm": 23.86149787902832,
      "learning_rate": 4.7845191820179146e-05,
      "loss": 1.3633,
      "step": 1520
    },
    {
      "epoch": 0.253227408142999,
      "grad_norm": 36.164154052734375,
      "learning_rate": 4.7824066249788744e-05,
      "loss": 1.0884,
      "step": 1530
    },
    {
      "epoch": 0.2548824892419729,
      "grad_norm": 55.85585403442383,
      "learning_rate": 4.780294067939834e-05,
      "loss": 0.8493,
      "step": 1540
    },
    {
      "epoch": 0.2565375703409467,
      "grad_norm": 27.191936492919922,
      "learning_rate": 4.778181510900794e-05,
      "loss": 0.9305,
      "step": 1550
    },
    {
      "epoch": 0.25819265143992054,
      "grad_norm": 40.60062026977539,
      "learning_rate": 4.776068953861754e-05,
      "loss": 1.1499,
      "step": 1560
    },
    {
      "epoch": 0.2598477325388944,
      "grad_norm": 30.577640533447266,
      "learning_rate": 4.7739563968227145e-05,
      "loss": 1.0569,
      "step": 1570
    },
    {
      "epoch": 0.26150281363786826,
      "grad_norm": 18.682828903198242,
      "learning_rate": 4.771843839783674e-05,
      "loss": 0.8512,
      "step": 1580
    },
    {
      "epoch": 0.2631578947368421,
      "grad_norm": 17.841514587402344,
      "learning_rate": 4.769731282744634e-05,
      "loss": 1.2974,
      "step": 1590
    },
    {
      "epoch": 0.264812975835816,
      "grad_norm": 28.501638412475586,
      "learning_rate": 4.767618725705594e-05,
      "loss": 1.141,
      "step": 1600
    },
    {
      "epoch": 0.2664680569347898,
      "grad_norm": 37.53023910522461,
      "learning_rate": 4.7655061686665545e-05,
      "loss": 0.7923,
      "step": 1610
    },
    {
      "epoch": 0.26812313803376364,
      "grad_norm": 22.709646224975586,
      "learning_rate": 4.763393611627514e-05,
      "loss": 1.319,
      "step": 1620
    },
    {
      "epoch": 0.26977821913273753,
      "grad_norm": 27.045103073120117,
      "learning_rate": 4.761281054588474e-05,
      "loss": 0.7483,
      "step": 1630
    },
    {
      "epoch": 0.27143330023171136,
      "grad_norm": 55.98433303833008,
      "learning_rate": 4.759168497549434e-05,
      "loss": 1.3818,
      "step": 1640
    },
    {
      "epoch": 0.2730883813306852,
      "grad_norm": 28.643022537231445,
      "learning_rate": 4.7570559405103945e-05,
      "loss": 1.173,
      "step": 1650
    },
    {
      "epoch": 0.2747434624296591,
      "grad_norm": 26.441133499145508,
      "learning_rate": 4.7549433834713544e-05,
      "loss": 1.2022,
      "step": 1660
    },
    {
      "epoch": 0.2763985435286329,
      "grad_norm": 46.60511016845703,
      "learning_rate": 4.752830826432314e-05,
      "loss": 1.0439,
      "step": 1670
    },
    {
      "epoch": 0.27805362462760674,
      "grad_norm": 20.532909393310547,
      "learning_rate": 4.750718269393274e-05,
      "loss": 0.997,
      "step": 1680
    },
    {
      "epoch": 0.2797087057265806,
      "grad_norm": 37.974918365478516,
      "learning_rate": 4.748605712354234e-05,
      "loss": 0.9507,
      "step": 1690
    },
    {
      "epoch": 0.28136378682555446,
      "grad_norm": 58.13558578491211,
      "learning_rate": 4.746493155315194e-05,
      "loss": 1.2208,
      "step": 1700
    },
    {
      "epoch": 0.2830188679245283,
      "grad_norm": 43.653099060058594,
      "learning_rate": 4.7443805982761536e-05,
      "loss": 1.4586,
      "step": 1710
    },
    {
      "epoch": 0.2846739490235021,
      "grad_norm": 11.706361770629883,
      "learning_rate": 4.7422680412371134e-05,
      "loss": 1.1155,
      "step": 1720
    },
    {
      "epoch": 0.286329030122476,
      "grad_norm": 24.717708587646484,
      "learning_rate": 4.740155484198073e-05,
      "loss": 1.1153,
      "step": 1730
    },
    {
      "epoch": 0.28798411122144985,
      "grad_norm": 28.47773551940918,
      "learning_rate": 4.738042927159034e-05,
      "loss": 1.3297,
      "step": 1740
    },
    {
      "epoch": 0.2896391923204237,
      "grad_norm": 55.19663619995117,
      "learning_rate": 4.7359303701199936e-05,
      "loss": 1.4315,
      "step": 1750
    },
    {
      "epoch": 0.29129427341939756,
      "grad_norm": 20.91109848022461,
      "learning_rate": 4.7338178130809534e-05,
      "loss": 1.0999,
      "step": 1760
    },
    {
      "epoch": 0.2929493545183714,
      "grad_norm": 26.083189010620117,
      "learning_rate": 4.731705256041913e-05,
      "loss": 1.0293,
      "step": 1770
    },
    {
      "epoch": 0.29460443561734523,
      "grad_norm": 42.47421646118164,
      "learning_rate": 4.729592699002873e-05,
      "loss": 1.2907,
      "step": 1780
    },
    {
      "epoch": 0.2962595167163191,
      "grad_norm": 24.587677001953125,
      "learning_rate": 4.727480141963833e-05,
      "loss": 1.0462,
      "step": 1790
    },
    {
      "epoch": 0.29791459781529295,
      "grad_norm": 54.55634307861328,
      "learning_rate": 4.725367584924793e-05,
      "loss": 0.9033,
      "step": 1800
    },
    {
      "epoch": 0.2995696789142668,
      "grad_norm": 24.767593383789062,
      "learning_rate": 4.7232550278857526e-05,
      "loss": 1.2028,
      "step": 1810
    },
    {
      "epoch": 0.30122476001324067,
      "grad_norm": 44.51886749267578,
      "learning_rate": 4.721142470846713e-05,
      "loss": 1.2678,
      "step": 1820
    },
    {
      "epoch": 0.3028798411122145,
      "grad_norm": 51.98725509643555,
      "learning_rate": 4.719029913807673e-05,
      "loss": 1.0257,
      "step": 1830
    },
    {
      "epoch": 0.30453492221118833,
      "grad_norm": 33.271419525146484,
      "learning_rate": 4.716917356768633e-05,
      "loss": 1.2286,
      "step": 1840
    },
    {
      "epoch": 0.3061900033101622,
      "grad_norm": 33.29391098022461,
      "learning_rate": 4.714804799729593e-05,
      "loss": 1.1303,
      "step": 1850
    },
    {
      "epoch": 0.30784508440913605,
      "grad_norm": 33.18928146362305,
      "learning_rate": 4.712692242690553e-05,
      "loss": 1.1277,
      "step": 1860
    },
    {
      "epoch": 0.3095001655081099,
      "grad_norm": 23.810184478759766,
      "learning_rate": 4.710579685651513e-05,
      "loss": 1.1335,
      "step": 1870
    },
    {
      "epoch": 0.31115524660708377,
      "grad_norm": 99.15519714355469,
      "learning_rate": 4.708467128612473e-05,
      "loss": 1.1959,
      "step": 1880
    },
    {
      "epoch": 0.3128103277060576,
      "grad_norm": 29.15110206604004,
      "learning_rate": 4.706354571573433e-05,
      "loss": 1.4197,
      "step": 1890
    },
    {
      "epoch": 0.31446540880503143,
      "grad_norm": 35.4796028137207,
      "learning_rate": 4.7042420145343925e-05,
      "loss": 1.0968,
      "step": 1900
    },
    {
      "epoch": 0.3161204899040053,
      "grad_norm": 33.30064010620117,
      "learning_rate": 4.702129457495353e-05,
      "loss": 1.0341,
      "step": 1910
    },
    {
      "epoch": 0.31777557100297915,
      "grad_norm": 33.59657287597656,
      "learning_rate": 4.700016900456313e-05,
      "loss": 1.1013,
      "step": 1920
    },
    {
      "epoch": 0.319430652101953,
      "grad_norm": 40.53507614135742,
      "learning_rate": 4.697904343417273e-05,
      "loss": 1.3462,
      "step": 1930
    },
    {
      "epoch": 0.32108573320092687,
      "grad_norm": 31.782255172729492,
      "learning_rate": 4.6957917863782325e-05,
      "loss": 1.1407,
      "step": 1940
    },
    {
      "epoch": 0.3227408142999007,
      "grad_norm": 21.611434936523438,
      "learning_rate": 4.6936792293391924e-05,
      "loss": 1.0156,
      "step": 1950
    },
    {
      "epoch": 0.32439589539887453,
      "grad_norm": 104.43136596679688,
      "learning_rate": 4.691566672300152e-05,
      "loss": 1.1035,
      "step": 1960
    },
    {
      "epoch": 0.3260509764978484,
      "grad_norm": 36.75350570678711,
      "learning_rate": 4.689454115261112e-05,
      "loss": 0.9366,
      "step": 1970
    },
    {
      "epoch": 0.32770605759682225,
      "grad_norm": 36.79549789428711,
      "learning_rate": 4.687341558222072e-05,
      "loss": 1.1395,
      "step": 1980
    },
    {
      "epoch": 0.3293611386957961,
      "grad_norm": 24.88180160522461,
      "learning_rate": 4.6852290011830324e-05,
      "loss": 0.856,
      "step": 1990
    },
    {
      "epoch": 0.3310162197947699,
      "grad_norm": 34.89110565185547,
      "learning_rate": 4.683116444143992e-05,
      "loss": 0.9102,
      "step": 2000
    },
    {
      "epoch": 0.3326713008937438,
      "grad_norm": 20.741331100463867,
      "learning_rate": 4.681003887104952e-05,
      "loss": 1.192,
      "step": 2010
    },
    {
      "epoch": 0.33432638199271764,
      "grad_norm": 36.0693244934082,
      "learning_rate": 4.678891330065912e-05,
      "loss": 1.1723,
      "step": 2020
    },
    {
      "epoch": 0.33598146309169147,
      "grad_norm": 69.0107650756836,
      "learning_rate": 4.676778773026872e-05,
      "loss": 0.9863,
      "step": 2030
    },
    {
      "epoch": 0.33763654419066536,
      "grad_norm": 29.610151290893555,
      "learning_rate": 4.6746662159878316e-05,
      "loss": 1.2898,
      "step": 2040
    },
    {
      "epoch": 0.3392916252896392,
      "grad_norm": 15.492105484008789,
      "learning_rate": 4.6725536589487914e-05,
      "loss": 1.0231,
      "step": 2050
    },
    {
      "epoch": 0.340946706388613,
      "grad_norm": 25.773880004882812,
      "learning_rate": 4.670441101909751e-05,
      "loss": 1.5425,
      "step": 2060
    },
    {
      "epoch": 0.3426017874875869,
      "grad_norm": 47.44260025024414,
      "learning_rate": 4.668328544870712e-05,
      "loss": 1.2476,
      "step": 2070
    },
    {
      "epoch": 0.34425686858656074,
      "grad_norm": 77.98101806640625,
      "learning_rate": 4.6662159878316716e-05,
      "loss": 1.378,
      "step": 2080
    },
    {
      "epoch": 0.34591194968553457,
      "grad_norm": 43.10260009765625,
      "learning_rate": 4.6641034307926315e-05,
      "loss": 1.2092,
      "step": 2090
    },
    {
      "epoch": 0.34756703078450846,
      "grad_norm": 38.76762771606445,
      "learning_rate": 4.661990873753592e-05,
      "loss": 1.0358,
      "step": 2100
    },
    {
      "epoch": 0.3492221118834823,
      "grad_norm": 27.373626708984375,
      "learning_rate": 4.659878316714552e-05,
      "loss": 0.8094,
      "step": 2110
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 24.611284255981445,
      "learning_rate": 4.657765759675512e-05,
      "loss": 0.9375,
      "step": 2120
    },
    {
      "epoch": 0.35253227408143,
      "grad_norm": 24.617549896240234,
      "learning_rate": 4.6556532026364715e-05,
      "loss": 1.377,
      "step": 2130
    },
    {
      "epoch": 0.35418735518040384,
      "grad_norm": 25.508323669433594,
      "learning_rate": 4.6535406455974313e-05,
      "loss": 1.0572,
      "step": 2140
    },
    {
      "epoch": 0.35584243627937767,
      "grad_norm": 34.27818298339844,
      "learning_rate": 4.651428088558391e-05,
      "loss": 1.2413,
      "step": 2150
    },
    {
      "epoch": 0.35749751737835156,
      "grad_norm": 86.87638854980469,
      "learning_rate": 4.649315531519352e-05,
      "loss": 1.1934,
      "step": 2160
    },
    {
      "epoch": 0.3591525984773254,
      "grad_norm": 11.199602127075195,
      "learning_rate": 4.6472029744803115e-05,
      "loss": 1.3045,
      "step": 2170
    },
    {
      "epoch": 0.3608076795762992,
      "grad_norm": 13.859945297241211,
      "learning_rate": 4.6450904174412714e-05,
      "loss": 0.9475,
      "step": 2180
    },
    {
      "epoch": 0.3624627606752731,
      "grad_norm": 31.423864364624023,
      "learning_rate": 4.642977860402231e-05,
      "loss": 1.1896,
      "step": 2190
    },
    {
      "epoch": 0.36411784177424694,
      "grad_norm": 29.091333389282227,
      "learning_rate": 4.640865303363191e-05,
      "loss": 1.1421,
      "step": 2200
    },
    {
      "epoch": 0.3657729228732208,
      "grad_norm": 40.34096145629883,
      "learning_rate": 4.638752746324151e-05,
      "loss": 1.0777,
      "step": 2210
    },
    {
      "epoch": 0.36742800397219466,
      "grad_norm": 21.893680572509766,
      "learning_rate": 4.636640189285111e-05,
      "loss": 1.0392,
      "step": 2220
    },
    {
      "epoch": 0.3690830850711685,
      "grad_norm": 29.696298599243164,
      "learning_rate": 4.6345276322460706e-05,
      "loss": 1.0009,
      "step": 2230
    },
    {
      "epoch": 0.3707381661701423,
      "grad_norm": 59.434635162353516,
      "learning_rate": 4.6324150752070304e-05,
      "loss": 1.1633,
      "step": 2240
    },
    {
      "epoch": 0.3723932472691162,
      "grad_norm": 10.650955200195312,
      "learning_rate": 4.630302518167991e-05,
      "loss": 1.1615,
      "step": 2250
    },
    {
      "epoch": 0.37404832836809004,
      "grad_norm": 35.60771179199219,
      "learning_rate": 4.628189961128951e-05,
      "loss": 0.9056,
      "step": 2260
    },
    {
      "epoch": 0.3757034094670639,
      "grad_norm": 32.109474182128906,
      "learning_rate": 4.6260774040899106e-05,
      "loss": 0.9758,
      "step": 2270
    },
    {
      "epoch": 0.37735849056603776,
      "grad_norm": 27.079797744750977,
      "learning_rate": 4.6239648470508704e-05,
      "loss": 0.8164,
      "step": 2280
    },
    {
      "epoch": 0.3790135716650116,
      "grad_norm": 52.39609146118164,
      "learning_rate": 4.62185229001183e-05,
      "loss": 1.3938,
      "step": 2290
    },
    {
      "epoch": 0.3806686527639854,
      "grad_norm": 38.14473342895508,
      "learning_rate": 4.61973973297279e-05,
      "loss": 1.2163,
      "step": 2300
    },
    {
      "epoch": 0.38232373386295926,
      "grad_norm": 28.690258026123047,
      "learning_rate": 4.61762717593375e-05,
      "loss": 1.1468,
      "step": 2310
    },
    {
      "epoch": 0.38397881496193315,
      "grad_norm": 67.65990447998047,
      "learning_rate": 4.6155146188947105e-05,
      "loss": 0.9443,
      "step": 2320
    },
    {
      "epoch": 0.385633896060907,
      "grad_norm": 35.638084411621094,
      "learning_rate": 4.61340206185567e-05,
      "loss": 0.7823,
      "step": 2330
    },
    {
      "epoch": 0.3872889771598808,
      "grad_norm": 262.21441650390625,
      "learning_rate": 4.61128950481663e-05,
      "loss": 1.3207,
      "step": 2340
    },
    {
      "epoch": 0.3889440582588547,
      "grad_norm": 31.428504943847656,
      "learning_rate": 4.6091769477775907e-05,
      "loss": 0.9319,
      "step": 2350
    },
    {
      "epoch": 0.39059913935782853,
      "grad_norm": 31.62726593017578,
      "learning_rate": 4.6070643907385505e-05,
      "loss": 1.1942,
      "step": 2360
    },
    {
      "epoch": 0.39225422045680236,
      "grad_norm": 44.401485443115234,
      "learning_rate": 4.60495183369951e-05,
      "loss": 0.8836,
      "step": 2370
    },
    {
      "epoch": 0.39390930155577625,
      "grad_norm": 58.04596710205078,
      "learning_rate": 4.60283927666047e-05,
      "loss": 1.3717,
      "step": 2380
    },
    {
      "epoch": 0.3955643826547501,
      "grad_norm": 92.31807708740234,
      "learning_rate": 4.60072671962143e-05,
      "loss": 1.1065,
      "step": 2390
    },
    {
      "epoch": 0.3972194637537239,
      "grad_norm": 14.041876792907715,
      "learning_rate": 4.59861416258239e-05,
      "loss": 0.9633,
      "step": 2400
    },
    {
      "epoch": 0.3988745448526978,
      "grad_norm": 18.356443405151367,
      "learning_rate": 4.59650160554335e-05,
      "loss": 0.921,
      "step": 2410
    }
  ],
  "logging_steps": 10,
  "max_steps": 24168,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 2417,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.7957478399787008e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
