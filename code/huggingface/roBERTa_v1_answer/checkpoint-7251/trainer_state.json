{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.2000993048659385,
  "eval_steps": 500,
  "global_step": 7251,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016550810989738498,
      "grad_norm": 94.93196868896484,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 1.4605,
      "step": 10
    },
    {
      "epoch": 0.0033101621979476996,
      "grad_norm": 49.61991882324219,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.8564,
      "step": 20
    },
    {
      "epoch": 0.004965243296921549,
      "grad_norm": 39.233978271484375,
      "learning_rate": 3e-06,
      "loss": 1.812,
      "step": 30
    },
    {
      "epoch": 0.006620324395895399,
      "grad_norm": 23.847949981689453,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.2042,
      "step": 40
    },
    {
      "epoch": 0.00827540549486925,
      "grad_norm": 29.515451431274414,
      "learning_rate": 5e-06,
      "loss": 1.0899,
      "step": 50
    },
    {
      "epoch": 0.009930486593843098,
      "grad_norm": 34.39731216430664,
      "learning_rate": 6e-06,
      "loss": 1.5913,
      "step": 60
    },
    {
      "epoch": 0.011585567692816948,
      "grad_norm": 23.16199493408203,
      "learning_rate": 7.000000000000001e-06,
      "loss": 1.2412,
      "step": 70
    },
    {
      "epoch": 0.013240648791790799,
      "grad_norm": 18.765995025634766,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.9137,
      "step": 80
    },
    {
      "epoch": 0.014895729890764648,
      "grad_norm": 47.13983154296875,
      "learning_rate": 9e-06,
      "loss": 0.9043,
      "step": 90
    },
    {
      "epoch": 0.0165508109897385,
      "grad_norm": 21.211231231689453,
      "learning_rate": 1e-05,
      "loss": 0.9752,
      "step": 100
    },
    {
      "epoch": 0.018205892088712348,
      "grad_norm": 46.94746780395508,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.0548,
      "step": 110
    },
    {
      "epoch": 0.019860973187686197,
      "grad_norm": 28.529935836791992,
      "learning_rate": 1.2e-05,
      "loss": 0.8669,
      "step": 120
    },
    {
      "epoch": 0.021516054286660046,
      "grad_norm": 26.23557472229004,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.1734,
      "step": 130
    },
    {
      "epoch": 0.023171135385633895,
      "grad_norm": 15.463247299194336,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.0461,
      "step": 140
    },
    {
      "epoch": 0.024826216484607744,
      "grad_norm": 13.037311553955078,
      "learning_rate": 1.5e-05,
      "loss": 1.0163,
      "step": 150
    },
    {
      "epoch": 0.026481297583581597,
      "grad_norm": 29.484344482421875,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.7433,
      "step": 160
    },
    {
      "epoch": 0.028136378682555446,
      "grad_norm": 27.589439392089844,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 1.0462,
      "step": 170
    },
    {
      "epoch": 0.029791459781529295,
      "grad_norm": 42.15110397338867,
      "learning_rate": 1.8e-05,
      "loss": 1.074,
      "step": 180
    },
    {
      "epoch": 0.031446540880503145,
      "grad_norm": 48.39501190185547,
      "learning_rate": 1.9e-05,
      "loss": 1.2777,
      "step": 190
    },
    {
      "epoch": 0.033101621979477,
      "grad_norm": 24.01774787902832,
      "learning_rate": 2e-05,
      "loss": 0.9772,
      "step": 200
    },
    {
      "epoch": 0.03475670307845084,
      "grad_norm": 14.437030792236328,
      "learning_rate": 2.1e-05,
      "loss": 0.9915,
      "step": 210
    },
    {
      "epoch": 0.036411784177424696,
      "grad_norm": 25.57666015625,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.8735,
      "step": 220
    },
    {
      "epoch": 0.03806686527639854,
      "grad_norm": 18.09803009033203,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.9756,
      "step": 230
    },
    {
      "epoch": 0.039721946375372394,
      "grad_norm": 27.545764923095703,
      "learning_rate": 2.4e-05,
      "loss": 1.1588,
      "step": 240
    },
    {
      "epoch": 0.04137702747434624,
      "grad_norm": 52.58906555175781,
      "learning_rate": 2.5e-05,
      "loss": 1.0716,
      "step": 250
    },
    {
      "epoch": 0.04303210857332009,
      "grad_norm": 56.39608383178711,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.3374,
      "step": 260
    },
    {
      "epoch": 0.044687189672293945,
      "grad_norm": 29.961017608642578,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.3407,
      "step": 270
    },
    {
      "epoch": 0.04634227077126779,
      "grad_norm": 28.311704635620117,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.8989,
      "step": 280
    },
    {
      "epoch": 0.04799735187024164,
      "grad_norm": 37.567115783691406,
      "learning_rate": 2.9e-05,
      "loss": 0.7976,
      "step": 290
    },
    {
      "epoch": 0.04965243296921549,
      "grad_norm": 38.55335235595703,
      "learning_rate": 3e-05,
      "loss": 1.3254,
      "step": 300
    },
    {
      "epoch": 0.05130751406818934,
      "grad_norm": 34.40132522583008,
      "learning_rate": 3.1e-05,
      "loss": 1.2579,
      "step": 310
    },
    {
      "epoch": 0.052962595167163194,
      "grad_norm": 18.60367202758789,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.6572,
      "step": 320
    },
    {
      "epoch": 0.05461767626613704,
      "grad_norm": 14.23806095123291,
      "learning_rate": 3.3e-05,
      "loss": 1.0558,
      "step": 330
    },
    {
      "epoch": 0.05627275736511089,
      "grad_norm": 23.523635864257812,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.9593,
      "step": 340
    },
    {
      "epoch": 0.05792783846408474,
      "grad_norm": 9.363369941711426,
      "learning_rate": 3.5e-05,
      "loss": 1.1994,
      "step": 350
    },
    {
      "epoch": 0.05958291956305859,
      "grad_norm": 9.844808578491211,
      "learning_rate": 3.6e-05,
      "loss": 1.0732,
      "step": 360
    },
    {
      "epoch": 0.06123800066203244,
      "grad_norm": 33.045509338378906,
      "learning_rate": 3.7e-05,
      "loss": 1.5828,
      "step": 370
    },
    {
      "epoch": 0.06289308176100629,
      "grad_norm": 19.92707633972168,
      "learning_rate": 3.8e-05,
      "loss": 1.1093,
      "step": 380
    },
    {
      "epoch": 0.06454816285998013,
      "grad_norm": 60.874629974365234,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.2993,
      "step": 390
    },
    {
      "epoch": 0.066203243958954,
      "grad_norm": 24.25271224975586,
      "learning_rate": 4e-05,
      "loss": 1.4202,
      "step": 400
    },
    {
      "epoch": 0.06785832505792784,
      "grad_norm": 25.432924270629883,
      "learning_rate": 4.1e-05,
      "loss": 1.3769,
      "step": 410
    },
    {
      "epoch": 0.06951340615690169,
      "grad_norm": 11.864958763122559,
      "learning_rate": 4.2e-05,
      "loss": 0.9763,
      "step": 420
    },
    {
      "epoch": 0.07116848725587553,
      "grad_norm": 5.082309246063232,
      "learning_rate": 4.3e-05,
      "loss": 1.0455,
      "step": 430
    },
    {
      "epoch": 0.07282356835484939,
      "grad_norm": 41.96894073486328,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.2106,
      "step": 440
    },
    {
      "epoch": 0.07447864945382324,
      "grad_norm": 25.940773010253906,
      "learning_rate": 4.5e-05,
      "loss": 1.4949,
      "step": 450
    },
    {
      "epoch": 0.07613373055279708,
      "grad_norm": 35.460819244384766,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.1292,
      "step": 460
    },
    {
      "epoch": 0.07778881165177094,
      "grad_norm": 87.54607391357422,
      "learning_rate": 4.7e-05,
      "loss": 0.7964,
      "step": 470
    },
    {
      "epoch": 0.07944389275074479,
      "grad_norm": 28.381498336791992,
      "learning_rate": 4.8e-05,
      "loss": 2.0016,
      "step": 480
    },
    {
      "epoch": 0.08109897384971863,
      "grad_norm": 41.20124816894531,
      "learning_rate": 4.9e-05,
      "loss": 1.3091,
      "step": 490
    },
    {
      "epoch": 0.08275405494869248,
      "grad_norm": 24.722789764404297,
      "learning_rate": 5e-05,
      "loss": 0.9915,
      "step": 500
    },
    {
      "epoch": 0.08440913604766634,
      "grad_norm": 56.54606246948242,
      "learning_rate": 4.99788744296096e-05,
      "loss": 0.9197,
      "step": 510
    },
    {
      "epoch": 0.08606421714664018,
      "grad_norm": 31.948501586914062,
      "learning_rate": 4.99577488592192e-05,
      "loss": 1.0886,
      "step": 520
    },
    {
      "epoch": 0.08771929824561403,
      "grad_norm": 30.25765609741211,
      "learning_rate": 4.99366232888288e-05,
      "loss": 1.3002,
      "step": 530
    },
    {
      "epoch": 0.08937437934458789,
      "grad_norm": 31.99591064453125,
      "learning_rate": 4.9915497718438396e-05,
      "loss": 1.2007,
      "step": 540
    },
    {
      "epoch": 0.09102946044356174,
      "grad_norm": 46.48484420776367,
      "learning_rate": 4.9894372148048e-05,
      "loss": 1.255,
      "step": 550
    },
    {
      "epoch": 0.09268454154253558,
      "grad_norm": 20.318845748901367,
      "learning_rate": 4.98732465776576e-05,
      "loss": 1.2243,
      "step": 560
    },
    {
      "epoch": 0.09433962264150944,
      "grad_norm": 21.3839168548584,
      "learning_rate": 4.98521210072672e-05,
      "loss": 1.344,
      "step": 570
    },
    {
      "epoch": 0.09599470374048329,
      "grad_norm": 47.355587005615234,
      "learning_rate": 4.9830995436876796e-05,
      "loss": 1.1516,
      "step": 580
    },
    {
      "epoch": 0.09764978483945713,
      "grad_norm": 28.223979949951172,
      "learning_rate": 4.9809869866486395e-05,
      "loss": 1.476,
      "step": 590
    },
    {
      "epoch": 0.09930486593843098,
      "grad_norm": 21.842832565307617,
      "learning_rate": 4.978874429609599e-05,
      "loss": 1.3641,
      "step": 600
    },
    {
      "epoch": 0.10095994703740484,
      "grad_norm": 29.433805465698242,
      "learning_rate": 4.97676187257056e-05,
      "loss": 1.4292,
      "step": 610
    },
    {
      "epoch": 0.10261502813637868,
      "grad_norm": 19.050249099731445,
      "learning_rate": 4.9746493155315197e-05,
      "loss": 1.0075,
      "step": 620
    },
    {
      "epoch": 0.10427010923535253,
      "grad_norm": 18.47532081604004,
      "learning_rate": 4.9725367584924795e-05,
      "loss": 1.3317,
      "step": 630
    },
    {
      "epoch": 0.10592519033432639,
      "grad_norm": 32.689239501953125,
      "learning_rate": 4.97042420145344e-05,
      "loss": 1.5647,
      "step": 640
    },
    {
      "epoch": 0.10758027143330023,
      "grad_norm": 35.68722152709961,
      "learning_rate": 4.9683116444144e-05,
      "loss": 1.35,
      "step": 650
    },
    {
      "epoch": 0.10923535253227408,
      "grad_norm": 42.5062141418457,
      "learning_rate": 4.96619908737536e-05,
      "loss": 1.2025,
      "step": 660
    },
    {
      "epoch": 0.11089043363124793,
      "grad_norm": 33.371925354003906,
      "learning_rate": 4.9640865303363195e-05,
      "loss": 1.2829,
      "step": 670
    },
    {
      "epoch": 0.11254551473022179,
      "grad_norm": 28.724153518676758,
      "learning_rate": 4.9619739732972794e-05,
      "loss": 1.1248,
      "step": 680
    },
    {
      "epoch": 0.11420059582919563,
      "grad_norm": 25.75612449645996,
      "learning_rate": 4.959861416258239e-05,
      "loss": 1.2202,
      "step": 690
    },
    {
      "epoch": 0.11585567692816948,
      "grad_norm": 35.77614212036133,
      "learning_rate": 4.957748859219199e-05,
      "loss": 1.1842,
      "step": 700
    },
    {
      "epoch": 0.11751075802714334,
      "grad_norm": 17.455219268798828,
      "learning_rate": 4.955636302180159e-05,
      "loss": 1.1268,
      "step": 710
    },
    {
      "epoch": 0.11916583912611718,
      "grad_norm": 47.575679779052734,
      "learning_rate": 4.9535237451411194e-05,
      "loss": 1.2153,
      "step": 720
    },
    {
      "epoch": 0.12082092022509103,
      "grad_norm": 23.51940155029297,
      "learning_rate": 4.951411188102079e-05,
      "loss": 1.5433,
      "step": 730
    },
    {
      "epoch": 0.12247600132406487,
      "grad_norm": 94.35320281982422,
      "learning_rate": 4.949298631063039e-05,
      "loss": 1.1476,
      "step": 740
    },
    {
      "epoch": 0.12413108242303873,
      "grad_norm": 21.964601516723633,
      "learning_rate": 4.947186074023999e-05,
      "loss": 1.1921,
      "step": 750
    },
    {
      "epoch": 0.12578616352201258,
      "grad_norm": 28.743871688842773,
      "learning_rate": 4.945073516984959e-05,
      "loss": 0.9337,
      "step": 760
    },
    {
      "epoch": 0.12744124462098644,
      "grad_norm": 29.54463005065918,
      "learning_rate": 4.9429609599459186e-05,
      "loss": 0.9638,
      "step": 770
    },
    {
      "epoch": 0.12909632571996027,
      "grad_norm": 35.154693603515625,
      "learning_rate": 4.9408484029068784e-05,
      "loss": 1.2569,
      "step": 780
    },
    {
      "epoch": 0.13075140681893413,
      "grad_norm": 24.85716438293457,
      "learning_rate": 4.938735845867838e-05,
      "loss": 1.4266,
      "step": 790
    },
    {
      "epoch": 0.132406487917908,
      "grad_norm": 62.14929962158203,
      "learning_rate": 4.936623288828799e-05,
      "loss": 1.2727,
      "step": 800
    },
    {
      "epoch": 0.13406156901688182,
      "grad_norm": 23.520668029785156,
      "learning_rate": 4.9345107317897586e-05,
      "loss": 1.2614,
      "step": 810
    },
    {
      "epoch": 0.13571665011585568,
      "grad_norm": 46.774497985839844,
      "learning_rate": 4.9323981747507185e-05,
      "loss": 1.1228,
      "step": 820
    },
    {
      "epoch": 0.13737173121482954,
      "grad_norm": 25.957347869873047,
      "learning_rate": 4.930285617711678e-05,
      "loss": 0.889,
      "step": 830
    },
    {
      "epoch": 0.13902681231380337,
      "grad_norm": 14.679350852966309,
      "learning_rate": 4.928173060672638e-05,
      "loss": 1.0498,
      "step": 840
    },
    {
      "epoch": 0.14068189341277723,
      "grad_norm": 56.847618103027344,
      "learning_rate": 4.926060503633598e-05,
      "loss": 1.0435,
      "step": 850
    },
    {
      "epoch": 0.14233697451175106,
      "grad_norm": 24.731678009033203,
      "learning_rate": 4.9239479465945585e-05,
      "loss": 1.283,
      "step": 860
    },
    {
      "epoch": 0.14399205561072492,
      "grad_norm": 55.34298324584961,
      "learning_rate": 4.921835389555518e-05,
      "loss": 1.5597,
      "step": 870
    },
    {
      "epoch": 0.14564713670969878,
      "grad_norm": 24.375551223754883,
      "learning_rate": 4.919722832516478e-05,
      "loss": 0.9162,
      "step": 880
    },
    {
      "epoch": 0.14730221780867261,
      "grad_norm": 22.159921646118164,
      "learning_rate": 4.917610275477439e-05,
      "loss": 1.4326,
      "step": 890
    },
    {
      "epoch": 0.14895729890764647,
      "grad_norm": 31.34564208984375,
      "learning_rate": 4.9154977184383985e-05,
      "loss": 1.2828,
      "step": 900
    },
    {
      "epoch": 0.15061238000662033,
      "grad_norm": 25.409488677978516,
      "learning_rate": 4.9133851613993584e-05,
      "loss": 1.0544,
      "step": 910
    },
    {
      "epoch": 0.15226746110559417,
      "grad_norm": 36.53563690185547,
      "learning_rate": 4.911272604360318e-05,
      "loss": 1.1645,
      "step": 920
    },
    {
      "epoch": 0.15392254220456802,
      "grad_norm": 29.02924919128418,
      "learning_rate": 4.909160047321278e-05,
      "loss": 1.2797,
      "step": 930
    },
    {
      "epoch": 0.15557762330354188,
      "grad_norm": 62.60102462768555,
      "learning_rate": 4.907047490282238e-05,
      "loss": 1.2179,
      "step": 940
    },
    {
      "epoch": 0.15723270440251572,
      "grad_norm": 63.68646240234375,
      "learning_rate": 4.904934933243198e-05,
      "loss": 0.8934,
      "step": 950
    },
    {
      "epoch": 0.15888778550148958,
      "grad_norm": 33.501556396484375,
      "learning_rate": 4.9028223762041575e-05,
      "loss": 1.7829,
      "step": 960
    },
    {
      "epoch": 0.16054286660046344,
      "grad_norm": 33.05158996582031,
      "learning_rate": 4.900709819165118e-05,
      "loss": 1.0657,
      "step": 970
    },
    {
      "epoch": 0.16219794769943727,
      "grad_norm": 61.646644592285156,
      "learning_rate": 4.898597262126078e-05,
      "loss": 1.532,
      "step": 980
    },
    {
      "epoch": 0.16385302879841113,
      "grad_norm": 35.512630462646484,
      "learning_rate": 4.896484705087038e-05,
      "loss": 1.3196,
      "step": 990
    },
    {
      "epoch": 0.16550810989738496,
      "grad_norm": 31.215553283691406,
      "learning_rate": 4.8943721480479976e-05,
      "loss": 1.2177,
      "step": 1000
    },
    {
      "epoch": 0.16716319099635882,
      "grad_norm": 56.37748718261719,
      "learning_rate": 4.8922595910089574e-05,
      "loss": 1.1638,
      "step": 1010
    },
    {
      "epoch": 0.16881827209533268,
      "grad_norm": 19.931238174438477,
      "learning_rate": 4.890147033969917e-05,
      "loss": 1.2221,
      "step": 1020
    },
    {
      "epoch": 0.1704733531943065,
      "grad_norm": 16.868911743164062,
      "learning_rate": 4.888034476930877e-05,
      "loss": 1.2528,
      "step": 1030
    },
    {
      "epoch": 0.17212843429328037,
      "grad_norm": 36.89094161987305,
      "learning_rate": 4.885921919891837e-05,
      "loss": 1.0083,
      "step": 1040
    },
    {
      "epoch": 0.17378351539225423,
      "grad_norm": 36.20716094970703,
      "learning_rate": 4.883809362852797e-05,
      "loss": 1.3969,
      "step": 1050
    },
    {
      "epoch": 0.17543859649122806,
      "grad_norm": 42.881309509277344,
      "learning_rate": 4.881696805813757e-05,
      "loss": 1.242,
      "step": 1060
    },
    {
      "epoch": 0.17709367759020192,
      "grad_norm": 17.409669876098633,
      "learning_rate": 4.879584248774717e-05,
      "loss": 1.4547,
      "step": 1070
    },
    {
      "epoch": 0.17874875868917578,
      "grad_norm": 34.6375617980957,
      "learning_rate": 4.877471691735677e-05,
      "loss": 1.3008,
      "step": 1080
    },
    {
      "epoch": 0.1804038397881496,
      "grad_norm": 64.32508087158203,
      "learning_rate": 4.875359134696637e-05,
      "loss": 1.2705,
      "step": 1090
    },
    {
      "epoch": 0.18205892088712347,
      "grad_norm": 18.446256637573242,
      "learning_rate": 4.8732465776575966e-05,
      "loss": 0.9743,
      "step": 1100
    },
    {
      "epoch": 0.18371400198609733,
      "grad_norm": 38.48124694824219,
      "learning_rate": 4.871134020618557e-05,
      "loss": 1.361,
      "step": 1110
    },
    {
      "epoch": 0.18536908308507116,
      "grad_norm": 31.099225997924805,
      "learning_rate": 4.869021463579517e-05,
      "loss": 0.8821,
      "step": 1120
    },
    {
      "epoch": 0.18702416418404502,
      "grad_norm": 30.995332717895508,
      "learning_rate": 4.866908906540477e-05,
      "loss": 1.2235,
      "step": 1130
    },
    {
      "epoch": 0.18867924528301888,
      "grad_norm": 33.05385971069336,
      "learning_rate": 4.8647963495014373e-05,
      "loss": 1.2368,
      "step": 1140
    },
    {
      "epoch": 0.1903343263819927,
      "grad_norm": 34.02387237548828,
      "learning_rate": 4.862683792462397e-05,
      "loss": 0.6727,
      "step": 1150
    },
    {
      "epoch": 0.19198940748096657,
      "grad_norm": 26.04374885559082,
      "learning_rate": 4.860571235423357e-05,
      "loss": 1.1808,
      "step": 1160
    },
    {
      "epoch": 0.1936444885799404,
      "grad_norm": 19.49028968811035,
      "learning_rate": 4.858458678384317e-05,
      "loss": 1.4659,
      "step": 1170
    },
    {
      "epoch": 0.19529956967891426,
      "grad_norm": 50.169246673583984,
      "learning_rate": 4.856346121345277e-05,
      "loss": 0.9053,
      "step": 1180
    },
    {
      "epoch": 0.19695465077788812,
      "grad_norm": 16.62969398498535,
      "learning_rate": 4.8542335643062365e-05,
      "loss": 1.3352,
      "step": 1190
    },
    {
      "epoch": 0.19860973187686196,
      "grad_norm": 78.2732925415039,
      "learning_rate": 4.8521210072671964e-05,
      "loss": 1.2553,
      "step": 1200
    },
    {
      "epoch": 0.20026481297583582,
      "grad_norm": 32.511512756347656,
      "learning_rate": 4.850008450228156e-05,
      "loss": 1.1074,
      "step": 1210
    },
    {
      "epoch": 0.20191989407480967,
      "grad_norm": 43.47333526611328,
      "learning_rate": 4.847895893189116e-05,
      "loss": 1.3581,
      "step": 1220
    },
    {
      "epoch": 0.2035749751737835,
      "grad_norm": 34.47276306152344,
      "learning_rate": 4.8457833361500766e-05,
      "loss": 1.1513,
      "step": 1230
    },
    {
      "epoch": 0.20523005627275737,
      "grad_norm": 33.01162338256836,
      "learning_rate": 4.8436707791110364e-05,
      "loss": 0.8879,
      "step": 1240
    },
    {
      "epoch": 0.20688513737173123,
      "grad_norm": 28.59977912902832,
      "learning_rate": 4.841558222071996e-05,
      "loss": 1.0483,
      "step": 1250
    },
    {
      "epoch": 0.20854021847070506,
      "grad_norm": 36.533111572265625,
      "learning_rate": 4.839445665032956e-05,
      "loss": 0.9967,
      "step": 1260
    },
    {
      "epoch": 0.21019529956967892,
      "grad_norm": 33.853511810302734,
      "learning_rate": 4.837333107993916e-05,
      "loss": 1.3891,
      "step": 1270
    },
    {
      "epoch": 0.21185038066865278,
      "grad_norm": 22.53427505493164,
      "learning_rate": 4.835220550954876e-05,
      "loss": 0.9819,
      "step": 1280
    },
    {
      "epoch": 0.2135054617676266,
      "grad_norm": 36.87285232543945,
      "learning_rate": 4.8331079939158356e-05,
      "loss": 1.109,
      "step": 1290
    },
    {
      "epoch": 0.21516054286660047,
      "grad_norm": 36.37614059448242,
      "learning_rate": 4.8309954368767954e-05,
      "loss": 1.2255,
      "step": 1300
    },
    {
      "epoch": 0.2168156239655743,
      "grad_norm": 67.95569610595703,
      "learning_rate": 4.828882879837756e-05,
      "loss": 1.0054,
      "step": 1310
    },
    {
      "epoch": 0.21847070506454816,
      "grad_norm": 37.321712493896484,
      "learning_rate": 4.826770322798716e-05,
      "loss": 1.4852,
      "step": 1320
    },
    {
      "epoch": 0.22012578616352202,
      "grad_norm": 35.06303787231445,
      "learning_rate": 4.8246577657596756e-05,
      "loss": 1.3332,
      "step": 1330
    },
    {
      "epoch": 0.22178086726249585,
      "grad_norm": 39.33171463012695,
      "learning_rate": 4.8225452087206355e-05,
      "loss": 0.6825,
      "step": 1340
    },
    {
      "epoch": 0.2234359483614697,
      "grad_norm": 41.969093322753906,
      "learning_rate": 4.820432651681595e-05,
      "loss": 1.1036,
      "step": 1350
    },
    {
      "epoch": 0.22509102946044357,
      "grad_norm": 35.6048583984375,
      "learning_rate": 4.818320094642556e-05,
      "loss": 1.0796,
      "step": 1360
    },
    {
      "epoch": 0.2267461105594174,
      "grad_norm": 35.25517272949219,
      "learning_rate": 4.816207537603516e-05,
      "loss": 1.7824,
      "step": 1370
    },
    {
      "epoch": 0.22840119165839126,
      "grad_norm": 42.24971008300781,
      "learning_rate": 4.8140949805644755e-05,
      "loss": 1.1511,
      "step": 1380
    },
    {
      "epoch": 0.23005627275736512,
      "grad_norm": 24.057832717895508,
      "learning_rate": 4.811982423525435e-05,
      "loss": 0.995,
      "step": 1390
    },
    {
      "epoch": 0.23171135385633895,
      "grad_norm": 54.834373474121094,
      "learning_rate": 4.809869866486396e-05,
      "loss": 1.1079,
      "step": 1400
    },
    {
      "epoch": 0.2333664349553128,
      "grad_norm": 54.928592681884766,
      "learning_rate": 4.807757309447356e-05,
      "loss": 1.5696,
      "step": 1410
    },
    {
      "epoch": 0.23502151605428667,
      "grad_norm": 44.82935333251953,
      "learning_rate": 4.8056447524083155e-05,
      "loss": 1.3746,
      "step": 1420
    },
    {
      "epoch": 0.2366765971532605,
      "grad_norm": 21.303272247314453,
      "learning_rate": 4.8035321953692754e-05,
      "loss": 0.9306,
      "step": 1430
    },
    {
      "epoch": 0.23833167825223436,
      "grad_norm": 80.04881286621094,
      "learning_rate": 4.801419638330235e-05,
      "loss": 1.08,
      "step": 1440
    },
    {
      "epoch": 0.2399867593512082,
      "grad_norm": 46.226810455322266,
      "learning_rate": 4.799307081291195e-05,
      "loss": 0.9101,
      "step": 1450
    },
    {
      "epoch": 0.24164184045018205,
      "grad_norm": 28.761930465698242,
      "learning_rate": 4.797194524252155e-05,
      "loss": 1.173,
      "step": 1460
    },
    {
      "epoch": 0.24329692154915591,
      "grad_norm": 39.339866638183594,
      "learning_rate": 4.795081967213115e-05,
      "loss": 1.2661,
      "step": 1470
    },
    {
      "epoch": 0.24495200264812975,
      "grad_norm": 24.74656867980957,
      "learning_rate": 4.792969410174075e-05,
      "loss": 0.9082,
      "step": 1480
    },
    {
      "epoch": 0.2466070837471036,
      "grad_norm": 50.91769027709961,
      "learning_rate": 4.790856853135035e-05,
      "loss": 1.4674,
      "step": 1490
    },
    {
      "epoch": 0.24826216484607747,
      "grad_norm": 18.758394241333008,
      "learning_rate": 4.788744296095995e-05,
      "loss": 1.17,
      "step": 1500
    },
    {
      "epoch": 0.2499172459450513,
      "grad_norm": 39.55379867553711,
      "learning_rate": 4.786631739056955e-05,
      "loss": 0.9029,
      "step": 1510
    },
    {
      "epoch": 0.25157232704402516,
      "grad_norm": 561.3009033203125,
      "learning_rate": 4.7845191820179146e-05,
      "loss": 1.4542,
      "step": 1520
    },
    {
      "epoch": 0.253227408142999,
      "grad_norm": 38.76213073730469,
      "learning_rate": 4.7824066249788744e-05,
      "loss": 1.1372,
      "step": 1530
    },
    {
      "epoch": 0.2548824892419729,
      "grad_norm": 41.69278335571289,
      "learning_rate": 4.780294067939834e-05,
      "loss": 0.9507,
      "step": 1540
    },
    {
      "epoch": 0.2565375703409467,
      "grad_norm": 22.300031661987305,
      "learning_rate": 4.778181510900794e-05,
      "loss": 1.0155,
      "step": 1550
    },
    {
      "epoch": 0.25819265143992054,
      "grad_norm": 36.31016540527344,
      "learning_rate": 4.776068953861754e-05,
      "loss": 0.9954,
      "step": 1560
    },
    {
      "epoch": 0.2598477325388944,
      "grad_norm": 40.576377868652344,
      "learning_rate": 4.7739563968227145e-05,
      "loss": 1.0021,
      "step": 1570
    },
    {
      "epoch": 0.26150281363786826,
      "grad_norm": 21.96670150756836,
      "learning_rate": 4.771843839783674e-05,
      "loss": 0.9412,
      "step": 1580
    },
    {
      "epoch": 0.2631578947368421,
      "grad_norm": 30.909685134887695,
      "learning_rate": 4.769731282744634e-05,
      "loss": 1.214,
      "step": 1590
    },
    {
      "epoch": 0.264812975835816,
      "grad_norm": 87.26956176757812,
      "learning_rate": 4.767618725705594e-05,
      "loss": 1.2862,
      "step": 1600
    },
    {
      "epoch": 0.2664680569347898,
      "grad_norm": 31.377893447875977,
      "learning_rate": 4.7655061686665545e-05,
      "loss": 0.9352,
      "step": 1610
    },
    {
      "epoch": 0.26812313803376364,
      "grad_norm": 35.796905517578125,
      "learning_rate": 4.763393611627514e-05,
      "loss": 1.5504,
      "step": 1620
    },
    {
      "epoch": 0.26977821913273753,
      "grad_norm": 20.72470474243164,
      "learning_rate": 4.761281054588474e-05,
      "loss": 0.9967,
      "step": 1630
    },
    {
      "epoch": 0.27143330023171136,
      "grad_norm": 35.54875183105469,
      "learning_rate": 4.759168497549434e-05,
      "loss": 1.5371,
      "step": 1640
    },
    {
      "epoch": 0.2730883813306852,
      "grad_norm": 32.84843444824219,
      "learning_rate": 4.7570559405103945e-05,
      "loss": 1.1145,
      "step": 1650
    },
    {
      "epoch": 0.2747434624296591,
      "grad_norm": 45.730796813964844,
      "learning_rate": 4.7549433834713544e-05,
      "loss": 1.0641,
      "step": 1660
    },
    {
      "epoch": 0.2763985435286329,
      "grad_norm": 38.7393798828125,
      "learning_rate": 4.752830826432314e-05,
      "loss": 1.1494,
      "step": 1670
    },
    {
      "epoch": 0.27805362462760674,
      "grad_norm": 30.51072120666504,
      "learning_rate": 4.750718269393274e-05,
      "loss": 0.9603,
      "step": 1680
    },
    {
      "epoch": 0.2797087057265806,
      "grad_norm": 91.09495544433594,
      "learning_rate": 4.748605712354234e-05,
      "loss": 0.9148,
      "step": 1690
    },
    {
      "epoch": 0.28136378682555446,
      "grad_norm": 83.44070434570312,
      "learning_rate": 4.746493155315194e-05,
      "loss": 1.1881,
      "step": 1700
    },
    {
      "epoch": 0.2830188679245283,
      "grad_norm": 59.126644134521484,
      "learning_rate": 4.7443805982761536e-05,
      "loss": 1.4904,
      "step": 1710
    },
    {
      "epoch": 0.2846739490235021,
      "grad_norm": 25.300601959228516,
      "learning_rate": 4.7422680412371134e-05,
      "loss": 1.1335,
      "step": 1720
    },
    {
      "epoch": 0.286329030122476,
      "grad_norm": 54.789833068847656,
      "learning_rate": 4.740155484198073e-05,
      "loss": 1.1538,
      "step": 1730
    },
    {
      "epoch": 0.28798411122144985,
      "grad_norm": 26.60367774963379,
      "learning_rate": 4.738042927159034e-05,
      "loss": 1.325,
      "step": 1740
    },
    {
      "epoch": 0.2896391923204237,
      "grad_norm": 62.403011322021484,
      "learning_rate": 4.7359303701199936e-05,
      "loss": 1.2374,
      "step": 1750
    },
    {
      "epoch": 0.29129427341939756,
      "grad_norm": 32.99116134643555,
      "learning_rate": 4.7338178130809534e-05,
      "loss": 1.028,
      "step": 1760
    },
    {
      "epoch": 0.2929493545183714,
      "grad_norm": 33.20037841796875,
      "learning_rate": 4.731705256041913e-05,
      "loss": 1.4632,
      "step": 1770
    },
    {
      "epoch": 0.29460443561734523,
      "grad_norm": 38.19801712036133,
      "learning_rate": 4.729592699002873e-05,
      "loss": 1.1601,
      "step": 1780
    },
    {
      "epoch": 0.2962595167163191,
      "grad_norm": 15.958377838134766,
      "learning_rate": 4.727480141963833e-05,
      "loss": 1.0955,
      "step": 1790
    },
    {
      "epoch": 0.29791459781529295,
      "grad_norm": 57.33251190185547,
      "learning_rate": 4.725367584924793e-05,
      "loss": 1.0043,
      "step": 1800
    },
    {
      "epoch": 0.2995696789142668,
      "grad_norm": 25.37035369873047,
      "learning_rate": 4.7232550278857526e-05,
      "loss": 0.955,
      "step": 1810
    },
    {
      "epoch": 0.30122476001324067,
      "grad_norm": 43.182395935058594,
      "learning_rate": 4.721142470846713e-05,
      "loss": 1.4133,
      "step": 1820
    },
    {
      "epoch": 0.3028798411122145,
      "grad_norm": 24.475969314575195,
      "learning_rate": 4.719029913807673e-05,
      "loss": 0.9858,
      "step": 1830
    },
    {
      "epoch": 0.30453492221118833,
      "grad_norm": 23.023963928222656,
      "learning_rate": 4.716917356768633e-05,
      "loss": 0.8741,
      "step": 1840
    },
    {
      "epoch": 0.3061900033101622,
      "grad_norm": 20.56555938720703,
      "learning_rate": 4.714804799729593e-05,
      "loss": 0.9324,
      "step": 1850
    },
    {
      "epoch": 0.30784508440913605,
      "grad_norm": 23.651470184326172,
      "learning_rate": 4.712692242690553e-05,
      "loss": 1.3162,
      "step": 1860
    },
    {
      "epoch": 0.3095001655081099,
      "grad_norm": 25.373672485351562,
      "learning_rate": 4.710579685651513e-05,
      "loss": 1.1498,
      "step": 1870
    },
    {
      "epoch": 0.31115524660708377,
      "grad_norm": 41.98109436035156,
      "learning_rate": 4.708467128612473e-05,
      "loss": 0.9848,
      "step": 1880
    },
    {
      "epoch": 0.3128103277060576,
      "grad_norm": 21.518543243408203,
      "learning_rate": 4.706354571573433e-05,
      "loss": 1.3222,
      "step": 1890
    },
    {
      "epoch": 0.31446540880503143,
      "grad_norm": 33.17843246459961,
      "learning_rate": 4.7042420145343925e-05,
      "loss": 1.106,
      "step": 1900
    },
    {
      "epoch": 0.3161204899040053,
      "grad_norm": 61.57261657714844,
      "learning_rate": 4.702129457495353e-05,
      "loss": 1.1968,
      "step": 1910
    },
    {
      "epoch": 0.31777557100297915,
      "grad_norm": 48.944618225097656,
      "learning_rate": 4.700016900456313e-05,
      "loss": 1.0751,
      "step": 1920
    },
    {
      "epoch": 0.319430652101953,
      "grad_norm": 20.99079704284668,
      "learning_rate": 4.697904343417273e-05,
      "loss": 1.2013,
      "step": 1930
    },
    {
      "epoch": 0.32108573320092687,
      "grad_norm": 38.452030181884766,
      "learning_rate": 4.6957917863782325e-05,
      "loss": 1.3166,
      "step": 1940
    },
    {
      "epoch": 0.3227408142999007,
      "grad_norm": 21.348737716674805,
      "learning_rate": 4.6936792293391924e-05,
      "loss": 0.9313,
      "step": 1950
    },
    {
      "epoch": 0.32439589539887453,
      "grad_norm": 68.90487670898438,
      "learning_rate": 4.691566672300152e-05,
      "loss": 0.9405,
      "step": 1960
    },
    {
      "epoch": 0.3260509764978484,
      "grad_norm": 22.760604858398438,
      "learning_rate": 4.689454115261112e-05,
      "loss": 0.9389,
      "step": 1970
    },
    {
      "epoch": 0.32770605759682225,
      "grad_norm": 35.51161575317383,
      "learning_rate": 4.687341558222072e-05,
      "loss": 0.9112,
      "step": 1980
    },
    {
      "epoch": 0.3293611386957961,
      "grad_norm": 24.886987686157227,
      "learning_rate": 4.6852290011830324e-05,
      "loss": 0.7875,
      "step": 1990
    },
    {
      "epoch": 0.3310162197947699,
      "grad_norm": 62.01565170288086,
      "learning_rate": 4.683116444143992e-05,
      "loss": 1.2205,
      "step": 2000
    },
    {
      "epoch": 0.3326713008937438,
      "grad_norm": 23.44072914123535,
      "learning_rate": 4.681003887104952e-05,
      "loss": 1.211,
      "step": 2010
    },
    {
      "epoch": 0.33432638199271764,
      "grad_norm": 9.802149772644043,
      "learning_rate": 4.678891330065912e-05,
      "loss": 1.18,
      "step": 2020
    },
    {
      "epoch": 0.33598146309169147,
      "grad_norm": 44.754112243652344,
      "learning_rate": 4.676778773026872e-05,
      "loss": 1.1424,
      "step": 2030
    },
    {
      "epoch": 0.33763654419066536,
      "grad_norm": 34.290706634521484,
      "learning_rate": 4.6746662159878316e-05,
      "loss": 1.4043,
      "step": 2040
    },
    {
      "epoch": 0.3392916252896392,
      "grad_norm": 24.2398681640625,
      "learning_rate": 4.6725536589487914e-05,
      "loss": 1.1603,
      "step": 2050
    },
    {
      "epoch": 0.340946706388613,
      "grad_norm": 45.33232879638672,
      "learning_rate": 4.670441101909751e-05,
      "loss": 1.542,
      "step": 2060
    },
    {
      "epoch": 0.3426017874875869,
      "grad_norm": 36.0558967590332,
      "learning_rate": 4.668328544870712e-05,
      "loss": 1.2271,
      "step": 2070
    },
    {
      "epoch": 0.34425686858656074,
      "grad_norm": 27.28056526184082,
      "learning_rate": 4.6662159878316716e-05,
      "loss": 1.2655,
      "step": 2080
    },
    {
      "epoch": 0.34591194968553457,
      "grad_norm": 38.7779655456543,
      "learning_rate": 4.6641034307926315e-05,
      "loss": 1.2294,
      "step": 2090
    },
    {
      "epoch": 0.34756703078450846,
      "grad_norm": 29.74987030029297,
      "learning_rate": 4.661990873753592e-05,
      "loss": 1.083,
      "step": 2100
    },
    {
      "epoch": 0.3492221118834823,
      "grad_norm": 43.002567291259766,
      "learning_rate": 4.659878316714552e-05,
      "loss": 1.0787,
      "step": 2110
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 41.27775955200195,
      "learning_rate": 4.657765759675512e-05,
      "loss": 0.7599,
      "step": 2120
    },
    {
      "epoch": 0.35253227408143,
      "grad_norm": 46.78726577758789,
      "learning_rate": 4.6556532026364715e-05,
      "loss": 1.1942,
      "step": 2130
    },
    {
      "epoch": 0.35418735518040384,
      "grad_norm": 36.71954345703125,
      "learning_rate": 4.6535406455974313e-05,
      "loss": 1.142,
      "step": 2140
    },
    {
      "epoch": 0.35584243627937767,
      "grad_norm": 35.49175262451172,
      "learning_rate": 4.651428088558391e-05,
      "loss": 1.1379,
      "step": 2150
    },
    {
      "epoch": 0.35749751737835156,
      "grad_norm": 25.779951095581055,
      "learning_rate": 4.649315531519352e-05,
      "loss": 1.365,
      "step": 2160
    },
    {
      "epoch": 0.3591525984773254,
      "grad_norm": 20.47720718383789,
      "learning_rate": 4.6472029744803115e-05,
      "loss": 1.1712,
      "step": 2170
    },
    {
      "epoch": 0.3608076795762992,
      "grad_norm": 21.131296157836914,
      "learning_rate": 4.6450904174412714e-05,
      "loss": 1.1084,
      "step": 2180
    },
    {
      "epoch": 0.3624627606752731,
      "grad_norm": 41.252567291259766,
      "learning_rate": 4.642977860402231e-05,
      "loss": 1.4163,
      "step": 2190
    },
    {
      "epoch": 0.36411784177424694,
      "grad_norm": 10.99903678894043,
      "learning_rate": 4.640865303363191e-05,
      "loss": 1.035,
      "step": 2200
    },
    {
      "epoch": 0.3657729228732208,
      "grad_norm": 51.228939056396484,
      "learning_rate": 4.638752746324151e-05,
      "loss": 1.2059,
      "step": 2210
    },
    {
      "epoch": 0.36742800397219466,
      "grad_norm": 50.516212463378906,
      "learning_rate": 4.636640189285111e-05,
      "loss": 1.0733,
      "step": 2220
    },
    {
      "epoch": 0.3690830850711685,
      "grad_norm": 67.74632263183594,
      "learning_rate": 4.6345276322460706e-05,
      "loss": 1.1689,
      "step": 2230
    },
    {
      "epoch": 0.3707381661701423,
      "grad_norm": 13.113480567932129,
      "learning_rate": 4.6324150752070304e-05,
      "loss": 1.0891,
      "step": 2240
    },
    {
      "epoch": 0.3723932472691162,
      "grad_norm": 29.59139633178711,
      "learning_rate": 4.630302518167991e-05,
      "loss": 1.3768,
      "step": 2250
    },
    {
      "epoch": 0.37404832836809004,
      "grad_norm": 24.58415412902832,
      "learning_rate": 4.628189961128951e-05,
      "loss": 1.043,
      "step": 2260
    },
    {
      "epoch": 0.3757034094670639,
      "grad_norm": 59.171051025390625,
      "learning_rate": 4.6260774040899106e-05,
      "loss": 0.9017,
      "step": 2270
    },
    {
      "epoch": 0.37735849056603776,
      "grad_norm": 30.52002716064453,
      "learning_rate": 4.6239648470508704e-05,
      "loss": 0.8053,
      "step": 2280
    },
    {
      "epoch": 0.3790135716650116,
      "grad_norm": 57.44437789916992,
      "learning_rate": 4.62185229001183e-05,
      "loss": 1.3044,
      "step": 2290
    },
    {
      "epoch": 0.3806686527639854,
      "grad_norm": 23.296039581298828,
      "learning_rate": 4.61973973297279e-05,
      "loss": 1.1087,
      "step": 2300
    },
    {
      "epoch": 0.38232373386295926,
      "grad_norm": 24.924442291259766,
      "learning_rate": 4.61762717593375e-05,
      "loss": 1.1995,
      "step": 2310
    },
    {
      "epoch": 0.38397881496193315,
      "grad_norm": 21.755882263183594,
      "learning_rate": 4.6155146188947105e-05,
      "loss": 0.9234,
      "step": 2320
    },
    {
      "epoch": 0.385633896060907,
      "grad_norm": 27.6358699798584,
      "learning_rate": 4.61340206185567e-05,
      "loss": 0.8497,
      "step": 2330
    },
    {
      "epoch": 0.3872889771598808,
      "grad_norm": 29.488656997680664,
      "learning_rate": 4.61128950481663e-05,
      "loss": 1.1556,
      "step": 2340
    },
    {
      "epoch": 0.3889440582588547,
      "grad_norm": 49.62392807006836,
      "learning_rate": 4.6091769477775907e-05,
      "loss": 0.8251,
      "step": 2350
    },
    {
      "epoch": 0.39059913935782853,
      "grad_norm": 26.177961349487305,
      "learning_rate": 4.6070643907385505e-05,
      "loss": 1.1338,
      "step": 2360
    },
    {
      "epoch": 0.39225422045680236,
      "grad_norm": 65.85770416259766,
      "learning_rate": 4.60495183369951e-05,
      "loss": 0.8249,
      "step": 2370
    },
    {
      "epoch": 0.39390930155577625,
      "grad_norm": 47.94070816040039,
      "learning_rate": 4.60283927666047e-05,
      "loss": 1.1611,
      "step": 2380
    },
    {
      "epoch": 0.3955643826547501,
      "grad_norm": 51.22091293334961,
      "learning_rate": 4.60072671962143e-05,
      "loss": 1.2501,
      "step": 2390
    },
    {
      "epoch": 0.3972194637537239,
      "grad_norm": 18.842723846435547,
      "learning_rate": 4.59861416258239e-05,
      "loss": 0.9628,
      "step": 2400
    },
    {
      "epoch": 0.3988745448526978,
      "grad_norm": 15.059087753295898,
      "learning_rate": 4.59650160554335e-05,
      "loss": 0.7939,
      "step": 2410
    },
    {
      "epoch": 0.40052962595167163,
      "grad_norm": 15.742000579833984,
      "learning_rate": 4.59438904850431e-05,
      "loss": 0.8103,
      "step": 2420
    },
    {
      "epoch": 0.40218470705064546,
      "grad_norm": 33.04116439819336,
      "learning_rate": 4.59227649146527e-05,
      "loss": 0.997,
      "step": 2430
    },
    {
      "epoch": 0.40383978814961935,
      "grad_norm": 28.583202362060547,
      "learning_rate": 4.59016393442623e-05,
      "loss": 0.883,
      "step": 2440
    },
    {
      "epoch": 0.4054948692485932,
      "grad_norm": 18.244342803955078,
      "learning_rate": 4.58805137738719e-05,
      "loss": 0.8775,
      "step": 2450
    },
    {
      "epoch": 0.407149950347567,
      "grad_norm": 53.03293228149414,
      "learning_rate": 4.5859388203481496e-05,
      "loss": 1.2301,
      "step": 2460
    },
    {
      "epoch": 0.4088050314465409,
      "grad_norm": 15.279807090759277,
      "learning_rate": 4.5838262633091094e-05,
      "loss": 0.7448,
      "step": 2470
    },
    {
      "epoch": 0.41046011254551473,
      "grad_norm": 16.459911346435547,
      "learning_rate": 4.581713706270069e-05,
      "loss": 0.852,
      "step": 2480
    },
    {
      "epoch": 0.41211519364448856,
      "grad_norm": 84.67054748535156,
      "learning_rate": 4.579601149231029e-05,
      "loss": 0.7746,
      "step": 2490
    },
    {
      "epoch": 0.41377027474346245,
      "grad_norm": 50.75457763671875,
      "learning_rate": 4.5774885921919896e-05,
      "loss": 1.4292,
      "step": 2500
    },
    {
      "epoch": 0.4154253558424363,
      "grad_norm": 24.217782974243164,
      "learning_rate": 4.5753760351529494e-05,
      "loss": 1.1432,
      "step": 2510
    },
    {
      "epoch": 0.4170804369414101,
      "grad_norm": 18.891891479492188,
      "learning_rate": 4.573263478113909e-05,
      "loss": 1.2085,
      "step": 2520
    },
    {
      "epoch": 0.418735518040384,
      "grad_norm": 26.55318832397461,
      "learning_rate": 4.571150921074869e-05,
      "loss": 0.8785,
      "step": 2530
    },
    {
      "epoch": 0.42039059913935783,
      "grad_norm": 30.518695831298828,
      "learning_rate": 4.569038364035829e-05,
      "loss": 1.0267,
      "step": 2540
    },
    {
      "epoch": 0.42204568023833167,
      "grad_norm": 47.7555046081543,
      "learning_rate": 4.566925806996789e-05,
      "loss": 0.9742,
      "step": 2550
    },
    {
      "epoch": 0.42370076133730555,
      "grad_norm": 20.539661407470703,
      "learning_rate": 4.5648132499577486e-05,
      "loss": 0.9212,
      "step": 2560
    },
    {
      "epoch": 0.4253558424362794,
      "grad_norm": 33.81751251220703,
      "learning_rate": 4.562700692918709e-05,
      "loss": 0.8025,
      "step": 2570
    },
    {
      "epoch": 0.4270109235352532,
      "grad_norm": 45.8291130065918,
      "learning_rate": 4.560588135879669e-05,
      "loss": 0.9576,
      "step": 2580
    },
    {
      "epoch": 0.4286660046342271,
      "grad_norm": 41.69477462768555,
      "learning_rate": 4.558475578840629e-05,
      "loss": 0.9541,
      "step": 2590
    },
    {
      "epoch": 0.43032108573320094,
      "grad_norm": 42.13896942138672,
      "learning_rate": 4.556363021801589e-05,
      "loss": 0.947,
      "step": 2600
    },
    {
      "epoch": 0.43197616683217477,
      "grad_norm": 23.745275497436523,
      "learning_rate": 4.554250464762549e-05,
      "loss": 1.2816,
      "step": 2610
    },
    {
      "epoch": 0.4336312479311486,
      "grad_norm": 21.12394905090332,
      "learning_rate": 4.552137907723509e-05,
      "loss": 0.8621,
      "step": 2620
    },
    {
      "epoch": 0.4352863290301225,
      "grad_norm": 32.90765380859375,
      "learning_rate": 4.550025350684469e-05,
      "loss": 1.3562,
      "step": 2630
    },
    {
      "epoch": 0.4369414101290963,
      "grad_norm": 24.078609466552734,
      "learning_rate": 4.547912793645429e-05,
      "loss": 1.3608,
      "step": 2640
    },
    {
      "epoch": 0.43859649122807015,
      "grad_norm": 13.141268730163574,
      "learning_rate": 4.5458002366063885e-05,
      "loss": 1.2233,
      "step": 2650
    },
    {
      "epoch": 0.44025157232704404,
      "grad_norm": 42.01589584350586,
      "learning_rate": 4.5436876795673484e-05,
      "loss": 1.1981,
      "step": 2660
    },
    {
      "epoch": 0.44190665342601787,
      "grad_norm": 76.83899688720703,
      "learning_rate": 4.541575122528309e-05,
      "loss": 1.258,
      "step": 2670
    },
    {
      "epoch": 0.4435617345249917,
      "grad_norm": 49.53511428833008,
      "learning_rate": 4.539462565489269e-05,
      "loss": 1.0348,
      "step": 2680
    },
    {
      "epoch": 0.4452168156239656,
      "grad_norm": 60.55226516723633,
      "learning_rate": 4.5373500084502286e-05,
      "loss": 1.0517,
      "step": 2690
    },
    {
      "epoch": 0.4468718967229394,
      "grad_norm": 53.95637893676758,
      "learning_rate": 4.5352374514111884e-05,
      "loss": 1.0085,
      "step": 2700
    },
    {
      "epoch": 0.44852697782191325,
      "grad_norm": 27.74187660217285,
      "learning_rate": 4.533124894372148e-05,
      "loss": 1.5072,
      "step": 2710
    },
    {
      "epoch": 0.45018205892088714,
      "grad_norm": 14.525424003601074,
      "learning_rate": 4.531012337333108e-05,
      "loss": 0.8195,
      "step": 2720
    },
    {
      "epoch": 0.45183714001986097,
      "grad_norm": 51.680599212646484,
      "learning_rate": 4.528899780294068e-05,
      "loss": 0.8585,
      "step": 2730
    },
    {
      "epoch": 0.4534922211188348,
      "grad_norm": 47.33660888671875,
      "learning_rate": 4.526787223255028e-05,
      "loss": 0.973,
      "step": 2740
    },
    {
      "epoch": 0.4551473022178087,
      "grad_norm": 29.995847702026367,
      "learning_rate": 4.5246746662159876e-05,
      "loss": 1.1677,
      "step": 2750
    },
    {
      "epoch": 0.4568023833167825,
      "grad_norm": 31.386474609375,
      "learning_rate": 4.522562109176948e-05,
      "loss": 1.0705,
      "step": 2760
    },
    {
      "epoch": 0.45845746441575635,
      "grad_norm": 18.126035690307617,
      "learning_rate": 4.520449552137908e-05,
      "loss": 0.984,
      "step": 2770
    },
    {
      "epoch": 0.46011254551473024,
      "grad_norm": 9.517047882080078,
      "learning_rate": 4.518336995098868e-05,
      "loss": 1.0103,
      "step": 2780
    },
    {
      "epoch": 0.4617676266137041,
      "grad_norm": 29.755075454711914,
      "learning_rate": 4.5162244380598276e-05,
      "loss": 1.1244,
      "step": 2790
    },
    {
      "epoch": 0.4634227077126779,
      "grad_norm": 51.23434066772461,
      "learning_rate": 4.5141118810207875e-05,
      "loss": 1.0505,
      "step": 2800
    },
    {
      "epoch": 0.4650777888116518,
      "grad_norm": 16.616374969482422,
      "learning_rate": 4.511999323981747e-05,
      "loss": 1.223,
      "step": 2810
    },
    {
      "epoch": 0.4667328699106256,
      "grad_norm": 51.38534927368164,
      "learning_rate": 4.509886766942708e-05,
      "loss": 0.9581,
      "step": 2820
    },
    {
      "epoch": 0.46838795100959946,
      "grad_norm": 41.693477630615234,
      "learning_rate": 4.5077742099036676e-05,
      "loss": 0.7692,
      "step": 2830
    },
    {
      "epoch": 0.47004303210857334,
      "grad_norm": 11.994704246520996,
      "learning_rate": 4.505661652864628e-05,
      "loss": 0.8272,
      "step": 2840
    },
    {
      "epoch": 0.4716981132075472,
      "grad_norm": 19.365615844726562,
      "learning_rate": 4.503549095825588e-05,
      "loss": 1.0408,
      "step": 2850
    },
    {
      "epoch": 0.473353194306521,
      "grad_norm": 29.6995849609375,
      "learning_rate": 4.501436538786548e-05,
      "loss": 1.1626,
      "step": 2860
    },
    {
      "epoch": 0.4750082754054949,
      "grad_norm": 24.254133224487305,
      "learning_rate": 4.499323981747508e-05,
      "loss": 1.1241,
      "step": 2870
    },
    {
      "epoch": 0.4766633565044687,
      "grad_norm": 29.963594436645508,
      "learning_rate": 4.4972114247084675e-05,
      "loss": 0.9395,
      "step": 2880
    },
    {
      "epoch": 0.47831843760344256,
      "grad_norm": 34.10828399658203,
      "learning_rate": 4.4950988676694274e-05,
      "loss": 1.1597,
      "step": 2890
    },
    {
      "epoch": 0.4799735187024164,
      "grad_norm": 33.3126106262207,
      "learning_rate": 4.492986310630387e-05,
      "loss": 1.265,
      "step": 2900
    },
    {
      "epoch": 0.4816285998013903,
      "grad_norm": 47.16437911987305,
      "learning_rate": 4.490873753591347e-05,
      "loss": 0.9345,
      "step": 2910
    },
    {
      "epoch": 0.4832836809003641,
      "grad_norm": 47.16258239746094,
      "learning_rate": 4.488761196552307e-05,
      "loss": 1.0233,
      "step": 2920
    },
    {
      "epoch": 0.48493876199933794,
      "grad_norm": 48.081687927246094,
      "learning_rate": 4.4866486395132674e-05,
      "loss": 1.1696,
      "step": 2930
    },
    {
      "epoch": 0.48659384309831183,
      "grad_norm": 49.111019134521484,
      "learning_rate": 4.484536082474227e-05,
      "loss": 0.9129,
      "step": 2940
    },
    {
      "epoch": 0.48824892419728566,
      "grad_norm": 48.45641326904297,
      "learning_rate": 4.482423525435187e-05,
      "loss": 0.9776,
      "step": 2950
    },
    {
      "epoch": 0.4899040052962595,
      "grad_norm": 19.74150848388672,
      "learning_rate": 4.480310968396147e-05,
      "loss": 0.9024,
      "step": 2960
    },
    {
      "epoch": 0.4915590863952334,
      "grad_norm": 6.187689781188965,
      "learning_rate": 4.478198411357107e-05,
      "loss": 0.8995,
      "step": 2970
    },
    {
      "epoch": 0.4932141674942072,
      "grad_norm": 129.9542999267578,
      "learning_rate": 4.4760858543180666e-05,
      "loss": 0.9229,
      "step": 2980
    },
    {
      "epoch": 0.49486924859318104,
      "grad_norm": 14.566239356994629,
      "learning_rate": 4.4739732972790264e-05,
      "loss": 1.0261,
      "step": 2990
    },
    {
      "epoch": 0.49652432969215493,
      "grad_norm": 25.0571231842041,
      "learning_rate": 4.471860740239986e-05,
      "loss": 1.0101,
      "step": 3000
    },
    {
      "epoch": 0.49817941079112876,
      "grad_norm": 33.502079010009766,
      "learning_rate": 4.469748183200947e-05,
      "loss": 1.148,
      "step": 3010
    },
    {
      "epoch": 0.4998344918901026,
      "grad_norm": 22.24547004699707,
      "learning_rate": 4.4676356261619066e-05,
      "loss": 1.1127,
      "step": 3020
    },
    {
      "epoch": 0.5014895729890765,
      "grad_norm": 31.913928985595703,
      "learning_rate": 4.4655230691228664e-05,
      "loss": 1.289,
      "step": 3030
    },
    {
      "epoch": 0.5031446540880503,
      "grad_norm": 46.591957092285156,
      "learning_rate": 4.463410512083826e-05,
      "loss": 0.7181,
      "step": 3040
    },
    {
      "epoch": 0.5047997351870241,
      "grad_norm": 29.779342651367188,
      "learning_rate": 4.461297955044786e-05,
      "loss": 1.1137,
      "step": 3050
    },
    {
      "epoch": 0.506454816285998,
      "grad_norm": 60.526493072509766,
      "learning_rate": 4.4591853980057466e-05,
      "loss": 1.0525,
      "step": 3060
    },
    {
      "epoch": 0.5081098973849718,
      "grad_norm": 42.228668212890625,
      "learning_rate": 4.4570728409667065e-05,
      "loss": 1.2382,
      "step": 3070
    },
    {
      "epoch": 0.5097649784839458,
      "grad_norm": 14.870965957641602,
      "learning_rate": 4.454960283927666e-05,
      "loss": 1.0068,
      "step": 3080
    },
    {
      "epoch": 0.5114200595829196,
      "grad_norm": 47.307918548583984,
      "learning_rate": 4.452847726888626e-05,
      "loss": 0.787,
      "step": 3090
    },
    {
      "epoch": 0.5130751406818934,
      "grad_norm": 75.28512573242188,
      "learning_rate": 4.450735169849587e-05,
      "loss": 0.9493,
      "step": 3100
    },
    {
      "epoch": 0.5147302217808672,
      "grad_norm": 17.681638717651367,
      "learning_rate": 4.4486226128105465e-05,
      "loss": 0.9243,
      "step": 3110
    },
    {
      "epoch": 0.5163853028798411,
      "grad_norm": 18.402727127075195,
      "learning_rate": 4.4465100557715063e-05,
      "loss": 0.8278,
      "step": 3120
    },
    {
      "epoch": 0.5180403839788149,
      "grad_norm": 64.42596435546875,
      "learning_rate": 4.444397498732466e-05,
      "loss": 1.4503,
      "step": 3130
    },
    {
      "epoch": 0.5196954650777889,
      "grad_norm": 26.840734481811523,
      "learning_rate": 4.442284941693426e-05,
      "loss": 1.1167,
      "step": 3140
    },
    {
      "epoch": 0.5213505461767627,
      "grad_norm": 6.106387615203857,
      "learning_rate": 4.440172384654386e-05,
      "loss": 1.0319,
      "step": 3150
    },
    {
      "epoch": 0.5230056272757365,
      "grad_norm": 9.54041862487793,
      "learning_rate": 4.438059827615346e-05,
      "loss": 0.9856,
      "step": 3160
    },
    {
      "epoch": 0.5246607083747103,
      "grad_norm": 51.608367919921875,
      "learning_rate": 4.4359472705763055e-05,
      "loss": 0.8853,
      "step": 3170
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 23.890981674194336,
      "learning_rate": 4.433834713537266e-05,
      "loss": 1.2501,
      "step": 3180
    },
    {
      "epoch": 0.527970870572658,
      "grad_norm": 22.985097885131836,
      "learning_rate": 4.431722156498226e-05,
      "loss": 1.0114,
      "step": 3190
    },
    {
      "epoch": 0.529625951671632,
      "grad_norm": 56.444828033447266,
      "learning_rate": 4.429609599459186e-05,
      "loss": 1.1895,
      "step": 3200
    },
    {
      "epoch": 0.5312810327706058,
      "grad_norm": 21.345722198486328,
      "learning_rate": 4.4274970424201456e-05,
      "loss": 0.8795,
      "step": 3210
    },
    {
      "epoch": 0.5329361138695796,
      "grad_norm": 61.34769058227539,
      "learning_rate": 4.4253844853811054e-05,
      "loss": 1.2211,
      "step": 3220
    },
    {
      "epoch": 0.5345911949685535,
      "grad_norm": 32.72557830810547,
      "learning_rate": 4.423271928342065e-05,
      "loss": 1.3167,
      "step": 3230
    },
    {
      "epoch": 0.5362462760675273,
      "grad_norm": 26.916698455810547,
      "learning_rate": 4.421159371303025e-05,
      "loss": 1.0784,
      "step": 3240
    },
    {
      "epoch": 0.5379013571665011,
      "grad_norm": 36.19459915161133,
      "learning_rate": 4.419046814263985e-05,
      "loss": 1.2127,
      "step": 3250
    },
    {
      "epoch": 0.5395564382654751,
      "grad_norm": 18.566329956054688,
      "learning_rate": 4.416934257224945e-05,
      "loss": 0.8837,
      "step": 3260
    },
    {
      "epoch": 0.5412115193644489,
      "grad_norm": 40.0125617980957,
      "learning_rate": 4.414821700185905e-05,
      "loss": 1.1717,
      "step": 3270
    },
    {
      "epoch": 0.5428666004634227,
      "grad_norm": 64.60932922363281,
      "learning_rate": 4.412709143146865e-05,
      "loss": 0.9544,
      "step": 3280
    },
    {
      "epoch": 0.5445216815623966,
      "grad_norm": 42.479454040527344,
      "learning_rate": 4.410596586107825e-05,
      "loss": 1.1997,
      "step": 3290
    },
    {
      "epoch": 0.5461767626613704,
      "grad_norm": 34.86271667480469,
      "learning_rate": 4.408484029068785e-05,
      "loss": 0.5354,
      "step": 3300
    },
    {
      "epoch": 0.5478318437603442,
      "grad_norm": 72.71646118164062,
      "learning_rate": 4.406371472029745e-05,
      "loss": 0.578,
      "step": 3310
    },
    {
      "epoch": 0.5494869248593182,
      "grad_norm": 20.737770080566406,
      "learning_rate": 4.404258914990705e-05,
      "loss": 1.0718,
      "step": 3320
    },
    {
      "epoch": 0.551142005958292,
      "grad_norm": 35.293800354003906,
      "learning_rate": 4.402146357951665e-05,
      "loss": 0.9742,
      "step": 3330
    },
    {
      "epoch": 0.5527970870572658,
      "grad_norm": 12.012450218200684,
      "learning_rate": 4.400033800912625e-05,
      "loss": 0.8382,
      "step": 3340
    },
    {
      "epoch": 0.5544521681562397,
      "grad_norm": 22.245031356811523,
      "learning_rate": 4.397921243873585e-05,
      "loss": 0.7901,
      "step": 3350
    },
    {
      "epoch": 0.5561072492552135,
      "grad_norm": 18.608829498291016,
      "learning_rate": 4.395808686834545e-05,
      "loss": 0.9909,
      "step": 3360
    },
    {
      "epoch": 0.5577623303541873,
      "grad_norm": 45.54827117919922,
      "learning_rate": 4.393696129795505e-05,
      "loss": 0.8151,
      "step": 3370
    },
    {
      "epoch": 0.5594174114531612,
      "grad_norm": 34.61511993408203,
      "learning_rate": 4.391583572756465e-05,
      "loss": 0.7967,
      "step": 3380
    },
    {
      "epoch": 0.5610724925521351,
      "grad_norm": 83.83863067626953,
      "learning_rate": 4.389471015717425e-05,
      "loss": 0.9728,
      "step": 3390
    },
    {
      "epoch": 0.5627275736511089,
      "grad_norm": 21.937610626220703,
      "learning_rate": 4.3873584586783845e-05,
      "loss": 0.697,
      "step": 3400
    },
    {
      "epoch": 0.5643826547500828,
      "grad_norm": 32.94719696044922,
      "learning_rate": 4.3852459016393444e-05,
      "loss": 1.2579,
      "step": 3410
    },
    {
      "epoch": 0.5660377358490566,
      "grad_norm": 25.933855056762695,
      "learning_rate": 4.383133344600304e-05,
      "loss": 1.0096,
      "step": 3420
    },
    {
      "epoch": 0.5676928169480304,
      "grad_norm": 29.334152221679688,
      "learning_rate": 4.381020787561264e-05,
      "loss": 0.7863,
      "step": 3430
    },
    {
      "epoch": 0.5693478980470043,
      "grad_norm": 34.82351303100586,
      "learning_rate": 4.3789082305222246e-05,
      "loss": 1.0507,
      "step": 3440
    },
    {
      "epoch": 0.5710029791459782,
      "grad_norm": 85.03363800048828,
      "learning_rate": 4.3767956734831844e-05,
      "loss": 1.2044,
      "step": 3450
    },
    {
      "epoch": 0.572658060244952,
      "grad_norm": 113.42005157470703,
      "learning_rate": 4.374683116444144e-05,
      "loss": 1.0027,
      "step": 3460
    },
    {
      "epoch": 0.5743131413439259,
      "grad_norm": 33.080318450927734,
      "learning_rate": 4.372570559405104e-05,
      "loss": 1.294,
      "step": 3470
    },
    {
      "epoch": 0.5759682224428997,
      "grad_norm": 34.29035186767578,
      "learning_rate": 4.370458002366064e-05,
      "loss": 0.7964,
      "step": 3480
    },
    {
      "epoch": 0.5776233035418735,
      "grad_norm": 34.41170120239258,
      "learning_rate": 4.368345445327024e-05,
      "loss": 1.0369,
      "step": 3490
    },
    {
      "epoch": 0.5792783846408474,
      "grad_norm": 48.581363677978516,
      "learning_rate": 4.3662328882879836e-05,
      "loss": 1.0079,
      "step": 3500
    },
    {
      "epoch": 0.5809334657398213,
      "grad_norm": 34.50174331665039,
      "learning_rate": 4.3641203312489434e-05,
      "loss": 0.8194,
      "step": 3510
    },
    {
      "epoch": 0.5825885468387951,
      "grad_norm": 62.804195404052734,
      "learning_rate": 4.362007774209904e-05,
      "loss": 1.1721,
      "step": 3520
    },
    {
      "epoch": 0.584243627937769,
      "grad_norm": 11.448285102844238,
      "learning_rate": 4.359895217170864e-05,
      "loss": 1.0368,
      "step": 3530
    },
    {
      "epoch": 0.5858987090367428,
      "grad_norm": 19.811426162719727,
      "learning_rate": 4.3577826601318236e-05,
      "loss": 1.298,
      "step": 3540
    },
    {
      "epoch": 0.5875537901357166,
      "grad_norm": 29.979482650756836,
      "learning_rate": 4.3556701030927835e-05,
      "loss": 1.063,
      "step": 3550
    },
    {
      "epoch": 0.5892088712346905,
      "grad_norm": 12.77958869934082,
      "learning_rate": 4.353557546053744e-05,
      "loss": 1.0148,
      "step": 3560
    },
    {
      "epoch": 0.5908639523336644,
      "grad_norm": 21.806713104248047,
      "learning_rate": 4.351444989014704e-05,
      "loss": 1.048,
      "step": 3570
    },
    {
      "epoch": 0.5925190334326382,
      "grad_norm": 52.04815673828125,
      "learning_rate": 4.3493324319756637e-05,
      "loss": 0.6887,
      "step": 3580
    },
    {
      "epoch": 0.5941741145316121,
      "grad_norm": 15.432953834533691,
      "learning_rate": 4.3472198749366235e-05,
      "loss": 1.1771,
      "step": 3590
    },
    {
      "epoch": 0.5958291956305859,
      "grad_norm": 17.309406280517578,
      "learning_rate": 4.345107317897583e-05,
      "loss": 1.1541,
      "step": 3600
    },
    {
      "epoch": 0.5974842767295597,
      "grad_norm": 33.38382339477539,
      "learning_rate": 4.342994760858544e-05,
      "loss": 1.0344,
      "step": 3610
    },
    {
      "epoch": 0.5991393578285336,
      "grad_norm": 19.473308563232422,
      "learning_rate": 4.340882203819504e-05,
      "loss": 1.1809,
      "step": 3620
    },
    {
      "epoch": 0.6007944389275075,
      "grad_norm": 28.181198120117188,
      "learning_rate": 4.3387696467804635e-05,
      "loss": 1.354,
      "step": 3630
    },
    {
      "epoch": 0.6024495200264813,
      "grad_norm": 93.92887115478516,
      "learning_rate": 4.3366570897414234e-05,
      "loss": 1.1341,
      "step": 3640
    },
    {
      "epoch": 0.6041046011254552,
      "grad_norm": 58.1821174621582,
      "learning_rate": 4.334544532702383e-05,
      "loss": 1.073,
      "step": 3650
    },
    {
      "epoch": 0.605759682224429,
      "grad_norm": 24.403579711914062,
      "learning_rate": 4.332431975663343e-05,
      "loss": 0.9675,
      "step": 3660
    },
    {
      "epoch": 0.6074147633234028,
      "grad_norm": 32.77725601196289,
      "learning_rate": 4.330319418624303e-05,
      "loss": 1.1368,
      "step": 3670
    },
    {
      "epoch": 0.6090698444223767,
      "grad_norm": 112.9271469116211,
      "learning_rate": 4.328206861585263e-05,
      "loss": 0.9697,
      "step": 3680
    },
    {
      "epoch": 0.6107249255213505,
      "grad_norm": 55.39676284790039,
      "learning_rate": 4.326094304546223e-05,
      "loss": 0.6896,
      "step": 3690
    },
    {
      "epoch": 0.6123800066203244,
      "grad_norm": 45.41059112548828,
      "learning_rate": 4.323981747507183e-05,
      "loss": 1.0041,
      "step": 3700
    },
    {
      "epoch": 0.6140350877192983,
      "grad_norm": 44.502227783203125,
      "learning_rate": 4.321869190468143e-05,
      "loss": 1.0185,
      "step": 3710
    },
    {
      "epoch": 0.6156901688182721,
      "grad_norm": 38.318965911865234,
      "learning_rate": 4.319756633429103e-05,
      "loss": 0.9425,
      "step": 3720
    },
    {
      "epoch": 0.6173452499172459,
      "grad_norm": 31.89158821105957,
      "learning_rate": 4.3176440763900626e-05,
      "loss": 0.9321,
      "step": 3730
    },
    {
      "epoch": 0.6190003310162198,
      "grad_norm": 28.556781768798828,
      "learning_rate": 4.3155315193510224e-05,
      "loss": 0.7806,
      "step": 3740
    },
    {
      "epoch": 0.6206554121151936,
      "grad_norm": 45.13645935058594,
      "learning_rate": 4.313418962311982e-05,
      "loss": 1.0425,
      "step": 3750
    },
    {
      "epoch": 0.6223104932141675,
      "grad_norm": 12.88387680053711,
      "learning_rate": 4.311306405272942e-05,
      "loss": 0.8504,
      "step": 3760
    },
    {
      "epoch": 0.6239655743131414,
      "grad_norm": 47.46189498901367,
      "learning_rate": 4.309193848233902e-05,
      "loss": 0.7715,
      "step": 3770
    },
    {
      "epoch": 0.6256206554121152,
      "grad_norm": 40.75315856933594,
      "learning_rate": 4.3070812911948625e-05,
      "loss": 1.0554,
      "step": 3780
    },
    {
      "epoch": 0.627275736511089,
      "grad_norm": 37.876182556152344,
      "learning_rate": 4.304968734155822e-05,
      "loss": 1.2458,
      "step": 3790
    },
    {
      "epoch": 0.6289308176100629,
      "grad_norm": 30.799652099609375,
      "learning_rate": 4.302856177116782e-05,
      "loss": 1.0244,
      "step": 3800
    },
    {
      "epoch": 0.6305858987090367,
      "grad_norm": 54.76056671142578,
      "learning_rate": 4.3007436200777426e-05,
      "loss": 1.0445,
      "step": 3810
    },
    {
      "epoch": 0.6322409798080106,
      "grad_norm": 41.50865173339844,
      "learning_rate": 4.2986310630387025e-05,
      "loss": 0.8986,
      "step": 3820
    },
    {
      "epoch": 0.6338960609069845,
      "grad_norm": 22.26169776916504,
      "learning_rate": 4.296518505999662e-05,
      "loss": 0.9525,
      "step": 3830
    },
    {
      "epoch": 0.6355511420059583,
      "grad_norm": 37.867061614990234,
      "learning_rate": 4.294405948960622e-05,
      "loss": 1.0535,
      "step": 3840
    },
    {
      "epoch": 0.6372062231049321,
      "grad_norm": 30.231611251831055,
      "learning_rate": 4.292293391921582e-05,
      "loss": 1.2385,
      "step": 3850
    },
    {
      "epoch": 0.638861304203906,
      "grad_norm": 35.224124908447266,
      "learning_rate": 4.2901808348825425e-05,
      "loss": 0.7999,
      "step": 3860
    },
    {
      "epoch": 0.6405163853028798,
      "grad_norm": 34.22627639770508,
      "learning_rate": 4.2880682778435024e-05,
      "loss": 0.9329,
      "step": 3870
    },
    {
      "epoch": 0.6421714664018537,
      "grad_norm": 38.03691482543945,
      "learning_rate": 4.285955720804462e-05,
      "loss": 1.0867,
      "step": 3880
    },
    {
      "epoch": 0.6438265475008276,
      "grad_norm": 29.962953567504883,
      "learning_rate": 4.283843163765422e-05,
      "loss": 0.9484,
      "step": 3890
    },
    {
      "epoch": 0.6454816285998014,
      "grad_norm": 34.05778503417969,
      "learning_rate": 4.281730606726382e-05,
      "loss": 1.199,
      "step": 3900
    },
    {
      "epoch": 0.6471367096987752,
      "grad_norm": 32.24484634399414,
      "learning_rate": 4.279618049687342e-05,
      "loss": 1.0652,
      "step": 3910
    },
    {
      "epoch": 0.6487917907977491,
      "grad_norm": 43.89640808105469,
      "learning_rate": 4.2775054926483015e-05,
      "loss": 0.6774,
      "step": 3920
    },
    {
      "epoch": 0.6504468718967229,
      "grad_norm": 68.62040710449219,
      "learning_rate": 4.2753929356092614e-05,
      "loss": 0.9146,
      "step": 3930
    },
    {
      "epoch": 0.6521019529956968,
      "grad_norm": 26.414653778076172,
      "learning_rate": 4.273280378570221e-05,
      "loss": 1.277,
      "step": 3940
    },
    {
      "epoch": 0.6537570340946707,
      "grad_norm": 26.844953536987305,
      "learning_rate": 4.271167821531182e-05,
      "loss": 1.1291,
      "step": 3950
    },
    {
      "epoch": 0.6554121151936445,
      "grad_norm": 59.392921447753906,
      "learning_rate": 4.2690552644921416e-05,
      "loss": 1.0473,
      "step": 3960
    },
    {
      "epoch": 0.6570671962926183,
      "grad_norm": 16.544017791748047,
      "learning_rate": 4.2669427074531014e-05,
      "loss": 1.1409,
      "step": 3970
    },
    {
      "epoch": 0.6587222773915922,
      "grad_norm": 46.55052947998047,
      "learning_rate": 4.264830150414061e-05,
      "loss": 1.3012,
      "step": 3980
    },
    {
      "epoch": 0.660377358490566,
      "grad_norm": 42.517845153808594,
      "learning_rate": 4.262717593375021e-05,
      "loss": 0.976,
      "step": 3990
    },
    {
      "epoch": 0.6620324395895398,
      "grad_norm": 19.613950729370117,
      "learning_rate": 4.260605036335981e-05,
      "loss": 1.2709,
      "step": 4000
    },
    {
      "epoch": 0.6636875206885138,
      "grad_norm": 25.446979522705078,
      "learning_rate": 4.258492479296941e-05,
      "loss": 0.86,
      "step": 4010
    },
    {
      "epoch": 0.6653426017874876,
      "grad_norm": 14.613615036010742,
      "learning_rate": 4.2563799222579006e-05,
      "loss": 0.8186,
      "step": 4020
    },
    {
      "epoch": 0.6669976828864614,
      "grad_norm": 14.685046195983887,
      "learning_rate": 4.254267365218861e-05,
      "loss": 0.6251,
      "step": 4030
    },
    {
      "epoch": 0.6686527639854353,
      "grad_norm": 48.23843002319336,
      "learning_rate": 4.252154808179821e-05,
      "loss": 1.2902,
      "step": 4040
    },
    {
      "epoch": 0.6703078450844091,
      "grad_norm": 24.807998657226562,
      "learning_rate": 4.2500422511407815e-05,
      "loss": 0.9393,
      "step": 4050
    },
    {
      "epoch": 0.6719629261833829,
      "grad_norm": 19.088146209716797,
      "learning_rate": 4.247929694101741e-05,
      "loss": 1.3358,
      "step": 4060
    },
    {
      "epoch": 0.6736180072823569,
      "grad_norm": 38.983734130859375,
      "learning_rate": 4.245817137062701e-05,
      "loss": 1.1017,
      "step": 4070
    },
    {
      "epoch": 0.6752730883813307,
      "grad_norm": 24.58608627319336,
      "learning_rate": 4.243704580023661e-05,
      "loss": 1.072,
      "step": 4080
    },
    {
      "epoch": 0.6769281694803045,
      "grad_norm": 13.285205841064453,
      "learning_rate": 4.241592022984621e-05,
      "loss": 0.7644,
      "step": 4090
    },
    {
      "epoch": 0.6785832505792784,
      "grad_norm": 28.360557556152344,
      "learning_rate": 4.239479465945581e-05,
      "loss": 1.2148,
      "step": 4100
    },
    {
      "epoch": 0.6802383316782522,
      "grad_norm": 13.010207176208496,
      "learning_rate": 4.2373669089065405e-05,
      "loss": 1.061,
      "step": 4110
    },
    {
      "epoch": 0.681893412777226,
      "grad_norm": 19.980510711669922,
      "learning_rate": 4.235254351867501e-05,
      "loss": 0.849,
      "step": 4120
    },
    {
      "epoch": 0.6835484938762,
      "grad_norm": 67.56499481201172,
      "learning_rate": 4.233141794828461e-05,
      "loss": 0.9764,
      "step": 4130
    },
    {
      "epoch": 0.6852035749751738,
      "grad_norm": 17.82050132751465,
      "learning_rate": 4.231029237789421e-05,
      "loss": 1.1407,
      "step": 4140
    },
    {
      "epoch": 0.6868586560741476,
      "grad_norm": 40.020790100097656,
      "learning_rate": 4.2289166807503805e-05,
      "loss": 1.1759,
      "step": 4150
    },
    {
      "epoch": 0.6885137371731215,
      "grad_norm": 40.529502868652344,
      "learning_rate": 4.2268041237113404e-05,
      "loss": 1.1339,
      "step": 4160
    },
    {
      "epoch": 0.6901688182720953,
      "grad_norm": 89.79834747314453,
      "learning_rate": 4.2246915666723e-05,
      "loss": 0.6817,
      "step": 4170
    },
    {
      "epoch": 0.6918238993710691,
      "grad_norm": 24.52212905883789,
      "learning_rate": 4.22257900963326e-05,
      "loss": 1.0708,
      "step": 4180
    },
    {
      "epoch": 0.6934789804700431,
      "grad_norm": 17.734771728515625,
      "learning_rate": 4.22046645259422e-05,
      "loss": 0.6651,
      "step": 4190
    },
    {
      "epoch": 0.6951340615690169,
      "grad_norm": 18.72201919555664,
      "learning_rate": 4.2183538955551804e-05,
      "loss": 0.9523,
      "step": 4200
    },
    {
      "epoch": 0.6967891426679907,
      "grad_norm": 25.724624633789062,
      "learning_rate": 4.21624133851614e-05,
      "loss": 0.9105,
      "step": 4210
    },
    {
      "epoch": 0.6984442237669646,
      "grad_norm": 59.815460205078125,
      "learning_rate": 4.2141287814771e-05,
      "loss": 0.7468,
      "step": 4220
    },
    {
      "epoch": 0.7000993048659384,
      "grad_norm": 11.220100402832031,
      "learning_rate": 4.21201622443806e-05,
      "loss": 1.0499,
      "step": 4230
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 25.086885452270508,
      "learning_rate": 4.20990366739902e-05,
      "loss": 0.8719,
      "step": 4240
    },
    {
      "epoch": 0.7034094670638862,
      "grad_norm": 33.66277313232422,
      "learning_rate": 4.2077911103599796e-05,
      "loss": 0.8356,
      "step": 4250
    },
    {
      "epoch": 0.70506454816286,
      "grad_norm": 49.99306106567383,
      "learning_rate": 4.2056785533209394e-05,
      "loss": 0.9382,
      "step": 4260
    },
    {
      "epoch": 0.7067196292618338,
      "grad_norm": 31.82387924194336,
      "learning_rate": 4.2035659962819e-05,
      "loss": 0.9529,
      "step": 4270
    },
    {
      "epoch": 0.7083747103608077,
      "grad_norm": 46.24699401855469,
      "learning_rate": 4.20145343924286e-05,
      "loss": 1.034,
      "step": 4280
    },
    {
      "epoch": 0.7100297914597815,
      "grad_norm": 36.28944778442383,
      "learning_rate": 4.1993408822038196e-05,
      "loss": 1.1619,
      "step": 4290
    },
    {
      "epoch": 0.7116848725587553,
      "grad_norm": 17.562015533447266,
      "learning_rate": 4.19722832516478e-05,
      "loss": 0.8711,
      "step": 4300
    },
    {
      "epoch": 0.7133399536577292,
      "grad_norm": 20.160202026367188,
      "learning_rate": 4.19511576812574e-05,
      "loss": 0.8318,
      "step": 4310
    },
    {
      "epoch": 0.7149950347567031,
      "grad_norm": 37.79857635498047,
      "learning_rate": 4.1930032110867e-05,
      "loss": 1.0378,
      "step": 4320
    },
    {
      "epoch": 0.716650115855677,
      "grad_norm": 18.58428382873535,
      "learning_rate": 4.1908906540476597e-05,
      "loss": 0.7658,
      "step": 4330
    },
    {
      "epoch": 0.7183051969546508,
      "grad_norm": 112.11043548583984,
      "learning_rate": 4.1887780970086195e-05,
      "loss": 0.7248,
      "step": 4340
    },
    {
      "epoch": 0.7199602780536246,
      "grad_norm": 27.68385887145996,
      "learning_rate": 4.186665539969579e-05,
      "loss": 0.4092,
      "step": 4350
    },
    {
      "epoch": 0.7216153591525984,
      "grad_norm": 17.037704467773438,
      "learning_rate": 4.184552982930539e-05,
      "loss": 1.2051,
      "step": 4360
    },
    {
      "epoch": 0.7232704402515723,
      "grad_norm": 18.39204216003418,
      "learning_rate": 4.1824404258915e-05,
      "loss": 0.9204,
      "step": 4370
    },
    {
      "epoch": 0.7249255213505462,
      "grad_norm": 20.9383487701416,
      "learning_rate": 4.1803278688524595e-05,
      "loss": 0.5885,
      "step": 4380
    },
    {
      "epoch": 0.72658060244952,
      "grad_norm": 40.035709381103516,
      "learning_rate": 4.1782153118134194e-05,
      "loss": 0.8874,
      "step": 4390
    },
    {
      "epoch": 0.7282356835484939,
      "grad_norm": 27.603206634521484,
      "learning_rate": 4.176102754774379e-05,
      "loss": 0.9238,
      "step": 4400
    },
    {
      "epoch": 0.7298907646474677,
      "grad_norm": 47.868778228759766,
      "learning_rate": 4.173990197735339e-05,
      "loss": 0.8204,
      "step": 4410
    },
    {
      "epoch": 0.7315458457464415,
      "grad_norm": 54.91796112060547,
      "learning_rate": 4.171877640696299e-05,
      "loss": 0.8304,
      "step": 4420
    },
    {
      "epoch": 0.7332009268454154,
      "grad_norm": 30.37213134765625,
      "learning_rate": 4.169765083657259e-05,
      "loss": 0.7506,
      "step": 4430
    },
    {
      "epoch": 0.7348560079443893,
      "grad_norm": 55.265262603759766,
      "learning_rate": 4.1676525266182186e-05,
      "loss": 1.2753,
      "step": 4440
    },
    {
      "epoch": 0.7365110890433632,
      "grad_norm": 21.631900787353516,
      "learning_rate": 4.1655399695791784e-05,
      "loss": 1.0233,
      "step": 4450
    },
    {
      "epoch": 0.738166170142337,
      "grad_norm": 12.731894493103027,
      "learning_rate": 4.163427412540139e-05,
      "loss": 0.9985,
      "step": 4460
    },
    {
      "epoch": 0.7398212512413108,
      "grad_norm": 38.49055099487305,
      "learning_rate": 4.161314855501099e-05,
      "loss": 1.0546,
      "step": 4470
    },
    {
      "epoch": 0.7414763323402846,
      "grad_norm": 29.420557022094727,
      "learning_rate": 4.1592022984620586e-05,
      "loss": 1.1254,
      "step": 4480
    },
    {
      "epoch": 0.7431314134392585,
      "grad_norm": 42.37807846069336,
      "learning_rate": 4.1570897414230184e-05,
      "loss": 0.9813,
      "step": 4490
    },
    {
      "epoch": 0.7447864945382324,
      "grad_norm": 43.05418395996094,
      "learning_rate": 4.154977184383978e-05,
      "loss": 0.8281,
      "step": 4500
    },
    {
      "epoch": 0.7464415756372063,
      "grad_norm": 30.92050552368164,
      "learning_rate": 4.152864627344938e-05,
      "loss": 1.0599,
      "step": 4510
    },
    {
      "epoch": 0.7480966567361801,
      "grad_norm": 17.37283706665039,
      "learning_rate": 4.1507520703058986e-05,
      "loss": 0.8848,
      "step": 4520
    },
    {
      "epoch": 0.7497517378351539,
      "grad_norm": 32.23815155029297,
      "learning_rate": 4.1486395132668585e-05,
      "loss": 0.9109,
      "step": 4530
    },
    {
      "epoch": 0.7514068189341278,
      "grad_norm": 21.870372772216797,
      "learning_rate": 4.146526956227818e-05,
      "loss": 0.772,
      "step": 4540
    },
    {
      "epoch": 0.7530619000331016,
      "grad_norm": 39.00139617919922,
      "learning_rate": 4.144414399188779e-05,
      "loss": 1.0175,
      "step": 4550
    },
    {
      "epoch": 0.7547169811320755,
      "grad_norm": 29.757577896118164,
      "learning_rate": 4.1423018421497387e-05,
      "loss": 0.9154,
      "step": 4560
    },
    {
      "epoch": 0.7563720622310494,
      "grad_norm": 29.229801177978516,
      "learning_rate": 4.1401892851106985e-05,
      "loss": 0.7105,
      "step": 4570
    },
    {
      "epoch": 0.7580271433300232,
      "grad_norm": 21.203264236450195,
      "learning_rate": 4.138076728071658e-05,
      "loss": 0.9338,
      "step": 4580
    },
    {
      "epoch": 0.759682224428997,
      "grad_norm": 32.385013580322266,
      "learning_rate": 4.135964171032618e-05,
      "loss": 0.8945,
      "step": 4590
    },
    {
      "epoch": 0.7613373055279709,
      "grad_norm": 33.82734298706055,
      "learning_rate": 4.133851613993578e-05,
      "loss": 1.3662,
      "step": 4600
    },
    {
      "epoch": 0.7629923866269447,
      "grad_norm": 24.08469009399414,
      "learning_rate": 4.131739056954538e-05,
      "loss": 1.2624,
      "step": 4610
    },
    {
      "epoch": 0.7646474677259185,
      "grad_norm": 22.48661231994629,
      "learning_rate": 4.129626499915498e-05,
      "loss": 0.9274,
      "step": 4620
    },
    {
      "epoch": 0.7663025488248925,
      "grad_norm": 29.62706756591797,
      "learning_rate": 4.127513942876458e-05,
      "loss": 1.0376,
      "step": 4630
    },
    {
      "epoch": 0.7679576299238663,
      "grad_norm": 36.48291015625,
      "learning_rate": 4.125401385837418e-05,
      "loss": 0.9274,
      "step": 4640
    },
    {
      "epoch": 0.7696127110228401,
      "grad_norm": 10.477531433105469,
      "learning_rate": 4.123288828798378e-05,
      "loss": 0.5048,
      "step": 4650
    },
    {
      "epoch": 0.771267792121814,
      "grad_norm": 49.21723556518555,
      "learning_rate": 4.121176271759338e-05,
      "loss": 0.7975,
      "step": 4660
    },
    {
      "epoch": 0.7729228732207878,
      "grad_norm": 40.18122100830078,
      "learning_rate": 4.1190637147202976e-05,
      "loss": 1.1503,
      "step": 4670
    },
    {
      "epoch": 0.7745779543197616,
      "grad_norm": 13.115181922912598,
      "learning_rate": 4.1169511576812574e-05,
      "loss": 1.1094,
      "step": 4680
    },
    {
      "epoch": 0.7762330354187356,
      "grad_norm": 12.524043083190918,
      "learning_rate": 4.114838600642217e-05,
      "loss": 0.5007,
      "step": 4690
    },
    {
      "epoch": 0.7778881165177094,
      "grad_norm": 17.053733825683594,
      "learning_rate": 4.112726043603177e-05,
      "loss": 0.8394,
      "step": 4700
    },
    {
      "epoch": 0.7795431976166832,
      "grad_norm": 30.350507736206055,
      "learning_rate": 4.1106134865641376e-05,
      "loss": 0.8769,
      "step": 4710
    },
    {
      "epoch": 0.7811982787156571,
      "grad_norm": 12.01939582824707,
      "learning_rate": 4.1085009295250974e-05,
      "loss": 1.0249,
      "step": 4720
    },
    {
      "epoch": 0.7828533598146309,
      "grad_norm": 18.823518753051758,
      "learning_rate": 4.106388372486057e-05,
      "loss": 0.8531,
      "step": 4730
    },
    {
      "epoch": 0.7845084409136047,
      "grad_norm": 34.41464614868164,
      "learning_rate": 4.104275815447017e-05,
      "loss": 1.0431,
      "step": 4740
    },
    {
      "epoch": 0.7861635220125787,
      "grad_norm": 111.41908264160156,
      "learning_rate": 4.102163258407977e-05,
      "loss": 0.9867,
      "step": 4750
    },
    {
      "epoch": 0.7878186031115525,
      "grad_norm": 72.9460220336914,
      "learning_rate": 4.100050701368937e-05,
      "loss": 0.899,
      "step": 4760
    },
    {
      "epoch": 0.7894736842105263,
      "grad_norm": 43.42085266113281,
      "learning_rate": 4.097938144329897e-05,
      "loss": 0.9092,
      "step": 4770
    },
    {
      "epoch": 0.7911287653095002,
      "grad_norm": 60.30282211303711,
      "learning_rate": 4.095825587290857e-05,
      "loss": 1.153,
      "step": 4780
    },
    {
      "epoch": 0.792783846408474,
      "grad_norm": 7.749949932098389,
      "learning_rate": 4.093713030251817e-05,
      "loss": 0.8745,
      "step": 4790
    },
    {
      "epoch": 0.7944389275074478,
      "grad_norm": 30.306625366210938,
      "learning_rate": 4.0916004732127775e-05,
      "loss": 0.8802,
      "step": 4800
    },
    {
      "epoch": 0.7960940086064218,
      "grad_norm": 32.5853385925293,
      "learning_rate": 4.089487916173737e-05,
      "loss": 0.8418,
      "step": 4810
    },
    {
      "epoch": 0.7977490897053956,
      "grad_norm": 47.25553512573242,
      "learning_rate": 4.087375359134697e-05,
      "loss": 0.8892,
      "step": 4820
    },
    {
      "epoch": 0.7994041708043694,
      "grad_norm": 38.854591369628906,
      "learning_rate": 4.085262802095657e-05,
      "loss": 1.376,
      "step": 4830
    },
    {
      "epoch": 0.8010592519033433,
      "grad_norm": 25.953882217407227,
      "learning_rate": 4.083150245056617e-05,
      "loss": 0.7152,
      "step": 4840
    },
    {
      "epoch": 0.8027143330023171,
      "grad_norm": 52.47404861450195,
      "learning_rate": 4.081037688017577e-05,
      "loss": 0.9498,
      "step": 4850
    },
    {
      "epoch": 0.8043694141012909,
      "grad_norm": 39.588294982910156,
      "learning_rate": 4.0789251309785365e-05,
      "loss": 0.6822,
      "step": 4860
    },
    {
      "epoch": 0.8060244952002649,
      "grad_norm": 20.295116424560547,
      "learning_rate": 4.0768125739394963e-05,
      "loss": 0.844,
      "step": 4870
    },
    {
      "epoch": 0.8076795762992387,
      "grad_norm": 35.075931549072266,
      "learning_rate": 4.074700016900457e-05,
      "loss": 0.9564,
      "step": 4880
    },
    {
      "epoch": 0.8093346573982125,
      "grad_norm": 30.06873321533203,
      "learning_rate": 4.072587459861417e-05,
      "loss": 0.6323,
      "step": 4890
    },
    {
      "epoch": 0.8109897384971864,
      "grad_norm": 15.681838989257812,
      "learning_rate": 4.0704749028223765e-05,
      "loss": 1.1238,
      "step": 4900
    },
    {
      "epoch": 0.8126448195961602,
      "grad_norm": 126.30139923095703,
      "learning_rate": 4.0683623457833364e-05,
      "loss": 0.9157,
      "step": 4910
    },
    {
      "epoch": 0.814299900695134,
      "grad_norm": 21.020286560058594,
      "learning_rate": 4.066249788744296e-05,
      "loss": 1.0257,
      "step": 4920
    },
    {
      "epoch": 0.8159549817941079,
      "grad_norm": 15.291295051574707,
      "learning_rate": 4.064137231705256e-05,
      "loss": 0.7021,
      "step": 4930
    },
    {
      "epoch": 0.8176100628930818,
      "grad_norm": 76.18698120117188,
      "learning_rate": 4.062024674666216e-05,
      "loss": 0.977,
      "step": 4940
    },
    {
      "epoch": 0.8192651439920556,
      "grad_norm": 35.83197784423828,
      "learning_rate": 4.059912117627176e-05,
      "loss": 0.8111,
      "step": 4950
    },
    {
      "epoch": 0.8209202250910295,
      "grad_norm": 14.97690200805664,
      "learning_rate": 4.0577995605881356e-05,
      "loss": 1.2279,
      "step": 4960
    },
    {
      "epoch": 0.8225753061900033,
      "grad_norm": 10.889110565185547,
      "learning_rate": 4.055687003549096e-05,
      "loss": 0.9001,
      "step": 4970
    },
    {
      "epoch": 0.8242303872889771,
      "grad_norm": 10.17762279510498,
      "learning_rate": 4.053574446510056e-05,
      "loss": 0.8683,
      "step": 4980
    },
    {
      "epoch": 0.825885468387951,
      "grad_norm": 74.28833770751953,
      "learning_rate": 4.051461889471016e-05,
      "loss": 1.1123,
      "step": 4990
    },
    {
      "epoch": 0.8275405494869249,
      "grad_norm": 22.76382064819336,
      "learning_rate": 4.0493493324319756e-05,
      "loss": 0.8691,
      "step": 5000
    },
    {
      "epoch": 0.8291956305858987,
      "grad_norm": 20.298860549926758,
      "learning_rate": 4.0472367753929354e-05,
      "loss": 0.9887,
      "step": 5010
    },
    {
      "epoch": 0.8308507116848726,
      "grad_norm": 25.43297576904297,
      "learning_rate": 4.045124218353896e-05,
      "loss": 0.7705,
      "step": 5020
    },
    {
      "epoch": 0.8325057927838464,
      "grad_norm": 32.17022705078125,
      "learning_rate": 4.043011661314856e-05,
      "loss": 0.842,
      "step": 5030
    },
    {
      "epoch": 0.8341608738828202,
      "grad_norm": 29.64518928527832,
      "learning_rate": 4.0408991042758156e-05,
      "loss": 0.6831,
      "step": 5040
    },
    {
      "epoch": 0.8358159549817941,
      "grad_norm": 33.59387969970703,
      "learning_rate": 4.038786547236776e-05,
      "loss": 0.6944,
      "step": 5050
    },
    {
      "epoch": 0.837471036080768,
      "grad_norm": 55.8568229675293,
      "learning_rate": 4.036673990197736e-05,
      "loss": 1.019,
      "step": 5060
    },
    {
      "epoch": 0.8391261171797418,
      "grad_norm": 14.394953727722168,
      "learning_rate": 4.034561433158696e-05,
      "loss": 0.8919,
      "step": 5070
    },
    {
      "epoch": 0.8407811982787157,
      "grad_norm": 80.77783203125,
      "learning_rate": 4.032448876119656e-05,
      "loss": 1.0842,
      "step": 5080
    },
    {
      "epoch": 0.8424362793776895,
      "grad_norm": 24.037731170654297,
      "learning_rate": 4.0303363190806155e-05,
      "loss": 1.1513,
      "step": 5090
    },
    {
      "epoch": 0.8440913604766633,
      "grad_norm": 24.804033279418945,
      "learning_rate": 4.0282237620415753e-05,
      "loss": 1.1227,
      "step": 5100
    },
    {
      "epoch": 0.8457464415756372,
      "grad_norm": 22.402841567993164,
      "learning_rate": 4.026111205002535e-05,
      "loss": 1.3215,
      "step": 5110
    },
    {
      "epoch": 0.8474015226746111,
      "grad_norm": 37.805419921875,
      "learning_rate": 4.023998647963495e-05,
      "loss": 1.2424,
      "step": 5120
    },
    {
      "epoch": 0.8490566037735849,
      "grad_norm": 16.74306297302246,
      "learning_rate": 4.021886090924455e-05,
      "loss": 1.0153,
      "step": 5130
    },
    {
      "epoch": 0.8507116848725588,
      "grad_norm": 21.006916046142578,
      "learning_rate": 4.0197735338854154e-05,
      "loss": 0.8299,
      "step": 5140
    },
    {
      "epoch": 0.8523667659715326,
      "grad_norm": 35.47661590576172,
      "learning_rate": 4.017660976846375e-05,
      "loss": 0.8202,
      "step": 5150
    },
    {
      "epoch": 0.8540218470705064,
      "grad_norm": 55.752315521240234,
      "learning_rate": 4.015548419807335e-05,
      "loss": 1.3186,
      "step": 5160
    },
    {
      "epoch": 0.8556769281694803,
      "grad_norm": 16.06988525390625,
      "learning_rate": 4.013435862768295e-05,
      "loss": 0.8507,
      "step": 5170
    },
    {
      "epoch": 0.8573320092684542,
      "grad_norm": 27.378360748291016,
      "learning_rate": 4.011323305729255e-05,
      "loss": 1.2523,
      "step": 5180
    },
    {
      "epoch": 0.858987090367428,
      "grad_norm": 45.20370864868164,
      "learning_rate": 4.0092107486902146e-05,
      "loss": 0.6968,
      "step": 5190
    },
    {
      "epoch": 0.8606421714664019,
      "grad_norm": 26.549150466918945,
      "learning_rate": 4.0070981916511744e-05,
      "loss": 0.9209,
      "step": 5200
    },
    {
      "epoch": 0.8622972525653757,
      "grad_norm": 49.325218200683594,
      "learning_rate": 4.004985634612134e-05,
      "loss": 1.2193,
      "step": 5210
    },
    {
      "epoch": 0.8639523336643495,
      "grad_norm": 15.602266311645508,
      "learning_rate": 4.002873077573095e-05,
      "loss": 1.0883,
      "step": 5220
    },
    {
      "epoch": 0.8656074147633234,
      "grad_norm": 51.72816848754883,
      "learning_rate": 4.0007605205340546e-05,
      "loss": 0.9799,
      "step": 5230
    },
    {
      "epoch": 0.8672624958622972,
      "grad_norm": 44.26960372924805,
      "learning_rate": 3.9986479634950144e-05,
      "loss": 0.7453,
      "step": 5240
    },
    {
      "epoch": 0.8689175769612711,
      "grad_norm": 28.475341796875,
      "learning_rate": 3.996535406455974e-05,
      "loss": 1.0269,
      "step": 5250
    },
    {
      "epoch": 0.870572658060245,
      "grad_norm": 17.49791717529297,
      "learning_rate": 3.994422849416935e-05,
      "loss": 0.8412,
      "step": 5260
    },
    {
      "epoch": 0.8722277391592188,
      "grad_norm": 12.145014762878418,
      "learning_rate": 3.9923102923778946e-05,
      "loss": 0.7369,
      "step": 5270
    },
    {
      "epoch": 0.8738828202581926,
      "grad_norm": 25.482913970947266,
      "learning_rate": 3.9901977353388545e-05,
      "loss": 0.789,
      "step": 5280
    },
    {
      "epoch": 0.8755379013571665,
      "grad_norm": 32.25702667236328,
      "learning_rate": 3.988085178299814e-05,
      "loss": 0.9842,
      "step": 5290
    },
    {
      "epoch": 0.8771929824561403,
      "grad_norm": 46.49248504638672,
      "learning_rate": 3.985972621260774e-05,
      "loss": 1.1531,
      "step": 5300
    },
    {
      "epoch": 0.8788480635551142,
      "grad_norm": 13.853187561035156,
      "learning_rate": 3.9838600642217347e-05,
      "loss": 1.0884,
      "step": 5310
    },
    {
      "epoch": 0.8805031446540881,
      "grad_norm": 18.454940795898438,
      "learning_rate": 3.9817475071826945e-05,
      "loss": 0.9324,
      "step": 5320
    },
    {
      "epoch": 0.8821582257530619,
      "grad_norm": 22.40740394592285,
      "learning_rate": 3.979634950143654e-05,
      "loss": 0.7641,
      "step": 5330
    },
    {
      "epoch": 0.8838133068520357,
      "grad_norm": 174.1632537841797,
      "learning_rate": 3.977522393104614e-05,
      "loss": 0.8721,
      "step": 5340
    },
    {
      "epoch": 0.8854683879510096,
      "grad_norm": 25.572917938232422,
      "learning_rate": 3.975409836065574e-05,
      "loss": 0.626,
      "step": 5350
    },
    {
      "epoch": 0.8871234690499834,
      "grad_norm": 29.094751358032227,
      "learning_rate": 3.973297279026534e-05,
      "loss": 1.057,
      "step": 5360
    },
    {
      "epoch": 0.8887785501489573,
      "grad_norm": 62.47285461425781,
      "learning_rate": 3.971184721987494e-05,
      "loss": 0.9203,
      "step": 5370
    },
    {
      "epoch": 0.8904336312479312,
      "grad_norm": 31.753328323364258,
      "learning_rate": 3.9690721649484535e-05,
      "loss": 0.7212,
      "step": 5380
    },
    {
      "epoch": 0.892088712346905,
      "grad_norm": 29.556612014770508,
      "learning_rate": 3.966959607909414e-05,
      "loss": 1.1472,
      "step": 5390
    },
    {
      "epoch": 0.8937437934458788,
      "grad_norm": 49.43680191040039,
      "learning_rate": 3.964847050870374e-05,
      "loss": 0.8792,
      "step": 5400
    },
    {
      "epoch": 0.8953988745448527,
      "grad_norm": 37.89870834350586,
      "learning_rate": 3.962734493831334e-05,
      "loss": 0.7196,
      "step": 5410
    },
    {
      "epoch": 0.8970539556438265,
      "grad_norm": 51.64850616455078,
      "learning_rate": 3.9606219367922936e-05,
      "loss": 1.0338,
      "step": 5420
    },
    {
      "epoch": 0.8987090367428004,
      "grad_norm": 76.54161071777344,
      "learning_rate": 3.9585093797532534e-05,
      "loss": 0.9517,
      "step": 5430
    },
    {
      "epoch": 0.9003641178417743,
      "grad_norm": 25.51974868774414,
      "learning_rate": 3.956396822714213e-05,
      "loss": 0.7484,
      "step": 5440
    },
    {
      "epoch": 0.9020191989407481,
      "grad_norm": 64.34526824951172,
      "learning_rate": 3.954284265675173e-05,
      "loss": 0.6259,
      "step": 5450
    },
    {
      "epoch": 0.9036742800397219,
      "grad_norm": 32.29912185668945,
      "learning_rate": 3.952171708636133e-05,
      "loss": 0.7585,
      "step": 5460
    },
    {
      "epoch": 0.9053293611386958,
      "grad_norm": 86.34689331054688,
      "learning_rate": 3.950059151597093e-05,
      "loss": 0.6086,
      "step": 5470
    },
    {
      "epoch": 0.9069844422376696,
      "grad_norm": 12.112634658813477,
      "learning_rate": 3.947946594558053e-05,
      "loss": 1.1722,
      "step": 5480
    },
    {
      "epoch": 0.9086395233366436,
      "grad_norm": 27.78068733215332,
      "learning_rate": 3.945834037519013e-05,
      "loss": 0.963,
      "step": 5490
    },
    {
      "epoch": 0.9102946044356174,
      "grad_norm": 37.203094482421875,
      "learning_rate": 3.943721480479973e-05,
      "loss": 1.2438,
      "step": 5500
    },
    {
      "epoch": 0.9119496855345912,
      "grad_norm": 38.40298080444336,
      "learning_rate": 3.9416089234409335e-05,
      "loss": 1.0261,
      "step": 5510
    },
    {
      "epoch": 0.913604766633565,
      "grad_norm": 39.25109100341797,
      "learning_rate": 3.939496366401893e-05,
      "loss": 0.8199,
      "step": 5520
    },
    {
      "epoch": 0.9152598477325389,
      "grad_norm": 43.16952133178711,
      "learning_rate": 3.937383809362853e-05,
      "loss": 0.9221,
      "step": 5530
    },
    {
      "epoch": 0.9169149288315127,
      "grad_norm": 28.587217330932617,
      "learning_rate": 3.935271252323813e-05,
      "loss": 1.0735,
      "step": 5540
    },
    {
      "epoch": 0.9185700099304865,
      "grad_norm": 40.173553466796875,
      "learning_rate": 3.933158695284773e-05,
      "loss": 0.942,
      "step": 5550
    },
    {
      "epoch": 0.9202250910294605,
      "grad_norm": 24.115835189819336,
      "learning_rate": 3.931046138245733e-05,
      "loss": 0.9262,
      "step": 5560
    },
    {
      "epoch": 0.9218801721284343,
      "grad_norm": 47.29297637939453,
      "learning_rate": 3.928933581206693e-05,
      "loss": 0.7428,
      "step": 5570
    },
    {
      "epoch": 0.9235352532274081,
      "grad_norm": 14.116887092590332,
      "learning_rate": 3.926821024167653e-05,
      "loss": 0.6369,
      "step": 5580
    },
    {
      "epoch": 0.925190334326382,
      "grad_norm": 36.847469329833984,
      "learning_rate": 3.924708467128613e-05,
      "loss": 1.0496,
      "step": 5590
    },
    {
      "epoch": 0.9268454154253558,
      "grad_norm": 39.12117004394531,
      "learning_rate": 3.922595910089573e-05,
      "loss": 0.9473,
      "step": 5600
    },
    {
      "epoch": 0.9285004965243296,
      "grad_norm": 25.145307540893555,
      "learning_rate": 3.9204833530505325e-05,
      "loss": 0.7678,
      "step": 5610
    },
    {
      "epoch": 0.9301555776233036,
      "grad_norm": 79.89407348632812,
      "learning_rate": 3.9183707960114924e-05,
      "loss": 0.7787,
      "step": 5620
    },
    {
      "epoch": 0.9318106587222774,
      "grad_norm": 59.21900177001953,
      "learning_rate": 3.916258238972452e-05,
      "loss": 0.8701,
      "step": 5630
    },
    {
      "epoch": 0.9334657398212513,
      "grad_norm": 60.02852249145508,
      "learning_rate": 3.914145681933412e-05,
      "loss": 0.8521,
      "step": 5640
    },
    {
      "epoch": 0.9351208209202251,
      "grad_norm": 16.344213485717773,
      "learning_rate": 3.9120331248943725e-05,
      "loss": 1.0592,
      "step": 5650
    },
    {
      "epoch": 0.9367759020191989,
      "grad_norm": 18.850765228271484,
      "learning_rate": 3.9099205678553324e-05,
      "loss": 0.967,
      "step": 5660
    },
    {
      "epoch": 0.9384309831181727,
      "grad_norm": 34.41431427001953,
      "learning_rate": 3.907808010816292e-05,
      "loss": 0.665,
      "step": 5670
    },
    {
      "epoch": 0.9400860642171467,
      "grad_norm": 14.427048683166504,
      "learning_rate": 3.905695453777252e-05,
      "loss": 0.6207,
      "step": 5680
    },
    {
      "epoch": 0.9417411453161205,
      "grad_norm": 47.749629974365234,
      "learning_rate": 3.903582896738212e-05,
      "loss": 0.855,
      "step": 5690
    },
    {
      "epoch": 0.9433962264150944,
      "grad_norm": 21.521379470825195,
      "learning_rate": 3.901470339699172e-05,
      "loss": 0.8471,
      "step": 5700
    },
    {
      "epoch": 0.9450513075140682,
      "grad_norm": 32.83076858520508,
      "learning_rate": 3.8993577826601316e-05,
      "loss": 1.1208,
      "step": 5710
    },
    {
      "epoch": 0.946706388613042,
      "grad_norm": 31.383573532104492,
      "learning_rate": 3.8972452256210914e-05,
      "loss": 0.7538,
      "step": 5720
    },
    {
      "epoch": 0.9483614697120158,
      "grad_norm": 40.423213958740234,
      "learning_rate": 3.895132668582052e-05,
      "loss": 0.8583,
      "step": 5730
    },
    {
      "epoch": 0.9500165508109898,
      "grad_norm": 27.19219970703125,
      "learning_rate": 3.893020111543012e-05,
      "loss": 1.0746,
      "step": 5740
    },
    {
      "epoch": 0.9516716319099636,
      "grad_norm": 38.645328521728516,
      "learning_rate": 3.8909075545039716e-05,
      "loss": 0.7944,
      "step": 5750
    },
    {
      "epoch": 0.9533267130089375,
      "grad_norm": 57.03192138671875,
      "learning_rate": 3.888794997464932e-05,
      "loss": 0.9767,
      "step": 5760
    },
    {
      "epoch": 0.9549817941079113,
      "grad_norm": 12.768770217895508,
      "learning_rate": 3.886682440425892e-05,
      "loss": 1.1274,
      "step": 5770
    },
    {
      "epoch": 0.9566368752068851,
      "grad_norm": 30.169437408447266,
      "learning_rate": 3.884569883386852e-05,
      "loss": 0.6774,
      "step": 5780
    },
    {
      "epoch": 0.958291956305859,
      "grad_norm": 4.46786642074585,
      "learning_rate": 3.8824573263478116e-05,
      "loss": 0.8425,
      "step": 5790
    },
    {
      "epoch": 0.9599470374048328,
      "grad_norm": 20.018781661987305,
      "learning_rate": 3.8803447693087715e-05,
      "loss": 0.5979,
      "step": 5800
    },
    {
      "epoch": 0.9616021185038067,
      "grad_norm": 47.40055465698242,
      "learning_rate": 3.878232212269731e-05,
      "loss": 0.7762,
      "step": 5810
    },
    {
      "epoch": 0.9632571996027806,
      "grad_norm": 15.472782135009766,
      "learning_rate": 3.876119655230692e-05,
      "loss": 0.854,
      "step": 5820
    },
    {
      "epoch": 0.9649122807017544,
      "grad_norm": 23.917409896850586,
      "learning_rate": 3.874007098191652e-05,
      "loss": 0.8295,
      "step": 5830
    },
    {
      "epoch": 0.9665673618007282,
      "grad_norm": 24.1065616607666,
      "learning_rate": 3.8718945411526115e-05,
      "loss": 1.3322,
      "step": 5840
    },
    {
      "epoch": 0.968222442899702,
      "grad_norm": 47.54595184326172,
      "learning_rate": 3.8697819841135713e-05,
      "loss": 1.0664,
      "step": 5850
    },
    {
      "epoch": 0.9698775239986759,
      "grad_norm": 44.00333786010742,
      "learning_rate": 3.867669427074531e-05,
      "loss": 0.7677,
      "step": 5860
    },
    {
      "epoch": 0.9715326050976498,
      "grad_norm": 37.38096618652344,
      "learning_rate": 3.865556870035491e-05,
      "loss": 0.6258,
      "step": 5870
    },
    {
      "epoch": 0.9731876861966237,
      "grad_norm": 28.08974266052246,
      "learning_rate": 3.863444312996451e-05,
      "loss": 0.9079,
      "step": 5880
    },
    {
      "epoch": 0.9748427672955975,
      "grad_norm": 37.26587677001953,
      "learning_rate": 3.861331755957411e-05,
      "loss": 0.9559,
      "step": 5890
    },
    {
      "epoch": 0.9764978483945713,
      "grad_norm": 37.71920394897461,
      "learning_rate": 3.859219198918371e-05,
      "loss": 0.8776,
      "step": 5900
    },
    {
      "epoch": 0.9781529294935452,
      "grad_norm": 16.08941650390625,
      "learning_rate": 3.857106641879331e-05,
      "loss": 1.1016,
      "step": 5910
    },
    {
      "epoch": 0.979808010592519,
      "grad_norm": 21.53485107421875,
      "learning_rate": 3.854994084840291e-05,
      "loss": 0.567,
      "step": 5920
    },
    {
      "epoch": 0.9814630916914929,
      "grad_norm": 28.464494705200195,
      "learning_rate": 3.852881527801251e-05,
      "loss": 1.1981,
      "step": 5930
    },
    {
      "epoch": 0.9831181727904668,
      "grad_norm": 44.08273696899414,
      "learning_rate": 3.8507689707622106e-05,
      "loss": 0.8573,
      "step": 5940
    },
    {
      "epoch": 0.9847732538894406,
      "grad_norm": 8.161031723022461,
      "learning_rate": 3.8486564137231704e-05,
      "loss": 0.7857,
      "step": 5950
    },
    {
      "epoch": 0.9864283349884144,
      "grad_norm": 39.70032501220703,
      "learning_rate": 3.84654385668413e-05,
      "loss": 1.0351,
      "step": 5960
    },
    {
      "epoch": 0.9880834160873883,
      "grad_norm": 36.06893539428711,
      "learning_rate": 3.84443129964509e-05,
      "loss": 0.667,
      "step": 5970
    },
    {
      "epoch": 0.9897384971863621,
      "grad_norm": 19.99225425720215,
      "learning_rate": 3.8423187426060506e-05,
      "loss": 0.8651,
      "step": 5980
    },
    {
      "epoch": 0.991393578285336,
      "grad_norm": 27.617633819580078,
      "learning_rate": 3.8402061855670104e-05,
      "loss": 1.0822,
      "step": 5990
    },
    {
      "epoch": 0.9930486593843099,
      "grad_norm": 14.423460006713867,
      "learning_rate": 3.83809362852797e-05,
      "loss": 0.6206,
      "step": 6000
    },
    {
      "epoch": 0.9947037404832837,
      "grad_norm": 95.06928253173828,
      "learning_rate": 3.835981071488931e-05,
      "loss": 0.8463,
      "step": 6010
    },
    {
      "epoch": 0.9963588215822575,
      "grad_norm": 37.445518493652344,
      "learning_rate": 3.8338685144498906e-05,
      "loss": 1.3678,
      "step": 6020
    },
    {
      "epoch": 0.9980139026812314,
      "grad_norm": 27.538061141967773,
      "learning_rate": 3.8317559574108505e-05,
      "loss": 1.0214,
      "step": 6030
    },
    {
      "epoch": 0.9996689837802052,
      "grad_norm": 43.167423248291016,
      "learning_rate": 3.82964340037181e-05,
      "loss": 0.5818,
      "step": 6040
    },
    {
      "epoch": 1.001324064879179,
      "grad_norm": 37.752986907958984,
      "learning_rate": 3.82753084333277e-05,
      "loss": 0.9795,
      "step": 6050
    },
    {
      "epoch": 1.002979145978153,
      "grad_norm": 19.304664611816406,
      "learning_rate": 3.82541828629373e-05,
      "loss": 0.6996,
      "step": 6060
    },
    {
      "epoch": 1.0046342270771267,
      "grad_norm": 12.632060050964355,
      "learning_rate": 3.8233057292546905e-05,
      "loss": 0.727,
      "step": 6070
    },
    {
      "epoch": 1.0062893081761006,
      "grad_norm": 21.13995361328125,
      "learning_rate": 3.8211931722156503e-05,
      "loss": 0.6604,
      "step": 6080
    },
    {
      "epoch": 1.0079443892750746,
      "grad_norm": 45.248504638671875,
      "learning_rate": 3.81908061517661e-05,
      "loss": 0.5215,
      "step": 6090
    },
    {
      "epoch": 1.0095994703740483,
      "grad_norm": 12.257265090942383,
      "learning_rate": 3.81696805813757e-05,
      "loss": 0.7822,
      "step": 6100
    },
    {
      "epoch": 1.0112545514730222,
      "grad_norm": 59.16961669921875,
      "learning_rate": 3.81485550109853e-05,
      "loss": 0.6344,
      "step": 6110
    },
    {
      "epoch": 1.012909632571996,
      "grad_norm": 25.8714656829834,
      "learning_rate": 3.81274294405949e-05,
      "loss": 0.7427,
      "step": 6120
    },
    {
      "epoch": 1.01456471367097,
      "grad_norm": 9.000753402709961,
      "learning_rate": 3.8106303870204495e-05,
      "loss": 0.6498,
      "step": 6130
    },
    {
      "epoch": 1.0162197947699436,
      "grad_norm": 11.507047653198242,
      "learning_rate": 3.8085178299814094e-05,
      "loss": 0.3788,
      "step": 6140
    },
    {
      "epoch": 1.0178748758689176,
      "grad_norm": 54.407169342041016,
      "learning_rate": 3.806405272942369e-05,
      "loss": 0.7636,
      "step": 6150
    },
    {
      "epoch": 1.0195299569678915,
      "grad_norm": 17.4193115234375,
      "learning_rate": 3.80429271590333e-05,
      "loss": 0.9838,
      "step": 6160
    },
    {
      "epoch": 1.0211850380668652,
      "grad_norm": 10.248483657836914,
      "learning_rate": 3.8021801588642896e-05,
      "loss": 0.7732,
      "step": 6170
    },
    {
      "epoch": 1.0228401191658392,
      "grad_norm": 23.52617645263672,
      "learning_rate": 3.8000676018252494e-05,
      "loss": 0.6393,
      "step": 6180
    },
    {
      "epoch": 1.0244952002648129,
      "grad_norm": 12.6254243850708,
      "learning_rate": 3.797955044786209e-05,
      "loss": 0.6813,
      "step": 6190
    },
    {
      "epoch": 1.0261502813637868,
      "grad_norm": 3.0969808101654053,
      "learning_rate": 3.795842487747169e-05,
      "loss": 0.7513,
      "step": 6200
    },
    {
      "epoch": 1.0278053624627608,
      "grad_norm": 27.86314582824707,
      "learning_rate": 3.793729930708129e-05,
      "loss": 0.5865,
      "step": 6210
    },
    {
      "epoch": 1.0294604435617345,
      "grad_norm": 24.43865394592285,
      "learning_rate": 3.791617373669089e-05,
      "loss": 0.5103,
      "step": 6220
    },
    {
      "epoch": 1.0311155246607084,
      "grad_norm": 31.367033004760742,
      "learning_rate": 3.789504816630049e-05,
      "loss": 0.5947,
      "step": 6230
    },
    {
      "epoch": 1.0327706057596822,
      "grad_norm": 17.810279846191406,
      "learning_rate": 3.787392259591009e-05,
      "loss": 1.3212,
      "step": 6240
    },
    {
      "epoch": 1.034425686858656,
      "grad_norm": 41.580692291259766,
      "learning_rate": 3.785279702551969e-05,
      "loss": 1.0531,
      "step": 6250
    },
    {
      "epoch": 1.0360807679576298,
      "grad_norm": 9.995942115783691,
      "learning_rate": 3.7831671455129295e-05,
      "loss": 0.6802,
      "step": 6260
    },
    {
      "epoch": 1.0377358490566038,
      "grad_norm": 47.17183303833008,
      "learning_rate": 3.781054588473889e-05,
      "loss": 0.5955,
      "step": 6270
    },
    {
      "epoch": 1.0393909301555777,
      "grad_norm": 2.97607159614563,
      "learning_rate": 3.778942031434849e-05,
      "loss": 0.5215,
      "step": 6280
    },
    {
      "epoch": 1.0410460112545514,
      "grad_norm": 25.46758460998535,
      "learning_rate": 3.776829474395809e-05,
      "loss": 0.6982,
      "step": 6290
    },
    {
      "epoch": 1.0427010923535254,
      "grad_norm": 23.56912612915039,
      "learning_rate": 3.774716917356769e-05,
      "loss": 0.5638,
      "step": 6300
    },
    {
      "epoch": 1.044356173452499,
      "grad_norm": 52.05971145629883,
      "learning_rate": 3.7726043603177287e-05,
      "loss": 0.7154,
      "step": 6310
    },
    {
      "epoch": 1.046011254551473,
      "grad_norm": 16.270938873291016,
      "learning_rate": 3.7704918032786885e-05,
      "loss": 0.5219,
      "step": 6320
    },
    {
      "epoch": 1.047666335650447,
      "grad_norm": 78.11312103271484,
      "learning_rate": 3.768379246239649e-05,
      "loss": 0.8122,
      "step": 6330
    },
    {
      "epoch": 1.0493214167494207,
      "grad_norm": 18.700693130493164,
      "learning_rate": 3.766266689200609e-05,
      "loss": 0.6804,
      "step": 6340
    },
    {
      "epoch": 1.0509764978483946,
      "grad_norm": 43.23332214355469,
      "learning_rate": 3.764154132161569e-05,
      "loss": 0.499,
      "step": 6350
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 74.5335693359375,
      "learning_rate": 3.7620415751225285e-05,
      "loss": 0.7657,
      "step": 6360
    },
    {
      "epoch": 1.0542866600463423,
      "grad_norm": 14.923569679260254,
      "learning_rate": 3.7599290180834884e-05,
      "loss": 0.5859,
      "step": 6370
    },
    {
      "epoch": 1.055941741145316,
      "grad_norm": 15.170279502868652,
      "learning_rate": 3.757816461044448e-05,
      "loss": 0.6257,
      "step": 6380
    },
    {
      "epoch": 1.05759682224429,
      "grad_norm": 17.125713348388672,
      "learning_rate": 3.755703904005408e-05,
      "loss": 0.6472,
      "step": 6390
    },
    {
      "epoch": 1.059251903343264,
      "grad_norm": 50.579566955566406,
      "learning_rate": 3.753591346966368e-05,
      "loss": 0.4811,
      "step": 6400
    },
    {
      "epoch": 1.0609069844422376,
      "grad_norm": 8.866579055786133,
      "learning_rate": 3.7514787899273284e-05,
      "loss": 0.6102,
      "step": 6410
    },
    {
      "epoch": 1.0625620655412116,
      "grad_norm": 51.77092742919922,
      "learning_rate": 3.749366232888288e-05,
      "loss": 0.6753,
      "step": 6420
    },
    {
      "epoch": 1.0642171466401853,
      "grad_norm": 40.88679122924805,
      "learning_rate": 3.747253675849248e-05,
      "loss": 0.7229,
      "step": 6430
    },
    {
      "epoch": 1.0658722277391592,
      "grad_norm": 39.0700569152832,
      "learning_rate": 3.745141118810208e-05,
      "loss": 0.5696,
      "step": 6440
    },
    {
      "epoch": 1.067527308838133,
      "grad_norm": 102.9173355102539,
      "learning_rate": 3.743028561771168e-05,
      "loss": 0.702,
      "step": 6450
    },
    {
      "epoch": 1.069182389937107,
      "grad_norm": 74.14219665527344,
      "learning_rate": 3.7409160047321276e-05,
      "loss": 0.7574,
      "step": 6460
    },
    {
      "epoch": 1.0708374710360808,
      "grad_norm": 45.568145751953125,
      "learning_rate": 3.738803447693088e-05,
      "loss": 0.6362,
      "step": 6470
    },
    {
      "epoch": 1.0724925521350546,
      "grad_norm": 27.640533447265625,
      "learning_rate": 3.736690890654048e-05,
      "loss": 0.6337,
      "step": 6480
    },
    {
      "epoch": 1.0741476332340285,
      "grad_norm": 34.935264587402344,
      "learning_rate": 3.734578333615008e-05,
      "loss": 0.6704,
      "step": 6490
    },
    {
      "epoch": 1.0758027143330022,
      "grad_norm": 14.070734024047852,
      "learning_rate": 3.732465776575968e-05,
      "loss": 0.5193,
      "step": 6500
    },
    {
      "epoch": 1.0774577954319762,
      "grad_norm": 17.43857192993164,
      "learning_rate": 3.730353219536928e-05,
      "loss": 0.7861,
      "step": 6510
    },
    {
      "epoch": 1.0791128765309501,
      "grad_norm": 22.43534278869629,
      "learning_rate": 3.728240662497888e-05,
      "loss": 0.4731,
      "step": 6520
    },
    {
      "epoch": 1.0807679576299238,
      "grad_norm": 43.19188690185547,
      "learning_rate": 3.726128105458848e-05,
      "loss": 0.4583,
      "step": 6530
    },
    {
      "epoch": 1.0824230387288978,
      "grad_norm": 6.877590656280518,
      "learning_rate": 3.7240155484198076e-05,
      "loss": 0.4702,
      "step": 6540
    },
    {
      "epoch": 1.0840781198278715,
      "grad_norm": 16.909996032714844,
      "learning_rate": 3.7219029913807675e-05,
      "loss": 0.7783,
      "step": 6550
    },
    {
      "epoch": 1.0857332009268454,
      "grad_norm": 56.77177810668945,
      "learning_rate": 3.719790434341727e-05,
      "loss": 1.0009,
      "step": 6560
    },
    {
      "epoch": 1.0873882820258192,
      "grad_norm": 8.679547309875488,
      "learning_rate": 3.717677877302687e-05,
      "loss": 0.5264,
      "step": 6570
    },
    {
      "epoch": 1.089043363124793,
      "grad_norm": 19.298259735107422,
      "learning_rate": 3.715565320263648e-05,
      "loss": 0.573,
      "step": 6580
    },
    {
      "epoch": 1.090698444223767,
      "grad_norm": 20.778139114379883,
      "learning_rate": 3.7134527632246075e-05,
      "loss": 0.6305,
      "step": 6590
    },
    {
      "epoch": 1.0923535253227408,
      "grad_norm": 41.1273193359375,
      "learning_rate": 3.7113402061855674e-05,
      "loss": 0.6069,
      "step": 6600
    },
    {
      "epoch": 1.0940086064217147,
      "grad_norm": 64.73125457763672,
      "learning_rate": 3.709227649146527e-05,
      "loss": 0.6787,
      "step": 6610
    },
    {
      "epoch": 1.0956636875206884,
      "grad_norm": 1.799364686012268,
      "learning_rate": 3.707115092107487e-05,
      "loss": 0.3908,
      "step": 6620
    },
    {
      "epoch": 1.0973187686196624,
      "grad_norm": 43.76002883911133,
      "learning_rate": 3.705002535068447e-05,
      "loss": 0.6485,
      "step": 6630
    },
    {
      "epoch": 1.0989738497186363,
      "grad_norm": 229.66001892089844,
      "learning_rate": 3.702889978029407e-05,
      "loss": 0.9455,
      "step": 6640
    },
    {
      "epoch": 1.10062893081761,
      "grad_norm": 27.529861450195312,
      "learning_rate": 3.7007774209903665e-05,
      "loss": 0.5003,
      "step": 6650
    },
    {
      "epoch": 1.102284011916584,
      "grad_norm": 18.008073806762695,
      "learning_rate": 3.6986648639513264e-05,
      "loss": 0.6389,
      "step": 6660
    },
    {
      "epoch": 1.1039390930155577,
      "grad_norm": 23.12146759033203,
      "learning_rate": 3.696552306912287e-05,
      "loss": 0.7457,
      "step": 6670
    },
    {
      "epoch": 1.1055941741145316,
      "grad_norm": 34.19845962524414,
      "learning_rate": 3.694439749873247e-05,
      "loss": 0.7756,
      "step": 6680
    },
    {
      "epoch": 1.1072492552135054,
      "grad_norm": 25.23284912109375,
      "learning_rate": 3.6923271928342066e-05,
      "loss": 0.7626,
      "step": 6690
    },
    {
      "epoch": 1.1089043363124793,
      "grad_norm": 14.101091384887695,
      "learning_rate": 3.6902146357951664e-05,
      "loss": 0.3924,
      "step": 6700
    },
    {
      "epoch": 1.1105594174114533,
      "grad_norm": 51.01255798339844,
      "learning_rate": 3.688102078756126e-05,
      "loss": 0.7922,
      "step": 6710
    },
    {
      "epoch": 1.112214498510427,
      "grad_norm": 16.10783576965332,
      "learning_rate": 3.685989521717087e-05,
      "loss": 0.8373,
      "step": 6720
    },
    {
      "epoch": 1.113869579609401,
      "grad_norm": 31.584129333496094,
      "learning_rate": 3.6838769646780466e-05,
      "loss": 0.6722,
      "step": 6730
    },
    {
      "epoch": 1.1155246607083746,
      "grad_norm": 32.611385345458984,
      "learning_rate": 3.6817644076390064e-05,
      "loss": 0.5653,
      "step": 6740
    },
    {
      "epoch": 1.1171797418073486,
      "grad_norm": 17.086959838867188,
      "learning_rate": 3.679651850599967e-05,
      "loss": 0.6537,
      "step": 6750
    },
    {
      "epoch": 1.1188348229063223,
      "grad_norm": 56.85791015625,
      "learning_rate": 3.677539293560927e-05,
      "loss": 0.7826,
      "step": 6760
    },
    {
      "epoch": 1.1204899040052962,
      "grad_norm": 19.696575164794922,
      "learning_rate": 3.6754267365218866e-05,
      "loss": 0.5748,
      "step": 6770
    },
    {
      "epoch": 1.1221449851042702,
      "grad_norm": 44.26768112182617,
      "learning_rate": 3.6733141794828465e-05,
      "loss": 0.5986,
      "step": 6780
    },
    {
      "epoch": 1.123800066203244,
      "grad_norm": 21.24663543701172,
      "learning_rate": 3.671201622443806e-05,
      "loss": 0.4471,
      "step": 6790
    },
    {
      "epoch": 1.1254551473022179,
      "grad_norm": 30.505199432373047,
      "learning_rate": 3.669089065404766e-05,
      "loss": 0.5534,
      "step": 6800
    },
    {
      "epoch": 1.1271102284011916,
      "grad_norm": 12.979656219482422,
      "learning_rate": 3.666976508365726e-05,
      "loss": 0.5924,
      "step": 6810
    },
    {
      "epoch": 1.1287653095001655,
      "grad_norm": 26.43694305419922,
      "learning_rate": 3.664863951326686e-05,
      "loss": 0.7999,
      "step": 6820
    },
    {
      "epoch": 1.1304203905991392,
      "grad_norm": 43.48200607299805,
      "learning_rate": 3.662751394287646e-05,
      "loss": 0.7086,
      "step": 6830
    },
    {
      "epoch": 1.1320754716981132,
      "grad_norm": 35.45294952392578,
      "learning_rate": 3.660638837248606e-05,
      "loss": 0.6338,
      "step": 6840
    },
    {
      "epoch": 1.1337305527970871,
      "grad_norm": 8.988092422485352,
      "learning_rate": 3.658526280209566e-05,
      "loss": 0.768,
      "step": 6850
    },
    {
      "epoch": 1.1353856338960608,
      "grad_norm": 33.509521484375,
      "learning_rate": 3.656413723170526e-05,
      "loss": 0.5208,
      "step": 6860
    },
    {
      "epoch": 1.1370407149950348,
      "grad_norm": 33.20020294189453,
      "learning_rate": 3.654301166131486e-05,
      "loss": 0.7413,
      "step": 6870
    },
    {
      "epoch": 1.1386957960940087,
      "grad_norm": 21.984722137451172,
      "learning_rate": 3.6521886090924455e-05,
      "loss": 0.682,
      "step": 6880
    },
    {
      "epoch": 1.1403508771929824,
      "grad_norm": 15.980286598205566,
      "learning_rate": 3.6500760520534054e-05,
      "loss": 0.6417,
      "step": 6890
    },
    {
      "epoch": 1.1420059582919564,
      "grad_norm": 21.761869430541992,
      "learning_rate": 3.647963495014365e-05,
      "loss": 0.7434,
      "step": 6900
    },
    {
      "epoch": 1.1436610393909301,
      "grad_norm": 9.546459197998047,
      "learning_rate": 3.645850937975325e-05,
      "loss": 0.5317,
      "step": 6910
    },
    {
      "epoch": 1.145316120489904,
      "grad_norm": 42.5108528137207,
      "learning_rate": 3.6437383809362856e-05,
      "loss": 0.8655,
      "step": 6920
    },
    {
      "epoch": 1.1469712015888778,
      "grad_norm": 31.79107093811035,
      "learning_rate": 3.6416258238972454e-05,
      "loss": 0.9241,
      "step": 6930
    },
    {
      "epoch": 1.1486262826878517,
      "grad_norm": 28.168354034423828,
      "learning_rate": 3.639513266858205e-05,
      "loss": 0.426,
      "step": 6940
    },
    {
      "epoch": 1.1502813637868257,
      "grad_norm": 26.191940307617188,
      "learning_rate": 3.637400709819165e-05,
      "loss": 0.641,
      "step": 6950
    },
    {
      "epoch": 1.1519364448857994,
      "grad_norm": 26.523212432861328,
      "learning_rate": 3.635288152780125e-05,
      "loss": 1.1123,
      "step": 6960
    },
    {
      "epoch": 1.1535915259847733,
      "grad_norm": 18.408836364746094,
      "learning_rate": 3.6331755957410854e-05,
      "loss": 0.514,
      "step": 6970
    },
    {
      "epoch": 1.155246607083747,
      "grad_norm": 33.0765380859375,
      "learning_rate": 3.631063038702045e-05,
      "loss": 0.6331,
      "step": 6980
    },
    {
      "epoch": 1.156901688182721,
      "grad_norm": 11.573594093322754,
      "learning_rate": 3.628950481663005e-05,
      "loss": 0.4605,
      "step": 6990
    },
    {
      "epoch": 1.1585567692816947,
      "grad_norm": 111.41014099121094,
      "learning_rate": 3.626837924623965e-05,
      "loss": 0.6089,
      "step": 7000
    },
    {
      "epoch": 1.1602118503806687,
      "grad_norm": 81.26359558105469,
      "learning_rate": 3.6247253675849255e-05,
      "loss": 0.7777,
      "step": 7010
    },
    {
      "epoch": 1.1618669314796426,
      "grad_norm": 46.88666915893555,
      "learning_rate": 3.622612810545885e-05,
      "loss": 0.8033,
      "step": 7020
    },
    {
      "epoch": 1.1635220125786163,
      "grad_norm": 46.20853805541992,
      "learning_rate": 3.620500253506845e-05,
      "loss": 0.7225,
      "step": 7030
    },
    {
      "epoch": 1.1651770936775903,
      "grad_norm": 25.22488021850586,
      "learning_rate": 3.618387696467805e-05,
      "loss": 0.5266,
      "step": 7040
    },
    {
      "epoch": 1.166832174776564,
      "grad_norm": 14.771267890930176,
      "learning_rate": 3.616275139428765e-05,
      "loss": 0.4356,
      "step": 7050
    },
    {
      "epoch": 1.168487255875538,
      "grad_norm": 30.170421600341797,
      "learning_rate": 3.614162582389725e-05,
      "loss": 0.777,
      "step": 7060
    },
    {
      "epoch": 1.1701423369745116,
      "grad_norm": 68.53997802734375,
      "learning_rate": 3.6120500253506845e-05,
      "loss": 0.8352,
      "step": 7070
    },
    {
      "epoch": 1.1717974180734856,
      "grad_norm": 52.23305892944336,
      "learning_rate": 3.6099374683116443e-05,
      "loss": 0.843,
      "step": 7080
    },
    {
      "epoch": 1.1734524991724595,
      "grad_norm": 28.146604537963867,
      "learning_rate": 3.607824911272605e-05,
      "loss": 0.475,
      "step": 7090
    },
    {
      "epoch": 1.1751075802714332,
      "grad_norm": 49.62084197998047,
      "learning_rate": 3.605712354233565e-05,
      "loss": 0.6228,
      "step": 7100
    },
    {
      "epoch": 1.1767626613704072,
      "grad_norm": 57.373199462890625,
      "learning_rate": 3.6035997971945245e-05,
      "loss": 0.7248,
      "step": 7110
    },
    {
      "epoch": 1.178417742469381,
      "grad_norm": 26.766395568847656,
      "learning_rate": 3.6014872401554844e-05,
      "loss": 0.7371,
      "step": 7120
    },
    {
      "epoch": 1.1800728235683549,
      "grad_norm": 62.24235153198242,
      "learning_rate": 3.599374683116444e-05,
      "loss": 0.6198,
      "step": 7130
    },
    {
      "epoch": 1.1817279046673286,
      "grad_norm": 31.707149505615234,
      "learning_rate": 3.597262126077404e-05,
      "loss": 0.6986,
      "step": 7140
    },
    {
      "epoch": 1.1833829857663025,
      "grad_norm": 29.700735092163086,
      "learning_rate": 3.595149569038364e-05,
      "loss": 0.7544,
      "step": 7150
    },
    {
      "epoch": 1.1850380668652765,
      "grad_norm": 20.38401222229004,
      "learning_rate": 3.593037011999324e-05,
      "loss": 0.7615,
      "step": 7160
    },
    {
      "epoch": 1.1866931479642502,
      "grad_norm": 12.296006202697754,
      "learning_rate": 3.5909244549602836e-05,
      "loss": 0.7073,
      "step": 7170
    },
    {
      "epoch": 1.1883482290632241,
      "grad_norm": 20.974931716918945,
      "learning_rate": 3.588811897921244e-05,
      "loss": 0.5321,
      "step": 7180
    },
    {
      "epoch": 1.190003310162198,
      "grad_norm": 5.526302337646484,
      "learning_rate": 3.586699340882204e-05,
      "loss": 0.4475,
      "step": 7190
    },
    {
      "epoch": 1.1916583912611718,
      "grad_norm": 12.04627799987793,
      "learning_rate": 3.584586783843164e-05,
      "loss": 0.3978,
      "step": 7200
    },
    {
      "epoch": 1.1933134723601457,
      "grad_norm": 28.984127044677734,
      "learning_rate": 3.5824742268041236e-05,
      "loss": 0.7094,
      "step": 7210
    },
    {
      "epoch": 1.1949685534591195,
      "grad_norm": 11.149456024169922,
      "learning_rate": 3.580361669765084e-05,
      "loss": 0.5015,
      "step": 7220
    },
    {
      "epoch": 1.1966236345580934,
      "grad_norm": 47.32598114013672,
      "learning_rate": 3.578249112726044e-05,
      "loss": 0.8053,
      "step": 7230
    },
    {
      "epoch": 1.1982787156570671,
      "grad_norm": 28.34724998474121,
      "learning_rate": 3.576136555687004e-05,
      "loss": 0.773,
      "step": 7240
    },
    {
      "epoch": 1.199933796756041,
      "grad_norm": 49.34343719482422,
      "learning_rate": 3.5740239986479636e-05,
      "loss": 0.6888,
      "step": 7250
    }
  ],
  "logging_steps": 10,
  "max_steps": 24168,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 2417,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.386964907838464e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
