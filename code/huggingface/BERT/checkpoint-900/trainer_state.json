{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.3003003003003003,
  "eval_steps": 500,
  "global_step": 900,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00333667000333667,
      "grad_norm": 8.857905387878418,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 4.4678,
      "step": 10
    },
    {
      "epoch": 0.00667334000667334,
      "grad_norm": 8.59739875793457,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 3.9352,
      "step": 20
    },
    {
      "epoch": 0.01001001001001001,
      "grad_norm": 8.053893089294434,
      "learning_rate": 3e-06,
      "loss": 3.9121,
      "step": 30
    },
    {
      "epoch": 0.01334668001334668,
      "grad_norm": 7.265538215637207,
      "learning_rate": 4.000000000000001e-06,
      "loss": 3.3505,
      "step": 40
    },
    {
      "epoch": 0.01668335001668335,
      "grad_norm": 8.49083137512207,
      "learning_rate": 5e-06,
      "loss": 3.7749,
      "step": 50
    },
    {
      "epoch": 0.02002002002002002,
      "grad_norm": 11.937213897705078,
      "learning_rate": 6e-06,
      "loss": 4.3121,
      "step": 60
    },
    {
      "epoch": 0.02335669002335669,
      "grad_norm": 6.517889976501465,
      "learning_rate": 7.000000000000001e-06,
      "loss": 3.7168,
      "step": 70
    },
    {
      "epoch": 0.02669336002669336,
      "grad_norm": 7.611666202545166,
      "learning_rate": 8.000000000000001e-06,
      "loss": 3.3343,
      "step": 80
    },
    {
      "epoch": 0.03003003003003003,
      "grad_norm": 11.604483604431152,
      "learning_rate": 9e-06,
      "loss": 3.3564,
      "step": 90
    },
    {
      "epoch": 0.0333667000333667,
      "grad_norm": 5.543286323547363,
      "learning_rate": 1e-05,
      "loss": 2.8424,
      "step": 100
    },
    {
      "epoch": 0.03670337003670337,
      "grad_norm": 9.923837661743164,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 3.0617,
      "step": 110
    },
    {
      "epoch": 0.04004004004004004,
      "grad_norm": 8.410821914672852,
      "learning_rate": 1.2e-05,
      "loss": 3.654,
      "step": 120
    },
    {
      "epoch": 0.04337671004337671,
      "grad_norm": 7.855680465698242,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 3.8245,
      "step": 130
    },
    {
      "epoch": 0.04671338004671338,
      "grad_norm": 7.0160417556762695,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 3.0403,
      "step": 140
    },
    {
      "epoch": 0.05005005005005005,
      "grad_norm": 8.539226531982422,
      "learning_rate": 1.5e-05,
      "loss": 4.0287,
      "step": 150
    },
    {
      "epoch": 0.05338672005338672,
      "grad_norm": 7.546672821044922,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 4.2148,
      "step": 160
    },
    {
      "epoch": 0.05672339005672339,
      "grad_norm": 9.074827194213867,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 3.3234,
      "step": 170
    },
    {
      "epoch": 0.06006006006006006,
      "grad_norm": 8.012894630432129,
      "learning_rate": 1.8e-05,
      "loss": 3.4108,
      "step": 180
    },
    {
      "epoch": 0.06339673006339673,
      "grad_norm": 7.305278301239014,
      "learning_rate": 1.9e-05,
      "loss": 3.8104,
      "step": 190
    },
    {
      "epoch": 0.0667334000667334,
      "grad_norm": 9.536160469055176,
      "learning_rate": 2e-05,
      "loss": 2.6948,
      "step": 200
    },
    {
      "epoch": 0.07007007007007007,
      "grad_norm": 7.950663089752197,
      "learning_rate": 2.1e-05,
      "loss": 3.3356,
      "step": 210
    },
    {
      "epoch": 0.07340674007340674,
      "grad_norm": 7.788869380950928,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 2.2596,
      "step": 220
    },
    {
      "epoch": 0.07674341007674342,
      "grad_norm": 7.822196006774902,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 2.5057,
      "step": 230
    },
    {
      "epoch": 0.08008008008008008,
      "grad_norm": 7.548488140106201,
      "learning_rate": 2.4e-05,
      "loss": 2.6202,
      "step": 240
    },
    {
      "epoch": 0.08341675008341674,
      "grad_norm": 6.796125888824463,
      "learning_rate": 2.5e-05,
      "loss": 1.9175,
      "step": 250
    },
    {
      "epoch": 0.08675342008675342,
      "grad_norm": 7.555082321166992,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.7628,
      "step": 260
    },
    {
      "epoch": 0.09009009009009009,
      "grad_norm": 6.521301746368408,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 2.0625,
      "step": 270
    },
    {
      "epoch": 0.09342676009342676,
      "grad_norm": 5.06267786026001,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.8207,
      "step": 280
    },
    {
      "epoch": 0.09676343009676343,
      "grad_norm": 6.200467586517334,
      "learning_rate": 2.9e-05,
      "loss": 1.9648,
      "step": 290
    },
    {
      "epoch": 0.1001001001001001,
      "grad_norm": 5.347590446472168,
      "learning_rate": 3e-05,
      "loss": 1.5949,
      "step": 300
    },
    {
      "epoch": 0.10343677010343677,
      "grad_norm": 2.64927077293396,
      "learning_rate": 3.1e-05,
      "loss": 1.5226,
      "step": 310
    },
    {
      "epoch": 0.10677344010677343,
      "grad_norm": 3.8787083625793457,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.8517,
      "step": 320
    },
    {
      "epoch": 0.11011011011011011,
      "grad_norm": 5.351945400238037,
      "learning_rate": 3.3e-05,
      "loss": 1.9147,
      "step": 330
    },
    {
      "epoch": 0.11344678011344678,
      "grad_norm": 4.14184045791626,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.732,
      "step": 340
    },
    {
      "epoch": 0.11678345011678345,
      "grad_norm": 5.556326389312744,
      "learning_rate": 3.5e-05,
      "loss": 1.5702,
      "step": 350
    },
    {
      "epoch": 0.12012012012012012,
      "grad_norm": 4.808996200561523,
      "learning_rate": 3.6e-05,
      "loss": 1.4381,
      "step": 360
    },
    {
      "epoch": 0.12345679012345678,
      "grad_norm": 5.18758487701416,
      "learning_rate": 3.7e-05,
      "loss": 1.6771,
      "step": 370
    },
    {
      "epoch": 0.12679346012679346,
      "grad_norm": 4.98621940612793,
      "learning_rate": 3.8e-05,
      "loss": 1.6097,
      "step": 380
    },
    {
      "epoch": 0.13013013013013014,
      "grad_norm": 4.125382423400879,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.383,
      "step": 390
    },
    {
      "epoch": 0.1334668001334668,
      "grad_norm": 4.260393142700195,
      "learning_rate": 4e-05,
      "loss": 1.4135,
      "step": 400
    },
    {
      "epoch": 0.13680347013680347,
      "grad_norm": 4.048669338226318,
      "learning_rate": 4.1e-05,
      "loss": 1.5159,
      "step": 410
    },
    {
      "epoch": 0.14014014014014015,
      "grad_norm": 4.261200428009033,
      "learning_rate": 4.2e-05,
      "loss": 1.3645,
      "step": 420
    },
    {
      "epoch": 0.14347681014347682,
      "grad_norm": 4.28817892074585,
      "learning_rate": 4.3e-05,
      "loss": 1.5483,
      "step": 430
    },
    {
      "epoch": 0.14681348014681347,
      "grad_norm": 3.5364677906036377,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.4444,
      "step": 440
    },
    {
      "epoch": 0.15015015015015015,
      "grad_norm": 3.8340559005737305,
      "learning_rate": 4.5e-05,
      "loss": 1.2625,
      "step": 450
    },
    {
      "epoch": 0.15348682015348683,
      "grad_norm": 5.591009616851807,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.3474,
      "step": 460
    },
    {
      "epoch": 0.15682349015682348,
      "grad_norm": 4.024402141571045,
      "learning_rate": 4.7e-05,
      "loss": 1.1617,
      "step": 470
    },
    {
      "epoch": 0.16016016016016016,
      "grad_norm": 5.415898323059082,
      "learning_rate": 4.8e-05,
      "loss": 1.7149,
      "step": 480
    },
    {
      "epoch": 0.16349683016349684,
      "grad_norm": 4.3966264724731445,
      "learning_rate": 4.9e-05,
      "loss": 1.3024,
      "step": 490
    },
    {
      "epoch": 0.1668335001668335,
      "grad_norm": 4.0056047439575195,
      "learning_rate": 5e-05,
      "loss": 1.5646,
      "step": 500
    },
    {
      "epoch": 0.17017017017017017,
      "grad_norm": 4.605757236480713,
      "learning_rate": 4.9941114120833824e-05,
      "loss": 1.4762,
      "step": 510
    },
    {
      "epoch": 0.17350684017350684,
      "grad_norm": 2.5374536514282227,
      "learning_rate": 4.988222824166765e-05,
      "loss": 1.1877,
      "step": 520
    },
    {
      "epoch": 0.17684351017684352,
      "grad_norm": 3.0233702659606934,
      "learning_rate": 4.9823342362501474e-05,
      "loss": 1.2266,
      "step": 530
    },
    {
      "epoch": 0.18018018018018017,
      "grad_norm": 4.547967433929443,
      "learning_rate": 4.9764456483335296e-05,
      "loss": 1.3976,
      "step": 540
    },
    {
      "epoch": 0.18351685018351685,
      "grad_norm": 3.5690975189208984,
      "learning_rate": 4.9705570604169125e-05,
      "loss": 1.387,
      "step": 550
    },
    {
      "epoch": 0.18685352018685353,
      "grad_norm": 6.602469444274902,
      "learning_rate": 4.9646684725002947e-05,
      "loss": 1.3689,
      "step": 560
    },
    {
      "epoch": 0.19019019019019018,
      "grad_norm": 6.749355316162109,
      "learning_rate": 4.958779884583677e-05,
      "loss": 1.2421,
      "step": 570
    },
    {
      "epoch": 0.19352686019352686,
      "grad_norm": 2.9303011894226074,
      "learning_rate": 4.952891296667059e-05,
      "loss": 1.0158,
      "step": 580
    },
    {
      "epoch": 0.19686353019686353,
      "grad_norm": 5.631228446960449,
      "learning_rate": 4.947002708750442e-05,
      "loss": 1.295,
      "step": 590
    },
    {
      "epoch": 0.2002002002002002,
      "grad_norm": 4.6857991218566895,
      "learning_rate": 4.941114120833824e-05,
      "loss": 1.3221,
      "step": 600
    },
    {
      "epoch": 0.20353687020353686,
      "grad_norm": 4.097028732299805,
      "learning_rate": 4.935225532917206e-05,
      "loss": 1.3916,
      "step": 610
    },
    {
      "epoch": 0.20687354020687354,
      "grad_norm": 3.8089182376861572,
      "learning_rate": 4.929336945000589e-05,
      "loss": 1.2672,
      "step": 620
    },
    {
      "epoch": 0.21021021021021022,
      "grad_norm": 3.4777469635009766,
      "learning_rate": 4.923448357083971e-05,
      "loss": 1.1686,
      "step": 630
    },
    {
      "epoch": 0.21354688021354687,
      "grad_norm": 6.217365741729736,
      "learning_rate": 4.917559769167354e-05,
      "loss": 1.3046,
      "step": 640
    },
    {
      "epoch": 0.21688355021688355,
      "grad_norm": 2.9496960639953613,
      "learning_rate": 4.911671181250736e-05,
      "loss": 1.1695,
      "step": 650
    },
    {
      "epoch": 0.22022022022022023,
      "grad_norm": 3.933528423309326,
      "learning_rate": 4.905782593334119e-05,
      "loss": 1.2356,
      "step": 660
    },
    {
      "epoch": 0.2235568902235569,
      "grad_norm": 3.9220006465911865,
      "learning_rate": 4.899894005417501e-05,
      "loss": 1.1246,
      "step": 670
    },
    {
      "epoch": 0.22689356022689355,
      "grad_norm": 7.431880474090576,
      "learning_rate": 4.8940054175008835e-05,
      "loss": 1.0677,
      "step": 680
    },
    {
      "epoch": 0.23023023023023023,
      "grad_norm": 4.475502014160156,
      "learning_rate": 4.8881168295842663e-05,
      "loss": 1.3185,
      "step": 690
    },
    {
      "epoch": 0.2335669002335669,
      "grad_norm": 5.457873344421387,
      "learning_rate": 4.8822282416676485e-05,
      "loss": 1.2404,
      "step": 700
    },
    {
      "epoch": 0.23690357023690356,
      "grad_norm": 5.106561183929443,
      "learning_rate": 4.876339653751031e-05,
      "loss": 1.4241,
      "step": 710
    },
    {
      "epoch": 0.24024024024024024,
      "grad_norm": 6.592267036437988,
      "learning_rate": 4.8704510658344136e-05,
      "loss": 1.366,
      "step": 720
    },
    {
      "epoch": 0.24357691024357692,
      "grad_norm": 3.60618257522583,
      "learning_rate": 4.864562477917796e-05,
      "loss": 1.1119,
      "step": 730
    },
    {
      "epoch": 0.24691358024691357,
      "grad_norm": 3.0848686695098877,
      "learning_rate": 4.858673890001178e-05,
      "loss": 1.0512,
      "step": 740
    },
    {
      "epoch": 0.2502502502502503,
      "grad_norm": 4.673299789428711,
      "learning_rate": 4.85278530208456e-05,
      "loss": 1.2696,
      "step": 750
    },
    {
      "epoch": 0.2535869202535869,
      "grad_norm": 4.895904064178467,
      "learning_rate": 4.846896714167943e-05,
      "loss": 1.1805,
      "step": 760
    },
    {
      "epoch": 0.2569235902569236,
      "grad_norm": 4.752594947814941,
      "learning_rate": 4.841008126251325e-05,
      "loss": 1.1475,
      "step": 770
    },
    {
      "epoch": 0.2602602602602603,
      "grad_norm": 4.059188365936279,
      "learning_rate": 4.835119538334707e-05,
      "loss": 1.2825,
      "step": 780
    },
    {
      "epoch": 0.26359693026359693,
      "grad_norm": 3.561336040496826,
      "learning_rate": 4.82923095041809e-05,
      "loss": 1.1613,
      "step": 790
    },
    {
      "epoch": 0.2669336002669336,
      "grad_norm": 4.691658973693848,
      "learning_rate": 4.823342362501472e-05,
      "loss": 1.5574,
      "step": 800
    },
    {
      "epoch": 0.2702702702702703,
      "grad_norm": 3.560206413269043,
      "learning_rate": 4.8174537745848545e-05,
      "loss": 1.212,
      "step": 810
    },
    {
      "epoch": 0.27360694027360694,
      "grad_norm": 4.498392581939697,
      "learning_rate": 4.8115651866682374e-05,
      "loss": 1.0687,
      "step": 820
    },
    {
      "epoch": 0.2769436102769436,
      "grad_norm": 3.535398483276367,
      "learning_rate": 4.8056765987516195e-05,
      "loss": 1.2544,
      "step": 830
    },
    {
      "epoch": 0.2802802802802803,
      "grad_norm": 5.058297157287598,
      "learning_rate": 4.799788010835002e-05,
      "loss": 1.4341,
      "step": 840
    },
    {
      "epoch": 0.28361695028361694,
      "grad_norm": 3.874671697616577,
      "learning_rate": 4.7938994229183846e-05,
      "loss": 1.1299,
      "step": 850
    },
    {
      "epoch": 0.28695362028695365,
      "grad_norm": 5.592775821685791,
      "learning_rate": 4.788010835001767e-05,
      "loss": 1.0973,
      "step": 860
    },
    {
      "epoch": 0.2902902902902903,
      "grad_norm": 3.8135714530944824,
      "learning_rate": 4.782122247085149e-05,
      "loss": 1.1867,
      "step": 870
    },
    {
      "epoch": 0.29362696029362695,
      "grad_norm": 5.178786754608154,
      "learning_rate": 4.776233659168531e-05,
      "loss": 1.1778,
      "step": 880
    },
    {
      "epoch": 0.29696363029696365,
      "grad_norm": 3.325193166732788,
      "learning_rate": 4.770345071251914e-05,
      "loss": 1.022,
      "step": 890
    },
    {
      "epoch": 0.3003003003003003,
      "grad_norm": 3.699855089187622,
      "learning_rate": 4.764456483335296e-05,
      "loss": 1.3642,
      "step": 900
    }
  ],
  "logging_steps": 10,
  "max_steps": 8991,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 900,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3801811186483200.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
