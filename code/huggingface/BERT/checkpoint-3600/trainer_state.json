{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.2012012012012012,
  "eval_steps": 500,
  "global_step": 3600,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00333667000333667,
      "grad_norm": 8.857905387878418,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 4.4678,
      "step": 10
    },
    {
      "epoch": 0.00667334000667334,
      "grad_norm": 8.59739875793457,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 3.9352,
      "step": 20
    },
    {
      "epoch": 0.01001001001001001,
      "grad_norm": 8.053893089294434,
      "learning_rate": 3e-06,
      "loss": 3.9121,
      "step": 30
    },
    {
      "epoch": 0.01334668001334668,
      "grad_norm": 7.265538215637207,
      "learning_rate": 4.000000000000001e-06,
      "loss": 3.3505,
      "step": 40
    },
    {
      "epoch": 0.01668335001668335,
      "grad_norm": 8.49083137512207,
      "learning_rate": 5e-06,
      "loss": 3.7749,
      "step": 50
    },
    {
      "epoch": 0.02002002002002002,
      "grad_norm": 11.937213897705078,
      "learning_rate": 6e-06,
      "loss": 4.3121,
      "step": 60
    },
    {
      "epoch": 0.02335669002335669,
      "grad_norm": 6.517889976501465,
      "learning_rate": 7.000000000000001e-06,
      "loss": 3.7168,
      "step": 70
    },
    {
      "epoch": 0.02669336002669336,
      "grad_norm": 7.611666202545166,
      "learning_rate": 8.000000000000001e-06,
      "loss": 3.3343,
      "step": 80
    },
    {
      "epoch": 0.03003003003003003,
      "grad_norm": 11.604483604431152,
      "learning_rate": 9e-06,
      "loss": 3.3564,
      "step": 90
    },
    {
      "epoch": 0.0333667000333667,
      "grad_norm": 5.543286323547363,
      "learning_rate": 1e-05,
      "loss": 2.8424,
      "step": 100
    },
    {
      "epoch": 0.03670337003670337,
      "grad_norm": 9.923837661743164,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 3.0617,
      "step": 110
    },
    {
      "epoch": 0.04004004004004004,
      "grad_norm": 8.410821914672852,
      "learning_rate": 1.2e-05,
      "loss": 3.654,
      "step": 120
    },
    {
      "epoch": 0.04337671004337671,
      "grad_norm": 7.855680465698242,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 3.8245,
      "step": 130
    },
    {
      "epoch": 0.04671338004671338,
      "grad_norm": 7.0160417556762695,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 3.0403,
      "step": 140
    },
    {
      "epoch": 0.05005005005005005,
      "grad_norm": 8.539226531982422,
      "learning_rate": 1.5e-05,
      "loss": 4.0287,
      "step": 150
    },
    {
      "epoch": 0.05338672005338672,
      "grad_norm": 7.546672821044922,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 4.2148,
      "step": 160
    },
    {
      "epoch": 0.05672339005672339,
      "grad_norm": 9.074827194213867,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 3.3234,
      "step": 170
    },
    {
      "epoch": 0.06006006006006006,
      "grad_norm": 8.012894630432129,
      "learning_rate": 1.8e-05,
      "loss": 3.4108,
      "step": 180
    },
    {
      "epoch": 0.06339673006339673,
      "grad_norm": 7.305278301239014,
      "learning_rate": 1.9e-05,
      "loss": 3.8104,
      "step": 190
    },
    {
      "epoch": 0.0667334000667334,
      "grad_norm": 9.536160469055176,
      "learning_rate": 2e-05,
      "loss": 2.6948,
      "step": 200
    },
    {
      "epoch": 0.07007007007007007,
      "grad_norm": 7.950663089752197,
      "learning_rate": 2.1e-05,
      "loss": 3.3356,
      "step": 210
    },
    {
      "epoch": 0.07340674007340674,
      "grad_norm": 7.788869380950928,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 2.2596,
      "step": 220
    },
    {
      "epoch": 0.07674341007674342,
      "grad_norm": 7.822196006774902,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 2.5057,
      "step": 230
    },
    {
      "epoch": 0.08008008008008008,
      "grad_norm": 7.548488140106201,
      "learning_rate": 2.4e-05,
      "loss": 2.6202,
      "step": 240
    },
    {
      "epoch": 0.08341675008341674,
      "grad_norm": 6.796125888824463,
      "learning_rate": 2.5e-05,
      "loss": 1.9175,
      "step": 250
    },
    {
      "epoch": 0.08675342008675342,
      "grad_norm": 7.555082321166992,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.7628,
      "step": 260
    },
    {
      "epoch": 0.09009009009009009,
      "grad_norm": 6.521301746368408,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 2.0625,
      "step": 270
    },
    {
      "epoch": 0.09342676009342676,
      "grad_norm": 5.06267786026001,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.8207,
      "step": 280
    },
    {
      "epoch": 0.09676343009676343,
      "grad_norm": 6.200467586517334,
      "learning_rate": 2.9e-05,
      "loss": 1.9648,
      "step": 290
    },
    {
      "epoch": 0.1001001001001001,
      "grad_norm": 5.347590446472168,
      "learning_rate": 3e-05,
      "loss": 1.5949,
      "step": 300
    },
    {
      "epoch": 0.10343677010343677,
      "grad_norm": 2.64927077293396,
      "learning_rate": 3.1e-05,
      "loss": 1.5226,
      "step": 310
    },
    {
      "epoch": 0.10677344010677343,
      "grad_norm": 3.8787083625793457,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.8517,
      "step": 320
    },
    {
      "epoch": 0.11011011011011011,
      "grad_norm": 5.351945400238037,
      "learning_rate": 3.3e-05,
      "loss": 1.9147,
      "step": 330
    },
    {
      "epoch": 0.11344678011344678,
      "grad_norm": 4.14184045791626,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.732,
      "step": 340
    },
    {
      "epoch": 0.11678345011678345,
      "grad_norm": 5.556326389312744,
      "learning_rate": 3.5e-05,
      "loss": 1.5702,
      "step": 350
    },
    {
      "epoch": 0.12012012012012012,
      "grad_norm": 4.808996200561523,
      "learning_rate": 3.6e-05,
      "loss": 1.4381,
      "step": 360
    },
    {
      "epoch": 0.12345679012345678,
      "grad_norm": 5.18758487701416,
      "learning_rate": 3.7e-05,
      "loss": 1.6771,
      "step": 370
    },
    {
      "epoch": 0.12679346012679346,
      "grad_norm": 4.98621940612793,
      "learning_rate": 3.8e-05,
      "loss": 1.6097,
      "step": 380
    },
    {
      "epoch": 0.13013013013013014,
      "grad_norm": 4.125382423400879,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.383,
      "step": 390
    },
    {
      "epoch": 0.1334668001334668,
      "grad_norm": 4.260393142700195,
      "learning_rate": 4e-05,
      "loss": 1.4135,
      "step": 400
    },
    {
      "epoch": 0.13680347013680347,
      "grad_norm": 4.048669338226318,
      "learning_rate": 4.1e-05,
      "loss": 1.5159,
      "step": 410
    },
    {
      "epoch": 0.14014014014014015,
      "grad_norm": 4.261200428009033,
      "learning_rate": 4.2e-05,
      "loss": 1.3645,
      "step": 420
    },
    {
      "epoch": 0.14347681014347682,
      "grad_norm": 4.28817892074585,
      "learning_rate": 4.3e-05,
      "loss": 1.5483,
      "step": 430
    },
    {
      "epoch": 0.14681348014681347,
      "grad_norm": 3.5364677906036377,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.4444,
      "step": 440
    },
    {
      "epoch": 0.15015015015015015,
      "grad_norm": 3.8340559005737305,
      "learning_rate": 4.5e-05,
      "loss": 1.2625,
      "step": 450
    },
    {
      "epoch": 0.15348682015348683,
      "grad_norm": 5.591009616851807,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.3474,
      "step": 460
    },
    {
      "epoch": 0.15682349015682348,
      "grad_norm": 4.024402141571045,
      "learning_rate": 4.7e-05,
      "loss": 1.1617,
      "step": 470
    },
    {
      "epoch": 0.16016016016016016,
      "grad_norm": 5.415898323059082,
      "learning_rate": 4.8e-05,
      "loss": 1.7149,
      "step": 480
    },
    {
      "epoch": 0.16349683016349684,
      "grad_norm": 4.3966264724731445,
      "learning_rate": 4.9e-05,
      "loss": 1.3024,
      "step": 490
    },
    {
      "epoch": 0.1668335001668335,
      "grad_norm": 4.0056047439575195,
      "learning_rate": 5e-05,
      "loss": 1.5646,
      "step": 500
    },
    {
      "epoch": 0.17017017017017017,
      "grad_norm": 4.605757236480713,
      "learning_rate": 4.9941114120833824e-05,
      "loss": 1.4762,
      "step": 510
    },
    {
      "epoch": 0.17350684017350684,
      "grad_norm": 2.5374536514282227,
      "learning_rate": 4.988222824166765e-05,
      "loss": 1.1877,
      "step": 520
    },
    {
      "epoch": 0.17684351017684352,
      "grad_norm": 3.0233702659606934,
      "learning_rate": 4.9823342362501474e-05,
      "loss": 1.2266,
      "step": 530
    },
    {
      "epoch": 0.18018018018018017,
      "grad_norm": 4.547967433929443,
      "learning_rate": 4.9764456483335296e-05,
      "loss": 1.3976,
      "step": 540
    },
    {
      "epoch": 0.18351685018351685,
      "grad_norm": 3.5690975189208984,
      "learning_rate": 4.9705570604169125e-05,
      "loss": 1.387,
      "step": 550
    },
    {
      "epoch": 0.18685352018685353,
      "grad_norm": 6.602469444274902,
      "learning_rate": 4.9646684725002947e-05,
      "loss": 1.3689,
      "step": 560
    },
    {
      "epoch": 0.19019019019019018,
      "grad_norm": 6.749355316162109,
      "learning_rate": 4.958779884583677e-05,
      "loss": 1.2421,
      "step": 570
    },
    {
      "epoch": 0.19352686019352686,
      "grad_norm": 2.9303011894226074,
      "learning_rate": 4.952891296667059e-05,
      "loss": 1.0158,
      "step": 580
    },
    {
      "epoch": 0.19686353019686353,
      "grad_norm": 5.631228446960449,
      "learning_rate": 4.947002708750442e-05,
      "loss": 1.295,
      "step": 590
    },
    {
      "epoch": 0.2002002002002002,
      "grad_norm": 4.6857991218566895,
      "learning_rate": 4.941114120833824e-05,
      "loss": 1.3221,
      "step": 600
    },
    {
      "epoch": 0.20353687020353686,
      "grad_norm": 4.097028732299805,
      "learning_rate": 4.935225532917206e-05,
      "loss": 1.3916,
      "step": 610
    },
    {
      "epoch": 0.20687354020687354,
      "grad_norm": 3.8089182376861572,
      "learning_rate": 4.929336945000589e-05,
      "loss": 1.2672,
      "step": 620
    },
    {
      "epoch": 0.21021021021021022,
      "grad_norm": 3.4777469635009766,
      "learning_rate": 4.923448357083971e-05,
      "loss": 1.1686,
      "step": 630
    },
    {
      "epoch": 0.21354688021354687,
      "grad_norm": 6.217365741729736,
      "learning_rate": 4.917559769167354e-05,
      "loss": 1.3046,
      "step": 640
    },
    {
      "epoch": 0.21688355021688355,
      "grad_norm": 2.9496960639953613,
      "learning_rate": 4.911671181250736e-05,
      "loss": 1.1695,
      "step": 650
    },
    {
      "epoch": 0.22022022022022023,
      "grad_norm": 3.933528423309326,
      "learning_rate": 4.905782593334119e-05,
      "loss": 1.2356,
      "step": 660
    },
    {
      "epoch": 0.2235568902235569,
      "grad_norm": 3.9220006465911865,
      "learning_rate": 4.899894005417501e-05,
      "loss": 1.1246,
      "step": 670
    },
    {
      "epoch": 0.22689356022689355,
      "grad_norm": 7.431880474090576,
      "learning_rate": 4.8940054175008835e-05,
      "loss": 1.0677,
      "step": 680
    },
    {
      "epoch": 0.23023023023023023,
      "grad_norm": 4.475502014160156,
      "learning_rate": 4.8881168295842663e-05,
      "loss": 1.3185,
      "step": 690
    },
    {
      "epoch": 0.2335669002335669,
      "grad_norm": 5.457873344421387,
      "learning_rate": 4.8822282416676485e-05,
      "loss": 1.2404,
      "step": 700
    },
    {
      "epoch": 0.23690357023690356,
      "grad_norm": 5.106561183929443,
      "learning_rate": 4.876339653751031e-05,
      "loss": 1.4241,
      "step": 710
    },
    {
      "epoch": 0.24024024024024024,
      "grad_norm": 6.592267036437988,
      "learning_rate": 4.8704510658344136e-05,
      "loss": 1.366,
      "step": 720
    },
    {
      "epoch": 0.24357691024357692,
      "grad_norm": 3.60618257522583,
      "learning_rate": 4.864562477917796e-05,
      "loss": 1.1119,
      "step": 730
    },
    {
      "epoch": 0.24691358024691357,
      "grad_norm": 3.0848686695098877,
      "learning_rate": 4.858673890001178e-05,
      "loss": 1.0512,
      "step": 740
    },
    {
      "epoch": 0.2502502502502503,
      "grad_norm": 4.673299789428711,
      "learning_rate": 4.85278530208456e-05,
      "loss": 1.2696,
      "step": 750
    },
    {
      "epoch": 0.2535869202535869,
      "grad_norm": 4.895904064178467,
      "learning_rate": 4.846896714167943e-05,
      "loss": 1.1805,
      "step": 760
    },
    {
      "epoch": 0.2569235902569236,
      "grad_norm": 4.752594947814941,
      "learning_rate": 4.841008126251325e-05,
      "loss": 1.1475,
      "step": 770
    },
    {
      "epoch": 0.2602602602602603,
      "grad_norm": 4.059188365936279,
      "learning_rate": 4.835119538334707e-05,
      "loss": 1.2825,
      "step": 780
    },
    {
      "epoch": 0.26359693026359693,
      "grad_norm": 3.561336040496826,
      "learning_rate": 4.82923095041809e-05,
      "loss": 1.1613,
      "step": 790
    },
    {
      "epoch": 0.2669336002669336,
      "grad_norm": 4.691658973693848,
      "learning_rate": 4.823342362501472e-05,
      "loss": 1.5574,
      "step": 800
    },
    {
      "epoch": 0.2702702702702703,
      "grad_norm": 3.560206413269043,
      "learning_rate": 4.8174537745848545e-05,
      "loss": 1.212,
      "step": 810
    },
    {
      "epoch": 0.27360694027360694,
      "grad_norm": 4.498392581939697,
      "learning_rate": 4.8115651866682374e-05,
      "loss": 1.0687,
      "step": 820
    },
    {
      "epoch": 0.2769436102769436,
      "grad_norm": 3.535398483276367,
      "learning_rate": 4.8056765987516195e-05,
      "loss": 1.2544,
      "step": 830
    },
    {
      "epoch": 0.2802802802802803,
      "grad_norm": 5.058297157287598,
      "learning_rate": 4.799788010835002e-05,
      "loss": 1.4341,
      "step": 840
    },
    {
      "epoch": 0.28361695028361694,
      "grad_norm": 3.874671697616577,
      "learning_rate": 4.7938994229183846e-05,
      "loss": 1.1299,
      "step": 850
    },
    {
      "epoch": 0.28695362028695365,
      "grad_norm": 5.592775821685791,
      "learning_rate": 4.788010835001767e-05,
      "loss": 1.0973,
      "step": 860
    },
    {
      "epoch": 0.2902902902902903,
      "grad_norm": 3.8135714530944824,
      "learning_rate": 4.782122247085149e-05,
      "loss": 1.1867,
      "step": 870
    },
    {
      "epoch": 0.29362696029362695,
      "grad_norm": 5.178786754608154,
      "learning_rate": 4.776233659168531e-05,
      "loss": 1.1778,
      "step": 880
    },
    {
      "epoch": 0.29696363029696365,
      "grad_norm": 3.325193166732788,
      "learning_rate": 4.770345071251914e-05,
      "loss": 1.022,
      "step": 890
    },
    {
      "epoch": 0.3003003003003003,
      "grad_norm": 3.699855089187622,
      "learning_rate": 4.764456483335296e-05,
      "loss": 1.3642,
      "step": 900
    },
    {
      "epoch": 0.30363697030363695,
      "grad_norm": 4.583566665649414,
      "learning_rate": 4.758567895418679e-05,
      "loss": 1.0707,
      "step": 910
    },
    {
      "epoch": 0.30697364030697366,
      "grad_norm": 5.991292476654053,
      "learning_rate": 4.752679307502061e-05,
      "loss": 1.1113,
      "step": 920
    },
    {
      "epoch": 0.3103103103103103,
      "grad_norm": 3.690126419067383,
      "learning_rate": 4.746790719585444e-05,
      "loss": 1.294,
      "step": 930
    },
    {
      "epoch": 0.31364698031364696,
      "grad_norm": 6.788210391998291,
      "learning_rate": 4.740902131668826e-05,
      "loss": 1.0856,
      "step": 940
    },
    {
      "epoch": 0.31698365031698367,
      "grad_norm": 3.347337484359741,
      "learning_rate": 4.7350135437522084e-05,
      "loss": 0.9876,
      "step": 950
    },
    {
      "epoch": 0.3203203203203203,
      "grad_norm": 4.475565433502197,
      "learning_rate": 4.729124955835591e-05,
      "loss": 1.0511,
      "step": 960
    },
    {
      "epoch": 0.32365699032365697,
      "grad_norm": 7.910323143005371,
      "learning_rate": 4.7232363679189734e-05,
      "loss": 1.4289,
      "step": 970
    },
    {
      "epoch": 0.3269936603269937,
      "grad_norm": 3.852656364440918,
      "learning_rate": 4.7173477800023556e-05,
      "loss": 1.1075,
      "step": 980
    },
    {
      "epoch": 0.3303303303303303,
      "grad_norm": 6.85919189453125,
      "learning_rate": 4.7114591920857384e-05,
      "loss": 1.1374,
      "step": 990
    },
    {
      "epoch": 0.333667000333667,
      "grad_norm": 4.76578426361084,
      "learning_rate": 4.7055706041691206e-05,
      "loss": 1.1148,
      "step": 1000
    },
    {
      "epoch": 0.3370036703370037,
      "grad_norm": 6.559088706970215,
      "learning_rate": 4.699682016252503e-05,
      "loss": 1.1183,
      "step": 1010
    },
    {
      "epoch": 0.34034034034034033,
      "grad_norm": 6.129230499267578,
      "learning_rate": 4.6937934283358856e-05,
      "loss": 1.1669,
      "step": 1020
    },
    {
      "epoch": 0.34367701034367704,
      "grad_norm": 4.710047721862793,
      "learning_rate": 4.687904840419268e-05,
      "loss": 1.2316,
      "step": 1030
    },
    {
      "epoch": 0.3470136803470137,
      "grad_norm": 5.12595272064209,
      "learning_rate": 4.68201625250265e-05,
      "loss": 1.1542,
      "step": 1040
    },
    {
      "epoch": 0.35035035035035034,
      "grad_norm": 2.357759952545166,
      "learning_rate": 4.676127664586032e-05,
      "loss": 0.9957,
      "step": 1050
    },
    {
      "epoch": 0.35368702035368704,
      "grad_norm": 3.131687641143799,
      "learning_rate": 4.670239076669415e-05,
      "loss": 1.2648,
      "step": 1060
    },
    {
      "epoch": 0.3570236903570237,
      "grad_norm": 3.833237886428833,
      "learning_rate": 4.664350488752797e-05,
      "loss": 1.0358,
      "step": 1070
    },
    {
      "epoch": 0.36036036036036034,
      "grad_norm": 4.858419895172119,
      "learning_rate": 4.6584619008361794e-05,
      "loss": 1.292,
      "step": 1080
    },
    {
      "epoch": 0.36369703036369705,
      "grad_norm": 4.332526206970215,
      "learning_rate": 4.652573312919562e-05,
      "loss": 1.3125,
      "step": 1090
    },
    {
      "epoch": 0.3670337003670337,
      "grad_norm": 3.6166481971740723,
      "learning_rate": 4.6466847250029444e-05,
      "loss": 1.1565,
      "step": 1100
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 4.405731678009033,
      "learning_rate": 4.6407961370863266e-05,
      "loss": 1.1211,
      "step": 1110
    },
    {
      "epoch": 0.37370704037370706,
      "grad_norm": 6.173271179199219,
      "learning_rate": 4.6349075491697094e-05,
      "loss": 1.0745,
      "step": 1120
    },
    {
      "epoch": 0.3770437103770437,
      "grad_norm": 4.33131217956543,
      "learning_rate": 4.6290189612530916e-05,
      "loss": 1.0042,
      "step": 1130
    },
    {
      "epoch": 0.38038038038038036,
      "grad_norm": 4.473048210144043,
      "learning_rate": 4.623130373336474e-05,
      "loss": 1.2826,
      "step": 1140
    },
    {
      "epoch": 0.38371705038371706,
      "grad_norm": 4.881191253662109,
      "learning_rate": 4.617241785419856e-05,
      "loss": 0.9516,
      "step": 1150
    },
    {
      "epoch": 0.3870537203870537,
      "grad_norm": 5.25804328918457,
      "learning_rate": 4.611353197503239e-05,
      "loss": 0.9502,
      "step": 1160
    },
    {
      "epoch": 0.39039039039039036,
      "grad_norm": 4.6736369132995605,
      "learning_rate": 4.605464609586622e-05,
      "loss": 1.1411,
      "step": 1170
    },
    {
      "epoch": 0.39372706039372707,
      "grad_norm": 5.670393466949463,
      "learning_rate": 4.599576021670004e-05,
      "loss": 1.2091,
      "step": 1180
    },
    {
      "epoch": 0.3970637303970637,
      "grad_norm": 4.273220539093018,
      "learning_rate": 4.593687433753387e-05,
      "loss": 1.3297,
      "step": 1190
    },
    {
      "epoch": 0.4004004004004004,
      "grad_norm": 4.789172649383545,
      "learning_rate": 4.587798845836769e-05,
      "loss": 1.1299,
      "step": 1200
    },
    {
      "epoch": 0.4037370704037371,
      "grad_norm": 4.748783588409424,
      "learning_rate": 4.581910257920151e-05,
      "loss": 1.0546,
      "step": 1210
    },
    {
      "epoch": 0.4070737404070737,
      "grad_norm": 5.400569915771484,
      "learning_rate": 4.576021670003533e-05,
      "loss": 1.4714,
      "step": 1220
    },
    {
      "epoch": 0.41041041041041043,
      "grad_norm": 5.7481369972229,
      "learning_rate": 4.570133082086916e-05,
      "loss": 1.2662,
      "step": 1230
    },
    {
      "epoch": 0.4137470804137471,
      "grad_norm": 3.5832130908966064,
      "learning_rate": 4.564244494170298e-05,
      "loss": 1.1563,
      "step": 1240
    },
    {
      "epoch": 0.41708375041708373,
      "grad_norm": 4.613012313842773,
      "learning_rate": 4.5583559062536805e-05,
      "loss": 1.0004,
      "step": 1250
    },
    {
      "epoch": 0.42042042042042044,
      "grad_norm": 5.867781162261963,
      "learning_rate": 4.552467318337063e-05,
      "loss": 1.0469,
      "step": 1260
    },
    {
      "epoch": 0.4237570904237571,
      "grad_norm": 3.820117712020874,
      "learning_rate": 4.5465787304204455e-05,
      "loss": 1.2976,
      "step": 1270
    },
    {
      "epoch": 0.42709376042709374,
      "grad_norm": 6.1656928062438965,
      "learning_rate": 4.540690142503828e-05,
      "loss": 1.1519,
      "step": 1280
    },
    {
      "epoch": 0.43043043043043044,
      "grad_norm": 6.3360700607299805,
      "learning_rate": 4.5348015545872105e-05,
      "loss": 1.2347,
      "step": 1290
    },
    {
      "epoch": 0.4337671004337671,
      "grad_norm": 5.503486156463623,
      "learning_rate": 4.528912966670593e-05,
      "loss": 1.2732,
      "step": 1300
    },
    {
      "epoch": 0.43710377043710374,
      "grad_norm": 3.3686165809631348,
      "learning_rate": 4.523024378753975e-05,
      "loss": 0.989,
      "step": 1310
    },
    {
      "epoch": 0.44044044044044045,
      "grad_norm": 6.149549961090088,
      "learning_rate": 4.517135790837357e-05,
      "loss": 1.2925,
      "step": 1320
    },
    {
      "epoch": 0.4437771104437771,
      "grad_norm": 5.150977611541748,
      "learning_rate": 4.51124720292074e-05,
      "loss": 0.9846,
      "step": 1330
    },
    {
      "epoch": 0.4471137804471138,
      "grad_norm": 6.125264644622803,
      "learning_rate": 4.505358615004122e-05,
      "loss": 1.0251,
      "step": 1340
    },
    {
      "epoch": 0.45045045045045046,
      "grad_norm": 3.403777837753296,
      "learning_rate": 4.499470027087504e-05,
      "loss": 1.0786,
      "step": 1350
    },
    {
      "epoch": 0.4537871204537871,
      "grad_norm": 3.3144290447235107,
      "learning_rate": 4.493581439170887e-05,
      "loss": 1.3567,
      "step": 1360
    },
    {
      "epoch": 0.4571237904571238,
      "grad_norm": 4.10409688949585,
      "learning_rate": 4.487692851254269e-05,
      "loss": 1.2828,
      "step": 1370
    },
    {
      "epoch": 0.46046046046046046,
      "grad_norm": 3.7806947231292725,
      "learning_rate": 4.4818042633376515e-05,
      "loss": 1.2158,
      "step": 1380
    },
    {
      "epoch": 0.4637971304637971,
      "grad_norm": 6.468990802764893,
      "learning_rate": 4.475915675421034e-05,
      "loss": 1.1004,
      "step": 1390
    },
    {
      "epoch": 0.4671338004671338,
      "grad_norm": 6.160029888153076,
      "learning_rate": 4.4700270875044165e-05,
      "loss": 1.4794,
      "step": 1400
    },
    {
      "epoch": 0.47047047047047047,
      "grad_norm": 5.245044708251953,
      "learning_rate": 4.464138499587799e-05,
      "loss": 1.1136,
      "step": 1410
    },
    {
      "epoch": 0.4738071404738071,
      "grad_norm": 4.885804176330566,
      "learning_rate": 4.4582499116711815e-05,
      "loss": 1.2628,
      "step": 1420
    },
    {
      "epoch": 0.4771438104771438,
      "grad_norm": 4.391535758972168,
      "learning_rate": 4.452361323754564e-05,
      "loss": 0.8905,
      "step": 1430
    },
    {
      "epoch": 0.4804804804804805,
      "grad_norm": 4.875990867614746,
      "learning_rate": 4.4464727358379466e-05,
      "loss": 1.1126,
      "step": 1440
    },
    {
      "epoch": 0.4838171504838171,
      "grad_norm": 3.6667091846466064,
      "learning_rate": 4.440584147921329e-05,
      "loss": 1.0576,
      "step": 1450
    },
    {
      "epoch": 0.48715382048715383,
      "grad_norm": 4.883539199829102,
      "learning_rate": 4.4346955600047116e-05,
      "loss": 1.0036,
      "step": 1460
    },
    {
      "epoch": 0.4904904904904905,
      "grad_norm": 4.8002495765686035,
      "learning_rate": 4.428806972088094e-05,
      "loss": 0.97,
      "step": 1470
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 5.606390476226807,
      "learning_rate": 4.422918384171476e-05,
      "loss": 1.3469,
      "step": 1480
    },
    {
      "epoch": 0.49716383049716384,
      "grad_norm": 6.953462600708008,
      "learning_rate": 4.417029796254858e-05,
      "loss": 0.9795,
      "step": 1490
    },
    {
      "epoch": 0.5005005005005005,
      "grad_norm": 6.637098789215088,
      "learning_rate": 4.411141208338241e-05,
      "loss": 1.2463,
      "step": 1500
    },
    {
      "epoch": 0.5038371705038371,
      "grad_norm": 5.890302658081055,
      "learning_rate": 4.405252620421623e-05,
      "loss": 1.0039,
      "step": 1510
    },
    {
      "epoch": 0.5071738405071738,
      "grad_norm": 4.386608123779297,
      "learning_rate": 4.3993640325050053e-05,
      "loss": 1.1956,
      "step": 1520
    },
    {
      "epoch": 0.5105105105105106,
      "grad_norm": 2.929110050201416,
      "learning_rate": 4.393475444588388e-05,
      "loss": 1.0892,
      "step": 1530
    },
    {
      "epoch": 0.5138471805138471,
      "grad_norm": 4.3466410636901855,
      "learning_rate": 4.3875868566717704e-05,
      "loss": 0.9713,
      "step": 1540
    },
    {
      "epoch": 0.5171838505171839,
      "grad_norm": 5.009315490722656,
      "learning_rate": 4.3816982687551526e-05,
      "loss": 1.0007,
      "step": 1550
    },
    {
      "epoch": 0.5205205205205206,
      "grad_norm": 6.610705852508545,
      "learning_rate": 4.3758096808385354e-05,
      "loss": 0.9979,
      "step": 1560
    },
    {
      "epoch": 0.5238571905238572,
      "grad_norm": 6.347642421722412,
      "learning_rate": 4.3699210929219176e-05,
      "loss": 1.0789,
      "step": 1570
    },
    {
      "epoch": 0.5271938605271939,
      "grad_norm": 4.932490825653076,
      "learning_rate": 4.3640325050053e-05,
      "loss": 0.9119,
      "step": 1580
    },
    {
      "epoch": 0.5305305305305306,
      "grad_norm": 5.253088474273682,
      "learning_rate": 4.3581439170886826e-05,
      "loss": 1.2789,
      "step": 1590
    },
    {
      "epoch": 0.5338672005338672,
      "grad_norm": 3.9506020545959473,
      "learning_rate": 4.352255329172065e-05,
      "loss": 1.0147,
      "step": 1600
    },
    {
      "epoch": 0.5372038705372039,
      "grad_norm": 4.5636138916015625,
      "learning_rate": 4.346366741255447e-05,
      "loss": 1.1785,
      "step": 1610
    },
    {
      "epoch": 0.5405405405405406,
      "grad_norm": 4.6478590965271,
      "learning_rate": 4.340478153338829e-05,
      "loss": 1.1807,
      "step": 1620
    },
    {
      "epoch": 0.5438772105438772,
      "grad_norm": 5.021444320678711,
      "learning_rate": 4.334589565422212e-05,
      "loss": 1.012,
      "step": 1630
    },
    {
      "epoch": 0.5472138805472139,
      "grad_norm": 3.3565914630889893,
      "learning_rate": 4.328700977505594e-05,
      "loss": 0.7903,
      "step": 1640
    },
    {
      "epoch": 0.5505505505505506,
      "grad_norm": 3.42800235748291,
      "learning_rate": 4.3228123895889764e-05,
      "loss": 1.131,
      "step": 1650
    },
    {
      "epoch": 0.5538872205538872,
      "grad_norm": 5.5250396728515625,
      "learning_rate": 4.316923801672359e-05,
      "loss": 1.0237,
      "step": 1660
    },
    {
      "epoch": 0.5572238905572239,
      "grad_norm": 5.53750467300415,
      "learning_rate": 4.3110352137557414e-05,
      "loss": 0.9715,
      "step": 1670
    },
    {
      "epoch": 0.5605605605605606,
      "grad_norm": 3.6184206008911133,
      "learning_rate": 4.3051466258391236e-05,
      "loss": 1.0773,
      "step": 1680
    },
    {
      "epoch": 0.5638972305638972,
      "grad_norm": 2.6647891998291016,
      "learning_rate": 4.2992580379225064e-05,
      "loss": 1.0247,
      "step": 1690
    },
    {
      "epoch": 0.5672339005672339,
      "grad_norm": 6.049147605895996,
      "learning_rate": 4.2933694500058886e-05,
      "loss": 1.246,
      "step": 1700
    },
    {
      "epoch": 0.5705705705705706,
      "grad_norm": 5.908942222595215,
      "learning_rate": 4.2874808620892715e-05,
      "loss": 1.0188,
      "step": 1710
    },
    {
      "epoch": 0.5739072405739073,
      "grad_norm": 4.471683502197266,
      "learning_rate": 4.2815922741726536e-05,
      "loss": 0.92,
      "step": 1720
    },
    {
      "epoch": 0.5772439105772439,
      "grad_norm": 8.099821090698242,
      "learning_rate": 4.2757036862560365e-05,
      "loss": 1.1127,
      "step": 1730
    },
    {
      "epoch": 0.5805805805805806,
      "grad_norm": 6.777768611907959,
      "learning_rate": 4.2698150983394187e-05,
      "loss": 1.0623,
      "step": 1740
    },
    {
      "epoch": 0.5839172505839173,
      "grad_norm": 3.303414821624756,
      "learning_rate": 4.263926510422801e-05,
      "loss": 1.2589,
      "step": 1750
    },
    {
      "epoch": 0.5872539205872539,
      "grad_norm": 5.274767875671387,
      "learning_rate": 4.258037922506184e-05,
      "loss": 0.9983,
      "step": 1760
    },
    {
      "epoch": 0.5905905905905906,
      "grad_norm": 8.6080961227417,
      "learning_rate": 4.252149334589566e-05,
      "loss": 1.1449,
      "step": 1770
    },
    {
      "epoch": 0.5939272605939273,
      "grad_norm": 4.0620222091674805,
      "learning_rate": 4.246260746672948e-05,
      "loss": 1.0229,
      "step": 1780
    },
    {
      "epoch": 0.5972639305972639,
      "grad_norm": 4.511322498321533,
      "learning_rate": 4.24037215875633e-05,
      "loss": 0.922,
      "step": 1790
    },
    {
      "epoch": 0.6006006006006006,
      "grad_norm": 4.134535789489746,
      "learning_rate": 4.234483570839713e-05,
      "loss": 0.9174,
      "step": 1800
    },
    {
      "epoch": 0.6039372706039373,
      "grad_norm": 3.4883105754852295,
      "learning_rate": 4.228594982923095e-05,
      "loss": 1.0787,
      "step": 1810
    },
    {
      "epoch": 0.6072739406072739,
      "grad_norm": 4.850587844848633,
      "learning_rate": 4.2227063950064774e-05,
      "loss": 1.0797,
      "step": 1820
    },
    {
      "epoch": 0.6106106106106106,
      "grad_norm": 3.6119439601898193,
      "learning_rate": 4.21681780708986e-05,
      "loss": 1.1667,
      "step": 1830
    },
    {
      "epoch": 0.6139472806139473,
      "grad_norm": 5.857569694519043,
      "learning_rate": 4.2109292191732425e-05,
      "loss": 1.2417,
      "step": 1840
    },
    {
      "epoch": 0.6172839506172839,
      "grad_norm": 4.887453079223633,
      "learning_rate": 4.2050406312566246e-05,
      "loss": 1.0392,
      "step": 1850
    },
    {
      "epoch": 0.6206206206206206,
      "grad_norm": 3.1208999156951904,
      "learning_rate": 4.1991520433400075e-05,
      "loss": 1.1207,
      "step": 1860
    },
    {
      "epoch": 0.6239572906239573,
      "grad_norm": 3.7177977561950684,
      "learning_rate": 4.19326345542339e-05,
      "loss": 0.9417,
      "step": 1870
    },
    {
      "epoch": 0.6272939606272939,
      "grad_norm": 4.772613048553467,
      "learning_rate": 4.187374867506772e-05,
      "loss": 0.9288,
      "step": 1880
    },
    {
      "epoch": 0.6306306306306306,
      "grad_norm": 4.38753604888916,
      "learning_rate": 4.181486279590155e-05,
      "loss": 1.086,
      "step": 1890
    },
    {
      "epoch": 0.6339673006339673,
      "grad_norm": 4.068124294281006,
      "learning_rate": 4.175597691673537e-05,
      "loss": 1.0794,
      "step": 1900
    },
    {
      "epoch": 0.6373039706373039,
      "grad_norm": 4.410216808319092,
      "learning_rate": 4.169709103756919e-05,
      "loss": 0.8916,
      "step": 1910
    },
    {
      "epoch": 0.6406406406406406,
      "grad_norm": 6.215452671051025,
      "learning_rate": 4.163820515840301e-05,
      "loss": 1.1252,
      "step": 1920
    },
    {
      "epoch": 0.6439773106439773,
      "grad_norm": 4.841552257537842,
      "learning_rate": 4.157931927923684e-05,
      "loss": 1.0643,
      "step": 1930
    },
    {
      "epoch": 0.6473139806473139,
      "grad_norm": 3.549687147140503,
      "learning_rate": 4.152043340007066e-05,
      "loss": 1.0961,
      "step": 1940
    },
    {
      "epoch": 0.6506506506506506,
      "grad_norm": 4.294360160827637,
      "learning_rate": 4.1461547520904484e-05,
      "loss": 0.9167,
      "step": 1950
    },
    {
      "epoch": 0.6539873206539873,
      "grad_norm": 3.567294120788574,
      "learning_rate": 4.140266164173831e-05,
      "loss": 1.0573,
      "step": 1960
    },
    {
      "epoch": 0.6573239906573239,
      "grad_norm": 3.7703592777252197,
      "learning_rate": 4.1343775762572135e-05,
      "loss": 1.003,
      "step": 1970
    },
    {
      "epoch": 0.6606606606606606,
      "grad_norm": 5.200263023376465,
      "learning_rate": 4.128488988340596e-05,
      "loss": 0.987,
      "step": 1980
    },
    {
      "epoch": 0.6639973306639974,
      "grad_norm": 4.791322231292725,
      "learning_rate": 4.1226004004239785e-05,
      "loss": 1.1087,
      "step": 1990
    },
    {
      "epoch": 0.667334000667334,
      "grad_norm": 4.2763590812683105,
      "learning_rate": 4.1167118125073614e-05,
      "loss": 1.223,
      "step": 2000
    },
    {
      "epoch": 0.6706706706706707,
      "grad_norm": 5.089679718017578,
      "learning_rate": 4.1108232245907435e-05,
      "loss": 1.0585,
      "step": 2010
    },
    {
      "epoch": 0.6740073406740074,
      "grad_norm": 3.6672744750976562,
      "learning_rate": 4.104934636674126e-05,
      "loss": 0.9721,
      "step": 2020
    },
    {
      "epoch": 0.6773440106773441,
      "grad_norm": 6.16092586517334,
      "learning_rate": 4.0990460487575086e-05,
      "loss": 1.2306,
      "step": 2030
    },
    {
      "epoch": 0.6806806806806807,
      "grad_norm": 6.168591022491455,
      "learning_rate": 4.093157460840891e-05,
      "loss": 1.1752,
      "step": 2040
    },
    {
      "epoch": 0.6840173506840174,
      "grad_norm": 3.3493587970733643,
      "learning_rate": 4.087268872924273e-05,
      "loss": 0.9449,
      "step": 2050
    },
    {
      "epoch": 0.6873540206873541,
      "grad_norm": 4.546745777130127,
      "learning_rate": 4.081380285007656e-05,
      "loss": 1.0035,
      "step": 2060
    },
    {
      "epoch": 0.6906906906906907,
      "grad_norm": 5.1102399826049805,
      "learning_rate": 4.075491697091038e-05,
      "loss": 1.048,
      "step": 2070
    },
    {
      "epoch": 0.6940273606940274,
      "grad_norm": 7.655319690704346,
      "learning_rate": 4.06960310917442e-05,
      "loss": 0.9685,
      "step": 2080
    },
    {
      "epoch": 0.6973640306973641,
      "grad_norm": 5.581677436828613,
      "learning_rate": 4.063714521257802e-05,
      "loss": 1.1065,
      "step": 2090
    },
    {
      "epoch": 0.7007007007007007,
      "grad_norm": 5.615943431854248,
      "learning_rate": 4.057825933341185e-05,
      "loss": 1.0006,
      "step": 2100
    },
    {
      "epoch": 0.7040373707040374,
      "grad_norm": 3.5344676971435547,
      "learning_rate": 4.0519373454245673e-05,
      "loss": 1.0115,
      "step": 2110
    },
    {
      "epoch": 0.7073740407073741,
      "grad_norm": 5.101223945617676,
      "learning_rate": 4.0460487575079495e-05,
      "loss": 0.9675,
      "step": 2120
    },
    {
      "epoch": 0.7107107107107107,
      "grad_norm": 5.310048580169678,
      "learning_rate": 4.0401601695913324e-05,
      "loss": 1.015,
      "step": 2130
    },
    {
      "epoch": 0.7140473807140474,
      "grad_norm": 7.982122898101807,
      "learning_rate": 4.0342715816747146e-05,
      "loss": 0.9236,
      "step": 2140
    },
    {
      "epoch": 0.7173840507173841,
      "grad_norm": 7.643040180206299,
      "learning_rate": 4.028382993758097e-05,
      "loss": 1.1786,
      "step": 2150
    },
    {
      "epoch": 0.7207207207207207,
      "grad_norm": 4.349778175354004,
      "learning_rate": 4.0224944058414796e-05,
      "loss": 1.0453,
      "step": 2160
    },
    {
      "epoch": 0.7240573907240574,
      "grad_norm": 4.18424654006958,
      "learning_rate": 4.016605817924862e-05,
      "loss": 0.9332,
      "step": 2170
    },
    {
      "epoch": 0.7273940607273941,
      "grad_norm": 4.759496212005615,
      "learning_rate": 4.010717230008244e-05,
      "loss": 0.8998,
      "step": 2180
    },
    {
      "epoch": 0.7307307307307307,
      "grad_norm": 4.415217399597168,
      "learning_rate": 4.004828642091626e-05,
      "loss": 0.9952,
      "step": 2190
    },
    {
      "epoch": 0.7340674007340674,
      "grad_norm": 3.434067487716675,
      "learning_rate": 3.998940054175009e-05,
      "loss": 0.9663,
      "step": 2200
    },
    {
      "epoch": 0.7374040707374041,
      "grad_norm": 4.300767421722412,
      "learning_rate": 3.993051466258391e-05,
      "loss": 0.684,
      "step": 2210
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 6.224095344543457,
      "learning_rate": 3.987162878341773e-05,
      "loss": 1.343,
      "step": 2220
    },
    {
      "epoch": 0.7440774107440774,
      "grad_norm": 5.001160621643066,
      "learning_rate": 3.981274290425156e-05,
      "loss": 1.0934,
      "step": 2230
    },
    {
      "epoch": 0.7474140807474141,
      "grad_norm": 4.951452732086182,
      "learning_rate": 3.9753857025085384e-05,
      "loss": 0.9736,
      "step": 2240
    },
    {
      "epoch": 0.7507507507507507,
      "grad_norm": 3.463355541229248,
      "learning_rate": 3.969497114591921e-05,
      "loss": 1.0766,
      "step": 2250
    },
    {
      "epoch": 0.7540874207540874,
      "grad_norm": 4.635566711425781,
      "learning_rate": 3.9636085266753034e-05,
      "loss": 1.0207,
      "step": 2260
    },
    {
      "epoch": 0.7574240907574241,
      "grad_norm": 5.9635539054870605,
      "learning_rate": 3.957719938758686e-05,
      "loss": 1.0465,
      "step": 2270
    },
    {
      "epoch": 0.7607607607607607,
      "grad_norm": 4.321613311767578,
      "learning_rate": 3.9518313508420684e-05,
      "loss": 0.8636,
      "step": 2280
    },
    {
      "epoch": 0.7640974307640974,
      "grad_norm": 3.976816415786743,
      "learning_rate": 3.9459427629254506e-05,
      "loss": 1.0845,
      "step": 2290
    },
    {
      "epoch": 0.7674341007674341,
      "grad_norm": 3.870814085006714,
      "learning_rate": 3.9400541750088335e-05,
      "loss": 1.038,
      "step": 2300
    },
    {
      "epoch": 0.7707707707707707,
      "grad_norm": 6.248379707336426,
      "learning_rate": 3.9341655870922156e-05,
      "loss": 0.9804,
      "step": 2310
    },
    {
      "epoch": 0.7741074407741074,
      "grad_norm": 5.252345561981201,
      "learning_rate": 3.928276999175598e-05,
      "loss": 1.046,
      "step": 2320
    },
    {
      "epoch": 0.7774441107774441,
      "grad_norm": 4.6217522621154785,
      "learning_rate": 3.9223884112589807e-05,
      "loss": 1.2273,
      "step": 2330
    },
    {
      "epoch": 0.7807807807807807,
      "grad_norm": 6.875094890594482,
      "learning_rate": 3.916499823342363e-05,
      "loss": 1.1323,
      "step": 2340
    },
    {
      "epoch": 0.7841174507841174,
      "grad_norm": 4.957897663116455,
      "learning_rate": 3.910611235425745e-05,
      "loss": 0.7901,
      "step": 2350
    },
    {
      "epoch": 0.7874541207874541,
      "grad_norm": 3.6564555168151855,
      "learning_rate": 3.904722647509127e-05,
      "loss": 1.1289,
      "step": 2360
    },
    {
      "epoch": 0.7907907907907908,
      "grad_norm": 4.779222011566162,
      "learning_rate": 3.89883405959251e-05,
      "loss": 1.0269,
      "step": 2370
    },
    {
      "epoch": 0.7941274607941274,
      "grad_norm": 6.013369083404541,
      "learning_rate": 3.892945471675892e-05,
      "loss": 0.8014,
      "step": 2380
    },
    {
      "epoch": 0.7974641307974641,
      "grad_norm": 3.8574578762054443,
      "learning_rate": 3.8870568837592744e-05,
      "loss": 0.7752,
      "step": 2390
    },
    {
      "epoch": 0.8008008008008008,
      "grad_norm": 3.866179943084717,
      "learning_rate": 3.881168295842657e-05,
      "loss": 0.9554,
      "step": 2400
    },
    {
      "epoch": 0.8041374708041374,
      "grad_norm": 3.6769495010375977,
      "learning_rate": 3.8752797079260394e-05,
      "loss": 1.0146,
      "step": 2410
    },
    {
      "epoch": 0.8074741408074741,
      "grad_norm": 5.019022464752197,
      "learning_rate": 3.8693911200094216e-05,
      "loss": 1.212,
      "step": 2420
    },
    {
      "epoch": 0.8108108108108109,
      "grad_norm": 3.467775821685791,
      "learning_rate": 3.8635025320928045e-05,
      "loss": 0.8089,
      "step": 2430
    },
    {
      "epoch": 0.8141474808141475,
      "grad_norm": 6.591006755828857,
      "learning_rate": 3.8576139441761866e-05,
      "loss": 1.1846,
      "step": 2440
    },
    {
      "epoch": 0.8174841508174842,
      "grad_norm": 4.856524467468262,
      "learning_rate": 3.851725356259569e-05,
      "loss": 0.9581,
      "step": 2450
    },
    {
      "epoch": 0.8208208208208209,
      "grad_norm": 6.975013256072998,
      "learning_rate": 3.845836768342952e-05,
      "loss": 1.16,
      "step": 2460
    },
    {
      "epoch": 0.8241574908241575,
      "grad_norm": 5.8184428215026855,
      "learning_rate": 3.839948180426334e-05,
      "loss": 1.0805,
      "step": 2470
    },
    {
      "epoch": 0.8274941608274942,
      "grad_norm": 2.5651066303253174,
      "learning_rate": 3.834059592509716e-05,
      "loss": 0.8015,
      "step": 2480
    },
    {
      "epoch": 0.8308308308308309,
      "grad_norm": 4.734800815582275,
      "learning_rate": 3.828171004593098e-05,
      "loss": 1.0027,
      "step": 2490
    },
    {
      "epoch": 0.8341675008341675,
      "grad_norm": 7.390276908874512,
      "learning_rate": 3.822282416676481e-05,
      "loss": 1.3636,
      "step": 2500
    },
    {
      "epoch": 0.8375041708375042,
      "grad_norm": 5.166672706604004,
      "learning_rate": 3.816393828759864e-05,
      "loss": 0.9292,
      "step": 2510
    },
    {
      "epoch": 0.8408408408408409,
      "grad_norm": 4.3348493576049805,
      "learning_rate": 3.810505240843246e-05,
      "loss": 0.936,
      "step": 2520
    },
    {
      "epoch": 0.8441775108441775,
      "grad_norm": 4.861528396606445,
      "learning_rate": 3.804616652926629e-05,
      "loss": 1.1958,
      "step": 2530
    },
    {
      "epoch": 0.8475141808475142,
      "grad_norm": 3.543977975845337,
      "learning_rate": 3.798728065010011e-05,
      "loss": 1.197,
      "step": 2540
    },
    {
      "epoch": 0.8508508508508509,
      "grad_norm": 4.155296802520752,
      "learning_rate": 3.792839477093393e-05,
      "loss": 0.877,
      "step": 2550
    },
    {
      "epoch": 0.8541875208541875,
      "grad_norm": 4.168817520141602,
      "learning_rate": 3.7869508891767755e-05,
      "loss": 0.9527,
      "step": 2560
    },
    {
      "epoch": 0.8575241908575242,
      "grad_norm": 6.19243860244751,
      "learning_rate": 3.781062301260158e-05,
      "loss": 0.956,
      "step": 2570
    },
    {
      "epoch": 0.8608608608608609,
      "grad_norm": 3.9070050716400146,
      "learning_rate": 3.7751737133435405e-05,
      "loss": 0.8739,
      "step": 2580
    },
    {
      "epoch": 0.8641975308641975,
      "grad_norm": 4.868672847747803,
      "learning_rate": 3.769285125426923e-05,
      "loss": 1.2115,
      "step": 2590
    },
    {
      "epoch": 0.8675342008675342,
      "grad_norm": 4.675326347351074,
      "learning_rate": 3.7633965375103055e-05,
      "loss": 1.0481,
      "step": 2600
    },
    {
      "epoch": 0.8708708708708709,
      "grad_norm": 3.832808494567871,
      "learning_rate": 3.757507949593688e-05,
      "loss": 1.0738,
      "step": 2610
    },
    {
      "epoch": 0.8742075408742075,
      "grad_norm": 3.6455960273742676,
      "learning_rate": 3.75161936167707e-05,
      "loss": 0.8546,
      "step": 2620
    },
    {
      "epoch": 0.8775442108775442,
      "grad_norm": 3.998145580291748,
      "learning_rate": 3.745730773760453e-05,
      "loss": 0.8991,
      "step": 2630
    },
    {
      "epoch": 0.8808808808808809,
      "grad_norm": 4.532684803009033,
      "learning_rate": 3.739842185843835e-05,
      "loss": 1.0026,
      "step": 2640
    },
    {
      "epoch": 0.8842175508842175,
      "grad_norm": 4.930627822875977,
      "learning_rate": 3.733953597927217e-05,
      "loss": 0.8846,
      "step": 2650
    },
    {
      "epoch": 0.8875542208875542,
      "grad_norm": 4.162967205047607,
      "learning_rate": 3.728065010010599e-05,
      "loss": 1.1876,
      "step": 2660
    },
    {
      "epoch": 0.8908908908908909,
      "grad_norm": 3.4028093814849854,
      "learning_rate": 3.722176422093982e-05,
      "loss": 0.9645,
      "step": 2670
    },
    {
      "epoch": 0.8942275608942276,
      "grad_norm": 5.702974796295166,
      "learning_rate": 3.716287834177364e-05,
      "loss": 1.0159,
      "step": 2680
    },
    {
      "epoch": 0.8975642308975642,
      "grad_norm": 5.855227470397949,
      "learning_rate": 3.7103992462607465e-05,
      "loss": 1.0794,
      "step": 2690
    },
    {
      "epoch": 0.9009009009009009,
      "grad_norm": 4.076763153076172,
      "learning_rate": 3.7045106583441293e-05,
      "loss": 1.1099,
      "step": 2700
    },
    {
      "epoch": 0.9042375709042376,
      "grad_norm": 4.325056076049805,
      "learning_rate": 3.6986220704275115e-05,
      "loss": 1.1451,
      "step": 2710
    },
    {
      "epoch": 0.9075742409075742,
      "grad_norm": 6.172327995300293,
      "learning_rate": 3.692733482510894e-05,
      "loss": 0.8643,
      "step": 2720
    },
    {
      "epoch": 0.9109109109109109,
      "grad_norm": 6.047774314880371,
      "learning_rate": 3.6868448945942766e-05,
      "loss": 0.9686,
      "step": 2730
    },
    {
      "epoch": 0.9142475809142476,
      "grad_norm": 5.221303462982178,
      "learning_rate": 3.680956306677659e-05,
      "loss": 1.0739,
      "step": 2740
    },
    {
      "epoch": 0.9175842509175842,
      "grad_norm": 3.6698429584503174,
      "learning_rate": 3.675067718761041e-05,
      "loss": 1.0057,
      "step": 2750
    },
    {
      "epoch": 0.9209209209209209,
      "grad_norm": 4.3993024826049805,
      "learning_rate": 3.669179130844424e-05,
      "loss": 0.9893,
      "step": 2760
    },
    {
      "epoch": 0.9242575909242576,
      "grad_norm": 4.986630916595459,
      "learning_rate": 3.663290542927806e-05,
      "loss": 0.8239,
      "step": 2770
    },
    {
      "epoch": 0.9275942609275942,
      "grad_norm": 4.477352142333984,
      "learning_rate": 3.657401955011189e-05,
      "loss": 0.993,
      "step": 2780
    },
    {
      "epoch": 0.9309309309309309,
      "grad_norm": 5.3636322021484375,
      "learning_rate": 3.651513367094571e-05,
      "loss": 0.8912,
      "step": 2790
    },
    {
      "epoch": 0.9342676009342676,
      "grad_norm": 4.185081481933594,
      "learning_rate": 3.645624779177954e-05,
      "loss": 0.87,
      "step": 2800
    },
    {
      "epoch": 0.9376042709376042,
      "grad_norm": 5.61265754699707,
      "learning_rate": 3.639736191261336e-05,
      "loss": 1.063,
      "step": 2810
    },
    {
      "epoch": 0.9409409409409409,
      "grad_norm": 5.876018047332764,
      "learning_rate": 3.633847603344718e-05,
      "loss": 0.8806,
      "step": 2820
    },
    {
      "epoch": 0.9442776109442776,
      "grad_norm": 4.064873695373535,
      "learning_rate": 3.6279590154281004e-05,
      "loss": 1.0168,
      "step": 2830
    },
    {
      "epoch": 0.9476142809476142,
      "grad_norm": 2.6956260204315186,
      "learning_rate": 3.622070427511483e-05,
      "loss": 1.0221,
      "step": 2840
    },
    {
      "epoch": 0.950950950950951,
      "grad_norm": 4.487701416015625,
      "learning_rate": 3.6161818395948654e-05,
      "loss": 1.2001,
      "step": 2850
    },
    {
      "epoch": 0.9542876209542877,
      "grad_norm": 4.530964374542236,
      "learning_rate": 3.6102932516782476e-05,
      "loss": 1.0219,
      "step": 2860
    },
    {
      "epoch": 0.9576242909576242,
      "grad_norm": 4.48826789855957,
      "learning_rate": 3.6044046637616304e-05,
      "loss": 1.0092,
      "step": 2870
    },
    {
      "epoch": 0.960960960960961,
      "grad_norm": 5.333290100097656,
      "learning_rate": 3.5985160758450126e-05,
      "loss": 0.7527,
      "step": 2880
    },
    {
      "epoch": 0.9642976309642977,
      "grad_norm": 5.828945159912109,
      "learning_rate": 3.592627487928395e-05,
      "loss": 1.0424,
      "step": 2890
    },
    {
      "epoch": 0.9676343009676343,
      "grad_norm": 8.202764511108398,
      "learning_rate": 3.5867389000117776e-05,
      "loss": 1.1884,
      "step": 2900
    },
    {
      "epoch": 0.970970970970971,
      "grad_norm": 7.321183681488037,
      "learning_rate": 3.58085031209516e-05,
      "loss": 0.9176,
      "step": 2910
    },
    {
      "epoch": 0.9743076409743077,
      "grad_norm": 4.25532865524292,
      "learning_rate": 3.574961724178542e-05,
      "loss": 1.0212,
      "step": 2920
    },
    {
      "epoch": 0.9776443109776443,
      "grad_norm": 3.028611660003662,
      "learning_rate": 3.569073136261925e-05,
      "loss": 0.859,
      "step": 2930
    },
    {
      "epoch": 0.980980980980981,
      "grad_norm": 4.605105400085449,
      "learning_rate": 3.563184548345307e-05,
      "loss": 0.9476,
      "step": 2940
    },
    {
      "epoch": 0.9843176509843177,
      "grad_norm": 4.377236843109131,
      "learning_rate": 3.557295960428689e-05,
      "loss": 1.0387,
      "step": 2950
    },
    {
      "epoch": 0.9876543209876543,
      "grad_norm": 2.8962743282318115,
      "learning_rate": 3.5514073725120714e-05,
      "loss": 0.7762,
      "step": 2960
    },
    {
      "epoch": 0.990990990990991,
      "grad_norm": 5.851387023925781,
      "learning_rate": 3.545518784595454e-05,
      "loss": 1.0775,
      "step": 2970
    },
    {
      "epoch": 0.9943276609943277,
      "grad_norm": 4.287487030029297,
      "learning_rate": 3.5396301966788364e-05,
      "loss": 1.2277,
      "step": 2980
    },
    {
      "epoch": 0.9976643309976644,
      "grad_norm": 4.4991044998168945,
      "learning_rate": 3.5337416087622186e-05,
      "loss": 1.2668,
      "step": 2990
    },
    {
      "epoch": 1.001001001001001,
      "grad_norm": 6.107329845428467,
      "learning_rate": 3.5278530208456014e-05,
      "loss": 1.2734,
      "step": 3000
    },
    {
      "epoch": 1.0043376710043377,
      "grad_norm": 4.124959468841553,
      "learning_rate": 3.5219644329289836e-05,
      "loss": 1.0428,
      "step": 3010
    },
    {
      "epoch": 1.0076743410076743,
      "grad_norm": 4.946746826171875,
      "learning_rate": 3.516075845012366e-05,
      "loss": 1.0084,
      "step": 3020
    },
    {
      "epoch": 1.011011011011011,
      "grad_norm": 6.0722808837890625,
      "learning_rate": 3.5101872570957486e-05,
      "loss": 1.0328,
      "step": 3030
    },
    {
      "epoch": 1.0143476810143477,
      "grad_norm": 6.764739036560059,
      "learning_rate": 3.504298669179131e-05,
      "loss": 1.008,
      "step": 3040
    },
    {
      "epoch": 1.0176843510176843,
      "grad_norm": 1.3837063312530518,
      "learning_rate": 3.498410081262514e-05,
      "loss": 0.8684,
      "step": 3050
    },
    {
      "epoch": 1.021021021021021,
      "grad_norm": 6.128448009490967,
      "learning_rate": 3.492521493345896e-05,
      "loss": 1.2344,
      "step": 3060
    },
    {
      "epoch": 1.0243576910243577,
      "grad_norm": 6.455077171325684,
      "learning_rate": 3.486632905429279e-05,
      "loss": 1.167,
      "step": 3070
    },
    {
      "epoch": 1.0276943610276943,
      "grad_norm": 3.454843759536743,
      "learning_rate": 3.480744317512661e-05,
      "loss": 0.8408,
      "step": 3080
    },
    {
      "epoch": 1.031031031031031,
      "grad_norm": 4.980084419250488,
      "learning_rate": 3.474855729596043e-05,
      "loss": 1.0318,
      "step": 3090
    },
    {
      "epoch": 1.0343677010343677,
      "grad_norm": 5.9065775871276855,
      "learning_rate": 3.468967141679426e-05,
      "loss": 1.0834,
      "step": 3100
    },
    {
      "epoch": 1.0377043710377043,
      "grad_norm": 7.222415447235107,
      "learning_rate": 3.463078553762808e-05,
      "loss": 1.1339,
      "step": 3110
    },
    {
      "epoch": 1.0410410410410411,
      "grad_norm": 6.7444844245910645,
      "learning_rate": 3.45718996584619e-05,
      "loss": 0.924,
      "step": 3120
    },
    {
      "epoch": 1.0443777110443777,
      "grad_norm": 5.90389347076416,
      "learning_rate": 3.4513013779295724e-05,
      "loss": 0.868,
      "step": 3130
    },
    {
      "epoch": 1.0477143810477143,
      "grad_norm": 4.0358195304870605,
      "learning_rate": 3.445412790012955e-05,
      "loss": 1.0779,
      "step": 3140
    },
    {
      "epoch": 1.0510510510510511,
      "grad_norm": 6.531481742858887,
      "learning_rate": 3.4395242020963375e-05,
      "loss": 0.9412,
      "step": 3150
    },
    {
      "epoch": 1.0543877210543877,
      "grad_norm": 5.394105911254883,
      "learning_rate": 3.4336356141797197e-05,
      "loss": 0.7926,
      "step": 3160
    },
    {
      "epoch": 1.0577243910577243,
      "grad_norm": 5.516204833984375,
      "learning_rate": 3.4277470262631025e-05,
      "loss": 1.0698,
      "step": 3170
    },
    {
      "epoch": 1.0610610610610611,
      "grad_norm": 3.1294829845428467,
      "learning_rate": 3.421858438346485e-05,
      "loss": 0.8542,
      "step": 3180
    },
    {
      "epoch": 1.0643977310643977,
      "grad_norm": 5.033647537231445,
      "learning_rate": 3.415969850429867e-05,
      "loss": 0.808,
      "step": 3190
    },
    {
      "epoch": 1.0677344010677343,
      "grad_norm": 5.309226036071777,
      "learning_rate": 3.41008126251325e-05,
      "loss": 0.9268,
      "step": 3200
    },
    {
      "epoch": 1.0710710710710711,
      "grad_norm": 4.488002777099609,
      "learning_rate": 3.404192674596632e-05,
      "loss": 0.9067,
      "step": 3210
    },
    {
      "epoch": 1.0744077410744077,
      "grad_norm": 4.105489253997803,
      "learning_rate": 3.398304086680014e-05,
      "loss": 0.7346,
      "step": 3220
    },
    {
      "epoch": 1.0777444110777443,
      "grad_norm": 7.047926902770996,
      "learning_rate": 3.392415498763397e-05,
      "loss": 1.0552,
      "step": 3230
    },
    {
      "epoch": 1.0810810810810811,
      "grad_norm": 3.5358290672302246,
      "learning_rate": 3.386526910846779e-05,
      "loss": 1.1827,
      "step": 3240
    },
    {
      "epoch": 1.0844177510844177,
      "grad_norm": 7.566186904907227,
      "learning_rate": 3.380638322930161e-05,
      "loss": 0.8479,
      "step": 3250
    },
    {
      "epoch": 1.0877544210877543,
      "grad_norm": 3.9561822414398193,
      "learning_rate": 3.3747497350135435e-05,
      "loss": 0.7722,
      "step": 3260
    },
    {
      "epoch": 1.0910910910910911,
      "grad_norm": 4.123648166656494,
      "learning_rate": 3.368861147096926e-05,
      "loss": 0.9573,
      "step": 3270
    },
    {
      "epoch": 1.0944277610944277,
      "grad_norm": 4.367898464202881,
      "learning_rate": 3.3629725591803085e-05,
      "loss": 0.7562,
      "step": 3280
    },
    {
      "epoch": 1.0977644310977643,
      "grad_norm": 4.766269683837891,
      "learning_rate": 3.357083971263691e-05,
      "loss": 0.8558,
      "step": 3290
    },
    {
      "epoch": 1.1011011011011012,
      "grad_norm": 5.469362258911133,
      "learning_rate": 3.3511953833470735e-05,
      "loss": 0.8266,
      "step": 3300
    },
    {
      "epoch": 1.1044377711044377,
      "grad_norm": 5.945261001586914,
      "learning_rate": 3.345306795430456e-05,
      "loss": 0.9677,
      "step": 3310
    },
    {
      "epoch": 1.1077744411077743,
      "grad_norm": 5.4678168296813965,
      "learning_rate": 3.3394182075138386e-05,
      "loss": 0.8111,
      "step": 3320
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 4.683640480041504,
      "learning_rate": 3.333529619597221e-05,
      "loss": 0.9189,
      "step": 3330
    },
    {
      "epoch": 1.1144477811144478,
      "grad_norm": 4.292535305023193,
      "learning_rate": 3.3276410316806036e-05,
      "loss": 1.1633,
      "step": 3340
    },
    {
      "epoch": 1.1177844511177844,
      "grad_norm": 6.897686004638672,
      "learning_rate": 3.321752443763986e-05,
      "loss": 1.002,
      "step": 3350
    },
    {
      "epoch": 1.1211211211211212,
      "grad_norm": 4.257903099060059,
      "learning_rate": 3.315863855847368e-05,
      "loss": 0.9308,
      "step": 3360
    },
    {
      "epoch": 1.1244577911244578,
      "grad_norm": 4.94467306137085,
      "learning_rate": 3.309975267930751e-05,
      "loss": 1.0582,
      "step": 3370
    },
    {
      "epoch": 1.1277944611277944,
      "grad_norm": 6.096086502075195,
      "learning_rate": 3.304086680014133e-05,
      "loss": 0.9572,
      "step": 3380
    },
    {
      "epoch": 1.1311311311311312,
      "grad_norm": 5.628101825714111,
      "learning_rate": 3.298198092097515e-05,
      "loss": 0.9417,
      "step": 3390
    },
    {
      "epoch": 1.1344678011344678,
      "grad_norm": 5.101990222930908,
      "learning_rate": 3.292309504180898e-05,
      "loss": 1.0409,
      "step": 3400
    },
    {
      "epoch": 1.1378044711378044,
      "grad_norm": 6.251267910003662,
      "learning_rate": 3.28642091626428e-05,
      "loss": 1.0488,
      "step": 3410
    },
    {
      "epoch": 1.1411411411411412,
      "grad_norm": 4.987309455871582,
      "learning_rate": 3.2805323283476624e-05,
      "loss": 1.2284,
      "step": 3420
    },
    {
      "epoch": 1.1444778111444778,
      "grad_norm": 4.977793216705322,
      "learning_rate": 3.2746437404310445e-05,
      "loss": 0.8652,
      "step": 3430
    },
    {
      "epoch": 1.1478144811478144,
      "grad_norm": 4.497901439666748,
      "learning_rate": 3.2687551525144274e-05,
      "loss": 0.9389,
      "step": 3440
    },
    {
      "epoch": 1.1511511511511512,
      "grad_norm": 4.752035140991211,
      "learning_rate": 3.2628665645978096e-05,
      "loss": 1.0733,
      "step": 3450
    },
    {
      "epoch": 1.1544878211544878,
      "grad_norm": 3.401003122329712,
      "learning_rate": 3.256977976681192e-05,
      "loss": 0.9172,
      "step": 3460
    },
    {
      "epoch": 1.1578244911578244,
      "grad_norm": 4.490971565246582,
      "learning_rate": 3.2510893887645746e-05,
      "loss": 0.8989,
      "step": 3470
    },
    {
      "epoch": 1.1611611611611612,
      "grad_norm": 5.833939075469971,
      "learning_rate": 3.245200800847957e-05,
      "loss": 1.2388,
      "step": 3480
    },
    {
      "epoch": 1.1644978311644978,
      "grad_norm": 4.074257850646973,
      "learning_rate": 3.239312212931339e-05,
      "loss": 0.9609,
      "step": 3490
    },
    {
      "epoch": 1.1678345011678344,
      "grad_norm": 6.005265235900879,
      "learning_rate": 3.233423625014722e-05,
      "loss": 0.8939,
      "step": 3500
    },
    {
      "epoch": 1.1711711711711712,
      "grad_norm": 7.347655296325684,
      "learning_rate": 3.227535037098104e-05,
      "loss": 1.0849,
      "step": 3510
    },
    {
      "epoch": 1.1745078411745078,
      "grad_norm": 5.65125846862793,
      "learning_rate": 3.221646449181486e-05,
      "loss": 0.8719,
      "step": 3520
    },
    {
      "epoch": 1.1778445111778444,
      "grad_norm": 2.512094020843506,
      "learning_rate": 3.2157578612648683e-05,
      "loss": 0.9119,
      "step": 3530
    },
    {
      "epoch": 1.1811811811811812,
      "grad_norm": 3.9574453830718994,
      "learning_rate": 3.209869273348251e-05,
      "loss": 1.2197,
      "step": 3540
    },
    {
      "epoch": 1.1845178511845178,
      "grad_norm": 4.77421760559082,
      "learning_rate": 3.2039806854316334e-05,
      "loss": 0.917,
      "step": 3550
    },
    {
      "epoch": 1.1878545211878546,
      "grad_norm": 4.045111179351807,
      "learning_rate": 3.1980920975150155e-05,
      "loss": 1.0475,
      "step": 3560
    },
    {
      "epoch": 1.1911911911911912,
      "grad_norm": 4.536621570587158,
      "learning_rate": 3.1922035095983984e-05,
      "loss": 0.9208,
      "step": 3570
    },
    {
      "epoch": 1.1945278611945278,
      "grad_norm": 4.737792491912842,
      "learning_rate": 3.1863149216817806e-05,
      "loss": 0.9107,
      "step": 3580
    },
    {
      "epoch": 1.1978645311978646,
      "grad_norm": 5.389671325683594,
      "learning_rate": 3.1804263337651634e-05,
      "loss": 1.0436,
      "step": 3590
    },
    {
      "epoch": 1.2012012012012012,
      "grad_norm": 4.240820407867432,
      "learning_rate": 3.1745377458485456e-05,
      "loss": 0.9232,
      "step": 3600
    }
  ],
  "logging_steps": 10,
  "max_steps": 8991,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 900,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.5204868613941248e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
