{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.7027027027027026,
  "eval_steps": 500,
  "global_step": 8100,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00333667000333667,
      "grad_norm": 8.857905387878418,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 4.4678,
      "step": 10
    },
    {
      "epoch": 0.00667334000667334,
      "grad_norm": 8.59739875793457,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 3.9352,
      "step": 20
    },
    {
      "epoch": 0.01001001001001001,
      "grad_norm": 8.053893089294434,
      "learning_rate": 3e-06,
      "loss": 3.9121,
      "step": 30
    },
    {
      "epoch": 0.01334668001334668,
      "grad_norm": 7.265538215637207,
      "learning_rate": 4.000000000000001e-06,
      "loss": 3.3505,
      "step": 40
    },
    {
      "epoch": 0.01668335001668335,
      "grad_norm": 8.49083137512207,
      "learning_rate": 5e-06,
      "loss": 3.7749,
      "step": 50
    },
    {
      "epoch": 0.02002002002002002,
      "grad_norm": 11.937213897705078,
      "learning_rate": 6e-06,
      "loss": 4.3121,
      "step": 60
    },
    {
      "epoch": 0.02335669002335669,
      "grad_norm": 6.517889976501465,
      "learning_rate": 7.000000000000001e-06,
      "loss": 3.7168,
      "step": 70
    },
    {
      "epoch": 0.02669336002669336,
      "grad_norm": 7.611666202545166,
      "learning_rate": 8.000000000000001e-06,
      "loss": 3.3343,
      "step": 80
    },
    {
      "epoch": 0.03003003003003003,
      "grad_norm": 11.604483604431152,
      "learning_rate": 9e-06,
      "loss": 3.3564,
      "step": 90
    },
    {
      "epoch": 0.0333667000333667,
      "grad_norm": 5.543286323547363,
      "learning_rate": 1e-05,
      "loss": 2.8424,
      "step": 100
    },
    {
      "epoch": 0.03670337003670337,
      "grad_norm": 9.923837661743164,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 3.0617,
      "step": 110
    },
    {
      "epoch": 0.04004004004004004,
      "grad_norm": 8.410821914672852,
      "learning_rate": 1.2e-05,
      "loss": 3.654,
      "step": 120
    },
    {
      "epoch": 0.04337671004337671,
      "grad_norm": 7.855680465698242,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 3.8245,
      "step": 130
    },
    {
      "epoch": 0.04671338004671338,
      "grad_norm": 7.0160417556762695,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 3.0403,
      "step": 140
    },
    {
      "epoch": 0.05005005005005005,
      "grad_norm": 8.539226531982422,
      "learning_rate": 1.5e-05,
      "loss": 4.0287,
      "step": 150
    },
    {
      "epoch": 0.05338672005338672,
      "grad_norm": 7.546672821044922,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 4.2148,
      "step": 160
    },
    {
      "epoch": 0.05672339005672339,
      "grad_norm": 9.074827194213867,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 3.3234,
      "step": 170
    },
    {
      "epoch": 0.06006006006006006,
      "grad_norm": 8.012894630432129,
      "learning_rate": 1.8e-05,
      "loss": 3.4108,
      "step": 180
    },
    {
      "epoch": 0.06339673006339673,
      "grad_norm": 7.305278301239014,
      "learning_rate": 1.9e-05,
      "loss": 3.8104,
      "step": 190
    },
    {
      "epoch": 0.0667334000667334,
      "grad_norm": 9.536160469055176,
      "learning_rate": 2e-05,
      "loss": 2.6948,
      "step": 200
    },
    {
      "epoch": 0.07007007007007007,
      "grad_norm": 7.950663089752197,
      "learning_rate": 2.1e-05,
      "loss": 3.3356,
      "step": 210
    },
    {
      "epoch": 0.07340674007340674,
      "grad_norm": 7.788869380950928,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 2.2596,
      "step": 220
    },
    {
      "epoch": 0.07674341007674342,
      "grad_norm": 7.822196006774902,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 2.5057,
      "step": 230
    },
    {
      "epoch": 0.08008008008008008,
      "grad_norm": 7.548488140106201,
      "learning_rate": 2.4e-05,
      "loss": 2.6202,
      "step": 240
    },
    {
      "epoch": 0.08341675008341674,
      "grad_norm": 6.796125888824463,
      "learning_rate": 2.5e-05,
      "loss": 1.9175,
      "step": 250
    },
    {
      "epoch": 0.08675342008675342,
      "grad_norm": 7.555082321166992,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.7628,
      "step": 260
    },
    {
      "epoch": 0.09009009009009009,
      "grad_norm": 6.521301746368408,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 2.0625,
      "step": 270
    },
    {
      "epoch": 0.09342676009342676,
      "grad_norm": 5.06267786026001,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.8207,
      "step": 280
    },
    {
      "epoch": 0.09676343009676343,
      "grad_norm": 6.200467586517334,
      "learning_rate": 2.9e-05,
      "loss": 1.9648,
      "step": 290
    },
    {
      "epoch": 0.1001001001001001,
      "grad_norm": 5.347590446472168,
      "learning_rate": 3e-05,
      "loss": 1.5949,
      "step": 300
    },
    {
      "epoch": 0.10343677010343677,
      "grad_norm": 2.64927077293396,
      "learning_rate": 3.1e-05,
      "loss": 1.5226,
      "step": 310
    },
    {
      "epoch": 0.10677344010677343,
      "grad_norm": 3.8787083625793457,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.8517,
      "step": 320
    },
    {
      "epoch": 0.11011011011011011,
      "grad_norm": 5.351945400238037,
      "learning_rate": 3.3e-05,
      "loss": 1.9147,
      "step": 330
    },
    {
      "epoch": 0.11344678011344678,
      "grad_norm": 4.14184045791626,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.732,
      "step": 340
    },
    {
      "epoch": 0.11678345011678345,
      "grad_norm": 5.556326389312744,
      "learning_rate": 3.5e-05,
      "loss": 1.5702,
      "step": 350
    },
    {
      "epoch": 0.12012012012012012,
      "grad_norm": 4.808996200561523,
      "learning_rate": 3.6e-05,
      "loss": 1.4381,
      "step": 360
    },
    {
      "epoch": 0.12345679012345678,
      "grad_norm": 5.18758487701416,
      "learning_rate": 3.7e-05,
      "loss": 1.6771,
      "step": 370
    },
    {
      "epoch": 0.12679346012679346,
      "grad_norm": 4.98621940612793,
      "learning_rate": 3.8e-05,
      "loss": 1.6097,
      "step": 380
    },
    {
      "epoch": 0.13013013013013014,
      "grad_norm": 4.125382423400879,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.383,
      "step": 390
    },
    {
      "epoch": 0.1334668001334668,
      "grad_norm": 4.260393142700195,
      "learning_rate": 4e-05,
      "loss": 1.4135,
      "step": 400
    },
    {
      "epoch": 0.13680347013680347,
      "grad_norm": 4.048669338226318,
      "learning_rate": 4.1e-05,
      "loss": 1.5159,
      "step": 410
    },
    {
      "epoch": 0.14014014014014015,
      "grad_norm": 4.261200428009033,
      "learning_rate": 4.2e-05,
      "loss": 1.3645,
      "step": 420
    },
    {
      "epoch": 0.14347681014347682,
      "grad_norm": 4.28817892074585,
      "learning_rate": 4.3e-05,
      "loss": 1.5483,
      "step": 430
    },
    {
      "epoch": 0.14681348014681347,
      "grad_norm": 3.5364677906036377,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.4444,
      "step": 440
    },
    {
      "epoch": 0.15015015015015015,
      "grad_norm": 3.8340559005737305,
      "learning_rate": 4.5e-05,
      "loss": 1.2625,
      "step": 450
    },
    {
      "epoch": 0.15348682015348683,
      "grad_norm": 5.591009616851807,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.3474,
      "step": 460
    },
    {
      "epoch": 0.15682349015682348,
      "grad_norm": 4.024402141571045,
      "learning_rate": 4.7e-05,
      "loss": 1.1617,
      "step": 470
    },
    {
      "epoch": 0.16016016016016016,
      "grad_norm": 5.415898323059082,
      "learning_rate": 4.8e-05,
      "loss": 1.7149,
      "step": 480
    },
    {
      "epoch": 0.16349683016349684,
      "grad_norm": 4.3966264724731445,
      "learning_rate": 4.9e-05,
      "loss": 1.3024,
      "step": 490
    },
    {
      "epoch": 0.1668335001668335,
      "grad_norm": 4.0056047439575195,
      "learning_rate": 5e-05,
      "loss": 1.5646,
      "step": 500
    },
    {
      "epoch": 0.17017017017017017,
      "grad_norm": 4.605757236480713,
      "learning_rate": 4.9941114120833824e-05,
      "loss": 1.4762,
      "step": 510
    },
    {
      "epoch": 0.17350684017350684,
      "grad_norm": 2.5374536514282227,
      "learning_rate": 4.988222824166765e-05,
      "loss": 1.1877,
      "step": 520
    },
    {
      "epoch": 0.17684351017684352,
      "grad_norm": 3.0233702659606934,
      "learning_rate": 4.9823342362501474e-05,
      "loss": 1.2266,
      "step": 530
    },
    {
      "epoch": 0.18018018018018017,
      "grad_norm": 4.547967433929443,
      "learning_rate": 4.9764456483335296e-05,
      "loss": 1.3976,
      "step": 540
    },
    {
      "epoch": 0.18351685018351685,
      "grad_norm": 3.5690975189208984,
      "learning_rate": 4.9705570604169125e-05,
      "loss": 1.387,
      "step": 550
    },
    {
      "epoch": 0.18685352018685353,
      "grad_norm": 6.602469444274902,
      "learning_rate": 4.9646684725002947e-05,
      "loss": 1.3689,
      "step": 560
    },
    {
      "epoch": 0.19019019019019018,
      "grad_norm": 6.749355316162109,
      "learning_rate": 4.958779884583677e-05,
      "loss": 1.2421,
      "step": 570
    },
    {
      "epoch": 0.19352686019352686,
      "grad_norm": 2.9303011894226074,
      "learning_rate": 4.952891296667059e-05,
      "loss": 1.0158,
      "step": 580
    },
    {
      "epoch": 0.19686353019686353,
      "grad_norm": 5.631228446960449,
      "learning_rate": 4.947002708750442e-05,
      "loss": 1.295,
      "step": 590
    },
    {
      "epoch": 0.2002002002002002,
      "grad_norm": 4.6857991218566895,
      "learning_rate": 4.941114120833824e-05,
      "loss": 1.3221,
      "step": 600
    },
    {
      "epoch": 0.20353687020353686,
      "grad_norm": 4.097028732299805,
      "learning_rate": 4.935225532917206e-05,
      "loss": 1.3916,
      "step": 610
    },
    {
      "epoch": 0.20687354020687354,
      "grad_norm": 3.8089182376861572,
      "learning_rate": 4.929336945000589e-05,
      "loss": 1.2672,
      "step": 620
    },
    {
      "epoch": 0.21021021021021022,
      "grad_norm": 3.4777469635009766,
      "learning_rate": 4.923448357083971e-05,
      "loss": 1.1686,
      "step": 630
    },
    {
      "epoch": 0.21354688021354687,
      "grad_norm": 6.217365741729736,
      "learning_rate": 4.917559769167354e-05,
      "loss": 1.3046,
      "step": 640
    },
    {
      "epoch": 0.21688355021688355,
      "grad_norm": 2.9496960639953613,
      "learning_rate": 4.911671181250736e-05,
      "loss": 1.1695,
      "step": 650
    },
    {
      "epoch": 0.22022022022022023,
      "grad_norm": 3.933528423309326,
      "learning_rate": 4.905782593334119e-05,
      "loss": 1.2356,
      "step": 660
    },
    {
      "epoch": 0.2235568902235569,
      "grad_norm": 3.9220006465911865,
      "learning_rate": 4.899894005417501e-05,
      "loss": 1.1246,
      "step": 670
    },
    {
      "epoch": 0.22689356022689355,
      "grad_norm": 7.431880474090576,
      "learning_rate": 4.8940054175008835e-05,
      "loss": 1.0677,
      "step": 680
    },
    {
      "epoch": 0.23023023023023023,
      "grad_norm": 4.475502014160156,
      "learning_rate": 4.8881168295842663e-05,
      "loss": 1.3185,
      "step": 690
    },
    {
      "epoch": 0.2335669002335669,
      "grad_norm": 5.457873344421387,
      "learning_rate": 4.8822282416676485e-05,
      "loss": 1.2404,
      "step": 700
    },
    {
      "epoch": 0.23690357023690356,
      "grad_norm": 5.106561183929443,
      "learning_rate": 4.876339653751031e-05,
      "loss": 1.4241,
      "step": 710
    },
    {
      "epoch": 0.24024024024024024,
      "grad_norm": 6.592267036437988,
      "learning_rate": 4.8704510658344136e-05,
      "loss": 1.366,
      "step": 720
    },
    {
      "epoch": 0.24357691024357692,
      "grad_norm": 3.60618257522583,
      "learning_rate": 4.864562477917796e-05,
      "loss": 1.1119,
      "step": 730
    },
    {
      "epoch": 0.24691358024691357,
      "grad_norm": 3.0848686695098877,
      "learning_rate": 4.858673890001178e-05,
      "loss": 1.0512,
      "step": 740
    },
    {
      "epoch": 0.2502502502502503,
      "grad_norm": 4.673299789428711,
      "learning_rate": 4.85278530208456e-05,
      "loss": 1.2696,
      "step": 750
    },
    {
      "epoch": 0.2535869202535869,
      "grad_norm": 4.895904064178467,
      "learning_rate": 4.846896714167943e-05,
      "loss": 1.1805,
      "step": 760
    },
    {
      "epoch": 0.2569235902569236,
      "grad_norm": 4.752594947814941,
      "learning_rate": 4.841008126251325e-05,
      "loss": 1.1475,
      "step": 770
    },
    {
      "epoch": 0.2602602602602603,
      "grad_norm": 4.059188365936279,
      "learning_rate": 4.835119538334707e-05,
      "loss": 1.2825,
      "step": 780
    },
    {
      "epoch": 0.26359693026359693,
      "grad_norm": 3.561336040496826,
      "learning_rate": 4.82923095041809e-05,
      "loss": 1.1613,
      "step": 790
    },
    {
      "epoch": 0.2669336002669336,
      "grad_norm": 4.691658973693848,
      "learning_rate": 4.823342362501472e-05,
      "loss": 1.5574,
      "step": 800
    },
    {
      "epoch": 0.2702702702702703,
      "grad_norm": 3.560206413269043,
      "learning_rate": 4.8174537745848545e-05,
      "loss": 1.212,
      "step": 810
    },
    {
      "epoch": 0.27360694027360694,
      "grad_norm": 4.498392581939697,
      "learning_rate": 4.8115651866682374e-05,
      "loss": 1.0687,
      "step": 820
    },
    {
      "epoch": 0.2769436102769436,
      "grad_norm": 3.535398483276367,
      "learning_rate": 4.8056765987516195e-05,
      "loss": 1.2544,
      "step": 830
    },
    {
      "epoch": 0.2802802802802803,
      "grad_norm": 5.058297157287598,
      "learning_rate": 4.799788010835002e-05,
      "loss": 1.4341,
      "step": 840
    },
    {
      "epoch": 0.28361695028361694,
      "grad_norm": 3.874671697616577,
      "learning_rate": 4.7938994229183846e-05,
      "loss": 1.1299,
      "step": 850
    },
    {
      "epoch": 0.28695362028695365,
      "grad_norm": 5.592775821685791,
      "learning_rate": 4.788010835001767e-05,
      "loss": 1.0973,
      "step": 860
    },
    {
      "epoch": 0.2902902902902903,
      "grad_norm": 3.8135714530944824,
      "learning_rate": 4.782122247085149e-05,
      "loss": 1.1867,
      "step": 870
    },
    {
      "epoch": 0.29362696029362695,
      "grad_norm": 5.178786754608154,
      "learning_rate": 4.776233659168531e-05,
      "loss": 1.1778,
      "step": 880
    },
    {
      "epoch": 0.29696363029696365,
      "grad_norm": 3.325193166732788,
      "learning_rate": 4.770345071251914e-05,
      "loss": 1.022,
      "step": 890
    },
    {
      "epoch": 0.3003003003003003,
      "grad_norm": 3.699855089187622,
      "learning_rate": 4.764456483335296e-05,
      "loss": 1.3642,
      "step": 900
    },
    {
      "epoch": 0.30363697030363695,
      "grad_norm": 4.583566665649414,
      "learning_rate": 4.758567895418679e-05,
      "loss": 1.0707,
      "step": 910
    },
    {
      "epoch": 0.30697364030697366,
      "grad_norm": 5.991292476654053,
      "learning_rate": 4.752679307502061e-05,
      "loss": 1.1113,
      "step": 920
    },
    {
      "epoch": 0.3103103103103103,
      "grad_norm": 3.690126419067383,
      "learning_rate": 4.746790719585444e-05,
      "loss": 1.294,
      "step": 930
    },
    {
      "epoch": 0.31364698031364696,
      "grad_norm": 6.788210391998291,
      "learning_rate": 4.740902131668826e-05,
      "loss": 1.0856,
      "step": 940
    },
    {
      "epoch": 0.31698365031698367,
      "grad_norm": 3.347337484359741,
      "learning_rate": 4.7350135437522084e-05,
      "loss": 0.9876,
      "step": 950
    },
    {
      "epoch": 0.3203203203203203,
      "grad_norm": 4.475565433502197,
      "learning_rate": 4.729124955835591e-05,
      "loss": 1.0511,
      "step": 960
    },
    {
      "epoch": 0.32365699032365697,
      "grad_norm": 7.910323143005371,
      "learning_rate": 4.7232363679189734e-05,
      "loss": 1.4289,
      "step": 970
    },
    {
      "epoch": 0.3269936603269937,
      "grad_norm": 3.852656364440918,
      "learning_rate": 4.7173477800023556e-05,
      "loss": 1.1075,
      "step": 980
    },
    {
      "epoch": 0.3303303303303303,
      "grad_norm": 6.85919189453125,
      "learning_rate": 4.7114591920857384e-05,
      "loss": 1.1374,
      "step": 990
    },
    {
      "epoch": 0.333667000333667,
      "grad_norm": 4.76578426361084,
      "learning_rate": 4.7055706041691206e-05,
      "loss": 1.1148,
      "step": 1000
    },
    {
      "epoch": 0.3370036703370037,
      "grad_norm": 6.559088706970215,
      "learning_rate": 4.699682016252503e-05,
      "loss": 1.1183,
      "step": 1010
    },
    {
      "epoch": 0.34034034034034033,
      "grad_norm": 6.129230499267578,
      "learning_rate": 4.6937934283358856e-05,
      "loss": 1.1669,
      "step": 1020
    },
    {
      "epoch": 0.34367701034367704,
      "grad_norm": 4.710047721862793,
      "learning_rate": 4.687904840419268e-05,
      "loss": 1.2316,
      "step": 1030
    },
    {
      "epoch": 0.3470136803470137,
      "grad_norm": 5.12595272064209,
      "learning_rate": 4.68201625250265e-05,
      "loss": 1.1542,
      "step": 1040
    },
    {
      "epoch": 0.35035035035035034,
      "grad_norm": 2.357759952545166,
      "learning_rate": 4.676127664586032e-05,
      "loss": 0.9957,
      "step": 1050
    },
    {
      "epoch": 0.35368702035368704,
      "grad_norm": 3.131687641143799,
      "learning_rate": 4.670239076669415e-05,
      "loss": 1.2648,
      "step": 1060
    },
    {
      "epoch": 0.3570236903570237,
      "grad_norm": 3.833237886428833,
      "learning_rate": 4.664350488752797e-05,
      "loss": 1.0358,
      "step": 1070
    },
    {
      "epoch": 0.36036036036036034,
      "grad_norm": 4.858419895172119,
      "learning_rate": 4.6584619008361794e-05,
      "loss": 1.292,
      "step": 1080
    },
    {
      "epoch": 0.36369703036369705,
      "grad_norm": 4.332526206970215,
      "learning_rate": 4.652573312919562e-05,
      "loss": 1.3125,
      "step": 1090
    },
    {
      "epoch": 0.3670337003670337,
      "grad_norm": 3.6166481971740723,
      "learning_rate": 4.6466847250029444e-05,
      "loss": 1.1565,
      "step": 1100
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 4.405731678009033,
      "learning_rate": 4.6407961370863266e-05,
      "loss": 1.1211,
      "step": 1110
    },
    {
      "epoch": 0.37370704037370706,
      "grad_norm": 6.173271179199219,
      "learning_rate": 4.6349075491697094e-05,
      "loss": 1.0745,
      "step": 1120
    },
    {
      "epoch": 0.3770437103770437,
      "grad_norm": 4.33131217956543,
      "learning_rate": 4.6290189612530916e-05,
      "loss": 1.0042,
      "step": 1130
    },
    {
      "epoch": 0.38038038038038036,
      "grad_norm": 4.473048210144043,
      "learning_rate": 4.623130373336474e-05,
      "loss": 1.2826,
      "step": 1140
    },
    {
      "epoch": 0.38371705038371706,
      "grad_norm": 4.881191253662109,
      "learning_rate": 4.617241785419856e-05,
      "loss": 0.9516,
      "step": 1150
    },
    {
      "epoch": 0.3870537203870537,
      "grad_norm": 5.25804328918457,
      "learning_rate": 4.611353197503239e-05,
      "loss": 0.9502,
      "step": 1160
    },
    {
      "epoch": 0.39039039039039036,
      "grad_norm": 4.6736369132995605,
      "learning_rate": 4.605464609586622e-05,
      "loss": 1.1411,
      "step": 1170
    },
    {
      "epoch": 0.39372706039372707,
      "grad_norm": 5.670393466949463,
      "learning_rate": 4.599576021670004e-05,
      "loss": 1.2091,
      "step": 1180
    },
    {
      "epoch": 0.3970637303970637,
      "grad_norm": 4.273220539093018,
      "learning_rate": 4.593687433753387e-05,
      "loss": 1.3297,
      "step": 1190
    },
    {
      "epoch": 0.4004004004004004,
      "grad_norm": 4.789172649383545,
      "learning_rate": 4.587798845836769e-05,
      "loss": 1.1299,
      "step": 1200
    },
    {
      "epoch": 0.4037370704037371,
      "grad_norm": 4.748783588409424,
      "learning_rate": 4.581910257920151e-05,
      "loss": 1.0546,
      "step": 1210
    },
    {
      "epoch": 0.4070737404070737,
      "grad_norm": 5.400569915771484,
      "learning_rate": 4.576021670003533e-05,
      "loss": 1.4714,
      "step": 1220
    },
    {
      "epoch": 0.41041041041041043,
      "grad_norm": 5.7481369972229,
      "learning_rate": 4.570133082086916e-05,
      "loss": 1.2662,
      "step": 1230
    },
    {
      "epoch": 0.4137470804137471,
      "grad_norm": 3.5832130908966064,
      "learning_rate": 4.564244494170298e-05,
      "loss": 1.1563,
      "step": 1240
    },
    {
      "epoch": 0.41708375041708373,
      "grad_norm": 4.613012313842773,
      "learning_rate": 4.5583559062536805e-05,
      "loss": 1.0004,
      "step": 1250
    },
    {
      "epoch": 0.42042042042042044,
      "grad_norm": 5.867781162261963,
      "learning_rate": 4.552467318337063e-05,
      "loss": 1.0469,
      "step": 1260
    },
    {
      "epoch": 0.4237570904237571,
      "grad_norm": 3.820117712020874,
      "learning_rate": 4.5465787304204455e-05,
      "loss": 1.2976,
      "step": 1270
    },
    {
      "epoch": 0.42709376042709374,
      "grad_norm": 6.1656928062438965,
      "learning_rate": 4.540690142503828e-05,
      "loss": 1.1519,
      "step": 1280
    },
    {
      "epoch": 0.43043043043043044,
      "grad_norm": 6.3360700607299805,
      "learning_rate": 4.5348015545872105e-05,
      "loss": 1.2347,
      "step": 1290
    },
    {
      "epoch": 0.4337671004337671,
      "grad_norm": 5.503486156463623,
      "learning_rate": 4.528912966670593e-05,
      "loss": 1.2732,
      "step": 1300
    },
    {
      "epoch": 0.43710377043710374,
      "grad_norm": 3.3686165809631348,
      "learning_rate": 4.523024378753975e-05,
      "loss": 0.989,
      "step": 1310
    },
    {
      "epoch": 0.44044044044044045,
      "grad_norm": 6.149549961090088,
      "learning_rate": 4.517135790837357e-05,
      "loss": 1.2925,
      "step": 1320
    },
    {
      "epoch": 0.4437771104437771,
      "grad_norm": 5.150977611541748,
      "learning_rate": 4.51124720292074e-05,
      "loss": 0.9846,
      "step": 1330
    },
    {
      "epoch": 0.4471137804471138,
      "grad_norm": 6.125264644622803,
      "learning_rate": 4.505358615004122e-05,
      "loss": 1.0251,
      "step": 1340
    },
    {
      "epoch": 0.45045045045045046,
      "grad_norm": 3.403777837753296,
      "learning_rate": 4.499470027087504e-05,
      "loss": 1.0786,
      "step": 1350
    },
    {
      "epoch": 0.4537871204537871,
      "grad_norm": 3.3144290447235107,
      "learning_rate": 4.493581439170887e-05,
      "loss": 1.3567,
      "step": 1360
    },
    {
      "epoch": 0.4571237904571238,
      "grad_norm": 4.10409688949585,
      "learning_rate": 4.487692851254269e-05,
      "loss": 1.2828,
      "step": 1370
    },
    {
      "epoch": 0.46046046046046046,
      "grad_norm": 3.7806947231292725,
      "learning_rate": 4.4818042633376515e-05,
      "loss": 1.2158,
      "step": 1380
    },
    {
      "epoch": 0.4637971304637971,
      "grad_norm": 6.468990802764893,
      "learning_rate": 4.475915675421034e-05,
      "loss": 1.1004,
      "step": 1390
    },
    {
      "epoch": 0.4671338004671338,
      "grad_norm": 6.160029888153076,
      "learning_rate": 4.4700270875044165e-05,
      "loss": 1.4794,
      "step": 1400
    },
    {
      "epoch": 0.47047047047047047,
      "grad_norm": 5.245044708251953,
      "learning_rate": 4.464138499587799e-05,
      "loss": 1.1136,
      "step": 1410
    },
    {
      "epoch": 0.4738071404738071,
      "grad_norm": 4.885804176330566,
      "learning_rate": 4.4582499116711815e-05,
      "loss": 1.2628,
      "step": 1420
    },
    {
      "epoch": 0.4771438104771438,
      "grad_norm": 4.391535758972168,
      "learning_rate": 4.452361323754564e-05,
      "loss": 0.8905,
      "step": 1430
    },
    {
      "epoch": 0.4804804804804805,
      "grad_norm": 4.875990867614746,
      "learning_rate": 4.4464727358379466e-05,
      "loss": 1.1126,
      "step": 1440
    },
    {
      "epoch": 0.4838171504838171,
      "grad_norm": 3.6667091846466064,
      "learning_rate": 4.440584147921329e-05,
      "loss": 1.0576,
      "step": 1450
    },
    {
      "epoch": 0.48715382048715383,
      "grad_norm": 4.883539199829102,
      "learning_rate": 4.4346955600047116e-05,
      "loss": 1.0036,
      "step": 1460
    },
    {
      "epoch": 0.4904904904904905,
      "grad_norm": 4.8002495765686035,
      "learning_rate": 4.428806972088094e-05,
      "loss": 0.97,
      "step": 1470
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 5.606390476226807,
      "learning_rate": 4.422918384171476e-05,
      "loss": 1.3469,
      "step": 1480
    },
    {
      "epoch": 0.49716383049716384,
      "grad_norm": 6.953462600708008,
      "learning_rate": 4.417029796254858e-05,
      "loss": 0.9795,
      "step": 1490
    },
    {
      "epoch": 0.5005005005005005,
      "grad_norm": 6.637098789215088,
      "learning_rate": 4.411141208338241e-05,
      "loss": 1.2463,
      "step": 1500
    },
    {
      "epoch": 0.5038371705038371,
      "grad_norm": 5.890302658081055,
      "learning_rate": 4.405252620421623e-05,
      "loss": 1.0039,
      "step": 1510
    },
    {
      "epoch": 0.5071738405071738,
      "grad_norm": 4.386608123779297,
      "learning_rate": 4.3993640325050053e-05,
      "loss": 1.1956,
      "step": 1520
    },
    {
      "epoch": 0.5105105105105106,
      "grad_norm": 2.929110050201416,
      "learning_rate": 4.393475444588388e-05,
      "loss": 1.0892,
      "step": 1530
    },
    {
      "epoch": 0.5138471805138471,
      "grad_norm": 4.3466410636901855,
      "learning_rate": 4.3875868566717704e-05,
      "loss": 0.9713,
      "step": 1540
    },
    {
      "epoch": 0.5171838505171839,
      "grad_norm": 5.009315490722656,
      "learning_rate": 4.3816982687551526e-05,
      "loss": 1.0007,
      "step": 1550
    },
    {
      "epoch": 0.5205205205205206,
      "grad_norm": 6.610705852508545,
      "learning_rate": 4.3758096808385354e-05,
      "loss": 0.9979,
      "step": 1560
    },
    {
      "epoch": 0.5238571905238572,
      "grad_norm": 6.347642421722412,
      "learning_rate": 4.3699210929219176e-05,
      "loss": 1.0789,
      "step": 1570
    },
    {
      "epoch": 0.5271938605271939,
      "grad_norm": 4.932490825653076,
      "learning_rate": 4.3640325050053e-05,
      "loss": 0.9119,
      "step": 1580
    },
    {
      "epoch": 0.5305305305305306,
      "grad_norm": 5.253088474273682,
      "learning_rate": 4.3581439170886826e-05,
      "loss": 1.2789,
      "step": 1590
    },
    {
      "epoch": 0.5338672005338672,
      "grad_norm": 3.9506020545959473,
      "learning_rate": 4.352255329172065e-05,
      "loss": 1.0147,
      "step": 1600
    },
    {
      "epoch": 0.5372038705372039,
      "grad_norm": 4.5636138916015625,
      "learning_rate": 4.346366741255447e-05,
      "loss": 1.1785,
      "step": 1610
    },
    {
      "epoch": 0.5405405405405406,
      "grad_norm": 4.6478590965271,
      "learning_rate": 4.340478153338829e-05,
      "loss": 1.1807,
      "step": 1620
    },
    {
      "epoch": 0.5438772105438772,
      "grad_norm": 5.021444320678711,
      "learning_rate": 4.334589565422212e-05,
      "loss": 1.012,
      "step": 1630
    },
    {
      "epoch": 0.5472138805472139,
      "grad_norm": 3.3565914630889893,
      "learning_rate": 4.328700977505594e-05,
      "loss": 0.7903,
      "step": 1640
    },
    {
      "epoch": 0.5505505505505506,
      "grad_norm": 3.42800235748291,
      "learning_rate": 4.3228123895889764e-05,
      "loss": 1.131,
      "step": 1650
    },
    {
      "epoch": 0.5538872205538872,
      "grad_norm": 5.5250396728515625,
      "learning_rate": 4.316923801672359e-05,
      "loss": 1.0237,
      "step": 1660
    },
    {
      "epoch": 0.5572238905572239,
      "grad_norm": 5.53750467300415,
      "learning_rate": 4.3110352137557414e-05,
      "loss": 0.9715,
      "step": 1670
    },
    {
      "epoch": 0.5605605605605606,
      "grad_norm": 3.6184206008911133,
      "learning_rate": 4.3051466258391236e-05,
      "loss": 1.0773,
      "step": 1680
    },
    {
      "epoch": 0.5638972305638972,
      "grad_norm": 2.6647891998291016,
      "learning_rate": 4.2992580379225064e-05,
      "loss": 1.0247,
      "step": 1690
    },
    {
      "epoch": 0.5672339005672339,
      "grad_norm": 6.049147605895996,
      "learning_rate": 4.2933694500058886e-05,
      "loss": 1.246,
      "step": 1700
    },
    {
      "epoch": 0.5705705705705706,
      "grad_norm": 5.908942222595215,
      "learning_rate": 4.2874808620892715e-05,
      "loss": 1.0188,
      "step": 1710
    },
    {
      "epoch": 0.5739072405739073,
      "grad_norm": 4.471683502197266,
      "learning_rate": 4.2815922741726536e-05,
      "loss": 0.92,
      "step": 1720
    },
    {
      "epoch": 0.5772439105772439,
      "grad_norm": 8.099821090698242,
      "learning_rate": 4.2757036862560365e-05,
      "loss": 1.1127,
      "step": 1730
    },
    {
      "epoch": 0.5805805805805806,
      "grad_norm": 6.777768611907959,
      "learning_rate": 4.2698150983394187e-05,
      "loss": 1.0623,
      "step": 1740
    },
    {
      "epoch": 0.5839172505839173,
      "grad_norm": 3.303414821624756,
      "learning_rate": 4.263926510422801e-05,
      "loss": 1.2589,
      "step": 1750
    },
    {
      "epoch": 0.5872539205872539,
      "grad_norm": 5.274767875671387,
      "learning_rate": 4.258037922506184e-05,
      "loss": 0.9983,
      "step": 1760
    },
    {
      "epoch": 0.5905905905905906,
      "grad_norm": 8.6080961227417,
      "learning_rate": 4.252149334589566e-05,
      "loss": 1.1449,
      "step": 1770
    },
    {
      "epoch": 0.5939272605939273,
      "grad_norm": 4.0620222091674805,
      "learning_rate": 4.246260746672948e-05,
      "loss": 1.0229,
      "step": 1780
    },
    {
      "epoch": 0.5972639305972639,
      "grad_norm": 4.511322498321533,
      "learning_rate": 4.24037215875633e-05,
      "loss": 0.922,
      "step": 1790
    },
    {
      "epoch": 0.6006006006006006,
      "grad_norm": 4.134535789489746,
      "learning_rate": 4.234483570839713e-05,
      "loss": 0.9174,
      "step": 1800
    },
    {
      "epoch": 0.6039372706039373,
      "grad_norm": 3.4883105754852295,
      "learning_rate": 4.228594982923095e-05,
      "loss": 1.0787,
      "step": 1810
    },
    {
      "epoch": 0.6072739406072739,
      "grad_norm": 4.850587844848633,
      "learning_rate": 4.2227063950064774e-05,
      "loss": 1.0797,
      "step": 1820
    },
    {
      "epoch": 0.6106106106106106,
      "grad_norm": 3.6119439601898193,
      "learning_rate": 4.21681780708986e-05,
      "loss": 1.1667,
      "step": 1830
    },
    {
      "epoch": 0.6139472806139473,
      "grad_norm": 5.857569694519043,
      "learning_rate": 4.2109292191732425e-05,
      "loss": 1.2417,
      "step": 1840
    },
    {
      "epoch": 0.6172839506172839,
      "grad_norm": 4.887453079223633,
      "learning_rate": 4.2050406312566246e-05,
      "loss": 1.0392,
      "step": 1850
    },
    {
      "epoch": 0.6206206206206206,
      "grad_norm": 3.1208999156951904,
      "learning_rate": 4.1991520433400075e-05,
      "loss": 1.1207,
      "step": 1860
    },
    {
      "epoch": 0.6239572906239573,
      "grad_norm": 3.7177977561950684,
      "learning_rate": 4.19326345542339e-05,
      "loss": 0.9417,
      "step": 1870
    },
    {
      "epoch": 0.6272939606272939,
      "grad_norm": 4.772613048553467,
      "learning_rate": 4.187374867506772e-05,
      "loss": 0.9288,
      "step": 1880
    },
    {
      "epoch": 0.6306306306306306,
      "grad_norm": 4.38753604888916,
      "learning_rate": 4.181486279590155e-05,
      "loss": 1.086,
      "step": 1890
    },
    {
      "epoch": 0.6339673006339673,
      "grad_norm": 4.068124294281006,
      "learning_rate": 4.175597691673537e-05,
      "loss": 1.0794,
      "step": 1900
    },
    {
      "epoch": 0.6373039706373039,
      "grad_norm": 4.410216808319092,
      "learning_rate": 4.169709103756919e-05,
      "loss": 0.8916,
      "step": 1910
    },
    {
      "epoch": 0.6406406406406406,
      "grad_norm": 6.215452671051025,
      "learning_rate": 4.163820515840301e-05,
      "loss": 1.1252,
      "step": 1920
    },
    {
      "epoch": 0.6439773106439773,
      "grad_norm": 4.841552257537842,
      "learning_rate": 4.157931927923684e-05,
      "loss": 1.0643,
      "step": 1930
    },
    {
      "epoch": 0.6473139806473139,
      "grad_norm": 3.549687147140503,
      "learning_rate": 4.152043340007066e-05,
      "loss": 1.0961,
      "step": 1940
    },
    {
      "epoch": 0.6506506506506506,
      "grad_norm": 4.294360160827637,
      "learning_rate": 4.1461547520904484e-05,
      "loss": 0.9167,
      "step": 1950
    },
    {
      "epoch": 0.6539873206539873,
      "grad_norm": 3.567294120788574,
      "learning_rate": 4.140266164173831e-05,
      "loss": 1.0573,
      "step": 1960
    },
    {
      "epoch": 0.6573239906573239,
      "grad_norm": 3.7703592777252197,
      "learning_rate": 4.1343775762572135e-05,
      "loss": 1.003,
      "step": 1970
    },
    {
      "epoch": 0.6606606606606606,
      "grad_norm": 5.200263023376465,
      "learning_rate": 4.128488988340596e-05,
      "loss": 0.987,
      "step": 1980
    },
    {
      "epoch": 0.6639973306639974,
      "grad_norm": 4.791322231292725,
      "learning_rate": 4.1226004004239785e-05,
      "loss": 1.1087,
      "step": 1990
    },
    {
      "epoch": 0.667334000667334,
      "grad_norm": 4.2763590812683105,
      "learning_rate": 4.1167118125073614e-05,
      "loss": 1.223,
      "step": 2000
    },
    {
      "epoch": 0.6706706706706707,
      "grad_norm": 5.089679718017578,
      "learning_rate": 4.1108232245907435e-05,
      "loss": 1.0585,
      "step": 2010
    },
    {
      "epoch": 0.6740073406740074,
      "grad_norm": 3.6672744750976562,
      "learning_rate": 4.104934636674126e-05,
      "loss": 0.9721,
      "step": 2020
    },
    {
      "epoch": 0.6773440106773441,
      "grad_norm": 6.16092586517334,
      "learning_rate": 4.0990460487575086e-05,
      "loss": 1.2306,
      "step": 2030
    },
    {
      "epoch": 0.6806806806806807,
      "grad_norm": 6.168591022491455,
      "learning_rate": 4.093157460840891e-05,
      "loss": 1.1752,
      "step": 2040
    },
    {
      "epoch": 0.6840173506840174,
      "grad_norm": 3.3493587970733643,
      "learning_rate": 4.087268872924273e-05,
      "loss": 0.9449,
      "step": 2050
    },
    {
      "epoch": 0.6873540206873541,
      "grad_norm": 4.546745777130127,
      "learning_rate": 4.081380285007656e-05,
      "loss": 1.0035,
      "step": 2060
    },
    {
      "epoch": 0.6906906906906907,
      "grad_norm": 5.1102399826049805,
      "learning_rate": 4.075491697091038e-05,
      "loss": 1.048,
      "step": 2070
    },
    {
      "epoch": 0.6940273606940274,
      "grad_norm": 7.655319690704346,
      "learning_rate": 4.06960310917442e-05,
      "loss": 0.9685,
      "step": 2080
    },
    {
      "epoch": 0.6973640306973641,
      "grad_norm": 5.581677436828613,
      "learning_rate": 4.063714521257802e-05,
      "loss": 1.1065,
      "step": 2090
    },
    {
      "epoch": 0.7007007007007007,
      "grad_norm": 5.615943431854248,
      "learning_rate": 4.057825933341185e-05,
      "loss": 1.0006,
      "step": 2100
    },
    {
      "epoch": 0.7040373707040374,
      "grad_norm": 3.5344676971435547,
      "learning_rate": 4.0519373454245673e-05,
      "loss": 1.0115,
      "step": 2110
    },
    {
      "epoch": 0.7073740407073741,
      "grad_norm": 5.101223945617676,
      "learning_rate": 4.0460487575079495e-05,
      "loss": 0.9675,
      "step": 2120
    },
    {
      "epoch": 0.7107107107107107,
      "grad_norm": 5.310048580169678,
      "learning_rate": 4.0401601695913324e-05,
      "loss": 1.015,
      "step": 2130
    },
    {
      "epoch": 0.7140473807140474,
      "grad_norm": 7.982122898101807,
      "learning_rate": 4.0342715816747146e-05,
      "loss": 0.9236,
      "step": 2140
    },
    {
      "epoch": 0.7173840507173841,
      "grad_norm": 7.643040180206299,
      "learning_rate": 4.028382993758097e-05,
      "loss": 1.1786,
      "step": 2150
    },
    {
      "epoch": 0.7207207207207207,
      "grad_norm": 4.349778175354004,
      "learning_rate": 4.0224944058414796e-05,
      "loss": 1.0453,
      "step": 2160
    },
    {
      "epoch": 0.7240573907240574,
      "grad_norm": 4.18424654006958,
      "learning_rate": 4.016605817924862e-05,
      "loss": 0.9332,
      "step": 2170
    },
    {
      "epoch": 0.7273940607273941,
      "grad_norm": 4.759496212005615,
      "learning_rate": 4.010717230008244e-05,
      "loss": 0.8998,
      "step": 2180
    },
    {
      "epoch": 0.7307307307307307,
      "grad_norm": 4.415217399597168,
      "learning_rate": 4.004828642091626e-05,
      "loss": 0.9952,
      "step": 2190
    },
    {
      "epoch": 0.7340674007340674,
      "grad_norm": 3.434067487716675,
      "learning_rate": 3.998940054175009e-05,
      "loss": 0.9663,
      "step": 2200
    },
    {
      "epoch": 0.7374040707374041,
      "grad_norm": 4.300767421722412,
      "learning_rate": 3.993051466258391e-05,
      "loss": 0.684,
      "step": 2210
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 6.224095344543457,
      "learning_rate": 3.987162878341773e-05,
      "loss": 1.343,
      "step": 2220
    },
    {
      "epoch": 0.7440774107440774,
      "grad_norm": 5.001160621643066,
      "learning_rate": 3.981274290425156e-05,
      "loss": 1.0934,
      "step": 2230
    },
    {
      "epoch": 0.7474140807474141,
      "grad_norm": 4.951452732086182,
      "learning_rate": 3.9753857025085384e-05,
      "loss": 0.9736,
      "step": 2240
    },
    {
      "epoch": 0.7507507507507507,
      "grad_norm": 3.463355541229248,
      "learning_rate": 3.969497114591921e-05,
      "loss": 1.0766,
      "step": 2250
    },
    {
      "epoch": 0.7540874207540874,
      "grad_norm": 4.635566711425781,
      "learning_rate": 3.9636085266753034e-05,
      "loss": 1.0207,
      "step": 2260
    },
    {
      "epoch": 0.7574240907574241,
      "grad_norm": 5.9635539054870605,
      "learning_rate": 3.957719938758686e-05,
      "loss": 1.0465,
      "step": 2270
    },
    {
      "epoch": 0.7607607607607607,
      "grad_norm": 4.321613311767578,
      "learning_rate": 3.9518313508420684e-05,
      "loss": 0.8636,
      "step": 2280
    },
    {
      "epoch": 0.7640974307640974,
      "grad_norm": 3.976816415786743,
      "learning_rate": 3.9459427629254506e-05,
      "loss": 1.0845,
      "step": 2290
    },
    {
      "epoch": 0.7674341007674341,
      "grad_norm": 3.870814085006714,
      "learning_rate": 3.9400541750088335e-05,
      "loss": 1.038,
      "step": 2300
    },
    {
      "epoch": 0.7707707707707707,
      "grad_norm": 6.248379707336426,
      "learning_rate": 3.9341655870922156e-05,
      "loss": 0.9804,
      "step": 2310
    },
    {
      "epoch": 0.7741074407741074,
      "grad_norm": 5.252345561981201,
      "learning_rate": 3.928276999175598e-05,
      "loss": 1.046,
      "step": 2320
    },
    {
      "epoch": 0.7774441107774441,
      "grad_norm": 4.6217522621154785,
      "learning_rate": 3.9223884112589807e-05,
      "loss": 1.2273,
      "step": 2330
    },
    {
      "epoch": 0.7807807807807807,
      "grad_norm": 6.875094890594482,
      "learning_rate": 3.916499823342363e-05,
      "loss": 1.1323,
      "step": 2340
    },
    {
      "epoch": 0.7841174507841174,
      "grad_norm": 4.957897663116455,
      "learning_rate": 3.910611235425745e-05,
      "loss": 0.7901,
      "step": 2350
    },
    {
      "epoch": 0.7874541207874541,
      "grad_norm": 3.6564555168151855,
      "learning_rate": 3.904722647509127e-05,
      "loss": 1.1289,
      "step": 2360
    },
    {
      "epoch": 0.7907907907907908,
      "grad_norm": 4.779222011566162,
      "learning_rate": 3.89883405959251e-05,
      "loss": 1.0269,
      "step": 2370
    },
    {
      "epoch": 0.7941274607941274,
      "grad_norm": 6.013369083404541,
      "learning_rate": 3.892945471675892e-05,
      "loss": 0.8014,
      "step": 2380
    },
    {
      "epoch": 0.7974641307974641,
      "grad_norm": 3.8574578762054443,
      "learning_rate": 3.8870568837592744e-05,
      "loss": 0.7752,
      "step": 2390
    },
    {
      "epoch": 0.8008008008008008,
      "grad_norm": 3.866179943084717,
      "learning_rate": 3.881168295842657e-05,
      "loss": 0.9554,
      "step": 2400
    },
    {
      "epoch": 0.8041374708041374,
      "grad_norm": 3.6769495010375977,
      "learning_rate": 3.8752797079260394e-05,
      "loss": 1.0146,
      "step": 2410
    },
    {
      "epoch": 0.8074741408074741,
      "grad_norm": 5.019022464752197,
      "learning_rate": 3.8693911200094216e-05,
      "loss": 1.212,
      "step": 2420
    },
    {
      "epoch": 0.8108108108108109,
      "grad_norm": 3.467775821685791,
      "learning_rate": 3.8635025320928045e-05,
      "loss": 0.8089,
      "step": 2430
    },
    {
      "epoch": 0.8141474808141475,
      "grad_norm": 6.591006755828857,
      "learning_rate": 3.8576139441761866e-05,
      "loss": 1.1846,
      "step": 2440
    },
    {
      "epoch": 0.8174841508174842,
      "grad_norm": 4.856524467468262,
      "learning_rate": 3.851725356259569e-05,
      "loss": 0.9581,
      "step": 2450
    },
    {
      "epoch": 0.8208208208208209,
      "grad_norm": 6.975013256072998,
      "learning_rate": 3.845836768342952e-05,
      "loss": 1.16,
      "step": 2460
    },
    {
      "epoch": 0.8241574908241575,
      "grad_norm": 5.8184428215026855,
      "learning_rate": 3.839948180426334e-05,
      "loss": 1.0805,
      "step": 2470
    },
    {
      "epoch": 0.8274941608274942,
      "grad_norm": 2.5651066303253174,
      "learning_rate": 3.834059592509716e-05,
      "loss": 0.8015,
      "step": 2480
    },
    {
      "epoch": 0.8308308308308309,
      "grad_norm": 4.734800815582275,
      "learning_rate": 3.828171004593098e-05,
      "loss": 1.0027,
      "step": 2490
    },
    {
      "epoch": 0.8341675008341675,
      "grad_norm": 7.390276908874512,
      "learning_rate": 3.822282416676481e-05,
      "loss": 1.3636,
      "step": 2500
    },
    {
      "epoch": 0.8375041708375042,
      "grad_norm": 5.166672706604004,
      "learning_rate": 3.816393828759864e-05,
      "loss": 0.9292,
      "step": 2510
    },
    {
      "epoch": 0.8408408408408409,
      "grad_norm": 4.3348493576049805,
      "learning_rate": 3.810505240843246e-05,
      "loss": 0.936,
      "step": 2520
    },
    {
      "epoch": 0.8441775108441775,
      "grad_norm": 4.861528396606445,
      "learning_rate": 3.804616652926629e-05,
      "loss": 1.1958,
      "step": 2530
    },
    {
      "epoch": 0.8475141808475142,
      "grad_norm": 3.543977975845337,
      "learning_rate": 3.798728065010011e-05,
      "loss": 1.197,
      "step": 2540
    },
    {
      "epoch": 0.8508508508508509,
      "grad_norm": 4.155296802520752,
      "learning_rate": 3.792839477093393e-05,
      "loss": 0.877,
      "step": 2550
    },
    {
      "epoch": 0.8541875208541875,
      "grad_norm": 4.168817520141602,
      "learning_rate": 3.7869508891767755e-05,
      "loss": 0.9527,
      "step": 2560
    },
    {
      "epoch": 0.8575241908575242,
      "grad_norm": 6.19243860244751,
      "learning_rate": 3.781062301260158e-05,
      "loss": 0.956,
      "step": 2570
    },
    {
      "epoch": 0.8608608608608609,
      "grad_norm": 3.9070050716400146,
      "learning_rate": 3.7751737133435405e-05,
      "loss": 0.8739,
      "step": 2580
    },
    {
      "epoch": 0.8641975308641975,
      "grad_norm": 4.868672847747803,
      "learning_rate": 3.769285125426923e-05,
      "loss": 1.2115,
      "step": 2590
    },
    {
      "epoch": 0.8675342008675342,
      "grad_norm": 4.675326347351074,
      "learning_rate": 3.7633965375103055e-05,
      "loss": 1.0481,
      "step": 2600
    },
    {
      "epoch": 0.8708708708708709,
      "grad_norm": 3.832808494567871,
      "learning_rate": 3.757507949593688e-05,
      "loss": 1.0738,
      "step": 2610
    },
    {
      "epoch": 0.8742075408742075,
      "grad_norm": 3.6455960273742676,
      "learning_rate": 3.75161936167707e-05,
      "loss": 0.8546,
      "step": 2620
    },
    {
      "epoch": 0.8775442108775442,
      "grad_norm": 3.998145580291748,
      "learning_rate": 3.745730773760453e-05,
      "loss": 0.8991,
      "step": 2630
    },
    {
      "epoch": 0.8808808808808809,
      "grad_norm": 4.532684803009033,
      "learning_rate": 3.739842185843835e-05,
      "loss": 1.0026,
      "step": 2640
    },
    {
      "epoch": 0.8842175508842175,
      "grad_norm": 4.930627822875977,
      "learning_rate": 3.733953597927217e-05,
      "loss": 0.8846,
      "step": 2650
    },
    {
      "epoch": 0.8875542208875542,
      "grad_norm": 4.162967205047607,
      "learning_rate": 3.728065010010599e-05,
      "loss": 1.1876,
      "step": 2660
    },
    {
      "epoch": 0.8908908908908909,
      "grad_norm": 3.4028093814849854,
      "learning_rate": 3.722176422093982e-05,
      "loss": 0.9645,
      "step": 2670
    },
    {
      "epoch": 0.8942275608942276,
      "grad_norm": 5.702974796295166,
      "learning_rate": 3.716287834177364e-05,
      "loss": 1.0159,
      "step": 2680
    },
    {
      "epoch": 0.8975642308975642,
      "grad_norm": 5.855227470397949,
      "learning_rate": 3.7103992462607465e-05,
      "loss": 1.0794,
      "step": 2690
    },
    {
      "epoch": 0.9009009009009009,
      "grad_norm": 4.076763153076172,
      "learning_rate": 3.7045106583441293e-05,
      "loss": 1.1099,
      "step": 2700
    },
    {
      "epoch": 0.9042375709042376,
      "grad_norm": 4.325056076049805,
      "learning_rate": 3.6986220704275115e-05,
      "loss": 1.1451,
      "step": 2710
    },
    {
      "epoch": 0.9075742409075742,
      "grad_norm": 6.172327995300293,
      "learning_rate": 3.692733482510894e-05,
      "loss": 0.8643,
      "step": 2720
    },
    {
      "epoch": 0.9109109109109109,
      "grad_norm": 6.047774314880371,
      "learning_rate": 3.6868448945942766e-05,
      "loss": 0.9686,
      "step": 2730
    },
    {
      "epoch": 0.9142475809142476,
      "grad_norm": 5.221303462982178,
      "learning_rate": 3.680956306677659e-05,
      "loss": 1.0739,
      "step": 2740
    },
    {
      "epoch": 0.9175842509175842,
      "grad_norm": 3.6698429584503174,
      "learning_rate": 3.675067718761041e-05,
      "loss": 1.0057,
      "step": 2750
    },
    {
      "epoch": 0.9209209209209209,
      "grad_norm": 4.3993024826049805,
      "learning_rate": 3.669179130844424e-05,
      "loss": 0.9893,
      "step": 2760
    },
    {
      "epoch": 0.9242575909242576,
      "grad_norm": 4.986630916595459,
      "learning_rate": 3.663290542927806e-05,
      "loss": 0.8239,
      "step": 2770
    },
    {
      "epoch": 0.9275942609275942,
      "grad_norm": 4.477352142333984,
      "learning_rate": 3.657401955011189e-05,
      "loss": 0.993,
      "step": 2780
    },
    {
      "epoch": 0.9309309309309309,
      "grad_norm": 5.3636322021484375,
      "learning_rate": 3.651513367094571e-05,
      "loss": 0.8912,
      "step": 2790
    },
    {
      "epoch": 0.9342676009342676,
      "grad_norm": 4.185081481933594,
      "learning_rate": 3.645624779177954e-05,
      "loss": 0.87,
      "step": 2800
    },
    {
      "epoch": 0.9376042709376042,
      "grad_norm": 5.61265754699707,
      "learning_rate": 3.639736191261336e-05,
      "loss": 1.063,
      "step": 2810
    },
    {
      "epoch": 0.9409409409409409,
      "grad_norm": 5.876018047332764,
      "learning_rate": 3.633847603344718e-05,
      "loss": 0.8806,
      "step": 2820
    },
    {
      "epoch": 0.9442776109442776,
      "grad_norm": 4.064873695373535,
      "learning_rate": 3.6279590154281004e-05,
      "loss": 1.0168,
      "step": 2830
    },
    {
      "epoch": 0.9476142809476142,
      "grad_norm": 2.6956260204315186,
      "learning_rate": 3.622070427511483e-05,
      "loss": 1.0221,
      "step": 2840
    },
    {
      "epoch": 0.950950950950951,
      "grad_norm": 4.487701416015625,
      "learning_rate": 3.6161818395948654e-05,
      "loss": 1.2001,
      "step": 2850
    },
    {
      "epoch": 0.9542876209542877,
      "grad_norm": 4.530964374542236,
      "learning_rate": 3.6102932516782476e-05,
      "loss": 1.0219,
      "step": 2860
    },
    {
      "epoch": 0.9576242909576242,
      "grad_norm": 4.48826789855957,
      "learning_rate": 3.6044046637616304e-05,
      "loss": 1.0092,
      "step": 2870
    },
    {
      "epoch": 0.960960960960961,
      "grad_norm": 5.333290100097656,
      "learning_rate": 3.5985160758450126e-05,
      "loss": 0.7527,
      "step": 2880
    },
    {
      "epoch": 0.9642976309642977,
      "grad_norm": 5.828945159912109,
      "learning_rate": 3.592627487928395e-05,
      "loss": 1.0424,
      "step": 2890
    },
    {
      "epoch": 0.9676343009676343,
      "grad_norm": 8.202764511108398,
      "learning_rate": 3.5867389000117776e-05,
      "loss": 1.1884,
      "step": 2900
    },
    {
      "epoch": 0.970970970970971,
      "grad_norm": 7.321183681488037,
      "learning_rate": 3.58085031209516e-05,
      "loss": 0.9176,
      "step": 2910
    },
    {
      "epoch": 0.9743076409743077,
      "grad_norm": 4.25532865524292,
      "learning_rate": 3.574961724178542e-05,
      "loss": 1.0212,
      "step": 2920
    },
    {
      "epoch": 0.9776443109776443,
      "grad_norm": 3.028611660003662,
      "learning_rate": 3.569073136261925e-05,
      "loss": 0.859,
      "step": 2930
    },
    {
      "epoch": 0.980980980980981,
      "grad_norm": 4.605105400085449,
      "learning_rate": 3.563184548345307e-05,
      "loss": 0.9476,
      "step": 2940
    },
    {
      "epoch": 0.9843176509843177,
      "grad_norm": 4.377236843109131,
      "learning_rate": 3.557295960428689e-05,
      "loss": 1.0387,
      "step": 2950
    },
    {
      "epoch": 0.9876543209876543,
      "grad_norm": 2.8962743282318115,
      "learning_rate": 3.5514073725120714e-05,
      "loss": 0.7762,
      "step": 2960
    },
    {
      "epoch": 0.990990990990991,
      "grad_norm": 5.851387023925781,
      "learning_rate": 3.545518784595454e-05,
      "loss": 1.0775,
      "step": 2970
    },
    {
      "epoch": 0.9943276609943277,
      "grad_norm": 4.287487030029297,
      "learning_rate": 3.5396301966788364e-05,
      "loss": 1.2277,
      "step": 2980
    },
    {
      "epoch": 0.9976643309976644,
      "grad_norm": 4.4991044998168945,
      "learning_rate": 3.5337416087622186e-05,
      "loss": 1.2668,
      "step": 2990
    },
    {
      "epoch": 1.001001001001001,
      "grad_norm": 6.107329845428467,
      "learning_rate": 3.5278530208456014e-05,
      "loss": 1.2734,
      "step": 3000
    },
    {
      "epoch": 1.0043376710043377,
      "grad_norm": 4.124959468841553,
      "learning_rate": 3.5219644329289836e-05,
      "loss": 1.0428,
      "step": 3010
    },
    {
      "epoch": 1.0076743410076743,
      "grad_norm": 4.946746826171875,
      "learning_rate": 3.516075845012366e-05,
      "loss": 1.0084,
      "step": 3020
    },
    {
      "epoch": 1.011011011011011,
      "grad_norm": 6.0722808837890625,
      "learning_rate": 3.5101872570957486e-05,
      "loss": 1.0328,
      "step": 3030
    },
    {
      "epoch": 1.0143476810143477,
      "grad_norm": 6.764739036560059,
      "learning_rate": 3.504298669179131e-05,
      "loss": 1.008,
      "step": 3040
    },
    {
      "epoch": 1.0176843510176843,
      "grad_norm": 1.3837063312530518,
      "learning_rate": 3.498410081262514e-05,
      "loss": 0.8684,
      "step": 3050
    },
    {
      "epoch": 1.021021021021021,
      "grad_norm": 6.128448009490967,
      "learning_rate": 3.492521493345896e-05,
      "loss": 1.2344,
      "step": 3060
    },
    {
      "epoch": 1.0243576910243577,
      "grad_norm": 6.455077171325684,
      "learning_rate": 3.486632905429279e-05,
      "loss": 1.167,
      "step": 3070
    },
    {
      "epoch": 1.0276943610276943,
      "grad_norm": 3.454843759536743,
      "learning_rate": 3.480744317512661e-05,
      "loss": 0.8408,
      "step": 3080
    },
    {
      "epoch": 1.031031031031031,
      "grad_norm": 4.980084419250488,
      "learning_rate": 3.474855729596043e-05,
      "loss": 1.0318,
      "step": 3090
    },
    {
      "epoch": 1.0343677010343677,
      "grad_norm": 5.9065775871276855,
      "learning_rate": 3.468967141679426e-05,
      "loss": 1.0834,
      "step": 3100
    },
    {
      "epoch": 1.0377043710377043,
      "grad_norm": 7.222415447235107,
      "learning_rate": 3.463078553762808e-05,
      "loss": 1.1339,
      "step": 3110
    },
    {
      "epoch": 1.0410410410410411,
      "grad_norm": 6.7444844245910645,
      "learning_rate": 3.45718996584619e-05,
      "loss": 0.924,
      "step": 3120
    },
    {
      "epoch": 1.0443777110443777,
      "grad_norm": 5.90389347076416,
      "learning_rate": 3.4513013779295724e-05,
      "loss": 0.868,
      "step": 3130
    },
    {
      "epoch": 1.0477143810477143,
      "grad_norm": 4.0358195304870605,
      "learning_rate": 3.445412790012955e-05,
      "loss": 1.0779,
      "step": 3140
    },
    {
      "epoch": 1.0510510510510511,
      "grad_norm": 6.531481742858887,
      "learning_rate": 3.4395242020963375e-05,
      "loss": 0.9412,
      "step": 3150
    },
    {
      "epoch": 1.0543877210543877,
      "grad_norm": 5.394105911254883,
      "learning_rate": 3.4336356141797197e-05,
      "loss": 0.7926,
      "step": 3160
    },
    {
      "epoch": 1.0577243910577243,
      "grad_norm": 5.516204833984375,
      "learning_rate": 3.4277470262631025e-05,
      "loss": 1.0698,
      "step": 3170
    },
    {
      "epoch": 1.0610610610610611,
      "grad_norm": 3.1294829845428467,
      "learning_rate": 3.421858438346485e-05,
      "loss": 0.8542,
      "step": 3180
    },
    {
      "epoch": 1.0643977310643977,
      "grad_norm": 5.033647537231445,
      "learning_rate": 3.415969850429867e-05,
      "loss": 0.808,
      "step": 3190
    },
    {
      "epoch": 1.0677344010677343,
      "grad_norm": 5.309226036071777,
      "learning_rate": 3.41008126251325e-05,
      "loss": 0.9268,
      "step": 3200
    },
    {
      "epoch": 1.0710710710710711,
      "grad_norm": 4.488002777099609,
      "learning_rate": 3.404192674596632e-05,
      "loss": 0.9067,
      "step": 3210
    },
    {
      "epoch": 1.0744077410744077,
      "grad_norm": 4.105489253997803,
      "learning_rate": 3.398304086680014e-05,
      "loss": 0.7346,
      "step": 3220
    },
    {
      "epoch": 1.0777444110777443,
      "grad_norm": 7.047926902770996,
      "learning_rate": 3.392415498763397e-05,
      "loss": 1.0552,
      "step": 3230
    },
    {
      "epoch": 1.0810810810810811,
      "grad_norm": 3.5358290672302246,
      "learning_rate": 3.386526910846779e-05,
      "loss": 1.1827,
      "step": 3240
    },
    {
      "epoch": 1.0844177510844177,
      "grad_norm": 7.566186904907227,
      "learning_rate": 3.380638322930161e-05,
      "loss": 0.8479,
      "step": 3250
    },
    {
      "epoch": 1.0877544210877543,
      "grad_norm": 3.9561822414398193,
      "learning_rate": 3.3747497350135435e-05,
      "loss": 0.7722,
      "step": 3260
    },
    {
      "epoch": 1.0910910910910911,
      "grad_norm": 4.123648166656494,
      "learning_rate": 3.368861147096926e-05,
      "loss": 0.9573,
      "step": 3270
    },
    {
      "epoch": 1.0944277610944277,
      "grad_norm": 4.367898464202881,
      "learning_rate": 3.3629725591803085e-05,
      "loss": 0.7562,
      "step": 3280
    },
    {
      "epoch": 1.0977644310977643,
      "grad_norm": 4.766269683837891,
      "learning_rate": 3.357083971263691e-05,
      "loss": 0.8558,
      "step": 3290
    },
    {
      "epoch": 1.1011011011011012,
      "grad_norm": 5.469362258911133,
      "learning_rate": 3.3511953833470735e-05,
      "loss": 0.8266,
      "step": 3300
    },
    {
      "epoch": 1.1044377711044377,
      "grad_norm": 5.945261001586914,
      "learning_rate": 3.345306795430456e-05,
      "loss": 0.9677,
      "step": 3310
    },
    {
      "epoch": 1.1077744411077743,
      "grad_norm": 5.4678168296813965,
      "learning_rate": 3.3394182075138386e-05,
      "loss": 0.8111,
      "step": 3320
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 4.683640480041504,
      "learning_rate": 3.333529619597221e-05,
      "loss": 0.9189,
      "step": 3330
    },
    {
      "epoch": 1.1144477811144478,
      "grad_norm": 4.292535305023193,
      "learning_rate": 3.3276410316806036e-05,
      "loss": 1.1633,
      "step": 3340
    },
    {
      "epoch": 1.1177844511177844,
      "grad_norm": 6.897686004638672,
      "learning_rate": 3.321752443763986e-05,
      "loss": 1.002,
      "step": 3350
    },
    {
      "epoch": 1.1211211211211212,
      "grad_norm": 4.257903099060059,
      "learning_rate": 3.315863855847368e-05,
      "loss": 0.9308,
      "step": 3360
    },
    {
      "epoch": 1.1244577911244578,
      "grad_norm": 4.94467306137085,
      "learning_rate": 3.309975267930751e-05,
      "loss": 1.0582,
      "step": 3370
    },
    {
      "epoch": 1.1277944611277944,
      "grad_norm": 6.096086502075195,
      "learning_rate": 3.304086680014133e-05,
      "loss": 0.9572,
      "step": 3380
    },
    {
      "epoch": 1.1311311311311312,
      "grad_norm": 5.628101825714111,
      "learning_rate": 3.298198092097515e-05,
      "loss": 0.9417,
      "step": 3390
    },
    {
      "epoch": 1.1344678011344678,
      "grad_norm": 5.101990222930908,
      "learning_rate": 3.292309504180898e-05,
      "loss": 1.0409,
      "step": 3400
    },
    {
      "epoch": 1.1378044711378044,
      "grad_norm": 6.251267910003662,
      "learning_rate": 3.28642091626428e-05,
      "loss": 1.0488,
      "step": 3410
    },
    {
      "epoch": 1.1411411411411412,
      "grad_norm": 4.987309455871582,
      "learning_rate": 3.2805323283476624e-05,
      "loss": 1.2284,
      "step": 3420
    },
    {
      "epoch": 1.1444778111444778,
      "grad_norm": 4.977793216705322,
      "learning_rate": 3.2746437404310445e-05,
      "loss": 0.8652,
      "step": 3430
    },
    {
      "epoch": 1.1478144811478144,
      "grad_norm": 4.497901439666748,
      "learning_rate": 3.2687551525144274e-05,
      "loss": 0.9389,
      "step": 3440
    },
    {
      "epoch": 1.1511511511511512,
      "grad_norm": 4.752035140991211,
      "learning_rate": 3.2628665645978096e-05,
      "loss": 1.0733,
      "step": 3450
    },
    {
      "epoch": 1.1544878211544878,
      "grad_norm": 3.401003122329712,
      "learning_rate": 3.256977976681192e-05,
      "loss": 0.9172,
      "step": 3460
    },
    {
      "epoch": 1.1578244911578244,
      "grad_norm": 4.490971565246582,
      "learning_rate": 3.2510893887645746e-05,
      "loss": 0.8989,
      "step": 3470
    },
    {
      "epoch": 1.1611611611611612,
      "grad_norm": 5.833939075469971,
      "learning_rate": 3.245200800847957e-05,
      "loss": 1.2388,
      "step": 3480
    },
    {
      "epoch": 1.1644978311644978,
      "grad_norm": 4.074257850646973,
      "learning_rate": 3.239312212931339e-05,
      "loss": 0.9609,
      "step": 3490
    },
    {
      "epoch": 1.1678345011678344,
      "grad_norm": 6.005265235900879,
      "learning_rate": 3.233423625014722e-05,
      "loss": 0.8939,
      "step": 3500
    },
    {
      "epoch": 1.1711711711711712,
      "grad_norm": 7.347655296325684,
      "learning_rate": 3.227535037098104e-05,
      "loss": 1.0849,
      "step": 3510
    },
    {
      "epoch": 1.1745078411745078,
      "grad_norm": 5.65125846862793,
      "learning_rate": 3.221646449181486e-05,
      "loss": 0.8719,
      "step": 3520
    },
    {
      "epoch": 1.1778445111778444,
      "grad_norm": 2.512094020843506,
      "learning_rate": 3.2157578612648683e-05,
      "loss": 0.9119,
      "step": 3530
    },
    {
      "epoch": 1.1811811811811812,
      "grad_norm": 3.9574453830718994,
      "learning_rate": 3.209869273348251e-05,
      "loss": 1.2197,
      "step": 3540
    },
    {
      "epoch": 1.1845178511845178,
      "grad_norm": 4.77421760559082,
      "learning_rate": 3.2039806854316334e-05,
      "loss": 0.917,
      "step": 3550
    },
    {
      "epoch": 1.1878545211878546,
      "grad_norm": 4.045111179351807,
      "learning_rate": 3.1980920975150155e-05,
      "loss": 1.0475,
      "step": 3560
    },
    {
      "epoch": 1.1911911911911912,
      "grad_norm": 4.536621570587158,
      "learning_rate": 3.1922035095983984e-05,
      "loss": 0.9208,
      "step": 3570
    },
    {
      "epoch": 1.1945278611945278,
      "grad_norm": 4.737792491912842,
      "learning_rate": 3.1863149216817806e-05,
      "loss": 0.9107,
      "step": 3580
    },
    {
      "epoch": 1.1978645311978646,
      "grad_norm": 5.389671325683594,
      "learning_rate": 3.1804263337651634e-05,
      "loss": 1.0436,
      "step": 3590
    },
    {
      "epoch": 1.2012012012012012,
      "grad_norm": 4.240820407867432,
      "learning_rate": 3.1745377458485456e-05,
      "loss": 0.9232,
      "step": 3600
    },
    {
      "epoch": 1.2045378712045378,
      "grad_norm": 5.550611972808838,
      "learning_rate": 3.1686491579319285e-05,
      "loss": 0.9908,
      "step": 3610
    },
    {
      "epoch": 1.2078745412078746,
      "grad_norm": 5.638623237609863,
      "learning_rate": 3.1627605700153106e-05,
      "loss": 0.9232,
      "step": 3620
    },
    {
      "epoch": 1.2112112112112112,
      "grad_norm": 2.769810438156128,
      "learning_rate": 3.156871982098693e-05,
      "loss": 0.8521,
      "step": 3630
    },
    {
      "epoch": 1.2145478812145478,
      "grad_norm": 4.291597366333008,
      "learning_rate": 3.150983394182076e-05,
      "loss": 1.0598,
      "step": 3640
    },
    {
      "epoch": 1.2178845512178846,
      "grad_norm": 4.745553493499756,
      "learning_rate": 3.145094806265458e-05,
      "loss": 1.0929,
      "step": 3650
    },
    {
      "epoch": 1.2212212212212212,
      "grad_norm": 4.252098560333252,
      "learning_rate": 3.13920621834884e-05,
      "loss": 0.821,
      "step": 3660
    },
    {
      "epoch": 1.2245578912245578,
      "grad_norm": 6.897449970245361,
      "learning_rate": 3.133317630432223e-05,
      "loss": 1.045,
      "step": 3670
    },
    {
      "epoch": 1.2278945612278946,
      "grad_norm": 3.776538133621216,
      "learning_rate": 3.127429042515605e-05,
      "loss": 0.8433,
      "step": 3680
    },
    {
      "epoch": 1.2312312312312312,
      "grad_norm": 6.526803493499756,
      "learning_rate": 3.121540454598987e-05,
      "loss": 1.1335,
      "step": 3690
    },
    {
      "epoch": 1.2345679012345678,
      "grad_norm": 3.516669988632202,
      "learning_rate": 3.1156518666823694e-05,
      "loss": 0.998,
      "step": 3700
    },
    {
      "epoch": 1.2379045712379046,
      "grad_norm": 4.290889739990234,
      "learning_rate": 3.109763278765752e-05,
      "loss": 0.8146,
      "step": 3710
    },
    {
      "epoch": 1.2412412412412412,
      "grad_norm": 3.7117183208465576,
      "learning_rate": 3.1038746908491344e-05,
      "loss": 1.1121,
      "step": 3720
    },
    {
      "epoch": 1.2445779112445778,
      "grad_norm": 4.459873676300049,
      "learning_rate": 3.0979861029325166e-05,
      "loss": 1.0697,
      "step": 3730
    },
    {
      "epoch": 1.2479145812479147,
      "grad_norm": 2.504807233810425,
      "learning_rate": 3.0920975150158995e-05,
      "loss": 0.763,
      "step": 3740
    },
    {
      "epoch": 1.2512512512512513,
      "grad_norm": 2.3144776821136475,
      "learning_rate": 3.0862089270992817e-05,
      "loss": 0.9274,
      "step": 3750
    },
    {
      "epoch": 1.2545879212545878,
      "grad_norm": 4.982751369476318,
      "learning_rate": 3.080320339182664e-05,
      "loss": 0.8815,
      "step": 3760
    },
    {
      "epoch": 1.2579245912579247,
      "grad_norm": 3.1561760902404785,
      "learning_rate": 3.074431751266047e-05,
      "loss": 0.9043,
      "step": 3770
    },
    {
      "epoch": 1.2612612612612613,
      "grad_norm": 5.456984519958496,
      "learning_rate": 3.068543163349429e-05,
      "loss": 0.9088,
      "step": 3780
    },
    {
      "epoch": 1.2645979312645979,
      "grad_norm": 4.275904178619385,
      "learning_rate": 3.062654575432811e-05,
      "loss": 0.7777,
      "step": 3790
    },
    {
      "epoch": 1.2679346012679347,
      "grad_norm": 2.8046858310699463,
      "learning_rate": 3.056765987516194e-05,
      "loss": 0.8019,
      "step": 3800
    },
    {
      "epoch": 1.2712712712712713,
      "grad_norm": 5.2043137550354,
      "learning_rate": 3.050877399599576e-05,
      "loss": 0.9569,
      "step": 3810
    },
    {
      "epoch": 1.2746079412746079,
      "grad_norm": 9.147806167602539,
      "learning_rate": 3.0449888116829583e-05,
      "loss": 0.8815,
      "step": 3820
    },
    {
      "epoch": 1.2779446112779447,
      "grad_norm": 3.6333444118499756,
      "learning_rate": 3.0391002237663408e-05,
      "loss": 0.8952,
      "step": 3830
    },
    {
      "epoch": 1.2812812812812813,
      "grad_norm": 3.9710114002227783,
      "learning_rate": 3.0332116358497233e-05,
      "loss": 0.977,
      "step": 3840
    },
    {
      "epoch": 1.284617951284618,
      "grad_norm": 5.027102470397949,
      "learning_rate": 3.027323047933106e-05,
      "loss": 0.8453,
      "step": 3850
    },
    {
      "epoch": 1.2879546212879547,
      "grad_norm": 3.06876802444458,
      "learning_rate": 3.0214344600164883e-05,
      "loss": 0.7752,
      "step": 3860
    },
    {
      "epoch": 1.2912912912912913,
      "grad_norm": 4.96275520324707,
      "learning_rate": 3.0155458720998708e-05,
      "loss": 0.8512,
      "step": 3870
    },
    {
      "epoch": 1.294627961294628,
      "grad_norm": 9.179335594177246,
      "learning_rate": 3.0096572841832533e-05,
      "loss": 0.9714,
      "step": 3880
    },
    {
      "epoch": 1.2979646312979647,
      "grad_norm": 5.059041976928711,
      "learning_rate": 3.0037686962666355e-05,
      "loss": 1.0255,
      "step": 3890
    },
    {
      "epoch": 1.3013013013013013,
      "grad_norm": 5.9990315437316895,
      "learning_rate": 2.997880108350018e-05,
      "loss": 0.9838,
      "step": 3900
    },
    {
      "epoch": 1.304637971304638,
      "grad_norm": 5.283405780792236,
      "learning_rate": 2.9919915204334002e-05,
      "loss": 1.1363,
      "step": 3910
    },
    {
      "epoch": 1.3079746413079747,
      "grad_norm": 3.9749536514282227,
      "learning_rate": 2.9861029325167827e-05,
      "loss": 1.1509,
      "step": 3920
    },
    {
      "epoch": 1.3113113113113113,
      "grad_norm": 6.247188091278076,
      "learning_rate": 2.9802143446001652e-05,
      "loss": 0.9771,
      "step": 3930
    },
    {
      "epoch": 1.314647981314648,
      "grad_norm": 6.321484565734863,
      "learning_rate": 2.9743257566835474e-05,
      "loss": 1.1446,
      "step": 3940
    },
    {
      "epoch": 1.3179846513179847,
      "grad_norm": 4.5725321769714355,
      "learning_rate": 2.96843716876693e-05,
      "loss": 1.0048,
      "step": 3950
    },
    {
      "epoch": 1.3213213213213213,
      "grad_norm": 5.0589118003845215,
      "learning_rate": 2.962548580850312e-05,
      "loss": 0.9901,
      "step": 3960
    },
    {
      "epoch": 1.3246579913246581,
      "grad_norm": 5.355396270751953,
      "learning_rate": 2.9566599929336946e-05,
      "loss": 0.9579,
      "step": 3970
    },
    {
      "epoch": 1.3279946613279947,
      "grad_norm": 3.483748197555542,
      "learning_rate": 2.950771405017077e-05,
      "loss": 0.8272,
      "step": 3980
    },
    {
      "epoch": 1.3313313313313313,
      "grad_norm": 6.103470325469971,
      "learning_rate": 2.9448828171004593e-05,
      "loss": 1.1516,
      "step": 3990
    },
    {
      "epoch": 1.3346680013346681,
      "grad_norm": 6.181433200836182,
      "learning_rate": 2.938994229183842e-05,
      "loss": 0.9624,
      "step": 4000
    },
    {
      "epoch": 1.3380046713380047,
      "grad_norm": 6.731479644775391,
      "learning_rate": 2.9331056412672244e-05,
      "loss": 0.867,
      "step": 4010
    },
    {
      "epoch": 1.3413413413413413,
      "grad_norm": 4.046059608459473,
      "learning_rate": 2.9272170533506065e-05,
      "loss": 0.9681,
      "step": 4020
    },
    {
      "epoch": 1.3446780113446781,
      "grad_norm": 6.288428783416748,
      "learning_rate": 2.921328465433989e-05,
      "loss": 1.0233,
      "step": 4030
    },
    {
      "epoch": 1.3480146813480147,
      "grad_norm": 4.0094075202941895,
      "learning_rate": 2.9154398775173712e-05,
      "loss": 0.9227,
      "step": 4040
    },
    {
      "epoch": 1.3513513513513513,
      "grad_norm": 5.842947006225586,
      "learning_rate": 2.9095512896007537e-05,
      "loss": 0.7064,
      "step": 4050
    },
    {
      "epoch": 1.3546880213546881,
      "grad_norm": 6.909192085266113,
      "learning_rate": 2.9036627016841363e-05,
      "loss": 0.9683,
      "step": 4060
    },
    {
      "epoch": 1.3580246913580247,
      "grad_norm": 4.181108474731445,
      "learning_rate": 2.8977741137675184e-05,
      "loss": 0.9603,
      "step": 4070
    },
    {
      "epoch": 1.3613613613613613,
      "grad_norm": 3.281149387359619,
      "learning_rate": 2.891885525850901e-05,
      "loss": 0.7637,
      "step": 4080
    },
    {
      "epoch": 1.3646980313646981,
      "grad_norm": 5.270967483520508,
      "learning_rate": 2.885996937934283e-05,
      "loss": 0.8457,
      "step": 4090
    },
    {
      "epoch": 1.3680347013680347,
      "grad_norm": 2.8420588970184326,
      "learning_rate": 2.8801083500176656e-05,
      "loss": 0.9139,
      "step": 4100
    },
    {
      "epoch": 1.3713713713713713,
      "grad_norm": 7.043352127075195,
      "learning_rate": 2.874219762101048e-05,
      "loss": 0.8003,
      "step": 4110
    },
    {
      "epoch": 1.3747080413747081,
      "grad_norm": 4.445373058319092,
      "learning_rate": 2.868331174184431e-05,
      "loss": 0.8602,
      "step": 4120
    },
    {
      "epoch": 1.3780447113780447,
      "grad_norm": 3.730395555496216,
      "learning_rate": 2.8624425862678135e-05,
      "loss": 1.073,
      "step": 4130
    },
    {
      "epoch": 1.3813813813813813,
      "grad_norm": 4.4734344482421875,
      "learning_rate": 2.8565539983511957e-05,
      "loss": 0.8764,
      "step": 4140
    },
    {
      "epoch": 1.3847180513847182,
      "grad_norm": 5.652626991271973,
      "learning_rate": 2.8506654104345782e-05,
      "loss": 0.8866,
      "step": 4150
    },
    {
      "epoch": 1.3880547213880547,
      "grad_norm": 3.66182804107666,
      "learning_rate": 2.8447768225179604e-05,
      "loss": 0.9781,
      "step": 4160
    },
    {
      "epoch": 1.3913913913913913,
      "grad_norm": 4.079824447631836,
      "learning_rate": 2.838888234601343e-05,
      "loss": 0.9661,
      "step": 4170
    },
    {
      "epoch": 1.3947280613947282,
      "grad_norm": 6.768295764923096,
      "learning_rate": 2.8329996466847254e-05,
      "loss": 1.0863,
      "step": 4180
    },
    {
      "epoch": 1.3980647313980648,
      "grad_norm": 4.542452335357666,
      "learning_rate": 2.8271110587681076e-05,
      "loss": 0.838,
      "step": 4190
    },
    {
      "epoch": 1.4014014014014013,
      "grad_norm": 9.68766975402832,
      "learning_rate": 2.82122247085149e-05,
      "loss": 0.845,
      "step": 4200
    },
    {
      "epoch": 1.4047380714047382,
      "grad_norm": 6.584863662719727,
      "learning_rate": 2.8153338829348723e-05,
      "loss": 0.9489,
      "step": 4210
    },
    {
      "epoch": 1.4080747414080748,
      "grad_norm": 5.267181873321533,
      "learning_rate": 2.8094452950182548e-05,
      "loss": 1.049,
      "step": 4220
    },
    {
      "epoch": 1.4114114114114114,
      "grad_norm": 7.047474384307861,
      "learning_rate": 2.8035567071016373e-05,
      "loss": 1.2099,
      "step": 4230
    },
    {
      "epoch": 1.4147480814147482,
      "grad_norm": 5.066310405731201,
      "learning_rate": 2.7976681191850195e-05,
      "loss": 0.8903,
      "step": 4240
    },
    {
      "epoch": 1.4180847514180848,
      "grad_norm": 3.4786598682403564,
      "learning_rate": 2.791779531268402e-05,
      "loss": 0.897,
      "step": 4250
    },
    {
      "epoch": 1.4214214214214214,
      "grad_norm": 4.724745273590088,
      "learning_rate": 2.7858909433517842e-05,
      "loss": 0.9803,
      "step": 4260
    },
    {
      "epoch": 1.4247580914247582,
      "grad_norm": 7.0762715339660645,
      "learning_rate": 2.7800023554351667e-05,
      "loss": 0.9726,
      "step": 4270
    },
    {
      "epoch": 1.4280947614280948,
      "grad_norm": 2.4112284183502197,
      "learning_rate": 2.7741137675185492e-05,
      "loss": 0.8795,
      "step": 4280
    },
    {
      "epoch": 1.4314314314314314,
      "grad_norm": 5.849210739135742,
      "learning_rate": 2.7682251796019314e-05,
      "loss": 1.069,
      "step": 4290
    },
    {
      "epoch": 1.4347681014347682,
      "grad_norm": 3.9724981784820557,
      "learning_rate": 2.762336591685314e-05,
      "loss": 1.1141,
      "step": 4300
    },
    {
      "epoch": 1.4381047714381048,
      "grad_norm": 2.61849308013916,
      "learning_rate": 2.756448003768696e-05,
      "loss": 0.8702,
      "step": 4310
    },
    {
      "epoch": 1.4414414414414414,
      "grad_norm": 5.193563461303711,
      "learning_rate": 2.7505594158520786e-05,
      "loss": 1.0163,
      "step": 4320
    },
    {
      "epoch": 1.4447781114447782,
      "grad_norm": 3.933619499206543,
      "learning_rate": 2.744670827935461e-05,
      "loss": 1.0321,
      "step": 4330
    },
    {
      "epoch": 1.4481147814481148,
      "grad_norm": 3.344290256500244,
      "learning_rate": 2.7387822400188433e-05,
      "loss": 0.7083,
      "step": 4340
    },
    {
      "epoch": 1.4514514514514514,
      "grad_norm": 3.6999802589416504,
      "learning_rate": 2.732893652102226e-05,
      "loss": 1.1168,
      "step": 4350
    },
    {
      "epoch": 1.4547881214547882,
      "grad_norm": 4.76951789855957,
      "learning_rate": 2.7270050641856083e-05,
      "loss": 0.965,
      "step": 4360
    },
    {
      "epoch": 1.4581247914581248,
      "grad_norm": 4.481216907501221,
      "learning_rate": 2.7211164762689905e-05,
      "loss": 0.7576,
      "step": 4370
    },
    {
      "epoch": 1.4614614614614614,
      "grad_norm": 6.167853832244873,
      "learning_rate": 2.715227888352373e-05,
      "loss": 0.8353,
      "step": 4380
    },
    {
      "epoch": 1.4647981314647982,
      "grad_norm": 5.680375099182129,
      "learning_rate": 2.709339300435756e-05,
      "loss": 0.9288,
      "step": 4390
    },
    {
      "epoch": 1.4681348014681348,
      "grad_norm": 5.619714260101318,
      "learning_rate": 2.7034507125191384e-05,
      "loss": 0.9552,
      "step": 4400
    },
    {
      "epoch": 1.4714714714714714,
      "grad_norm": 4.768710136413574,
      "learning_rate": 2.6975621246025206e-05,
      "loss": 1.06,
      "step": 4410
    },
    {
      "epoch": 1.4748081414748082,
      "grad_norm": 3.6110777854919434,
      "learning_rate": 2.691673536685903e-05,
      "loss": 0.8779,
      "step": 4420
    },
    {
      "epoch": 1.4781448114781448,
      "grad_norm": 5.385594844818115,
      "learning_rate": 2.6857849487692853e-05,
      "loss": 0.9987,
      "step": 4430
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 5.52331018447876,
      "learning_rate": 2.6798963608526678e-05,
      "loss": 0.7728,
      "step": 4440
    },
    {
      "epoch": 1.4848181514848182,
      "grad_norm": 10.174220085144043,
      "learning_rate": 2.6740077729360503e-05,
      "loss": 0.9121,
      "step": 4450
    },
    {
      "epoch": 1.4881548214881548,
      "grad_norm": 7.104811668395996,
      "learning_rate": 2.6681191850194325e-05,
      "loss": 0.931,
      "step": 4460
    },
    {
      "epoch": 1.4914914914914914,
      "grad_norm": 6.431149005889893,
      "learning_rate": 2.662230597102815e-05,
      "loss": 0.9886,
      "step": 4470
    },
    {
      "epoch": 1.4948281614948282,
      "grad_norm": 6.63034200668335,
      "learning_rate": 2.6563420091861975e-05,
      "loss": 0.8432,
      "step": 4480
    },
    {
      "epoch": 1.4981648314981648,
      "grad_norm": 7.944759845733643,
      "learning_rate": 2.6504534212695797e-05,
      "loss": 0.9595,
      "step": 4490
    },
    {
      "epoch": 1.5015015015015014,
      "grad_norm": 2.8568007946014404,
      "learning_rate": 2.6445648333529622e-05,
      "loss": 1.0813,
      "step": 4500
    },
    {
      "epoch": 1.5048381715048382,
      "grad_norm": 6.614625453948975,
      "learning_rate": 2.6386762454363444e-05,
      "loss": 1.0809,
      "step": 4510
    },
    {
      "epoch": 1.5081748415081748,
      "grad_norm": 7.848061561584473,
      "learning_rate": 2.632787657519727e-05,
      "loss": 0.9352,
      "step": 4520
    },
    {
      "epoch": 1.5115115115115114,
      "grad_norm": 7.014749526977539,
      "learning_rate": 2.6268990696031094e-05,
      "loss": 0.9239,
      "step": 4530
    },
    {
      "epoch": 1.5148481815148482,
      "grad_norm": 5.027646541595459,
      "learning_rate": 2.6210104816864916e-05,
      "loss": 0.8579,
      "step": 4540
    },
    {
      "epoch": 1.5181848515181848,
      "grad_norm": 4.422837734222412,
      "learning_rate": 2.615121893769874e-05,
      "loss": 1.079,
      "step": 4550
    },
    {
      "epoch": 1.5215215215215214,
      "grad_norm": 6.415572166442871,
      "learning_rate": 2.6092333058532563e-05,
      "loss": 1.0195,
      "step": 4560
    },
    {
      "epoch": 1.5248581915248582,
      "grad_norm": 5.427745342254639,
      "learning_rate": 2.6033447179366388e-05,
      "loss": 0.8873,
      "step": 4570
    },
    {
      "epoch": 1.5281948615281948,
      "grad_norm": 6.462301254272461,
      "learning_rate": 2.5974561300200213e-05,
      "loss": 0.9535,
      "step": 4580
    },
    {
      "epoch": 1.5315315315315314,
      "grad_norm": 6.18765926361084,
      "learning_rate": 2.5915675421034035e-05,
      "loss": 1.0544,
      "step": 4590
    },
    {
      "epoch": 1.5348682015348682,
      "grad_norm": 7.067300319671631,
      "learning_rate": 2.585678954186786e-05,
      "loss": 1.0181,
      "step": 4600
    },
    {
      "epoch": 1.5382048715382048,
      "grad_norm": 4.788033962249756,
      "learning_rate": 2.5797903662701682e-05,
      "loss": 0.8565,
      "step": 4610
    },
    {
      "epoch": 1.5415415415415414,
      "grad_norm": 4.0219926834106445,
      "learning_rate": 2.5739017783535507e-05,
      "loss": 1.2158,
      "step": 4620
    },
    {
      "epoch": 1.5448782115448783,
      "grad_norm": 4.856924057006836,
      "learning_rate": 2.5680131904369332e-05,
      "loss": 0.9628,
      "step": 4630
    },
    {
      "epoch": 1.5482148815482148,
      "grad_norm": 4.065872669219971,
      "learning_rate": 2.5621246025203154e-05,
      "loss": 0.9511,
      "step": 4640
    },
    {
      "epoch": 1.5515515515515514,
      "grad_norm": 4.145706653594971,
      "learning_rate": 2.556236014603698e-05,
      "loss": 0.748,
      "step": 4650
    },
    {
      "epoch": 1.5548882215548883,
      "grad_norm": 4.888798713684082,
      "learning_rate": 2.5503474266870808e-05,
      "loss": 0.81,
      "step": 4660
    },
    {
      "epoch": 1.5582248915582249,
      "grad_norm": 5.7998809814453125,
      "learning_rate": 2.5444588387704633e-05,
      "loss": 1.0965,
      "step": 4670
    },
    {
      "epoch": 1.5615615615615615,
      "grad_norm": 3.8214259147644043,
      "learning_rate": 2.5385702508538455e-05,
      "loss": 1.0437,
      "step": 4680
    },
    {
      "epoch": 1.5648982315648983,
      "grad_norm": 4.867493152618408,
      "learning_rate": 2.532681662937228e-05,
      "loss": 0.9438,
      "step": 4690
    },
    {
      "epoch": 1.5682349015682349,
      "grad_norm": 3.0981714725494385,
      "learning_rate": 2.5267930750206105e-05,
      "loss": 0.7588,
      "step": 4700
    },
    {
      "epoch": 1.5715715715715715,
      "grad_norm": 2.934812307357788,
      "learning_rate": 2.5209044871039927e-05,
      "loss": 0.8293,
      "step": 4710
    },
    {
      "epoch": 1.5749082415749083,
      "grad_norm": 3.7560691833496094,
      "learning_rate": 2.5150158991873752e-05,
      "loss": 0.9055,
      "step": 4720
    },
    {
      "epoch": 1.5782449115782449,
      "grad_norm": 4.4222893714904785,
      "learning_rate": 2.5091273112707574e-05,
      "loss": 0.9761,
      "step": 4730
    },
    {
      "epoch": 1.5815815815815815,
      "grad_norm": 3.7061383724212646,
      "learning_rate": 2.50323872335414e-05,
      "loss": 1.0589,
      "step": 4740
    },
    {
      "epoch": 1.5849182515849183,
      "grad_norm": 5.439238548278809,
      "learning_rate": 2.4973501354375224e-05,
      "loss": 0.8043,
      "step": 4750
    },
    {
      "epoch": 1.5882549215882549,
      "grad_norm": 6.345290660858154,
      "learning_rate": 2.4914615475209046e-05,
      "loss": 1.1771,
      "step": 4760
    },
    {
      "epoch": 1.5915915915915915,
      "grad_norm": 4.001060485839844,
      "learning_rate": 2.485572959604287e-05,
      "loss": 0.7978,
      "step": 4770
    },
    {
      "epoch": 1.5949282615949283,
      "grad_norm": 3.999523639678955,
      "learning_rate": 2.4796843716876693e-05,
      "loss": 0.7515,
      "step": 4780
    },
    {
      "epoch": 1.5982649315982649,
      "grad_norm": 3.900792360305786,
      "learning_rate": 2.4737957837710518e-05,
      "loss": 0.8692,
      "step": 4790
    },
    {
      "epoch": 1.6016016016016015,
      "grad_norm": 3.60117506980896,
      "learning_rate": 2.4679071958544343e-05,
      "loss": 0.7335,
      "step": 4800
    },
    {
      "epoch": 1.6049382716049383,
      "grad_norm": 4.406067848205566,
      "learning_rate": 2.4620186079378165e-05,
      "loss": 1.2113,
      "step": 4810
    },
    {
      "epoch": 1.6082749416082749,
      "grad_norm": 3.0116846561431885,
      "learning_rate": 2.456130020021199e-05,
      "loss": 0.8775,
      "step": 4820
    },
    {
      "epoch": 1.6116116116116115,
      "grad_norm": 4.786301612854004,
      "learning_rate": 2.4502414321045815e-05,
      "loss": 0.9916,
      "step": 4830
    },
    {
      "epoch": 1.6149482816149483,
      "grad_norm": 4.318855285644531,
      "learning_rate": 2.4443528441879637e-05,
      "loss": 0.8857,
      "step": 4840
    },
    {
      "epoch": 1.6182849516182851,
      "grad_norm": 3.981743335723877,
      "learning_rate": 2.4384642562713462e-05,
      "loss": 0.7771,
      "step": 4850
    },
    {
      "epoch": 1.6216216216216215,
      "grad_norm": 7.738938331604004,
      "learning_rate": 2.4325756683547287e-05,
      "loss": 1.1116,
      "step": 4860
    },
    {
      "epoch": 1.6249582916249583,
      "grad_norm": 4.900479793548584,
      "learning_rate": 2.4266870804381112e-05,
      "loss": 1.0219,
      "step": 4870
    },
    {
      "epoch": 1.6282949616282951,
      "grad_norm": 4.699550151824951,
      "learning_rate": 2.4207984925214934e-05,
      "loss": 0.5644,
      "step": 4880
    },
    {
      "epoch": 1.6316316316316315,
      "grad_norm": 6.2433857917785645,
      "learning_rate": 2.414909904604876e-05,
      "loss": 1.0901,
      "step": 4890
    },
    {
      "epoch": 1.6349683016349683,
      "grad_norm": 3.9478719234466553,
      "learning_rate": 2.4090213166882584e-05,
      "loss": 0.9394,
      "step": 4900
    },
    {
      "epoch": 1.6383049716383051,
      "grad_norm": 5.000967025756836,
      "learning_rate": 2.4031327287716406e-05,
      "loss": 0.8312,
      "step": 4910
    },
    {
      "epoch": 1.6416416416416415,
      "grad_norm": 5.710293769836426,
      "learning_rate": 2.397244140855023e-05,
      "loss": 0.9712,
      "step": 4920
    },
    {
      "epoch": 1.6449783116449783,
      "grad_norm": 6.429892063140869,
      "learning_rate": 2.3913555529384053e-05,
      "loss": 1.0085,
      "step": 4930
    },
    {
      "epoch": 1.6483149816483151,
      "grad_norm": 5.284185409545898,
      "learning_rate": 2.385466965021788e-05,
      "loss": 1.0195,
      "step": 4940
    },
    {
      "epoch": 1.6516516516516515,
      "grad_norm": 5.044758319854736,
      "learning_rate": 2.3795783771051704e-05,
      "loss": 1.1279,
      "step": 4950
    },
    {
      "epoch": 1.6549883216549883,
      "grad_norm": 5.943666458129883,
      "learning_rate": 2.3736897891885525e-05,
      "loss": 0.8848,
      "step": 4960
    },
    {
      "epoch": 1.6583249916583251,
      "grad_norm": 3.88893985748291,
      "learning_rate": 2.367801201271935e-05,
      "loss": 0.9976,
      "step": 4970
    },
    {
      "epoch": 1.6616616616616615,
      "grad_norm": 4.05940055847168,
      "learning_rate": 2.3619126133553172e-05,
      "loss": 0.7933,
      "step": 4980
    },
    {
      "epoch": 1.6649983316649983,
      "grad_norm": 4.058497428894043,
      "learning_rate": 2.3560240254387e-05,
      "loss": 0.8127,
      "step": 4990
    },
    {
      "epoch": 1.6683350016683351,
      "grad_norm": 4.624009132385254,
      "learning_rate": 2.3501354375220826e-05,
      "loss": 0.8364,
      "step": 5000
    },
    {
      "epoch": 1.6716716716716715,
      "grad_norm": 4.875530242919922,
      "learning_rate": 2.3442468496054648e-05,
      "loss": 0.7804,
      "step": 5010
    },
    {
      "epoch": 1.6750083416750083,
      "grad_norm": 3.8115475177764893,
      "learning_rate": 2.3383582616888473e-05,
      "loss": 0.8612,
      "step": 5020
    },
    {
      "epoch": 1.6783450116783452,
      "grad_norm": 4.301511287689209,
      "learning_rate": 2.3324696737722295e-05,
      "loss": 0.7989,
      "step": 5030
    },
    {
      "epoch": 1.6816816816816815,
      "grad_norm": 7.760819435119629,
      "learning_rate": 2.326581085855612e-05,
      "loss": 1.1598,
      "step": 5040
    },
    {
      "epoch": 1.6850183516850183,
      "grad_norm": 7.268030166625977,
      "learning_rate": 2.3206924979389945e-05,
      "loss": 1.1316,
      "step": 5050
    },
    {
      "epoch": 1.6883550216883552,
      "grad_norm": 7.396394729614258,
      "learning_rate": 2.3148039100223767e-05,
      "loss": 0.9967,
      "step": 5060
    },
    {
      "epoch": 1.6916916916916915,
      "grad_norm": 3.36869215965271,
      "learning_rate": 2.3089153221057592e-05,
      "loss": 0.9185,
      "step": 5070
    },
    {
      "epoch": 1.6950283616950284,
      "grad_norm": 5.504220962524414,
      "learning_rate": 2.3030267341891414e-05,
      "loss": 0.9148,
      "step": 5080
    },
    {
      "epoch": 1.6983650316983652,
      "grad_norm": 3.9718313217163086,
      "learning_rate": 2.297138146272524e-05,
      "loss": 0.968,
      "step": 5090
    },
    {
      "epoch": 1.7017017017017015,
      "grad_norm": 4.696664333343506,
      "learning_rate": 2.2912495583559064e-05,
      "loss": 0.9419,
      "step": 5100
    },
    {
      "epoch": 1.7050383717050384,
      "grad_norm": 7.094948768615723,
      "learning_rate": 2.2853609704392886e-05,
      "loss": 1.0996,
      "step": 5110
    },
    {
      "epoch": 1.7083750417083752,
      "grad_norm": 3.79357647895813,
      "learning_rate": 2.279472382522671e-05,
      "loss": 0.8248,
      "step": 5120
    },
    {
      "epoch": 1.7117117117117115,
      "grad_norm": 4.002504348754883,
      "learning_rate": 2.2735837946060536e-05,
      "loss": 0.7523,
      "step": 5130
    },
    {
      "epoch": 1.7150483817150484,
      "grad_norm": 5.466933727264404,
      "learning_rate": 2.267695206689436e-05,
      "loss": 0.9159,
      "step": 5140
    },
    {
      "epoch": 1.7183850517183852,
      "grad_norm": 5.31099271774292,
      "learning_rate": 2.2618066187728183e-05,
      "loss": 0.9117,
      "step": 5150
    },
    {
      "epoch": 1.7217217217217218,
      "grad_norm": 5.264367580413818,
      "learning_rate": 2.2559180308562008e-05,
      "loss": 1.1738,
      "step": 5160
    },
    {
      "epoch": 1.7250583917250584,
      "grad_norm": 5.362513542175293,
      "learning_rate": 2.2500294429395833e-05,
      "loss": 1.1561,
      "step": 5170
    },
    {
      "epoch": 1.7283950617283952,
      "grad_norm": 5.140868186950684,
      "learning_rate": 2.2441408550229655e-05,
      "loss": 0.9682,
      "step": 5180
    },
    {
      "epoch": 1.7317317317317318,
      "grad_norm": 4.131853103637695,
      "learning_rate": 2.238252267106348e-05,
      "loss": 0.8582,
      "step": 5190
    },
    {
      "epoch": 1.7350684017350684,
      "grad_norm": 4.669349670410156,
      "learning_rate": 2.2323636791897305e-05,
      "loss": 1.1224,
      "step": 5200
    },
    {
      "epoch": 1.7384050717384052,
      "grad_norm": 4.853196144104004,
      "learning_rate": 2.2264750912731127e-05,
      "loss": 0.9605,
      "step": 5210
    },
    {
      "epoch": 1.7417417417417418,
      "grad_norm": 3.0719332695007324,
      "learning_rate": 2.2205865033564952e-05,
      "loss": 0.8795,
      "step": 5220
    },
    {
      "epoch": 1.7450784117450784,
      "grad_norm": 4.694395542144775,
      "learning_rate": 2.2146979154398774e-05,
      "loss": 0.9412,
      "step": 5230
    },
    {
      "epoch": 1.7484150817484152,
      "grad_norm": 3.7310755252838135,
      "learning_rate": 2.20880932752326e-05,
      "loss": 0.9017,
      "step": 5240
    },
    {
      "epoch": 1.7517517517517518,
      "grad_norm": 4.693175315856934,
      "learning_rate": 2.2029207396066424e-05,
      "loss": 0.92,
      "step": 5250
    },
    {
      "epoch": 1.7550884217550884,
      "grad_norm": 6.304305553436279,
      "learning_rate": 2.197032151690025e-05,
      "loss": 0.9453,
      "step": 5260
    },
    {
      "epoch": 1.7584250917584252,
      "grad_norm": 6.206532955169678,
      "learning_rate": 2.1911435637734075e-05,
      "loss": 0.9191,
      "step": 5270
    },
    {
      "epoch": 1.7617617617617618,
      "grad_norm": 4.682729244232178,
      "learning_rate": 2.1852549758567896e-05,
      "loss": 0.7072,
      "step": 5280
    },
    {
      "epoch": 1.7650984317650984,
      "grad_norm": 5.541351795196533,
      "learning_rate": 2.179366387940172e-05,
      "loss": 1.0698,
      "step": 5290
    },
    {
      "epoch": 1.7684351017684352,
      "grad_norm": 7.073324203491211,
      "learning_rate": 2.1734778000235543e-05,
      "loss": 0.9771,
      "step": 5300
    },
    {
      "epoch": 1.7717717717717718,
      "grad_norm": 6.106131076812744,
      "learning_rate": 2.167589212106937e-05,
      "loss": 1.0908,
      "step": 5310
    },
    {
      "epoch": 1.7751084417751084,
      "grad_norm": 4.724084377288818,
      "learning_rate": 2.1617006241903194e-05,
      "loss": 1.1158,
      "step": 5320
    },
    {
      "epoch": 1.7784451117784452,
      "grad_norm": 3.7488021850585938,
      "learning_rate": 2.1558120362737016e-05,
      "loss": 0.8532,
      "step": 5330
    },
    {
      "epoch": 1.7817817817817818,
      "grad_norm": 5.747878551483154,
      "learning_rate": 2.149923448357084e-05,
      "loss": 0.8871,
      "step": 5340
    },
    {
      "epoch": 1.7851184517851184,
      "grad_norm": 3.325592279434204,
      "learning_rate": 2.1440348604404666e-05,
      "loss": 0.7972,
      "step": 5350
    },
    {
      "epoch": 1.7884551217884552,
      "grad_norm": 5.427106857299805,
      "learning_rate": 2.1381462725238488e-05,
      "loss": 0.9369,
      "step": 5360
    },
    {
      "epoch": 1.7917917917917918,
      "grad_norm": 4.729701519012451,
      "learning_rate": 2.1322576846072313e-05,
      "loss": 1.042,
      "step": 5370
    },
    {
      "epoch": 1.7951284617951284,
      "grad_norm": 5.39504861831665,
      "learning_rate": 2.1263690966906135e-05,
      "loss": 1.0996,
      "step": 5380
    },
    {
      "epoch": 1.7984651317984652,
      "grad_norm": 5.084852695465088,
      "learning_rate": 2.1204805087739963e-05,
      "loss": 0.8213,
      "step": 5390
    },
    {
      "epoch": 1.8018018018018018,
      "grad_norm": 3.9791672229766846,
      "learning_rate": 2.1145919208573785e-05,
      "loss": 0.7506,
      "step": 5400
    },
    {
      "epoch": 1.8051384718051384,
      "grad_norm": 5.441821575164795,
      "learning_rate": 2.108703332940761e-05,
      "loss": 0.8619,
      "step": 5410
    },
    {
      "epoch": 1.8084751418084752,
      "grad_norm": 2.832183599472046,
      "learning_rate": 2.1028147450241435e-05,
      "loss": 1.0097,
      "step": 5420
    },
    {
      "epoch": 1.8118118118118118,
      "grad_norm": 2.9059581756591797,
      "learning_rate": 2.0969261571075257e-05,
      "loss": 0.855,
      "step": 5430
    },
    {
      "epoch": 1.8151484818151484,
      "grad_norm": 3.929295063018799,
      "learning_rate": 2.0910375691909082e-05,
      "loss": 0.8321,
      "step": 5440
    },
    {
      "epoch": 1.8184851518184852,
      "grad_norm": 5.422582626342773,
      "learning_rate": 2.0851489812742904e-05,
      "loss": 0.8886,
      "step": 5450
    },
    {
      "epoch": 1.8218218218218218,
      "grad_norm": 8.23167896270752,
      "learning_rate": 2.079260393357673e-05,
      "loss": 1.0817,
      "step": 5460
    },
    {
      "epoch": 1.8251584918251584,
      "grad_norm": 5.019721031188965,
      "learning_rate": 2.0733718054410554e-05,
      "loss": 0.988,
      "step": 5470
    },
    {
      "epoch": 1.8284951618284953,
      "grad_norm": 6.165898323059082,
      "learning_rate": 2.0674832175244376e-05,
      "loss": 0.8839,
      "step": 5480
    },
    {
      "epoch": 1.8318318318318318,
      "grad_norm": 3.6453328132629395,
      "learning_rate": 2.06159462960782e-05,
      "loss": 0.866,
      "step": 5490
    },
    {
      "epoch": 1.8351685018351684,
      "grad_norm": 6.28937292098999,
      "learning_rate": 2.0557060416912023e-05,
      "loss": 1.0241,
      "step": 5500
    },
    {
      "epoch": 1.8385051718385053,
      "grad_norm": 4.9938883781433105,
      "learning_rate": 2.0498174537745848e-05,
      "loss": 1.0056,
      "step": 5510
    },
    {
      "epoch": 1.8418418418418419,
      "grad_norm": 5.800065040588379,
      "learning_rate": 2.0439288658579673e-05,
      "loss": 1.1227,
      "step": 5520
    },
    {
      "epoch": 1.8451785118451784,
      "grad_norm": 3.2610182762145996,
      "learning_rate": 2.03804027794135e-05,
      "loss": 0.8758,
      "step": 5530
    },
    {
      "epoch": 1.8485151818485153,
      "grad_norm": 6.9058308601379395,
      "learning_rate": 2.0321516900247324e-05,
      "loss": 0.8656,
      "step": 5540
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 3.4954466819763184,
      "learning_rate": 2.0262631021081145e-05,
      "loss": 0.7662,
      "step": 5550
    },
    {
      "epoch": 1.8551885218551885,
      "grad_norm": 4.638798713684082,
      "learning_rate": 2.020374514191497e-05,
      "loss": 0.8754,
      "step": 5560
    },
    {
      "epoch": 1.8585251918585253,
      "grad_norm": 5.858693599700928,
      "learning_rate": 2.0144859262748796e-05,
      "loss": 0.8923,
      "step": 5570
    },
    {
      "epoch": 1.8618618618618619,
      "grad_norm": 4.702371597290039,
      "learning_rate": 2.0085973383582617e-05,
      "loss": 0.7969,
      "step": 5580
    },
    {
      "epoch": 1.8651985318651985,
      "grad_norm": 2.968923807144165,
      "learning_rate": 2.0027087504416443e-05,
      "loss": 0.6537,
      "step": 5590
    },
    {
      "epoch": 1.8685352018685353,
      "grad_norm": 5.99422550201416,
      "learning_rate": 1.9968201625250264e-05,
      "loss": 0.7977,
      "step": 5600
    },
    {
      "epoch": 1.8718718718718719,
      "grad_norm": 4.8941545486450195,
      "learning_rate": 1.990931574608409e-05,
      "loss": 0.7347,
      "step": 5610
    },
    {
      "epoch": 1.8752085418752085,
      "grad_norm": 5.707812786102295,
      "learning_rate": 1.9850429866917915e-05,
      "loss": 0.9891,
      "step": 5620
    },
    {
      "epoch": 1.8785452118785453,
      "grad_norm": 3.534576416015625,
      "learning_rate": 1.9791543987751736e-05,
      "loss": 0.8678,
      "step": 5630
    },
    {
      "epoch": 1.8818818818818819,
      "grad_norm": 3.7414798736572266,
      "learning_rate": 1.973265810858556e-05,
      "loss": 0.9536,
      "step": 5640
    },
    {
      "epoch": 1.8852185518852185,
      "grad_norm": 7.50349235534668,
      "learning_rate": 1.9673772229419383e-05,
      "loss": 0.9938,
      "step": 5650
    },
    {
      "epoch": 1.8885552218885553,
      "grad_norm": 4.329593181610107,
      "learning_rate": 1.9614886350253212e-05,
      "loss": 1.1525,
      "step": 5660
    },
    {
      "epoch": 1.8918918918918919,
      "grad_norm": 7.405785083770752,
      "learning_rate": 1.9556000471087037e-05,
      "loss": 0.8645,
      "step": 5670
    },
    {
      "epoch": 1.8952285618952285,
      "grad_norm": 4.554341793060303,
      "learning_rate": 1.949711459192086e-05,
      "loss": 0.7567,
      "step": 5680
    },
    {
      "epoch": 1.8985652318985653,
      "grad_norm": 5.50166654586792,
      "learning_rate": 1.9438228712754684e-05,
      "loss": 1.0302,
      "step": 5690
    },
    {
      "epoch": 1.901901901901902,
      "grad_norm": 5.851183891296387,
      "learning_rate": 1.9379342833588506e-05,
      "loss": 0.9633,
      "step": 5700
    },
    {
      "epoch": 1.9052385719052385,
      "grad_norm": 8.137897491455078,
      "learning_rate": 1.932045695442233e-05,
      "loss": 0.8459,
      "step": 5710
    },
    {
      "epoch": 1.9085752419085753,
      "grad_norm": 4.997340202331543,
      "learning_rate": 1.9261571075256156e-05,
      "loss": 0.9425,
      "step": 5720
    },
    {
      "epoch": 1.911911911911912,
      "grad_norm": 3.326988697052002,
      "learning_rate": 1.9202685196089978e-05,
      "loss": 0.8993,
      "step": 5730
    },
    {
      "epoch": 1.9152485819152485,
      "grad_norm": 5.360797882080078,
      "learning_rate": 1.9143799316923803e-05,
      "loss": 0.8621,
      "step": 5740
    },
    {
      "epoch": 1.9185852519185853,
      "grad_norm": 5.478371620178223,
      "learning_rate": 1.9084913437757625e-05,
      "loss": 0.8808,
      "step": 5750
    },
    {
      "epoch": 1.921921921921922,
      "grad_norm": 6.183421611785889,
      "learning_rate": 1.902602755859145e-05,
      "loss": 1.0987,
      "step": 5760
    },
    {
      "epoch": 1.9252585919252585,
      "grad_norm": 4.676942348480225,
      "learning_rate": 1.8967141679425275e-05,
      "loss": 1.0105,
      "step": 5770
    },
    {
      "epoch": 1.9285952619285953,
      "grad_norm": 3.41871976852417,
      "learning_rate": 1.8908255800259097e-05,
      "loss": 0.8062,
      "step": 5780
    },
    {
      "epoch": 1.931931931931932,
      "grad_norm": 3.338702917098999,
      "learning_rate": 1.8849369921092922e-05,
      "loss": 0.9273,
      "step": 5790
    },
    {
      "epoch": 1.9352686019352685,
      "grad_norm": 5.5353779792785645,
      "learning_rate": 1.8790484041926747e-05,
      "loss": 1.0641,
      "step": 5800
    },
    {
      "epoch": 1.9386052719386053,
      "grad_norm": 4.87351131439209,
      "learning_rate": 1.8731598162760572e-05,
      "loss": 0.926,
      "step": 5810
    },
    {
      "epoch": 1.941941941941942,
      "grad_norm": 4.008827209472656,
      "learning_rate": 1.8672712283594394e-05,
      "loss": 0.6096,
      "step": 5820
    },
    {
      "epoch": 1.9452786119452785,
      "grad_norm": 4.192124843597412,
      "learning_rate": 1.861382640442822e-05,
      "loss": 0.9273,
      "step": 5830
    },
    {
      "epoch": 1.9486152819486153,
      "grad_norm": 5.575512886047363,
      "learning_rate": 1.8554940525262044e-05,
      "loss": 0.9779,
      "step": 5840
    },
    {
      "epoch": 1.951951951951952,
      "grad_norm": 4.183220386505127,
      "learning_rate": 1.8496054646095866e-05,
      "loss": 0.8634,
      "step": 5850
    },
    {
      "epoch": 1.9552886219552885,
      "grad_norm": 2.647493362426758,
      "learning_rate": 1.843716876692969e-05,
      "loss": 0.8437,
      "step": 5860
    },
    {
      "epoch": 1.9586252919586253,
      "grad_norm": 4.902371406555176,
      "learning_rate": 1.8378282887763516e-05,
      "loss": 1.0746,
      "step": 5870
    },
    {
      "epoch": 1.961961961961962,
      "grad_norm": 4.100434303283691,
      "learning_rate": 1.8319397008597338e-05,
      "loss": 0.7541,
      "step": 5880
    },
    {
      "epoch": 1.9652986319652985,
      "grad_norm": 4.448179244995117,
      "learning_rate": 1.8260511129431163e-05,
      "loss": 0.896,
      "step": 5890
    },
    {
      "epoch": 1.9686353019686353,
      "grad_norm": 3.5408051013946533,
      "learning_rate": 1.8201625250264985e-05,
      "loss": 0.8555,
      "step": 5900
    },
    {
      "epoch": 1.971971971971972,
      "grad_norm": 6.36071252822876,
      "learning_rate": 1.814273937109881e-05,
      "loss": 0.9312,
      "step": 5910
    },
    {
      "epoch": 1.9753086419753085,
      "grad_norm": 2.734036922454834,
      "learning_rate": 1.8083853491932636e-05,
      "loss": 0.8123,
      "step": 5920
    },
    {
      "epoch": 1.9786453119786453,
      "grad_norm": 5.436046600341797,
      "learning_rate": 1.802496761276646e-05,
      "loss": 1.0673,
      "step": 5930
    },
    {
      "epoch": 1.981981981981982,
      "grad_norm": 6.309381484985352,
      "learning_rate": 1.7966081733600286e-05,
      "loss": 0.894,
      "step": 5940
    },
    {
      "epoch": 1.9853186519853185,
      "grad_norm": 5.3144707679748535,
      "learning_rate": 1.7907195854434108e-05,
      "loss": 0.8549,
      "step": 5950
    },
    {
      "epoch": 1.9886553219886554,
      "grad_norm": 4.392692565917969,
      "learning_rate": 1.7848309975267933e-05,
      "loss": 0.8554,
      "step": 5960
    },
    {
      "epoch": 1.991991991991992,
      "grad_norm": 3.9173834323883057,
      "learning_rate": 1.7789424096101755e-05,
      "loss": 0.9933,
      "step": 5970
    },
    {
      "epoch": 1.9953286619953285,
      "grad_norm": 4.9565277099609375,
      "learning_rate": 1.773053821693558e-05,
      "loss": 0.7982,
      "step": 5980
    },
    {
      "epoch": 1.9986653319986654,
      "grad_norm": 3.293752670288086,
      "learning_rate": 1.7671652337769405e-05,
      "loss": 0.7807,
      "step": 5990
    },
    {
      "epoch": 2.002002002002002,
      "grad_norm": 2.9699745178222656,
      "learning_rate": 1.7612766458603227e-05,
      "loss": 0.9868,
      "step": 6000
    },
    {
      "epoch": 2.0053386720053386,
      "grad_norm": 5.142185211181641,
      "learning_rate": 1.7553880579437052e-05,
      "loss": 0.915,
      "step": 6010
    },
    {
      "epoch": 2.0086753420086754,
      "grad_norm": 5.411505699157715,
      "learning_rate": 1.7494994700270877e-05,
      "loss": 0.7741,
      "step": 6020
    },
    {
      "epoch": 2.012012012012012,
      "grad_norm": 4.0805158615112305,
      "learning_rate": 1.74361088211047e-05,
      "loss": 0.996,
      "step": 6030
    },
    {
      "epoch": 2.0153486820153486,
      "grad_norm": 4.164783477783203,
      "learning_rate": 1.7377222941938524e-05,
      "loss": 0.9477,
      "step": 6040
    },
    {
      "epoch": 2.0186853520186854,
      "grad_norm": 3.642644166946411,
      "learning_rate": 1.7318337062772346e-05,
      "loss": 0.9997,
      "step": 6050
    },
    {
      "epoch": 2.022022022022022,
      "grad_norm": 9.850851058959961,
      "learning_rate": 1.7259451183606174e-05,
      "loss": 1.1888,
      "step": 6060
    },
    {
      "epoch": 2.0253586920253586,
      "grad_norm": 4.669140815734863,
      "learning_rate": 1.7200565304439996e-05,
      "loss": 0.7663,
      "step": 6070
    },
    {
      "epoch": 2.0286953620286954,
      "grad_norm": 5.902434349060059,
      "learning_rate": 1.714167942527382e-05,
      "loss": 0.8533,
      "step": 6080
    },
    {
      "epoch": 2.032032032032032,
      "grad_norm": 4.271953582763672,
      "learning_rate": 1.7082793546107646e-05,
      "loss": 0.8008,
      "step": 6090
    },
    {
      "epoch": 2.0353687020353686,
      "grad_norm": 4.07318115234375,
      "learning_rate": 1.7023907666941468e-05,
      "loss": 1.0907,
      "step": 6100
    },
    {
      "epoch": 2.0387053720387054,
      "grad_norm": 6.745945453643799,
      "learning_rate": 1.6965021787775293e-05,
      "loss": 1.0455,
      "step": 6110
    },
    {
      "epoch": 2.042042042042042,
      "grad_norm": 5.885935306549072,
      "learning_rate": 1.6906135908609115e-05,
      "loss": 0.9016,
      "step": 6120
    },
    {
      "epoch": 2.0453787120453786,
      "grad_norm": 7.77225923538208,
      "learning_rate": 1.684725002944294e-05,
      "loss": 0.9274,
      "step": 6130
    },
    {
      "epoch": 2.0487153820487154,
      "grad_norm": 5.085982322692871,
      "learning_rate": 1.6788364150276765e-05,
      "loss": 0.9233,
      "step": 6140
    },
    {
      "epoch": 2.052052052052052,
      "grad_norm": 7.08985710144043,
      "learning_rate": 1.6729478271110587e-05,
      "loss": 0.9882,
      "step": 6150
    },
    {
      "epoch": 2.0553887220553886,
      "grad_norm": 4.130739212036133,
      "learning_rate": 1.6670592391944412e-05,
      "loss": 0.7488,
      "step": 6160
    },
    {
      "epoch": 2.0587253920587254,
      "grad_norm": 3.936957359313965,
      "learning_rate": 1.6611706512778234e-05,
      "loss": 0.8409,
      "step": 6170
    },
    {
      "epoch": 2.062062062062062,
      "grad_norm": 4.870021820068359,
      "learning_rate": 1.655282063361206e-05,
      "loss": 0.8849,
      "step": 6180
    },
    {
      "epoch": 2.0653987320653986,
      "grad_norm": 6.4573163986206055,
      "learning_rate": 1.6493934754445884e-05,
      "loss": 1.1138,
      "step": 6190
    },
    {
      "epoch": 2.0687354020687354,
      "grad_norm": 6.541041851043701,
      "learning_rate": 1.643504887527971e-05,
      "loss": 0.9853,
      "step": 6200
    },
    {
      "epoch": 2.0720720720720722,
      "grad_norm": 4.399292945861816,
      "learning_rate": 1.6376162996113535e-05,
      "loss": 0.9166,
      "step": 6210
    },
    {
      "epoch": 2.0754087420754086,
      "grad_norm": 6.608387470245361,
      "learning_rate": 1.6317277116947356e-05,
      "loss": 1.0814,
      "step": 6220
    },
    {
      "epoch": 2.0787454120787454,
      "grad_norm": 4.985740661621094,
      "learning_rate": 1.625839123778118e-05,
      "loss": 0.9653,
      "step": 6230
    },
    {
      "epoch": 2.0820820820820822,
      "grad_norm": 3.5988619327545166,
      "learning_rate": 1.6199505358615007e-05,
      "loss": 0.772,
      "step": 6240
    },
    {
      "epoch": 2.0854187520854186,
      "grad_norm": 4.408260345458984,
      "learning_rate": 1.614061947944883e-05,
      "loss": 0.6676,
      "step": 6250
    },
    {
      "epoch": 2.0887554220887554,
      "grad_norm": 4.239713668823242,
      "learning_rate": 1.6081733600282654e-05,
      "loss": 0.7602,
      "step": 6260
    },
    {
      "epoch": 2.0920920920920922,
      "grad_norm": 3.6242785453796387,
      "learning_rate": 1.6022847721116475e-05,
      "loss": 0.6909,
      "step": 6270
    },
    {
      "epoch": 2.0954287620954286,
      "grad_norm": 3.029491662979126,
      "learning_rate": 1.59639618419503e-05,
      "loss": 1.0758,
      "step": 6280
    },
    {
      "epoch": 2.0987654320987654,
      "grad_norm": 5.272650241851807,
      "learning_rate": 1.5905075962784126e-05,
      "loss": 1.0016,
      "step": 6290
    },
    {
      "epoch": 2.1021021021021022,
      "grad_norm": 5.37189245223999,
      "learning_rate": 1.5846190083617948e-05,
      "loss": 0.9268,
      "step": 6300
    },
    {
      "epoch": 2.1054387721054386,
      "grad_norm": 5.0123467445373535,
      "learning_rate": 1.5787304204451773e-05,
      "loss": 0.7744,
      "step": 6310
    },
    {
      "epoch": 2.1087754421087754,
      "grad_norm": 5.7150139808654785,
      "learning_rate": 1.5728418325285594e-05,
      "loss": 0.9206,
      "step": 6320
    },
    {
      "epoch": 2.1121121121121122,
      "grad_norm": 4.771893501281738,
      "learning_rate": 1.5669532446119423e-05,
      "loss": 0.9017,
      "step": 6330
    },
    {
      "epoch": 2.1154487821154486,
      "grad_norm": 4.527965545654297,
      "learning_rate": 1.5610646566953245e-05,
      "loss": 0.7292,
      "step": 6340
    },
    {
      "epoch": 2.1187854521187854,
      "grad_norm": 4.183869361877441,
      "learning_rate": 1.555176068778707e-05,
      "loss": 1.0932,
      "step": 6350
    },
    {
      "epoch": 2.1221221221221223,
      "grad_norm": 4.669654846191406,
      "learning_rate": 1.5492874808620895e-05,
      "loss": 0.9786,
      "step": 6360
    },
    {
      "epoch": 2.1254587921254586,
      "grad_norm": 5.183159351348877,
      "learning_rate": 1.5433988929454717e-05,
      "loss": 0.8662,
      "step": 6370
    },
    {
      "epoch": 2.1287954621287954,
      "grad_norm": 1.620301365852356,
      "learning_rate": 1.5375103050288542e-05,
      "loss": 0.8619,
      "step": 6380
    },
    {
      "epoch": 2.1321321321321323,
      "grad_norm": 3.4083471298217773,
      "learning_rate": 1.5316217171122367e-05,
      "loss": 0.8404,
      "step": 6390
    },
    {
      "epoch": 2.1354688021354686,
      "grad_norm": 3.6774728298187256,
      "learning_rate": 1.5257331291956189e-05,
      "loss": 0.825,
      "step": 6400
    },
    {
      "epoch": 2.1388054721388055,
      "grad_norm": 6.050713062286377,
      "learning_rate": 1.5198445412790014e-05,
      "loss": 0.8696,
      "step": 6410
    },
    {
      "epoch": 2.1421421421421423,
      "grad_norm": 4.190037727355957,
      "learning_rate": 1.5139559533623838e-05,
      "loss": 0.7511,
      "step": 6420
    },
    {
      "epoch": 2.1454788121454786,
      "grad_norm": 4.947109699249268,
      "learning_rate": 1.5080673654457661e-05,
      "loss": 0.8442,
      "step": 6430
    },
    {
      "epoch": 2.1488154821488155,
      "grad_norm": 4.761785507202148,
      "learning_rate": 1.5021787775291484e-05,
      "loss": 0.8246,
      "step": 6440
    },
    {
      "epoch": 2.1521521521521523,
      "grad_norm": 8.90989875793457,
      "learning_rate": 1.4962901896125308e-05,
      "loss": 0.8883,
      "step": 6450
    },
    {
      "epoch": 2.1554888221554886,
      "grad_norm": 4.033025741577148,
      "learning_rate": 1.4904016016959135e-05,
      "loss": 0.8235,
      "step": 6460
    },
    {
      "epoch": 2.1588254921588255,
      "grad_norm": 5.808401107788086,
      "learning_rate": 1.484513013779296e-05,
      "loss": 1.0093,
      "step": 6470
    },
    {
      "epoch": 2.1621621621621623,
      "grad_norm": 5.0555524826049805,
      "learning_rate": 1.4786244258626783e-05,
      "loss": 0.8426,
      "step": 6480
    },
    {
      "epoch": 2.1654988321654987,
      "grad_norm": 7.746306896209717,
      "learning_rate": 1.4727358379460607e-05,
      "loss": 1.0316,
      "step": 6490
    },
    {
      "epoch": 2.1688355021688355,
      "grad_norm": 6.059556484222412,
      "learning_rate": 1.466847250029443e-05,
      "loss": 1.0438,
      "step": 6500
    },
    {
      "epoch": 2.1721721721721723,
      "grad_norm": 3.5329930782318115,
      "learning_rate": 1.4609586621128254e-05,
      "loss": 0.9047,
      "step": 6510
    },
    {
      "epoch": 2.1755088421755087,
      "grad_norm": 6.387434482574463,
      "learning_rate": 1.4550700741962079e-05,
      "loss": 0.9025,
      "step": 6520
    },
    {
      "epoch": 2.1788455121788455,
      "grad_norm": 7.642216205596924,
      "learning_rate": 1.4491814862795902e-05,
      "loss": 0.954,
      "step": 6530
    },
    {
      "epoch": 2.1821821821821823,
      "grad_norm": 7.342599868774414,
      "learning_rate": 1.4432928983629726e-05,
      "loss": 0.9097,
      "step": 6540
    },
    {
      "epoch": 2.1855188521855187,
      "grad_norm": 4.862120628356934,
      "learning_rate": 1.437404310446355e-05,
      "loss": 0.9157,
      "step": 6550
    },
    {
      "epoch": 2.1888555221888555,
      "grad_norm": 2.8272507190704346,
      "learning_rate": 1.4315157225297373e-05,
      "loss": 0.741,
      "step": 6560
    },
    {
      "epoch": 2.1921921921921923,
      "grad_norm": 6.162783622741699,
      "learning_rate": 1.4256271346131198e-05,
      "loss": 0.7616,
      "step": 6570
    },
    {
      "epoch": 2.1955288621955287,
      "grad_norm": 5.792452335357666,
      "learning_rate": 1.4197385466965021e-05,
      "loss": 0.898,
      "step": 6580
    },
    {
      "epoch": 2.1988655321988655,
      "grad_norm": 9.175507545471191,
      "learning_rate": 1.4138499587798845e-05,
      "loss": 0.914,
      "step": 6590
    },
    {
      "epoch": 2.2022022022022023,
      "grad_norm": 5.308396816253662,
      "learning_rate": 1.4079613708632672e-05,
      "loss": 0.9587,
      "step": 6600
    },
    {
      "epoch": 2.2055388722055387,
      "grad_norm": 6.998093128204346,
      "learning_rate": 1.4020727829466495e-05,
      "loss": 0.9407,
      "step": 6610
    },
    {
      "epoch": 2.2088755422088755,
      "grad_norm": 4.273682594299316,
      "learning_rate": 1.3961841950300319e-05,
      "loss": 0.6883,
      "step": 6620
    },
    {
      "epoch": 2.2122122122122123,
      "grad_norm": 3.6364569664001465,
      "learning_rate": 1.3902956071134144e-05,
      "loss": 0.7608,
      "step": 6630
    },
    {
      "epoch": 2.2155488822155487,
      "grad_norm": 4.156885147094727,
      "learning_rate": 1.3844070191967967e-05,
      "loss": 0.9188,
      "step": 6640
    },
    {
      "epoch": 2.2188855522188855,
      "grad_norm": 6.085949897766113,
      "learning_rate": 1.378518431280179e-05,
      "loss": 0.8951,
      "step": 6650
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 7.30843448638916,
      "learning_rate": 1.3726298433635614e-05,
      "loss": 1.0023,
      "step": 6660
    },
    {
      "epoch": 2.2255588922255587,
      "grad_norm": 4.495553970336914,
      "learning_rate": 1.366741255446944e-05,
      "loss": 0.984,
      "step": 6670
    },
    {
      "epoch": 2.2288955622288955,
      "grad_norm": 5.932806491851807,
      "learning_rate": 1.3608526675303263e-05,
      "loss": 0.9389,
      "step": 6680
    },
    {
      "epoch": 2.2322322322322323,
      "grad_norm": 5.663153171539307,
      "learning_rate": 1.3549640796137086e-05,
      "loss": 0.9522,
      "step": 6690
    },
    {
      "epoch": 2.2355689022355687,
      "grad_norm": 5.434256076812744,
      "learning_rate": 1.349075491697091e-05,
      "loss": 0.7702,
      "step": 6700
    },
    {
      "epoch": 2.2389055722389055,
      "grad_norm": 5.1048359870910645,
      "learning_rate": 1.3431869037804733e-05,
      "loss": 0.845,
      "step": 6710
    },
    {
      "epoch": 2.2422422422422423,
      "grad_norm": 7.671282768249512,
      "learning_rate": 1.3372983158638558e-05,
      "loss": 0.907,
      "step": 6720
    },
    {
      "epoch": 2.2455789122455787,
      "grad_norm": 4.845818996429443,
      "learning_rate": 1.3314097279472385e-05,
      "loss": 0.6987,
      "step": 6730
    },
    {
      "epoch": 2.2489155822489155,
      "grad_norm": 6.694758892059326,
      "learning_rate": 1.3255211400306209e-05,
      "loss": 0.9322,
      "step": 6740
    },
    {
      "epoch": 2.2522522522522523,
      "grad_norm": 4.263418197631836,
      "learning_rate": 1.3196325521140032e-05,
      "loss": 0.7708,
      "step": 6750
    },
    {
      "epoch": 2.2555889222555887,
      "grad_norm": 5.951828479766846,
      "learning_rate": 1.3137439641973856e-05,
      "loss": 0.9275,
      "step": 6760
    },
    {
      "epoch": 2.2589255922589255,
      "grad_norm": 9.562702178955078,
      "learning_rate": 1.307855376280768e-05,
      "loss": 0.9399,
      "step": 6770
    },
    {
      "epoch": 2.2622622622622623,
      "grad_norm": 1.8399078845977783,
      "learning_rate": 1.3019667883641504e-05,
      "loss": 0.8867,
      "step": 6780
    },
    {
      "epoch": 2.2655989322655987,
      "grad_norm": 4.848128795623779,
      "learning_rate": 1.2960782004475328e-05,
      "loss": 0.8561,
      "step": 6790
    },
    {
      "epoch": 2.2689356022689355,
      "grad_norm": 4.194354057312012,
      "learning_rate": 1.2901896125309151e-05,
      "loss": 1.1558,
      "step": 6800
    },
    {
      "epoch": 2.2722722722722724,
      "grad_norm": 5.059664249420166,
      "learning_rate": 1.2843010246142975e-05,
      "loss": 0.8142,
      "step": 6810
    },
    {
      "epoch": 2.2756089422756087,
      "grad_norm": 3.1696560382843018,
      "learning_rate": 1.27841243669768e-05,
      "loss": 0.8386,
      "step": 6820
    },
    {
      "epoch": 2.2789456122789455,
      "grad_norm": 4.1571736335754395,
      "learning_rate": 1.2725238487810623e-05,
      "loss": 0.852,
      "step": 6830
    },
    {
      "epoch": 2.2822822822822824,
      "grad_norm": 3.6578338146209717,
      "learning_rate": 1.2666352608644447e-05,
      "loss": 0.9736,
      "step": 6840
    },
    {
      "epoch": 2.2856189522856187,
      "grad_norm": 2.3299450874328613,
      "learning_rate": 1.260746672947827e-05,
      "loss": 0.7171,
      "step": 6850
    },
    {
      "epoch": 2.2889556222889555,
      "grad_norm": 5.1550750732421875,
      "learning_rate": 1.2548580850312094e-05,
      "loss": 1.0759,
      "step": 6860
    },
    {
      "epoch": 2.2922922922922924,
      "grad_norm": 5.465588569641113,
      "learning_rate": 1.2489694971145919e-05,
      "loss": 0.8083,
      "step": 6870
    },
    {
      "epoch": 2.2956289622956287,
      "grad_norm": 2.9892730712890625,
      "learning_rate": 1.2430809091979744e-05,
      "loss": 1.1139,
      "step": 6880
    },
    {
      "epoch": 2.2989656322989656,
      "grad_norm": 3.491985321044922,
      "learning_rate": 1.2371923212813568e-05,
      "loss": 0.8675,
      "step": 6890
    },
    {
      "epoch": 2.3023023023023024,
      "grad_norm": 3.9404876232147217,
      "learning_rate": 1.2313037333647393e-05,
      "loss": 0.9182,
      "step": 6900
    },
    {
      "epoch": 2.3056389723056387,
      "grad_norm": 5.569154262542725,
      "learning_rate": 1.2254151454481216e-05,
      "loss": 0.8896,
      "step": 6910
    },
    {
      "epoch": 2.3089756423089756,
      "grad_norm": 4.290768623352051,
      "learning_rate": 1.219526557531504e-05,
      "loss": 0.9226,
      "step": 6920
    },
    {
      "epoch": 2.3123123123123124,
      "grad_norm": 4.654914855957031,
      "learning_rate": 1.2136379696148865e-05,
      "loss": 1.0371,
      "step": 6930
    },
    {
      "epoch": 2.3156489823156488,
      "grad_norm": 3.7385847568511963,
      "learning_rate": 1.2077493816982688e-05,
      "loss": 0.7749,
      "step": 6940
    },
    {
      "epoch": 2.3189856523189856,
      "grad_norm": 3.8032267093658447,
      "learning_rate": 1.2018607937816512e-05,
      "loss": 0.7142,
      "step": 6950
    },
    {
      "epoch": 2.3223223223223224,
      "grad_norm": 6.011880874633789,
      "learning_rate": 1.1959722058650335e-05,
      "loss": 0.9153,
      "step": 6960
    },
    {
      "epoch": 2.3256589923256588,
      "grad_norm": 7.633078098297119,
      "learning_rate": 1.190083617948416e-05,
      "loss": 0.9332,
      "step": 6970
    },
    {
      "epoch": 2.3289956623289956,
      "grad_norm": 4.184731960296631,
      "learning_rate": 1.1841950300317984e-05,
      "loss": 0.8337,
      "step": 6980
    },
    {
      "epoch": 2.3323323323323324,
      "grad_norm": 5.877026081085205,
      "learning_rate": 1.1783064421151809e-05,
      "loss": 0.8332,
      "step": 6990
    },
    {
      "epoch": 2.3356690023356688,
      "grad_norm": 3.144505023956299,
      "learning_rate": 1.1724178541985632e-05,
      "loss": 0.7888,
      "step": 7000
    },
    {
      "epoch": 2.3390056723390056,
      "grad_norm": 3.2245595455169678,
      "learning_rate": 1.1665292662819456e-05,
      "loss": 0.7172,
      "step": 7010
    },
    {
      "epoch": 2.3423423423423424,
      "grad_norm": 5.3101677894592285,
      "learning_rate": 1.160640678365328e-05,
      "loss": 1.0201,
      "step": 7020
    },
    {
      "epoch": 2.3456790123456788,
      "grad_norm": 6.780621528625488,
      "learning_rate": 1.1547520904487105e-05,
      "loss": 1.036,
      "step": 7030
    },
    {
      "epoch": 2.3490156823490156,
      "grad_norm": 4.850260257720947,
      "learning_rate": 1.148863502532093e-05,
      "loss": 1.0246,
      "step": 7040
    },
    {
      "epoch": 2.3523523523523524,
      "grad_norm": 5.511544704437256,
      "learning_rate": 1.1429749146154753e-05,
      "loss": 1.1127,
      "step": 7050
    },
    {
      "epoch": 2.3556890223556888,
      "grad_norm": 4.91419792175293,
      "learning_rate": 1.1370863266988577e-05,
      "loss": 0.8624,
      "step": 7060
    },
    {
      "epoch": 2.3590256923590256,
      "grad_norm": 5.264565944671631,
      "learning_rate": 1.13119773878224e-05,
      "loss": 0.73,
      "step": 7070
    },
    {
      "epoch": 2.3623623623623624,
      "grad_norm": 6.102673530578613,
      "learning_rate": 1.1253091508656225e-05,
      "loss": 0.9634,
      "step": 7080
    },
    {
      "epoch": 2.365699032365699,
      "grad_norm": 5.434073448181152,
      "learning_rate": 1.1194205629490049e-05,
      "loss": 0.9055,
      "step": 7090
    },
    {
      "epoch": 2.3690357023690356,
      "grad_norm": 5.1500396728515625,
      "learning_rate": 1.1135319750323874e-05,
      "loss": 1.037,
      "step": 7100
    },
    {
      "epoch": 2.3723723723723724,
      "grad_norm": 6.597478866577148,
      "learning_rate": 1.1076433871157697e-05,
      "loss": 0.8468,
      "step": 7110
    },
    {
      "epoch": 2.3757090423757092,
      "grad_norm": 5.251009941101074,
      "learning_rate": 1.101754799199152e-05,
      "loss": 0.6811,
      "step": 7120
    },
    {
      "epoch": 2.3790457123790456,
      "grad_norm": 3.839900255203247,
      "learning_rate": 1.0958662112825344e-05,
      "loss": 0.7711,
      "step": 7130
    },
    {
      "epoch": 2.3823823823823824,
      "grad_norm": 4.943689346313477,
      "learning_rate": 1.089977623365917e-05,
      "loss": 1.0815,
      "step": 7140
    },
    {
      "epoch": 2.3857190523857192,
      "grad_norm": 5.846051216125488,
      "learning_rate": 1.0840890354492993e-05,
      "loss": 0.9529,
      "step": 7150
    },
    {
      "epoch": 2.3890557223890556,
      "grad_norm": 4.799659729003906,
      "learning_rate": 1.0782004475326816e-05,
      "loss": 0.992,
      "step": 7160
    },
    {
      "epoch": 2.3923923923923924,
      "grad_norm": 6.459100723266602,
      "learning_rate": 1.0723118596160641e-05,
      "loss": 1.1002,
      "step": 7170
    },
    {
      "epoch": 2.3957290623957292,
      "grad_norm": 6.696314811706543,
      "learning_rate": 1.0664232716994465e-05,
      "loss": 1.0743,
      "step": 7180
    },
    {
      "epoch": 2.3990657323990656,
      "grad_norm": 6.025881767272949,
      "learning_rate": 1.060534683782829e-05,
      "loss": 1.0286,
      "step": 7190
    },
    {
      "epoch": 2.4024024024024024,
      "grad_norm": 4.190756320953369,
      "learning_rate": 1.0546460958662114e-05,
      "loss": 0.8952,
      "step": 7200
    },
    {
      "epoch": 2.4057390724057393,
      "grad_norm": 4.588676929473877,
      "learning_rate": 1.0487575079495937e-05,
      "loss": 0.9472,
      "step": 7210
    },
    {
      "epoch": 2.4090757424090756,
      "grad_norm": 3.0968663692474365,
      "learning_rate": 1.042868920032976e-05,
      "loss": 0.7258,
      "step": 7220
    },
    {
      "epoch": 2.4124124124124124,
      "grad_norm": 4.935168743133545,
      "learning_rate": 1.0369803321163586e-05,
      "loss": 0.792,
      "step": 7230
    },
    {
      "epoch": 2.4157490824157493,
      "grad_norm": 4.85584020614624,
      "learning_rate": 1.031091744199741e-05,
      "loss": 0.9623,
      "step": 7240
    },
    {
      "epoch": 2.4190857524190856,
      "grad_norm": 4.033323764801025,
      "learning_rate": 1.0252031562831234e-05,
      "loss": 0.7853,
      "step": 7250
    },
    {
      "epoch": 2.4224224224224224,
      "grad_norm": 7.475490570068359,
      "learning_rate": 1.0193145683665058e-05,
      "loss": 1.0677,
      "step": 7260
    },
    {
      "epoch": 2.4257590924257593,
      "grad_norm": 5.315418720245361,
      "learning_rate": 1.0134259804498881e-05,
      "loss": 0.8264,
      "step": 7270
    },
    {
      "epoch": 2.4290957624290956,
      "grad_norm": 7.0898661613464355,
      "learning_rate": 1.0075373925332705e-05,
      "loss": 0.8238,
      "step": 7280
    },
    {
      "epoch": 2.4324324324324325,
      "grad_norm": 4.784377098083496,
      "learning_rate": 1.001648804616653e-05,
      "loss": 0.6752,
      "step": 7290
    },
    {
      "epoch": 2.4357691024357693,
      "grad_norm": 4.771017074584961,
      "learning_rate": 9.957602167000355e-06,
      "loss": 0.8437,
      "step": 7300
    },
    {
      "epoch": 2.4391057724391056,
      "grad_norm": 5.565734386444092,
      "learning_rate": 9.898716287834178e-06,
      "loss": 0.9153,
      "step": 7310
    },
    {
      "epoch": 2.4424424424424425,
      "grad_norm": 3.689666271209717,
      "learning_rate": 9.839830408668002e-06,
      "loss": 0.9517,
      "step": 7320
    },
    {
      "epoch": 2.4457791124457793,
      "grad_norm": 5.102993011474609,
      "learning_rate": 9.780944529501825e-06,
      "loss": 0.9132,
      "step": 7330
    },
    {
      "epoch": 2.4491157824491157,
      "grad_norm": 4.1635236740112305,
      "learning_rate": 9.72205865033565e-06,
      "loss": 0.8839,
      "step": 7340
    },
    {
      "epoch": 2.4524524524524525,
      "grad_norm": 4.58434534072876,
      "learning_rate": 9.663172771169474e-06,
      "loss": 0.8725,
      "step": 7350
    },
    {
      "epoch": 2.4557891224557893,
      "grad_norm": 4.456936359405518,
      "learning_rate": 9.604286892003297e-06,
      "loss": 0.8981,
      "step": 7360
    },
    {
      "epoch": 2.4591257924591257,
      "grad_norm": 6.6568732261657715,
      "learning_rate": 9.545401012837123e-06,
      "loss": 0.6132,
      "step": 7370
    },
    {
      "epoch": 2.4624624624624625,
      "grad_norm": 5.908835411071777,
      "learning_rate": 9.486515133670946e-06,
      "loss": 1.1111,
      "step": 7380
    },
    {
      "epoch": 2.4657991324657993,
      "grad_norm": 6.092319011688232,
      "learning_rate": 9.42762925450477e-06,
      "loss": 0.8553,
      "step": 7390
    },
    {
      "epoch": 2.4691358024691357,
      "grad_norm": 4.67083215713501,
      "learning_rate": 9.368743375338595e-06,
      "loss": 0.887,
      "step": 7400
    },
    {
      "epoch": 2.4724724724724725,
      "grad_norm": 5.005824565887451,
      "learning_rate": 9.309857496172418e-06,
      "loss": 0.8689,
      "step": 7410
    },
    {
      "epoch": 2.4758091424758093,
      "grad_norm": 5.243635654449463,
      "learning_rate": 9.250971617006242e-06,
      "loss": 0.9096,
      "step": 7420
    },
    {
      "epoch": 2.4791458124791457,
      "grad_norm": 3.7165770530700684,
      "learning_rate": 9.192085737840065e-06,
      "loss": 0.8971,
      "step": 7430
    },
    {
      "epoch": 2.4824824824824825,
      "grad_norm": 5.413182258605957,
      "learning_rate": 9.13319985867389e-06,
      "loss": 0.8735,
      "step": 7440
    },
    {
      "epoch": 2.4858191524858193,
      "grad_norm": 4.590153694152832,
      "learning_rate": 9.074313979507715e-06,
      "loss": 0.8357,
      "step": 7450
    },
    {
      "epoch": 2.4891558224891557,
      "grad_norm": 3.093687057495117,
      "learning_rate": 9.015428100341539e-06,
      "loss": 0.9671,
      "step": 7460
    },
    {
      "epoch": 2.4924924924924925,
      "grad_norm": 5.895611763000488,
      "learning_rate": 8.956542221175362e-06,
      "loss": 0.8645,
      "step": 7470
    },
    {
      "epoch": 2.4958291624958293,
      "grad_norm": 4.741138935089111,
      "learning_rate": 8.897656342009186e-06,
      "loss": 0.8929,
      "step": 7480
    },
    {
      "epoch": 2.4991658324991657,
      "grad_norm": 6.363923072814941,
      "learning_rate": 8.83877046284301e-06,
      "loss": 0.8886,
      "step": 7490
    },
    {
      "epoch": 2.5025025025025025,
      "grad_norm": 5.492615222930908,
      "learning_rate": 8.779884583676836e-06,
      "loss": 0.9779,
      "step": 7500
    },
    {
      "epoch": 2.5058391725058393,
      "grad_norm": 3.994258165359497,
      "learning_rate": 8.72099870451066e-06,
      "loss": 0.8575,
      "step": 7510
    },
    {
      "epoch": 2.5091758425091757,
      "grad_norm": 5.281205177307129,
      "learning_rate": 8.662112825344483e-06,
      "loss": 0.7995,
      "step": 7520
    },
    {
      "epoch": 2.5125125125125125,
      "grad_norm": 5.9471211433410645,
      "learning_rate": 8.603226946178307e-06,
      "loss": 0.8616,
      "step": 7530
    },
    {
      "epoch": 2.5158491825158493,
      "grad_norm": 7.096257209777832,
      "learning_rate": 8.54434106701213e-06,
      "loss": 0.8518,
      "step": 7540
    },
    {
      "epoch": 2.5191858525191857,
      "grad_norm": 6.527785301208496,
      "learning_rate": 8.485455187845955e-06,
      "loss": 0.8529,
      "step": 7550
    },
    {
      "epoch": 2.5225225225225225,
      "grad_norm": 5.434747219085693,
      "learning_rate": 8.426569308679779e-06,
      "loss": 0.7316,
      "step": 7560
    },
    {
      "epoch": 2.5258591925258593,
      "grad_norm": 5.88389253616333,
      "learning_rate": 8.367683429513604e-06,
      "loss": 0.8363,
      "step": 7570
    },
    {
      "epoch": 2.5291958625291957,
      "grad_norm": 5.025155544281006,
      "learning_rate": 8.308797550347427e-06,
      "loss": 0.9083,
      "step": 7580
    },
    {
      "epoch": 2.5325325325325325,
      "grad_norm": 6.36037015914917,
      "learning_rate": 8.24991167118125e-06,
      "loss": 1.0323,
      "step": 7590
    },
    {
      "epoch": 2.5358692025358693,
      "grad_norm": 4.042445182800293,
      "learning_rate": 8.191025792015076e-06,
      "loss": 0.8784,
      "step": 7600
    },
    {
      "epoch": 2.5392058725392057,
      "grad_norm": 4.943482875823975,
      "learning_rate": 8.1321399128489e-06,
      "loss": 0.8139,
      "step": 7610
    },
    {
      "epoch": 2.5425425425425425,
      "grad_norm": 3.5856266021728516,
      "learning_rate": 8.073254033682723e-06,
      "loss": 0.827,
      "step": 7620
    },
    {
      "epoch": 2.5458792125458793,
      "grad_norm": 5.161612033843994,
      "learning_rate": 8.014368154516546e-06,
      "loss": 1.0817,
      "step": 7630
    },
    {
      "epoch": 2.5492158825492157,
      "grad_norm": 5.344567775726318,
      "learning_rate": 7.955482275350371e-06,
      "loss": 0.8824,
      "step": 7640
    },
    {
      "epoch": 2.5525525525525525,
      "grad_norm": 4.497580051422119,
      "learning_rate": 7.896596396184195e-06,
      "loss": 0.7418,
      "step": 7650
    },
    {
      "epoch": 2.5558892225558894,
      "grad_norm": 3.7652747631073,
      "learning_rate": 7.83771051701802e-06,
      "loss": 0.7591,
      "step": 7660
    },
    {
      "epoch": 2.559225892559226,
      "grad_norm": 5.043215751647949,
      "learning_rate": 7.778824637851844e-06,
      "loss": 0.9683,
      "step": 7670
    },
    {
      "epoch": 2.5625625625625625,
      "grad_norm": 5.303695201873779,
      "learning_rate": 7.719938758685667e-06,
      "loss": 0.8861,
      "step": 7680
    },
    {
      "epoch": 2.5658992325658994,
      "grad_norm": 3.729618787765503,
      "learning_rate": 7.66105287951949e-06,
      "loss": 0.7813,
      "step": 7690
    },
    {
      "epoch": 2.569235902569236,
      "grad_norm": 4.301995277404785,
      "learning_rate": 7.6021670003533165e-06,
      "loss": 0.8809,
      "step": 7700
    },
    {
      "epoch": 2.5725725725725725,
      "grad_norm": 6.180739879608154,
      "learning_rate": 7.54328112118714e-06,
      "loss": 0.8408,
      "step": 7710
    },
    {
      "epoch": 2.5759092425759094,
      "grad_norm": 3.2503840923309326,
      "learning_rate": 7.484395242020964e-06,
      "loss": 0.7023,
      "step": 7720
    },
    {
      "epoch": 2.579245912579246,
      "grad_norm": 3.6919443607330322,
      "learning_rate": 7.425509362854788e-06,
      "loss": 0.8176,
      "step": 7730
    },
    {
      "epoch": 2.5825825825825826,
      "grad_norm": 4.421922206878662,
      "learning_rate": 7.366623483688612e-06,
      "loss": 0.7729,
      "step": 7740
    },
    {
      "epoch": 2.5859192525859194,
      "grad_norm": 4.308811664581299,
      "learning_rate": 7.3077376045224355e-06,
      "loss": 0.7402,
      "step": 7750
    },
    {
      "epoch": 2.589255922589256,
      "grad_norm": 3.277337074279785,
      "learning_rate": 7.248851725356259e-06,
      "loss": 0.7009,
      "step": 7760
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 3.3448739051818848,
      "learning_rate": 7.189965846190085e-06,
      "loss": 0.8275,
      "step": 7770
    },
    {
      "epoch": 2.5959292625959294,
      "grad_norm": 6.862736225128174,
      "learning_rate": 7.1310799670239084e-06,
      "loss": 0.8012,
      "step": 7780
    },
    {
      "epoch": 2.599265932599266,
      "grad_norm": 6.93810510635376,
      "learning_rate": 7.072194087857732e-06,
      "loss": 0.9493,
      "step": 7790
    },
    {
      "epoch": 2.6026026026026026,
      "grad_norm": 4.198426723480225,
      "learning_rate": 7.013308208691556e-06,
      "loss": 0.9297,
      "step": 7800
    },
    {
      "epoch": 2.6059392726059394,
      "grad_norm": 5.9219069480896,
      "learning_rate": 6.95442232952538e-06,
      "loss": 0.7271,
      "step": 7810
    },
    {
      "epoch": 2.609275942609276,
      "grad_norm": 5.787832260131836,
      "learning_rate": 6.895536450359204e-06,
      "loss": 1.1045,
      "step": 7820
    },
    {
      "epoch": 2.6126126126126126,
      "grad_norm": 4.537323951721191,
      "learning_rate": 6.8366505711930274e-06,
      "loss": 0.6826,
      "step": 7830
    },
    {
      "epoch": 2.6159492826159494,
      "grad_norm": 4.80109977722168,
      "learning_rate": 6.777764692026853e-06,
      "loss": 0.9697,
      "step": 7840
    },
    {
      "epoch": 2.619285952619286,
      "grad_norm": 6.26077127456665,
      "learning_rate": 6.718878812860677e-06,
      "loss": 0.8407,
      "step": 7850
    },
    {
      "epoch": 2.6226226226226226,
      "grad_norm": 5.495997428894043,
      "learning_rate": 6.6599929336945e-06,
      "loss": 0.8734,
      "step": 7860
    },
    {
      "epoch": 2.6259592926259594,
      "grad_norm": 6.640900135040283,
      "learning_rate": 6.601107054528325e-06,
      "loss": 0.8211,
      "step": 7870
    },
    {
      "epoch": 2.629295962629296,
      "grad_norm": 6.219351768493652,
      "learning_rate": 6.542221175362148e-06,
      "loss": 0.9124,
      "step": 7880
    },
    {
      "epoch": 2.6326326326326326,
      "grad_norm": 6.820228576660156,
      "learning_rate": 6.483335296195972e-06,
      "loss": 0.979,
      "step": 7890
    },
    {
      "epoch": 2.6359693026359694,
      "grad_norm": 4.173318862915039,
      "learning_rate": 6.424449417029798e-06,
      "loss": 0.8532,
      "step": 7900
    },
    {
      "epoch": 2.639305972639306,
      "grad_norm": 5.032829284667969,
      "learning_rate": 6.365563537863621e-06,
      "loss": 0.9507,
      "step": 7910
    },
    {
      "epoch": 2.6426426426426426,
      "grad_norm": 6.598764896392822,
      "learning_rate": 6.3066776586974446e-06,
      "loss": 0.9272,
      "step": 7920
    },
    {
      "epoch": 2.6459793126459794,
      "grad_norm": 3.6713969707489014,
      "learning_rate": 6.247791779531269e-06,
      "loss": 0.8568,
      "step": 7930
    },
    {
      "epoch": 2.6493159826493162,
      "grad_norm": 7.976811408996582,
      "learning_rate": 6.188905900365092e-06,
      "loss": 0.9468,
      "step": 7940
    },
    {
      "epoch": 2.6526526526526526,
      "grad_norm": 5.183959007263184,
      "learning_rate": 6.1300200211989175e-06,
      "loss": 0.8758,
      "step": 7950
    },
    {
      "epoch": 2.6559893226559894,
      "grad_norm": 6.63443660736084,
      "learning_rate": 6.071134142032741e-06,
      "loss": 0.7353,
      "step": 7960
    },
    {
      "epoch": 2.6593259926593262,
      "grad_norm": 3.557628870010376,
      "learning_rate": 6.0122482628665644e-06,
      "loss": 1.1285,
      "step": 7970
    },
    {
      "epoch": 2.6626626626626626,
      "grad_norm": 4.934995174407959,
      "learning_rate": 5.953362383700389e-06,
      "loss": 0.912,
      "step": 7980
    },
    {
      "epoch": 2.6659993326659994,
      "grad_norm": 3.6205852031707764,
      "learning_rate": 5.894476504534213e-06,
      "loss": 0.9593,
      "step": 7990
    },
    {
      "epoch": 2.6693360026693362,
      "grad_norm": 5.165246963500977,
      "learning_rate": 5.835590625368037e-06,
      "loss": 0.7048,
      "step": 8000
    },
    {
      "epoch": 2.6726726726726726,
      "grad_norm": 4.787269115447998,
      "learning_rate": 5.776704746201861e-06,
      "loss": 0.8668,
      "step": 8010
    },
    {
      "epoch": 2.6760093426760094,
      "grad_norm": 3.47517466545105,
      "learning_rate": 5.717818867035685e-06,
      "loss": 0.6857,
      "step": 8020
    },
    {
      "epoch": 2.6793460126793462,
      "grad_norm": 7.197416305541992,
      "learning_rate": 5.6589329878695094e-06,
      "loss": 0.837,
      "step": 8030
    },
    {
      "epoch": 2.6826826826826826,
      "grad_norm": 5.638045787811279,
      "learning_rate": 5.600047108703333e-06,
      "loss": 0.924,
      "step": 8040
    },
    {
      "epoch": 2.6860193526860194,
      "grad_norm": 4.551172256469727,
      "learning_rate": 5.541161229537157e-06,
      "loss": 0.8456,
      "step": 8050
    },
    {
      "epoch": 2.6893560226893563,
      "grad_norm": 2.4025495052337646,
      "learning_rate": 5.4822753503709815e-06,
      "loss": 0.6699,
      "step": 8060
    },
    {
      "epoch": 2.6926926926926926,
      "grad_norm": 5.2597784996032715,
      "learning_rate": 5.423389471204805e-06,
      "loss": 0.9482,
      "step": 8070
    },
    {
      "epoch": 2.6960293626960294,
      "grad_norm": 7.612503528594971,
      "learning_rate": 5.364503592038629e-06,
      "loss": 0.9134,
      "step": 8080
    },
    {
      "epoch": 2.6993660326993663,
      "grad_norm": 5.904001235961914,
      "learning_rate": 5.305617712872454e-06,
      "loss": 0.7613,
      "step": 8090
    },
    {
      "epoch": 2.7027027027027026,
      "grad_norm": 5.868046283721924,
      "learning_rate": 5.246731833706277e-06,
      "loss": 0.913,
      "step": 8100
    }
  ],
  "logging_steps": 10,
  "max_steps": 8991,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 900,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.4211548414365696e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
