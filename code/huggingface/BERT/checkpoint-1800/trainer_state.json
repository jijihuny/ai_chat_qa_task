{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.6006006006006006,
  "eval_steps": 500,
  "global_step": 1800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00333667000333667,
      "grad_norm": 8.857905387878418,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 4.4678,
      "step": 10
    },
    {
      "epoch": 0.00667334000667334,
      "grad_norm": 8.59739875793457,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 3.9352,
      "step": 20
    },
    {
      "epoch": 0.01001001001001001,
      "grad_norm": 8.053893089294434,
      "learning_rate": 3e-06,
      "loss": 3.9121,
      "step": 30
    },
    {
      "epoch": 0.01334668001334668,
      "grad_norm": 7.265538215637207,
      "learning_rate": 4.000000000000001e-06,
      "loss": 3.3505,
      "step": 40
    },
    {
      "epoch": 0.01668335001668335,
      "grad_norm": 8.49083137512207,
      "learning_rate": 5e-06,
      "loss": 3.7749,
      "step": 50
    },
    {
      "epoch": 0.02002002002002002,
      "grad_norm": 11.937213897705078,
      "learning_rate": 6e-06,
      "loss": 4.3121,
      "step": 60
    },
    {
      "epoch": 0.02335669002335669,
      "grad_norm": 6.517889976501465,
      "learning_rate": 7.000000000000001e-06,
      "loss": 3.7168,
      "step": 70
    },
    {
      "epoch": 0.02669336002669336,
      "grad_norm": 7.611666202545166,
      "learning_rate": 8.000000000000001e-06,
      "loss": 3.3343,
      "step": 80
    },
    {
      "epoch": 0.03003003003003003,
      "grad_norm": 11.604483604431152,
      "learning_rate": 9e-06,
      "loss": 3.3564,
      "step": 90
    },
    {
      "epoch": 0.0333667000333667,
      "grad_norm": 5.543286323547363,
      "learning_rate": 1e-05,
      "loss": 2.8424,
      "step": 100
    },
    {
      "epoch": 0.03670337003670337,
      "grad_norm": 9.923837661743164,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 3.0617,
      "step": 110
    },
    {
      "epoch": 0.04004004004004004,
      "grad_norm": 8.410821914672852,
      "learning_rate": 1.2e-05,
      "loss": 3.654,
      "step": 120
    },
    {
      "epoch": 0.04337671004337671,
      "grad_norm": 7.855680465698242,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 3.8245,
      "step": 130
    },
    {
      "epoch": 0.04671338004671338,
      "grad_norm": 7.0160417556762695,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 3.0403,
      "step": 140
    },
    {
      "epoch": 0.05005005005005005,
      "grad_norm": 8.539226531982422,
      "learning_rate": 1.5e-05,
      "loss": 4.0287,
      "step": 150
    },
    {
      "epoch": 0.05338672005338672,
      "grad_norm": 7.546672821044922,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 4.2148,
      "step": 160
    },
    {
      "epoch": 0.05672339005672339,
      "grad_norm": 9.074827194213867,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 3.3234,
      "step": 170
    },
    {
      "epoch": 0.06006006006006006,
      "grad_norm": 8.012894630432129,
      "learning_rate": 1.8e-05,
      "loss": 3.4108,
      "step": 180
    },
    {
      "epoch": 0.06339673006339673,
      "grad_norm": 7.305278301239014,
      "learning_rate": 1.9e-05,
      "loss": 3.8104,
      "step": 190
    },
    {
      "epoch": 0.0667334000667334,
      "grad_norm": 9.536160469055176,
      "learning_rate": 2e-05,
      "loss": 2.6948,
      "step": 200
    },
    {
      "epoch": 0.07007007007007007,
      "grad_norm": 7.950663089752197,
      "learning_rate": 2.1e-05,
      "loss": 3.3356,
      "step": 210
    },
    {
      "epoch": 0.07340674007340674,
      "grad_norm": 7.788869380950928,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 2.2596,
      "step": 220
    },
    {
      "epoch": 0.07674341007674342,
      "grad_norm": 7.822196006774902,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 2.5057,
      "step": 230
    },
    {
      "epoch": 0.08008008008008008,
      "grad_norm": 7.548488140106201,
      "learning_rate": 2.4e-05,
      "loss": 2.6202,
      "step": 240
    },
    {
      "epoch": 0.08341675008341674,
      "grad_norm": 6.796125888824463,
      "learning_rate": 2.5e-05,
      "loss": 1.9175,
      "step": 250
    },
    {
      "epoch": 0.08675342008675342,
      "grad_norm": 7.555082321166992,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.7628,
      "step": 260
    },
    {
      "epoch": 0.09009009009009009,
      "grad_norm": 6.521301746368408,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 2.0625,
      "step": 270
    },
    {
      "epoch": 0.09342676009342676,
      "grad_norm": 5.06267786026001,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.8207,
      "step": 280
    },
    {
      "epoch": 0.09676343009676343,
      "grad_norm": 6.200467586517334,
      "learning_rate": 2.9e-05,
      "loss": 1.9648,
      "step": 290
    },
    {
      "epoch": 0.1001001001001001,
      "grad_norm": 5.347590446472168,
      "learning_rate": 3e-05,
      "loss": 1.5949,
      "step": 300
    },
    {
      "epoch": 0.10343677010343677,
      "grad_norm": 2.64927077293396,
      "learning_rate": 3.1e-05,
      "loss": 1.5226,
      "step": 310
    },
    {
      "epoch": 0.10677344010677343,
      "grad_norm": 3.8787083625793457,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.8517,
      "step": 320
    },
    {
      "epoch": 0.11011011011011011,
      "grad_norm": 5.351945400238037,
      "learning_rate": 3.3e-05,
      "loss": 1.9147,
      "step": 330
    },
    {
      "epoch": 0.11344678011344678,
      "grad_norm": 4.14184045791626,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.732,
      "step": 340
    },
    {
      "epoch": 0.11678345011678345,
      "grad_norm": 5.556326389312744,
      "learning_rate": 3.5e-05,
      "loss": 1.5702,
      "step": 350
    },
    {
      "epoch": 0.12012012012012012,
      "grad_norm": 4.808996200561523,
      "learning_rate": 3.6e-05,
      "loss": 1.4381,
      "step": 360
    },
    {
      "epoch": 0.12345679012345678,
      "grad_norm": 5.18758487701416,
      "learning_rate": 3.7e-05,
      "loss": 1.6771,
      "step": 370
    },
    {
      "epoch": 0.12679346012679346,
      "grad_norm": 4.98621940612793,
      "learning_rate": 3.8e-05,
      "loss": 1.6097,
      "step": 380
    },
    {
      "epoch": 0.13013013013013014,
      "grad_norm": 4.125382423400879,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.383,
      "step": 390
    },
    {
      "epoch": 0.1334668001334668,
      "grad_norm": 4.260393142700195,
      "learning_rate": 4e-05,
      "loss": 1.4135,
      "step": 400
    },
    {
      "epoch": 0.13680347013680347,
      "grad_norm": 4.048669338226318,
      "learning_rate": 4.1e-05,
      "loss": 1.5159,
      "step": 410
    },
    {
      "epoch": 0.14014014014014015,
      "grad_norm": 4.261200428009033,
      "learning_rate": 4.2e-05,
      "loss": 1.3645,
      "step": 420
    },
    {
      "epoch": 0.14347681014347682,
      "grad_norm": 4.28817892074585,
      "learning_rate": 4.3e-05,
      "loss": 1.5483,
      "step": 430
    },
    {
      "epoch": 0.14681348014681347,
      "grad_norm": 3.5364677906036377,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.4444,
      "step": 440
    },
    {
      "epoch": 0.15015015015015015,
      "grad_norm": 3.8340559005737305,
      "learning_rate": 4.5e-05,
      "loss": 1.2625,
      "step": 450
    },
    {
      "epoch": 0.15348682015348683,
      "grad_norm": 5.591009616851807,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.3474,
      "step": 460
    },
    {
      "epoch": 0.15682349015682348,
      "grad_norm": 4.024402141571045,
      "learning_rate": 4.7e-05,
      "loss": 1.1617,
      "step": 470
    },
    {
      "epoch": 0.16016016016016016,
      "grad_norm": 5.415898323059082,
      "learning_rate": 4.8e-05,
      "loss": 1.7149,
      "step": 480
    },
    {
      "epoch": 0.16349683016349684,
      "grad_norm": 4.3966264724731445,
      "learning_rate": 4.9e-05,
      "loss": 1.3024,
      "step": 490
    },
    {
      "epoch": 0.1668335001668335,
      "grad_norm": 4.0056047439575195,
      "learning_rate": 5e-05,
      "loss": 1.5646,
      "step": 500
    },
    {
      "epoch": 0.17017017017017017,
      "grad_norm": 4.605757236480713,
      "learning_rate": 4.9941114120833824e-05,
      "loss": 1.4762,
      "step": 510
    },
    {
      "epoch": 0.17350684017350684,
      "grad_norm": 2.5374536514282227,
      "learning_rate": 4.988222824166765e-05,
      "loss": 1.1877,
      "step": 520
    },
    {
      "epoch": 0.17684351017684352,
      "grad_norm": 3.0233702659606934,
      "learning_rate": 4.9823342362501474e-05,
      "loss": 1.2266,
      "step": 530
    },
    {
      "epoch": 0.18018018018018017,
      "grad_norm": 4.547967433929443,
      "learning_rate": 4.9764456483335296e-05,
      "loss": 1.3976,
      "step": 540
    },
    {
      "epoch": 0.18351685018351685,
      "grad_norm": 3.5690975189208984,
      "learning_rate": 4.9705570604169125e-05,
      "loss": 1.387,
      "step": 550
    },
    {
      "epoch": 0.18685352018685353,
      "grad_norm": 6.602469444274902,
      "learning_rate": 4.9646684725002947e-05,
      "loss": 1.3689,
      "step": 560
    },
    {
      "epoch": 0.19019019019019018,
      "grad_norm": 6.749355316162109,
      "learning_rate": 4.958779884583677e-05,
      "loss": 1.2421,
      "step": 570
    },
    {
      "epoch": 0.19352686019352686,
      "grad_norm": 2.9303011894226074,
      "learning_rate": 4.952891296667059e-05,
      "loss": 1.0158,
      "step": 580
    },
    {
      "epoch": 0.19686353019686353,
      "grad_norm": 5.631228446960449,
      "learning_rate": 4.947002708750442e-05,
      "loss": 1.295,
      "step": 590
    },
    {
      "epoch": 0.2002002002002002,
      "grad_norm": 4.6857991218566895,
      "learning_rate": 4.941114120833824e-05,
      "loss": 1.3221,
      "step": 600
    },
    {
      "epoch": 0.20353687020353686,
      "grad_norm": 4.097028732299805,
      "learning_rate": 4.935225532917206e-05,
      "loss": 1.3916,
      "step": 610
    },
    {
      "epoch": 0.20687354020687354,
      "grad_norm": 3.8089182376861572,
      "learning_rate": 4.929336945000589e-05,
      "loss": 1.2672,
      "step": 620
    },
    {
      "epoch": 0.21021021021021022,
      "grad_norm": 3.4777469635009766,
      "learning_rate": 4.923448357083971e-05,
      "loss": 1.1686,
      "step": 630
    },
    {
      "epoch": 0.21354688021354687,
      "grad_norm": 6.217365741729736,
      "learning_rate": 4.917559769167354e-05,
      "loss": 1.3046,
      "step": 640
    },
    {
      "epoch": 0.21688355021688355,
      "grad_norm": 2.9496960639953613,
      "learning_rate": 4.911671181250736e-05,
      "loss": 1.1695,
      "step": 650
    },
    {
      "epoch": 0.22022022022022023,
      "grad_norm": 3.933528423309326,
      "learning_rate": 4.905782593334119e-05,
      "loss": 1.2356,
      "step": 660
    },
    {
      "epoch": 0.2235568902235569,
      "grad_norm": 3.9220006465911865,
      "learning_rate": 4.899894005417501e-05,
      "loss": 1.1246,
      "step": 670
    },
    {
      "epoch": 0.22689356022689355,
      "grad_norm": 7.431880474090576,
      "learning_rate": 4.8940054175008835e-05,
      "loss": 1.0677,
      "step": 680
    },
    {
      "epoch": 0.23023023023023023,
      "grad_norm": 4.475502014160156,
      "learning_rate": 4.8881168295842663e-05,
      "loss": 1.3185,
      "step": 690
    },
    {
      "epoch": 0.2335669002335669,
      "grad_norm": 5.457873344421387,
      "learning_rate": 4.8822282416676485e-05,
      "loss": 1.2404,
      "step": 700
    },
    {
      "epoch": 0.23690357023690356,
      "grad_norm": 5.106561183929443,
      "learning_rate": 4.876339653751031e-05,
      "loss": 1.4241,
      "step": 710
    },
    {
      "epoch": 0.24024024024024024,
      "grad_norm": 6.592267036437988,
      "learning_rate": 4.8704510658344136e-05,
      "loss": 1.366,
      "step": 720
    },
    {
      "epoch": 0.24357691024357692,
      "grad_norm": 3.60618257522583,
      "learning_rate": 4.864562477917796e-05,
      "loss": 1.1119,
      "step": 730
    },
    {
      "epoch": 0.24691358024691357,
      "grad_norm": 3.0848686695098877,
      "learning_rate": 4.858673890001178e-05,
      "loss": 1.0512,
      "step": 740
    },
    {
      "epoch": 0.2502502502502503,
      "grad_norm": 4.673299789428711,
      "learning_rate": 4.85278530208456e-05,
      "loss": 1.2696,
      "step": 750
    },
    {
      "epoch": 0.2535869202535869,
      "grad_norm": 4.895904064178467,
      "learning_rate": 4.846896714167943e-05,
      "loss": 1.1805,
      "step": 760
    },
    {
      "epoch": 0.2569235902569236,
      "grad_norm": 4.752594947814941,
      "learning_rate": 4.841008126251325e-05,
      "loss": 1.1475,
      "step": 770
    },
    {
      "epoch": 0.2602602602602603,
      "grad_norm": 4.059188365936279,
      "learning_rate": 4.835119538334707e-05,
      "loss": 1.2825,
      "step": 780
    },
    {
      "epoch": 0.26359693026359693,
      "grad_norm": 3.561336040496826,
      "learning_rate": 4.82923095041809e-05,
      "loss": 1.1613,
      "step": 790
    },
    {
      "epoch": 0.2669336002669336,
      "grad_norm": 4.691658973693848,
      "learning_rate": 4.823342362501472e-05,
      "loss": 1.5574,
      "step": 800
    },
    {
      "epoch": 0.2702702702702703,
      "grad_norm": 3.560206413269043,
      "learning_rate": 4.8174537745848545e-05,
      "loss": 1.212,
      "step": 810
    },
    {
      "epoch": 0.27360694027360694,
      "grad_norm": 4.498392581939697,
      "learning_rate": 4.8115651866682374e-05,
      "loss": 1.0687,
      "step": 820
    },
    {
      "epoch": 0.2769436102769436,
      "grad_norm": 3.535398483276367,
      "learning_rate": 4.8056765987516195e-05,
      "loss": 1.2544,
      "step": 830
    },
    {
      "epoch": 0.2802802802802803,
      "grad_norm": 5.058297157287598,
      "learning_rate": 4.799788010835002e-05,
      "loss": 1.4341,
      "step": 840
    },
    {
      "epoch": 0.28361695028361694,
      "grad_norm": 3.874671697616577,
      "learning_rate": 4.7938994229183846e-05,
      "loss": 1.1299,
      "step": 850
    },
    {
      "epoch": 0.28695362028695365,
      "grad_norm": 5.592775821685791,
      "learning_rate": 4.788010835001767e-05,
      "loss": 1.0973,
      "step": 860
    },
    {
      "epoch": 0.2902902902902903,
      "grad_norm": 3.8135714530944824,
      "learning_rate": 4.782122247085149e-05,
      "loss": 1.1867,
      "step": 870
    },
    {
      "epoch": 0.29362696029362695,
      "grad_norm": 5.178786754608154,
      "learning_rate": 4.776233659168531e-05,
      "loss": 1.1778,
      "step": 880
    },
    {
      "epoch": 0.29696363029696365,
      "grad_norm": 3.325193166732788,
      "learning_rate": 4.770345071251914e-05,
      "loss": 1.022,
      "step": 890
    },
    {
      "epoch": 0.3003003003003003,
      "grad_norm": 3.699855089187622,
      "learning_rate": 4.764456483335296e-05,
      "loss": 1.3642,
      "step": 900
    },
    {
      "epoch": 0.30363697030363695,
      "grad_norm": 4.583566665649414,
      "learning_rate": 4.758567895418679e-05,
      "loss": 1.0707,
      "step": 910
    },
    {
      "epoch": 0.30697364030697366,
      "grad_norm": 5.991292476654053,
      "learning_rate": 4.752679307502061e-05,
      "loss": 1.1113,
      "step": 920
    },
    {
      "epoch": 0.3103103103103103,
      "grad_norm": 3.690126419067383,
      "learning_rate": 4.746790719585444e-05,
      "loss": 1.294,
      "step": 930
    },
    {
      "epoch": 0.31364698031364696,
      "grad_norm": 6.788210391998291,
      "learning_rate": 4.740902131668826e-05,
      "loss": 1.0856,
      "step": 940
    },
    {
      "epoch": 0.31698365031698367,
      "grad_norm": 3.347337484359741,
      "learning_rate": 4.7350135437522084e-05,
      "loss": 0.9876,
      "step": 950
    },
    {
      "epoch": 0.3203203203203203,
      "grad_norm": 4.475565433502197,
      "learning_rate": 4.729124955835591e-05,
      "loss": 1.0511,
      "step": 960
    },
    {
      "epoch": 0.32365699032365697,
      "grad_norm": 7.910323143005371,
      "learning_rate": 4.7232363679189734e-05,
      "loss": 1.4289,
      "step": 970
    },
    {
      "epoch": 0.3269936603269937,
      "grad_norm": 3.852656364440918,
      "learning_rate": 4.7173477800023556e-05,
      "loss": 1.1075,
      "step": 980
    },
    {
      "epoch": 0.3303303303303303,
      "grad_norm": 6.85919189453125,
      "learning_rate": 4.7114591920857384e-05,
      "loss": 1.1374,
      "step": 990
    },
    {
      "epoch": 0.333667000333667,
      "grad_norm": 4.76578426361084,
      "learning_rate": 4.7055706041691206e-05,
      "loss": 1.1148,
      "step": 1000
    },
    {
      "epoch": 0.3370036703370037,
      "grad_norm": 6.559088706970215,
      "learning_rate": 4.699682016252503e-05,
      "loss": 1.1183,
      "step": 1010
    },
    {
      "epoch": 0.34034034034034033,
      "grad_norm": 6.129230499267578,
      "learning_rate": 4.6937934283358856e-05,
      "loss": 1.1669,
      "step": 1020
    },
    {
      "epoch": 0.34367701034367704,
      "grad_norm": 4.710047721862793,
      "learning_rate": 4.687904840419268e-05,
      "loss": 1.2316,
      "step": 1030
    },
    {
      "epoch": 0.3470136803470137,
      "grad_norm": 5.12595272064209,
      "learning_rate": 4.68201625250265e-05,
      "loss": 1.1542,
      "step": 1040
    },
    {
      "epoch": 0.35035035035035034,
      "grad_norm": 2.357759952545166,
      "learning_rate": 4.676127664586032e-05,
      "loss": 0.9957,
      "step": 1050
    },
    {
      "epoch": 0.35368702035368704,
      "grad_norm": 3.131687641143799,
      "learning_rate": 4.670239076669415e-05,
      "loss": 1.2648,
      "step": 1060
    },
    {
      "epoch": 0.3570236903570237,
      "grad_norm": 3.833237886428833,
      "learning_rate": 4.664350488752797e-05,
      "loss": 1.0358,
      "step": 1070
    },
    {
      "epoch": 0.36036036036036034,
      "grad_norm": 4.858419895172119,
      "learning_rate": 4.6584619008361794e-05,
      "loss": 1.292,
      "step": 1080
    },
    {
      "epoch": 0.36369703036369705,
      "grad_norm": 4.332526206970215,
      "learning_rate": 4.652573312919562e-05,
      "loss": 1.3125,
      "step": 1090
    },
    {
      "epoch": 0.3670337003670337,
      "grad_norm": 3.6166481971740723,
      "learning_rate": 4.6466847250029444e-05,
      "loss": 1.1565,
      "step": 1100
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 4.405731678009033,
      "learning_rate": 4.6407961370863266e-05,
      "loss": 1.1211,
      "step": 1110
    },
    {
      "epoch": 0.37370704037370706,
      "grad_norm": 6.173271179199219,
      "learning_rate": 4.6349075491697094e-05,
      "loss": 1.0745,
      "step": 1120
    },
    {
      "epoch": 0.3770437103770437,
      "grad_norm": 4.33131217956543,
      "learning_rate": 4.6290189612530916e-05,
      "loss": 1.0042,
      "step": 1130
    },
    {
      "epoch": 0.38038038038038036,
      "grad_norm": 4.473048210144043,
      "learning_rate": 4.623130373336474e-05,
      "loss": 1.2826,
      "step": 1140
    },
    {
      "epoch": 0.38371705038371706,
      "grad_norm": 4.881191253662109,
      "learning_rate": 4.617241785419856e-05,
      "loss": 0.9516,
      "step": 1150
    },
    {
      "epoch": 0.3870537203870537,
      "grad_norm": 5.25804328918457,
      "learning_rate": 4.611353197503239e-05,
      "loss": 0.9502,
      "step": 1160
    },
    {
      "epoch": 0.39039039039039036,
      "grad_norm": 4.6736369132995605,
      "learning_rate": 4.605464609586622e-05,
      "loss": 1.1411,
      "step": 1170
    },
    {
      "epoch": 0.39372706039372707,
      "grad_norm": 5.670393466949463,
      "learning_rate": 4.599576021670004e-05,
      "loss": 1.2091,
      "step": 1180
    },
    {
      "epoch": 0.3970637303970637,
      "grad_norm": 4.273220539093018,
      "learning_rate": 4.593687433753387e-05,
      "loss": 1.3297,
      "step": 1190
    },
    {
      "epoch": 0.4004004004004004,
      "grad_norm": 4.789172649383545,
      "learning_rate": 4.587798845836769e-05,
      "loss": 1.1299,
      "step": 1200
    },
    {
      "epoch": 0.4037370704037371,
      "grad_norm": 4.748783588409424,
      "learning_rate": 4.581910257920151e-05,
      "loss": 1.0546,
      "step": 1210
    },
    {
      "epoch": 0.4070737404070737,
      "grad_norm": 5.400569915771484,
      "learning_rate": 4.576021670003533e-05,
      "loss": 1.4714,
      "step": 1220
    },
    {
      "epoch": 0.41041041041041043,
      "grad_norm": 5.7481369972229,
      "learning_rate": 4.570133082086916e-05,
      "loss": 1.2662,
      "step": 1230
    },
    {
      "epoch": 0.4137470804137471,
      "grad_norm": 3.5832130908966064,
      "learning_rate": 4.564244494170298e-05,
      "loss": 1.1563,
      "step": 1240
    },
    {
      "epoch": 0.41708375041708373,
      "grad_norm": 4.613012313842773,
      "learning_rate": 4.5583559062536805e-05,
      "loss": 1.0004,
      "step": 1250
    },
    {
      "epoch": 0.42042042042042044,
      "grad_norm": 5.867781162261963,
      "learning_rate": 4.552467318337063e-05,
      "loss": 1.0469,
      "step": 1260
    },
    {
      "epoch": 0.4237570904237571,
      "grad_norm": 3.820117712020874,
      "learning_rate": 4.5465787304204455e-05,
      "loss": 1.2976,
      "step": 1270
    },
    {
      "epoch": 0.42709376042709374,
      "grad_norm": 6.1656928062438965,
      "learning_rate": 4.540690142503828e-05,
      "loss": 1.1519,
      "step": 1280
    },
    {
      "epoch": 0.43043043043043044,
      "grad_norm": 6.3360700607299805,
      "learning_rate": 4.5348015545872105e-05,
      "loss": 1.2347,
      "step": 1290
    },
    {
      "epoch": 0.4337671004337671,
      "grad_norm": 5.503486156463623,
      "learning_rate": 4.528912966670593e-05,
      "loss": 1.2732,
      "step": 1300
    },
    {
      "epoch": 0.43710377043710374,
      "grad_norm": 3.3686165809631348,
      "learning_rate": 4.523024378753975e-05,
      "loss": 0.989,
      "step": 1310
    },
    {
      "epoch": 0.44044044044044045,
      "grad_norm": 6.149549961090088,
      "learning_rate": 4.517135790837357e-05,
      "loss": 1.2925,
      "step": 1320
    },
    {
      "epoch": 0.4437771104437771,
      "grad_norm": 5.150977611541748,
      "learning_rate": 4.51124720292074e-05,
      "loss": 0.9846,
      "step": 1330
    },
    {
      "epoch": 0.4471137804471138,
      "grad_norm": 6.125264644622803,
      "learning_rate": 4.505358615004122e-05,
      "loss": 1.0251,
      "step": 1340
    },
    {
      "epoch": 0.45045045045045046,
      "grad_norm": 3.403777837753296,
      "learning_rate": 4.499470027087504e-05,
      "loss": 1.0786,
      "step": 1350
    },
    {
      "epoch": 0.4537871204537871,
      "grad_norm": 3.3144290447235107,
      "learning_rate": 4.493581439170887e-05,
      "loss": 1.3567,
      "step": 1360
    },
    {
      "epoch": 0.4571237904571238,
      "grad_norm": 4.10409688949585,
      "learning_rate": 4.487692851254269e-05,
      "loss": 1.2828,
      "step": 1370
    },
    {
      "epoch": 0.46046046046046046,
      "grad_norm": 3.7806947231292725,
      "learning_rate": 4.4818042633376515e-05,
      "loss": 1.2158,
      "step": 1380
    },
    {
      "epoch": 0.4637971304637971,
      "grad_norm": 6.468990802764893,
      "learning_rate": 4.475915675421034e-05,
      "loss": 1.1004,
      "step": 1390
    },
    {
      "epoch": 0.4671338004671338,
      "grad_norm": 6.160029888153076,
      "learning_rate": 4.4700270875044165e-05,
      "loss": 1.4794,
      "step": 1400
    },
    {
      "epoch": 0.47047047047047047,
      "grad_norm": 5.245044708251953,
      "learning_rate": 4.464138499587799e-05,
      "loss": 1.1136,
      "step": 1410
    },
    {
      "epoch": 0.4738071404738071,
      "grad_norm": 4.885804176330566,
      "learning_rate": 4.4582499116711815e-05,
      "loss": 1.2628,
      "step": 1420
    },
    {
      "epoch": 0.4771438104771438,
      "grad_norm": 4.391535758972168,
      "learning_rate": 4.452361323754564e-05,
      "loss": 0.8905,
      "step": 1430
    },
    {
      "epoch": 0.4804804804804805,
      "grad_norm": 4.875990867614746,
      "learning_rate": 4.4464727358379466e-05,
      "loss": 1.1126,
      "step": 1440
    },
    {
      "epoch": 0.4838171504838171,
      "grad_norm": 3.6667091846466064,
      "learning_rate": 4.440584147921329e-05,
      "loss": 1.0576,
      "step": 1450
    },
    {
      "epoch": 0.48715382048715383,
      "grad_norm": 4.883539199829102,
      "learning_rate": 4.4346955600047116e-05,
      "loss": 1.0036,
      "step": 1460
    },
    {
      "epoch": 0.4904904904904905,
      "grad_norm": 4.8002495765686035,
      "learning_rate": 4.428806972088094e-05,
      "loss": 0.97,
      "step": 1470
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 5.606390476226807,
      "learning_rate": 4.422918384171476e-05,
      "loss": 1.3469,
      "step": 1480
    },
    {
      "epoch": 0.49716383049716384,
      "grad_norm": 6.953462600708008,
      "learning_rate": 4.417029796254858e-05,
      "loss": 0.9795,
      "step": 1490
    },
    {
      "epoch": 0.5005005005005005,
      "grad_norm": 6.637098789215088,
      "learning_rate": 4.411141208338241e-05,
      "loss": 1.2463,
      "step": 1500
    },
    {
      "epoch": 0.5038371705038371,
      "grad_norm": 5.890302658081055,
      "learning_rate": 4.405252620421623e-05,
      "loss": 1.0039,
      "step": 1510
    },
    {
      "epoch": 0.5071738405071738,
      "grad_norm": 4.386608123779297,
      "learning_rate": 4.3993640325050053e-05,
      "loss": 1.1956,
      "step": 1520
    },
    {
      "epoch": 0.5105105105105106,
      "grad_norm": 2.929110050201416,
      "learning_rate": 4.393475444588388e-05,
      "loss": 1.0892,
      "step": 1530
    },
    {
      "epoch": 0.5138471805138471,
      "grad_norm": 4.3466410636901855,
      "learning_rate": 4.3875868566717704e-05,
      "loss": 0.9713,
      "step": 1540
    },
    {
      "epoch": 0.5171838505171839,
      "grad_norm": 5.009315490722656,
      "learning_rate": 4.3816982687551526e-05,
      "loss": 1.0007,
      "step": 1550
    },
    {
      "epoch": 0.5205205205205206,
      "grad_norm": 6.610705852508545,
      "learning_rate": 4.3758096808385354e-05,
      "loss": 0.9979,
      "step": 1560
    },
    {
      "epoch": 0.5238571905238572,
      "grad_norm": 6.347642421722412,
      "learning_rate": 4.3699210929219176e-05,
      "loss": 1.0789,
      "step": 1570
    },
    {
      "epoch": 0.5271938605271939,
      "grad_norm": 4.932490825653076,
      "learning_rate": 4.3640325050053e-05,
      "loss": 0.9119,
      "step": 1580
    },
    {
      "epoch": 0.5305305305305306,
      "grad_norm": 5.253088474273682,
      "learning_rate": 4.3581439170886826e-05,
      "loss": 1.2789,
      "step": 1590
    },
    {
      "epoch": 0.5338672005338672,
      "grad_norm": 3.9506020545959473,
      "learning_rate": 4.352255329172065e-05,
      "loss": 1.0147,
      "step": 1600
    },
    {
      "epoch": 0.5372038705372039,
      "grad_norm": 4.5636138916015625,
      "learning_rate": 4.346366741255447e-05,
      "loss": 1.1785,
      "step": 1610
    },
    {
      "epoch": 0.5405405405405406,
      "grad_norm": 4.6478590965271,
      "learning_rate": 4.340478153338829e-05,
      "loss": 1.1807,
      "step": 1620
    },
    {
      "epoch": 0.5438772105438772,
      "grad_norm": 5.021444320678711,
      "learning_rate": 4.334589565422212e-05,
      "loss": 1.012,
      "step": 1630
    },
    {
      "epoch": 0.5472138805472139,
      "grad_norm": 3.3565914630889893,
      "learning_rate": 4.328700977505594e-05,
      "loss": 0.7903,
      "step": 1640
    },
    {
      "epoch": 0.5505505505505506,
      "grad_norm": 3.42800235748291,
      "learning_rate": 4.3228123895889764e-05,
      "loss": 1.131,
      "step": 1650
    },
    {
      "epoch": 0.5538872205538872,
      "grad_norm": 5.5250396728515625,
      "learning_rate": 4.316923801672359e-05,
      "loss": 1.0237,
      "step": 1660
    },
    {
      "epoch": 0.5572238905572239,
      "grad_norm": 5.53750467300415,
      "learning_rate": 4.3110352137557414e-05,
      "loss": 0.9715,
      "step": 1670
    },
    {
      "epoch": 0.5605605605605606,
      "grad_norm": 3.6184206008911133,
      "learning_rate": 4.3051466258391236e-05,
      "loss": 1.0773,
      "step": 1680
    },
    {
      "epoch": 0.5638972305638972,
      "grad_norm": 2.6647891998291016,
      "learning_rate": 4.2992580379225064e-05,
      "loss": 1.0247,
      "step": 1690
    },
    {
      "epoch": 0.5672339005672339,
      "grad_norm": 6.049147605895996,
      "learning_rate": 4.2933694500058886e-05,
      "loss": 1.246,
      "step": 1700
    },
    {
      "epoch": 0.5705705705705706,
      "grad_norm": 5.908942222595215,
      "learning_rate": 4.2874808620892715e-05,
      "loss": 1.0188,
      "step": 1710
    },
    {
      "epoch": 0.5739072405739073,
      "grad_norm": 4.471683502197266,
      "learning_rate": 4.2815922741726536e-05,
      "loss": 0.92,
      "step": 1720
    },
    {
      "epoch": 0.5772439105772439,
      "grad_norm": 8.099821090698242,
      "learning_rate": 4.2757036862560365e-05,
      "loss": 1.1127,
      "step": 1730
    },
    {
      "epoch": 0.5805805805805806,
      "grad_norm": 6.777768611907959,
      "learning_rate": 4.2698150983394187e-05,
      "loss": 1.0623,
      "step": 1740
    },
    {
      "epoch": 0.5839172505839173,
      "grad_norm": 3.303414821624756,
      "learning_rate": 4.263926510422801e-05,
      "loss": 1.2589,
      "step": 1750
    },
    {
      "epoch": 0.5872539205872539,
      "grad_norm": 5.274767875671387,
      "learning_rate": 4.258037922506184e-05,
      "loss": 0.9983,
      "step": 1760
    },
    {
      "epoch": 0.5905905905905906,
      "grad_norm": 8.6080961227417,
      "learning_rate": 4.252149334589566e-05,
      "loss": 1.1449,
      "step": 1770
    },
    {
      "epoch": 0.5939272605939273,
      "grad_norm": 4.0620222091674805,
      "learning_rate": 4.246260746672948e-05,
      "loss": 1.0229,
      "step": 1780
    },
    {
      "epoch": 0.5972639305972639,
      "grad_norm": 4.511322498321533,
      "learning_rate": 4.24037215875633e-05,
      "loss": 0.922,
      "step": 1790
    },
    {
      "epoch": 0.6006006006006006,
      "grad_norm": 4.134535789489746,
      "learning_rate": 4.234483570839713e-05,
      "loss": 0.9174,
      "step": 1800
    }
  ],
  "logging_steps": 10,
  "max_steps": 8991,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 900,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7603622372966400.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
