{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.500083430669114,
  "eval_steps": 500,
  "global_step": 2997,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016686133822793258,
      "grad_norm": 3.79904842376709,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 2.2711,
      "step": 10
    },
    {
      "epoch": 0.0033372267645586516,
      "grad_norm": 7.182623386383057,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 2.1339,
      "step": 20
    },
    {
      "epoch": 0.005005840146837978,
      "grad_norm": 3.6051793098449707,
      "learning_rate": 3e-06,
      "loss": 1.34,
      "step": 30
    },
    {
      "epoch": 0.006674453529117303,
      "grad_norm": 10.527331352233887,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.6011,
      "step": 40
    },
    {
      "epoch": 0.00834306691139663,
      "grad_norm": 9.845771789550781,
      "learning_rate": 5e-06,
      "loss": 1.7923,
      "step": 50
    },
    {
      "epoch": 0.010011680293675955,
      "grad_norm": 5.402078151702881,
      "learning_rate": 6e-06,
      "loss": 1.4371,
      "step": 60
    },
    {
      "epoch": 0.011680293675955281,
      "grad_norm": 6.919439315795898,
      "learning_rate": 7.000000000000001e-06,
      "loss": 1.2766,
      "step": 70
    },
    {
      "epoch": 0.013348907058234607,
      "grad_norm": 3.590498447418213,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.7382,
      "step": 80
    },
    {
      "epoch": 0.015017520440513932,
      "grad_norm": 4.093526840209961,
      "learning_rate": 9e-06,
      "loss": 1.9864,
      "step": 90
    },
    {
      "epoch": 0.01668613382279326,
      "grad_norm": 3.692473888397217,
      "learning_rate": 1e-05,
      "loss": 2.0114,
      "step": 100
    },
    {
      "epoch": 0.018354747205072585,
      "grad_norm": 4.630216121673584,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.1169,
      "step": 110
    },
    {
      "epoch": 0.02002336058735191,
      "grad_norm": 5.28784704208374,
      "learning_rate": 1.2e-05,
      "loss": 2.0046,
      "step": 120
    },
    {
      "epoch": 0.021691973969631236,
      "grad_norm": 5.671827793121338,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 2.0312,
      "step": 130
    },
    {
      "epoch": 0.023360587351910562,
      "grad_norm": 8.2243013381958,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.5622,
      "step": 140
    },
    {
      "epoch": 0.025029200734189887,
      "grad_norm": 2.4041430950164795,
      "learning_rate": 1.5e-05,
      "loss": 1.461,
      "step": 150
    },
    {
      "epoch": 0.026697814116469213,
      "grad_norm": 4.042627334594727,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.7078,
      "step": 160
    },
    {
      "epoch": 0.02836642749874854,
      "grad_norm": 5.533896446228027,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 1.6739,
      "step": 170
    },
    {
      "epoch": 0.030035040881027864,
      "grad_norm": 6.103790760040283,
      "learning_rate": 1.8e-05,
      "loss": 1.2197,
      "step": 180
    },
    {
      "epoch": 0.03170365426330719,
      "grad_norm": 3.236377000808716,
      "learning_rate": 1.9e-05,
      "loss": 1.1634,
      "step": 190
    },
    {
      "epoch": 0.03337226764558652,
      "grad_norm": 3.6233606338500977,
      "learning_rate": 2e-05,
      "loss": 1.4596,
      "step": 200
    },
    {
      "epoch": 0.035040881027865844,
      "grad_norm": 2.819995403289795,
      "learning_rate": 2.1e-05,
      "loss": 0.883,
      "step": 210
    },
    {
      "epoch": 0.03670949441014517,
      "grad_norm": 3.301626443862915,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.9302,
      "step": 220
    },
    {
      "epoch": 0.038378107792424496,
      "grad_norm": 5.836217403411865,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 1.3534,
      "step": 230
    },
    {
      "epoch": 0.04004672117470382,
      "grad_norm": 2.4070804119110107,
      "learning_rate": 2.4e-05,
      "loss": 1.6678,
      "step": 240
    },
    {
      "epoch": 0.04171533455698315,
      "grad_norm": 5.188549041748047,
      "learning_rate": 2.5e-05,
      "loss": 1.5615,
      "step": 250
    },
    {
      "epoch": 0.04338394793926247,
      "grad_norm": 3.4946234226226807,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.1306,
      "step": 260
    },
    {
      "epoch": 0.0450525613215418,
      "grad_norm": 3.4348223209381104,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.9766,
      "step": 270
    },
    {
      "epoch": 0.046721174703821124,
      "grad_norm": 6.687024116516113,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.2911,
      "step": 280
    },
    {
      "epoch": 0.04838978808610045,
      "grad_norm": 8.801347732543945,
      "learning_rate": 2.9e-05,
      "loss": 1.1132,
      "step": 290
    },
    {
      "epoch": 0.050058401468379775,
      "grad_norm": 4.196038722991943,
      "learning_rate": 3e-05,
      "loss": 1.1248,
      "step": 300
    },
    {
      "epoch": 0.0517270148506591,
      "grad_norm": 2.8185057640075684,
      "learning_rate": 3.1e-05,
      "loss": 1.5625,
      "step": 310
    },
    {
      "epoch": 0.053395628232938426,
      "grad_norm": 7.821621417999268,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.1737,
      "step": 320
    },
    {
      "epoch": 0.05506424161521775,
      "grad_norm": 3.4825820922851562,
      "learning_rate": 3.3e-05,
      "loss": 1.2027,
      "step": 330
    },
    {
      "epoch": 0.05673285499749708,
      "grad_norm": 3.456397294998169,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.8523,
      "step": 340
    },
    {
      "epoch": 0.0584014683797764,
      "grad_norm": 6.144533634185791,
      "learning_rate": 3.5e-05,
      "loss": 1.1343,
      "step": 350
    },
    {
      "epoch": 0.06007008176205573,
      "grad_norm": 3.0288727283477783,
      "learning_rate": 3.6e-05,
      "loss": 1.0722,
      "step": 360
    },
    {
      "epoch": 0.06173869514433506,
      "grad_norm": 2.38199520111084,
      "learning_rate": 3.7e-05,
      "loss": 1.2691,
      "step": 370
    },
    {
      "epoch": 0.06340730852661439,
      "grad_norm": 9.471896171569824,
      "learning_rate": 3.8e-05,
      "loss": 1.1728,
      "step": 380
    },
    {
      "epoch": 0.0650759219088937,
      "grad_norm": 5.951301574707031,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.0393,
      "step": 390
    },
    {
      "epoch": 0.06674453529117304,
      "grad_norm": 4.838334560394287,
      "learning_rate": 4e-05,
      "loss": 0.6978,
      "step": 400
    },
    {
      "epoch": 0.06841314867345236,
      "grad_norm": 5.813498497009277,
      "learning_rate": 4.1e-05,
      "loss": 0.782,
      "step": 410
    },
    {
      "epoch": 0.07008176205573169,
      "grad_norm": 6.75109338760376,
      "learning_rate": 4.2e-05,
      "loss": 1.1449,
      "step": 420
    },
    {
      "epoch": 0.07175037543801101,
      "grad_norm": 1.5278546810150146,
      "learning_rate": 4.3e-05,
      "loss": 0.8661,
      "step": 430
    },
    {
      "epoch": 0.07341898882029034,
      "grad_norm": 4.583148002624512,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.058,
      "step": 440
    },
    {
      "epoch": 0.07508760220256966,
      "grad_norm": 7.415335178375244,
      "learning_rate": 4.5e-05,
      "loss": 1.0257,
      "step": 450
    },
    {
      "epoch": 0.07675621558484899,
      "grad_norm": 7.909468173980713,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.1085,
      "step": 460
    },
    {
      "epoch": 0.07842482896712831,
      "grad_norm": 3.7046473026275635,
      "learning_rate": 4.7e-05,
      "loss": 0.9718,
      "step": 470
    },
    {
      "epoch": 0.08009344234940764,
      "grad_norm": 5.0828118324279785,
      "learning_rate": 4.8e-05,
      "loss": 0.7885,
      "step": 480
    },
    {
      "epoch": 0.08176205573168698,
      "grad_norm": 1.0030765533447266,
      "learning_rate": 4.9e-05,
      "loss": 0.8161,
      "step": 490
    },
    {
      "epoch": 0.0834306691139663,
      "grad_norm": 14.221755027770996,
      "learning_rate": 5e-05,
      "loss": 0.8607,
      "step": 500
    },
    {
      "epoch": 0.08509928249624563,
      "grad_norm": 3.024747133255005,
      "learning_rate": 4.998303071440693e-05,
      "loss": 0.8952,
      "step": 510
    },
    {
      "epoch": 0.08676789587852494,
      "grad_norm": 8.669234275817871,
      "learning_rate": 4.996606142881385e-05,
      "loss": 0.782,
      "step": 520
    },
    {
      "epoch": 0.08843650926080428,
      "grad_norm": 5.924846649169922,
      "learning_rate": 4.994909214322077e-05,
      "loss": 1.2535,
      "step": 530
    },
    {
      "epoch": 0.0901051226430836,
      "grad_norm": 3.6464028358459473,
      "learning_rate": 4.993212285762769e-05,
      "loss": 0.8856,
      "step": 540
    },
    {
      "epoch": 0.09177373602536293,
      "grad_norm": 6.805990219116211,
      "learning_rate": 4.991515357203462e-05,
      "loss": 1.1457,
      "step": 550
    },
    {
      "epoch": 0.09344234940764225,
      "grad_norm": 5.023440837860107,
      "learning_rate": 4.989818428644154e-05,
      "loss": 0.6686,
      "step": 560
    },
    {
      "epoch": 0.09511096278992158,
      "grad_norm": 7.8002753257751465,
      "learning_rate": 4.988121500084847e-05,
      "loss": 1.3947,
      "step": 570
    },
    {
      "epoch": 0.0967795761722009,
      "grad_norm": 6.081882953643799,
      "learning_rate": 4.986424571525539e-05,
      "loss": 0.7415,
      "step": 580
    },
    {
      "epoch": 0.09844818955448023,
      "grad_norm": 3.987962007522583,
      "learning_rate": 4.9847276429662315e-05,
      "loss": 0.8556,
      "step": 590
    },
    {
      "epoch": 0.10011680293675955,
      "grad_norm": 8.489911079406738,
      "learning_rate": 4.983030714406924e-05,
      "loss": 0.7891,
      "step": 600
    },
    {
      "epoch": 0.10178541631903888,
      "grad_norm": 7.404081344604492,
      "learning_rate": 4.981333785847616e-05,
      "loss": 1.1191,
      "step": 610
    },
    {
      "epoch": 0.1034540297013182,
      "grad_norm": 4.346790313720703,
      "learning_rate": 4.9796368572883086e-05,
      "loss": 0.8707,
      "step": 620
    },
    {
      "epoch": 0.10512264308359753,
      "grad_norm": 7.410800933837891,
      "learning_rate": 4.9779399287290005e-05,
      "loss": 0.9092,
      "step": 630
    },
    {
      "epoch": 0.10679125646587685,
      "grad_norm": 2.9268417358398438,
      "learning_rate": 4.976243000169693e-05,
      "loss": 1.0369,
      "step": 640
    },
    {
      "epoch": 0.10845986984815618,
      "grad_norm": 4.379398822784424,
      "learning_rate": 4.974546071610386e-05,
      "loss": 0.8231,
      "step": 650
    },
    {
      "epoch": 0.1101284832304355,
      "grad_norm": 5.61354923248291,
      "learning_rate": 4.9728491430510776e-05,
      "loss": 0.9985,
      "step": 660
    },
    {
      "epoch": 0.11179709661271484,
      "grad_norm": 6.251413822174072,
      "learning_rate": 4.97115221449177e-05,
      "loss": 1.087,
      "step": 670
    },
    {
      "epoch": 0.11346570999499415,
      "grad_norm": 4.300239086151123,
      "learning_rate": 4.969455285932462e-05,
      "loss": 0.9326,
      "step": 680
    },
    {
      "epoch": 0.11513432337727349,
      "grad_norm": 1.7682503461837769,
      "learning_rate": 4.967758357373155e-05,
      "loss": 0.9457,
      "step": 690
    },
    {
      "epoch": 0.1168029367595528,
      "grad_norm": 6.0161309242248535,
      "learning_rate": 4.966061428813847e-05,
      "loss": 0.7711,
      "step": 700
    },
    {
      "epoch": 0.11847155014183214,
      "grad_norm": 7.376018047332764,
      "learning_rate": 4.96436450025454e-05,
      "loss": 0.7476,
      "step": 710
    },
    {
      "epoch": 0.12014016352411146,
      "grad_norm": 5.102439880371094,
      "learning_rate": 4.962667571695232e-05,
      "loss": 0.7983,
      "step": 720
    },
    {
      "epoch": 0.12180877690639079,
      "grad_norm": 5.592205047607422,
      "learning_rate": 4.960970643135924e-05,
      "loss": 0.7777,
      "step": 730
    },
    {
      "epoch": 0.12347739028867012,
      "grad_norm": 10.7653169631958,
      "learning_rate": 4.959273714576617e-05,
      "loss": 1.2737,
      "step": 740
    },
    {
      "epoch": 0.12514600367094944,
      "grad_norm": 4.526876926422119,
      "learning_rate": 4.957576786017309e-05,
      "loss": 1.1343,
      "step": 750
    },
    {
      "epoch": 0.12681461705322877,
      "grad_norm": 4.812863826751709,
      "learning_rate": 4.9558798574580014e-05,
      "loss": 0.9902,
      "step": 760
    },
    {
      "epoch": 0.1284832304355081,
      "grad_norm": 7.757281303405762,
      "learning_rate": 4.954182928898693e-05,
      "loss": 0.9167,
      "step": 770
    },
    {
      "epoch": 0.1301518438177874,
      "grad_norm": 6.603519916534424,
      "learning_rate": 4.952486000339386e-05,
      "loss": 0.975,
      "step": 780
    },
    {
      "epoch": 0.13182045720006674,
      "grad_norm": 7.927140712738037,
      "learning_rate": 4.950789071780078e-05,
      "loss": 1.0208,
      "step": 790
    },
    {
      "epoch": 0.13348907058234608,
      "grad_norm": 3.15924072265625,
      "learning_rate": 4.9490921432207704e-05,
      "loss": 0.8477,
      "step": 800
    },
    {
      "epoch": 0.1351576839646254,
      "grad_norm": 4.649066925048828,
      "learning_rate": 4.947395214661463e-05,
      "loss": 1.0416,
      "step": 810
    },
    {
      "epoch": 0.1368262973469047,
      "grad_norm": 13.645825386047363,
      "learning_rate": 4.9456982861021556e-05,
      "loss": 0.8851,
      "step": 820
    },
    {
      "epoch": 0.13849491072918405,
      "grad_norm": 1.7864187955856323,
      "learning_rate": 4.944001357542848e-05,
      "loss": 0.6225,
      "step": 830
    },
    {
      "epoch": 0.14016352411146338,
      "grad_norm": 6.079190731048584,
      "learning_rate": 4.94230442898354e-05,
      "loss": 0.8376,
      "step": 840
    },
    {
      "epoch": 0.1418321374937427,
      "grad_norm": 7.027696132659912,
      "learning_rate": 4.940607500424233e-05,
      "loss": 1.2121,
      "step": 850
    },
    {
      "epoch": 0.14350075087602202,
      "grad_norm": 6.695984840393066,
      "learning_rate": 4.9389105718649246e-05,
      "loss": 0.965,
      "step": 860
    },
    {
      "epoch": 0.14516936425830135,
      "grad_norm": 7.41980504989624,
      "learning_rate": 4.937213643305617e-05,
      "loss": 1.0054,
      "step": 870
    },
    {
      "epoch": 0.14683797764058068,
      "grad_norm": 6.862626075744629,
      "learning_rate": 4.935516714746309e-05,
      "loss": 0.9562,
      "step": 880
    },
    {
      "epoch": 0.14850659102286,
      "grad_norm": 6.646551609039307,
      "learning_rate": 4.933819786187002e-05,
      "loss": 0.868,
      "step": 890
    },
    {
      "epoch": 0.15017520440513932,
      "grad_norm": 5.551462650299072,
      "learning_rate": 4.932122857627694e-05,
      "loss": 0.7635,
      "step": 900
    },
    {
      "epoch": 0.15184381778741865,
      "grad_norm": 1.2757699489593506,
      "learning_rate": 4.930425929068386e-05,
      "loss": 0.8866,
      "step": 910
    },
    {
      "epoch": 0.15351243116969798,
      "grad_norm": 9.283583641052246,
      "learning_rate": 4.928729000509079e-05,
      "loss": 0.8781,
      "step": 920
    },
    {
      "epoch": 0.15518104455197732,
      "grad_norm": 8.223098754882812,
      "learning_rate": 4.9270320719497707e-05,
      "loss": 0.8231,
      "step": 930
    },
    {
      "epoch": 0.15684965793425662,
      "grad_norm": 6.533298015594482,
      "learning_rate": 4.925335143390464e-05,
      "loss": 0.885,
      "step": 940
    },
    {
      "epoch": 0.15851827131653595,
      "grad_norm": 10.064355850219727,
      "learning_rate": 4.923638214831156e-05,
      "loss": 1.0058,
      "step": 950
    },
    {
      "epoch": 0.16018688469881529,
      "grad_norm": 3.179572582244873,
      "learning_rate": 4.9219412862718484e-05,
      "loss": 1.0824,
      "step": 960
    },
    {
      "epoch": 0.16185549808109462,
      "grad_norm": 6.268158435821533,
      "learning_rate": 4.92024435771254e-05,
      "loss": 0.8083,
      "step": 970
    },
    {
      "epoch": 0.16352411146337395,
      "grad_norm": 7.978918552398682,
      "learning_rate": 4.918547429153233e-05,
      "loss": 0.8201,
      "step": 980
    },
    {
      "epoch": 0.16519272484565325,
      "grad_norm": 7.36130952835083,
      "learning_rate": 4.9168505005939255e-05,
      "loss": 0.8972,
      "step": 990
    },
    {
      "epoch": 0.1668613382279326,
      "grad_norm": 6.709752559661865,
      "learning_rate": 4.9151535720346174e-05,
      "loss": 1.0784,
      "step": 1000
    },
    {
      "epoch": 0.16852995161021192,
      "grad_norm": 3.5908749103546143,
      "learning_rate": 4.91345664347531e-05,
      "loss": 0.8012,
      "step": 1010
    },
    {
      "epoch": 0.17019856499249125,
      "grad_norm": 6.520646572113037,
      "learning_rate": 4.911759714916002e-05,
      "loss": 0.7217,
      "step": 1020
    },
    {
      "epoch": 0.17186717837477056,
      "grad_norm": 9.398969650268555,
      "learning_rate": 4.9100627863566945e-05,
      "loss": 0.6783,
      "step": 1030
    },
    {
      "epoch": 0.1735357917570499,
      "grad_norm": 4.233824253082275,
      "learning_rate": 4.908365857797387e-05,
      "loss": 0.7253,
      "step": 1040
    },
    {
      "epoch": 0.17520440513932922,
      "grad_norm": 7.398596286773682,
      "learning_rate": 4.906668929238079e-05,
      "loss": 0.7637,
      "step": 1050
    },
    {
      "epoch": 0.17687301852160855,
      "grad_norm": 6.739957809448242,
      "learning_rate": 4.9049720006787716e-05,
      "loss": 0.8031,
      "step": 1060
    },
    {
      "epoch": 0.17854163190388786,
      "grad_norm": 7.9486985206604,
      "learning_rate": 4.903275072119464e-05,
      "loss": 0.9621,
      "step": 1070
    },
    {
      "epoch": 0.1802102452861672,
      "grad_norm": 7.202178955078125,
      "learning_rate": 4.901578143560157e-05,
      "loss": 0.9566,
      "step": 1080
    },
    {
      "epoch": 0.18187885866844652,
      "grad_norm": 6.278532028198242,
      "learning_rate": 4.899881215000849e-05,
      "loss": 0.9441,
      "step": 1090
    },
    {
      "epoch": 0.18354747205072586,
      "grad_norm": 5.335439205169678,
      "learning_rate": 4.898184286441541e-05,
      "loss": 0.6441,
      "step": 1100
    },
    {
      "epoch": 0.18521608543300516,
      "grad_norm": 4.642460346221924,
      "learning_rate": 4.896487357882233e-05,
      "loss": 0.7104,
      "step": 1110
    },
    {
      "epoch": 0.1868846988152845,
      "grad_norm": 8.298638343811035,
      "learning_rate": 4.894790429322926e-05,
      "loss": 1.1014,
      "step": 1120
    },
    {
      "epoch": 0.18855331219756383,
      "grad_norm": 4.595211982727051,
      "learning_rate": 4.8930935007636183e-05,
      "loss": 0.7587,
      "step": 1130
    },
    {
      "epoch": 0.19022192557984316,
      "grad_norm": 3.740720748901367,
      "learning_rate": 4.89139657220431e-05,
      "loss": 0.9371,
      "step": 1140
    },
    {
      "epoch": 0.19189053896212246,
      "grad_norm": 9.767218589782715,
      "learning_rate": 4.889699643645003e-05,
      "loss": 0.6969,
      "step": 1150
    },
    {
      "epoch": 0.1935591523444018,
      "grad_norm": 7.882074356079102,
      "learning_rate": 4.888002715085695e-05,
      "loss": 0.4989,
      "step": 1160
    },
    {
      "epoch": 0.19522776572668113,
      "grad_norm": 8.713257789611816,
      "learning_rate": 4.886305786526387e-05,
      "loss": 0.6804,
      "step": 1170
    },
    {
      "epoch": 0.19689637910896046,
      "grad_norm": 6.494941711425781,
      "learning_rate": 4.884608857967079e-05,
      "loss": 0.6599,
      "step": 1180
    },
    {
      "epoch": 0.19856499249123977,
      "grad_norm": 5.320250511169434,
      "learning_rate": 4.8829119294077725e-05,
      "loss": 1.2213,
      "step": 1190
    },
    {
      "epoch": 0.2002336058735191,
      "grad_norm": 6.940951824188232,
      "learning_rate": 4.8812150008484644e-05,
      "loss": 1.151,
      "step": 1200
    },
    {
      "epoch": 0.20190221925579843,
      "grad_norm": 3.36543869972229,
      "learning_rate": 4.879518072289157e-05,
      "loss": 0.9777,
      "step": 1210
    },
    {
      "epoch": 0.20357083263807776,
      "grad_norm": 3.555760145187378,
      "learning_rate": 4.8778211437298496e-05,
      "loss": 0.9639,
      "step": 1220
    },
    {
      "epoch": 0.2052394460203571,
      "grad_norm": 7.245345592498779,
      "learning_rate": 4.8761242151705415e-05,
      "loss": 0.8724,
      "step": 1230
    },
    {
      "epoch": 0.2069080594026364,
      "grad_norm": 6.3500189781188965,
      "learning_rate": 4.874427286611234e-05,
      "loss": 0.7266,
      "step": 1240
    },
    {
      "epoch": 0.20857667278491573,
      "grad_norm": 7.472283363342285,
      "learning_rate": 4.872730358051926e-05,
      "loss": 0.7371,
      "step": 1250
    },
    {
      "epoch": 0.21024528616719507,
      "grad_norm": 7.047811985015869,
      "learning_rate": 4.8710334294926186e-05,
      "loss": 0.6164,
      "step": 1260
    },
    {
      "epoch": 0.2119138995494744,
      "grad_norm": 6.9337639808654785,
      "learning_rate": 4.869336500933311e-05,
      "loss": 0.7654,
      "step": 1270
    },
    {
      "epoch": 0.2135825129317537,
      "grad_norm": 15.973174095153809,
      "learning_rate": 4.867639572374003e-05,
      "loss": 1.0935,
      "step": 1280
    },
    {
      "epoch": 0.21525112631403304,
      "grad_norm": 8.758491516113281,
      "learning_rate": 4.865942643814696e-05,
      "loss": 0.7974,
      "step": 1290
    },
    {
      "epoch": 0.21691973969631237,
      "grad_norm": 1.8754631280899048,
      "learning_rate": 4.8642457152553876e-05,
      "loss": 0.5935,
      "step": 1300
    },
    {
      "epoch": 0.2185883530785917,
      "grad_norm": 7.009585857391357,
      "learning_rate": 4.862548786696081e-05,
      "loss": 1.0534,
      "step": 1310
    },
    {
      "epoch": 0.220256966460871,
      "grad_norm": 4.205259799957275,
      "learning_rate": 4.860851858136773e-05,
      "loss": 0.9154,
      "step": 1320
    },
    {
      "epoch": 0.22192557984315034,
      "grad_norm": 6.721474647521973,
      "learning_rate": 4.8591549295774653e-05,
      "loss": 0.6583,
      "step": 1330
    },
    {
      "epoch": 0.22359419322542967,
      "grad_norm": 3.5803515911102295,
      "learning_rate": 4.857458001018157e-05,
      "loss": 0.8278,
      "step": 1340
    },
    {
      "epoch": 0.225262806607709,
      "grad_norm": 4.124467849731445,
      "learning_rate": 4.85576107245885e-05,
      "loss": 0.5003,
      "step": 1350
    },
    {
      "epoch": 0.2269314199899883,
      "grad_norm": 10.031257629394531,
      "learning_rate": 4.8540641438995424e-05,
      "loss": 0.9419,
      "step": 1360
    },
    {
      "epoch": 0.22860003337226764,
      "grad_norm": 7.770595073699951,
      "learning_rate": 4.8523672153402343e-05,
      "loss": 0.9462,
      "step": 1370
    },
    {
      "epoch": 0.23026864675454697,
      "grad_norm": 6.804042339324951,
      "learning_rate": 4.850670286780927e-05,
      "loss": 0.862,
      "step": 1380
    },
    {
      "epoch": 0.2319372601368263,
      "grad_norm": 3.2381014823913574,
      "learning_rate": 4.848973358221619e-05,
      "loss": 0.8612,
      "step": 1390
    },
    {
      "epoch": 0.2336058735191056,
      "grad_norm": 5.524078845977783,
      "learning_rate": 4.8472764296623114e-05,
      "loss": 0.7656,
      "step": 1400
    },
    {
      "epoch": 0.23527448690138494,
      "grad_norm": 5.035533905029297,
      "learning_rate": 4.845579501103003e-05,
      "loss": 1.071,
      "step": 1410
    },
    {
      "epoch": 0.23694310028366428,
      "grad_norm": 9.536332130432129,
      "learning_rate": 4.843882572543696e-05,
      "loss": 0.6668,
      "step": 1420
    },
    {
      "epoch": 0.2386117136659436,
      "grad_norm": 5.131458282470703,
      "learning_rate": 4.8421856439843885e-05,
      "loss": 1.0141,
      "step": 1430
    },
    {
      "epoch": 0.2402803270482229,
      "grad_norm": 5.364581108093262,
      "learning_rate": 4.840488715425081e-05,
      "loss": 1.0744,
      "step": 1440
    },
    {
      "epoch": 0.24194894043050225,
      "grad_norm": 10.462005615234375,
      "learning_rate": 4.838791786865774e-05,
      "loss": 0.7962,
      "step": 1450
    },
    {
      "epoch": 0.24361755381278158,
      "grad_norm": 3.568871021270752,
      "learning_rate": 4.8370948583064656e-05,
      "loss": 0.8062,
      "step": 1460
    },
    {
      "epoch": 0.2452861671950609,
      "grad_norm": 5.848616600036621,
      "learning_rate": 4.835397929747158e-05,
      "loss": 0.8054,
      "step": 1470
    },
    {
      "epoch": 0.24695478057734024,
      "grad_norm": 2.287451982498169,
      "learning_rate": 4.83370100118785e-05,
      "loss": 0.7035,
      "step": 1480
    },
    {
      "epoch": 0.24862339395961955,
      "grad_norm": 7.200664043426514,
      "learning_rate": 4.832004072628543e-05,
      "loss": 0.8759,
      "step": 1490
    },
    {
      "epoch": 0.2502920073418989,
      "grad_norm": 4.867833614349365,
      "learning_rate": 4.8303071440692346e-05,
      "loss": 0.8476,
      "step": 1500
    },
    {
      "epoch": 0.2519606207241782,
      "grad_norm": 4.327005863189697,
      "learning_rate": 4.828610215509927e-05,
      "loss": 0.877,
      "step": 1510
    },
    {
      "epoch": 0.25362923410645755,
      "grad_norm": 7.988686561584473,
      "learning_rate": 4.82691328695062e-05,
      "loss": 0.7017,
      "step": 1520
    },
    {
      "epoch": 0.25529784748873685,
      "grad_norm": 3.5522189140319824,
      "learning_rate": 4.825216358391312e-05,
      "loss": 0.8189,
      "step": 1530
    },
    {
      "epoch": 0.2569664608710162,
      "grad_norm": 9.3996000289917,
      "learning_rate": 4.823519429832004e-05,
      "loss": 0.8931,
      "step": 1540
    },
    {
      "epoch": 0.2586350742532955,
      "grad_norm": 6.500752925872803,
      "learning_rate": 4.821822501272696e-05,
      "loss": 1.0194,
      "step": 1550
    },
    {
      "epoch": 0.2603036876355748,
      "grad_norm": 5.607722282409668,
      "learning_rate": 4.8201255727133894e-05,
      "loss": 0.841,
      "step": 1560
    },
    {
      "epoch": 0.2619723010178542,
      "grad_norm": 7.975852966308594,
      "learning_rate": 4.8184286441540813e-05,
      "loss": 0.9532,
      "step": 1570
    },
    {
      "epoch": 0.2636409144001335,
      "grad_norm": 4.294488906860352,
      "learning_rate": 4.816731715594774e-05,
      "loss": 0.5532,
      "step": 1580
    },
    {
      "epoch": 0.2653095277824128,
      "grad_norm": 5.220550537109375,
      "learning_rate": 4.815034787035466e-05,
      "loss": 1.0821,
      "step": 1590
    },
    {
      "epoch": 0.26697814116469215,
      "grad_norm": 10.952414512634277,
      "learning_rate": 4.8133378584761584e-05,
      "loss": 0.9575,
      "step": 1600
    },
    {
      "epoch": 0.26864675454697146,
      "grad_norm": 3.701032876968384,
      "learning_rate": 4.811640929916851e-05,
      "loss": 0.8681,
      "step": 1610
    },
    {
      "epoch": 0.2703153679292508,
      "grad_norm": 4.70545768737793,
      "learning_rate": 4.809944001357543e-05,
      "loss": 0.9569,
      "step": 1620
    },
    {
      "epoch": 0.2719839813115301,
      "grad_norm": 7.7636284828186035,
      "learning_rate": 4.8082470727982355e-05,
      "loss": 0.7812,
      "step": 1630
    },
    {
      "epoch": 0.2736525946938094,
      "grad_norm": 6.987236022949219,
      "learning_rate": 4.8065501442389274e-05,
      "loss": 0.7828,
      "step": 1640
    },
    {
      "epoch": 0.2753212080760888,
      "grad_norm": 4.573073863983154,
      "learning_rate": 4.80485321567962e-05,
      "loss": 0.9015,
      "step": 1650
    },
    {
      "epoch": 0.2769898214583681,
      "grad_norm": 3.060145854949951,
      "learning_rate": 4.8031562871203126e-05,
      "loss": 0.6581,
      "step": 1660
    },
    {
      "epoch": 0.2786584348406474,
      "grad_norm": 9.09227466583252,
      "learning_rate": 4.8014593585610045e-05,
      "loss": 0.9648,
      "step": 1670
    },
    {
      "epoch": 0.28032704822292676,
      "grad_norm": 7.815892219543457,
      "learning_rate": 4.799762430001697e-05,
      "loss": 0.8771,
      "step": 1680
    },
    {
      "epoch": 0.28199566160520606,
      "grad_norm": 8.1954984664917,
      "learning_rate": 4.79806550144239e-05,
      "loss": 0.4498,
      "step": 1690
    },
    {
      "epoch": 0.2836642749874854,
      "grad_norm": 4.973302364349365,
      "learning_rate": 4.796368572883082e-05,
      "loss": 0.9246,
      "step": 1700
    },
    {
      "epoch": 0.2853328883697647,
      "grad_norm": 8.766337394714355,
      "learning_rate": 4.794671644323774e-05,
      "loss": 0.5424,
      "step": 1710
    },
    {
      "epoch": 0.28700150175204403,
      "grad_norm": 5.705307960510254,
      "learning_rate": 4.792974715764467e-05,
      "loss": 1.1216,
      "step": 1720
    },
    {
      "epoch": 0.2886701151343234,
      "grad_norm": 5.839878082275391,
      "learning_rate": 4.791277787205159e-05,
      "loss": 0.5954,
      "step": 1730
    },
    {
      "epoch": 0.2903387285166027,
      "grad_norm": 6.5529465675354,
      "learning_rate": 4.789580858645851e-05,
      "loss": 0.8163,
      "step": 1740
    },
    {
      "epoch": 0.29200734189888206,
      "grad_norm": 8.933732986450195,
      "learning_rate": 4.787883930086544e-05,
      "loss": 0.8461,
      "step": 1750
    },
    {
      "epoch": 0.29367595528116136,
      "grad_norm": 4.6097187995910645,
      "learning_rate": 4.786187001527236e-05,
      "loss": 0.5946,
      "step": 1760
    },
    {
      "epoch": 0.29534456866344067,
      "grad_norm": 7.7182183265686035,
      "learning_rate": 4.7844900729679284e-05,
      "loss": 0.6555,
      "step": 1770
    },
    {
      "epoch": 0.29701318204572,
      "grad_norm": 2.280984878540039,
      "learning_rate": 4.78279314440862e-05,
      "loss": 0.7076,
      "step": 1780
    },
    {
      "epoch": 0.29868179542799933,
      "grad_norm": 8.5516357421875,
      "learning_rate": 4.781096215849313e-05,
      "loss": 0.9988,
      "step": 1790
    },
    {
      "epoch": 0.30035040881027864,
      "grad_norm": 4.809074878692627,
      "learning_rate": 4.779399287290005e-05,
      "loss": 0.8486,
      "step": 1800
    },
    {
      "epoch": 0.302019022192558,
      "grad_norm": 3.1991074085235596,
      "learning_rate": 4.777702358730698e-05,
      "loss": 0.6643,
      "step": 1810
    },
    {
      "epoch": 0.3036876355748373,
      "grad_norm": 7.109113693237305,
      "learning_rate": 4.77600543017139e-05,
      "loss": 0.7072,
      "step": 1820
    },
    {
      "epoch": 0.30535624895711666,
      "grad_norm": 5.751452922821045,
      "learning_rate": 4.7743085016120825e-05,
      "loss": 0.4635,
      "step": 1830
    },
    {
      "epoch": 0.30702486233939597,
      "grad_norm": 6.025784015655518,
      "learning_rate": 4.772611573052775e-05,
      "loss": 0.7978,
      "step": 1840
    },
    {
      "epoch": 0.30869347572167527,
      "grad_norm": 7.925596714019775,
      "learning_rate": 4.770914644493467e-05,
      "loss": 0.7224,
      "step": 1850
    },
    {
      "epoch": 0.31036208910395463,
      "grad_norm": 2.901942729949951,
      "learning_rate": 4.7692177159341596e-05,
      "loss": 0.8664,
      "step": 1860
    },
    {
      "epoch": 0.31203070248623394,
      "grad_norm": 4.568856239318848,
      "learning_rate": 4.7675207873748515e-05,
      "loss": 0.8431,
      "step": 1870
    },
    {
      "epoch": 0.31369931586851324,
      "grad_norm": 3.7095916271209717,
      "learning_rate": 4.765823858815544e-05,
      "loss": 0.6565,
      "step": 1880
    },
    {
      "epoch": 0.3153679292507926,
      "grad_norm": 6.966523170471191,
      "learning_rate": 4.764126930256237e-05,
      "loss": 0.6631,
      "step": 1890
    },
    {
      "epoch": 0.3170365426330719,
      "grad_norm": 3.690739631652832,
      "learning_rate": 4.7624300016969286e-05,
      "loss": 1.0129,
      "step": 1900
    },
    {
      "epoch": 0.31870515601535127,
      "grad_norm": 10.0293550491333,
      "learning_rate": 4.760733073137621e-05,
      "loss": 0.8941,
      "step": 1910
    },
    {
      "epoch": 0.32037376939763057,
      "grad_norm": 5.941582679748535,
      "learning_rate": 4.759036144578313e-05,
      "loss": 0.727,
      "step": 1920
    },
    {
      "epoch": 0.3220423827799099,
      "grad_norm": 7.233775615692139,
      "learning_rate": 4.757339216019006e-05,
      "loss": 0.8189,
      "step": 1930
    },
    {
      "epoch": 0.32371099616218924,
      "grad_norm": 6.904343605041504,
      "learning_rate": 4.755642287459698e-05,
      "loss": 0.9145,
      "step": 1940
    },
    {
      "epoch": 0.32537960954446854,
      "grad_norm": 7.455501079559326,
      "learning_rate": 4.753945358900391e-05,
      "loss": 0.639,
      "step": 1950
    },
    {
      "epoch": 0.3270482229267479,
      "grad_norm": 2.02082896232605,
      "learning_rate": 4.752248430341083e-05,
      "loss": 0.62,
      "step": 1960
    },
    {
      "epoch": 0.3287168363090272,
      "grad_norm": 0.8728573322296143,
      "learning_rate": 4.7505515017817754e-05,
      "loss": 0.4651,
      "step": 1970
    },
    {
      "epoch": 0.3303854496913065,
      "grad_norm": 6.1950836181640625,
      "learning_rate": 4.748854573222468e-05,
      "loss": 1.0393,
      "step": 1980
    },
    {
      "epoch": 0.33205406307358587,
      "grad_norm": 6.6434712409973145,
      "learning_rate": 4.74715764466316e-05,
      "loss": 0.5031,
      "step": 1990
    },
    {
      "epoch": 0.3337226764558652,
      "grad_norm": 5.445943355560303,
      "learning_rate": 4.7454607161038524e-05,
      "loss": 0.5834,
      "step": 2000
    },
    {
      "epoch": 0.3353912898381445,
      "grad_norm": 2.0814809799194336,
      "learning_rate": 4.7437637875445444e-05,
      "loss": 0.7365,
      "step": 2010
    },
    {
      "epoch": 0.33705990322042384,
      "grad_norm": 8.248190879821777,
      "learning_rate": 4.742066858985237e-05,
      "loss": 0.5451,
      "step": 2020
    },
    {
      "epoch": 0.33872851660270314,
      "grad_norm": 5.852273464202881,
      "learning_rate": 4.740369930425929e-05,
      "loss": 0.6407,
      "step": 2030
    },
    {
      "epoch": 0.3403971299849825,
      "grad_norm": 10.305346488952637,
      "learning_rate": 4.7386730018666214e-05,
      "loss": 0.7857,
      "step": 2040
    },
    {
      "epoch": 0.3420657433672618,
      "grad_norm": 3.9916467666625977,
      "learning_rate": 4.736976073307314e-05,
      "loss": 0.595,
      "step": 2050
    },
    {
      "epoch": 0.3437343567495411,
      "grad_norm": 7.858857154846191,
      "learning_rate": 4.7352791447480066e-05,
      "loss": 0.8684,
      "step": 2060
    },
    {
      "epoch": 0.3454029701318205,
      "grad_norm": 4.695647716522217,
      "learning_rate": 4.733582216188699e-05,
      "loss": 0.5155,
      "step": 2070
    },
    {
      "epoch": 0.3470715835140998,
      "grad_norm": 3.2922067642211914,
      "learning_rate": 4.731885287629391e-05,
      "loss": 0.9256,
      "step": 2080
    },
    {
      "epoch": 0.3487401968963791,
      "grad_norm": 8.713995933532715,
      "learning_rate": 4.730188359070084e-05,
      "loss": 0.5911,
      "step": 2090
    },
    {
      "epoch": 0.35040881027865844,
      "grad_norm": 9.679231643676758,
      "learning_rate": 4.7284914305107756e-05,
      "loss": 0.7517,
      "step": 2100
    },
    {
      "epoch": 0.35207742366093775,
      "grad_norm": 10.645646095275879,
      "learning_rate": 4.726794501951468e-05,
      "loss": 1.1755,
      "step": 2110
    },
    {
      "epoch": 0.3537460370432171,
      "grad_norm": 6.195444107055664,
      "learning_rate": 4.72509757339216e-05,
      "loss": 0.7128,
      "step": 2120
    },
    {
      "epoch": 0.3554146504254964,
      "grad_norm": 9.371164321899414,
      "learning_rate": 4.723400644832853e-05,
      "loss": 0.5542,
      "step": 2130
    },
    {
      "epoch": 0.3570832638077757,
      "grad_norm": 0.7653611302375793,
      "learning_rate": 4.721703716273545e-05,
      "loss": 0.6224,
      "step": 2140
    },
    {
      "epoch": 0.3587518771900551,
      "grad_norm": 7.059931755065918,
      "learning_rate": 4.720006787714237e-05,
      "loss": 0.7012,
      "step": 2150
    },
    {
      "epoch": 0.3604204905723344,
      "grad_norm": 7.114423751831055,
      "learning_rate": 4.71830985915493e-05,
      "loss": 0.8296,
      "step": 2160
    },
    {
      "epoch": 0.3620891039546137,
      "grad_norm": 8.467098236083984,
      "learning_rate": 4.716612930595622e-05,
      "loss": 1.0739,
      "step": 2170
    },
    {
      "epoch": 0.36375771733689305,
      "grad_norm": 8.871882438659668,
      "learning_rate": 4.714916002036314e-05,
      "loss": 0.6668,
      "step": 2180
    },
    {
      "epoch": 0.36542633071917235,
      "grad_norm": 5.060971260070801,
      "learning_rate": 4.713219073477007e-05,
      "loss": 0.8976,
      "step": 2190
    },
    {
      "epoch": 0.3670949441014517,
      "grad_norm": 4.836977005004883,
      "learning_rate": 4.7115221449176994e-05,
      "loss": 0.6857,
      "step": 2200
    },
    {
      "epoch": 0.368763557483731,
      "grad_norm": 1.0327324867248535,
      "learning_rate": 4.7098252163583914e-05,
      "loss": 0.5267,
      "step": 2210
    },
    {
      "epoch": 0.3704321708660103,
      "grad_norm": 11.744599342346191,
      "learning_rate": 4.708128287799084e-05,
      "loss": 0.8479,
      "step": 2220
    },
    {
      "epoch": 0.3721007842482897,
      "grad_norm": 2.2491724491119385,
      "learning_rate": 4.7064313592397765e-05,
      "loss": 0.5676,
      "step": 2230
    },
    {
      "epoch": 0.373769397630569,
      "grad_norm": 2.801563024520874,
      "learning_rate": 4.7047344306804684e-05,
      "loss": 0.7472,
      "step": 2240
    },
    {
      "epoch": 0.37543801101284835,
      "grad_norm": 4.369708061218262,
      "learning_rate": 4.703037502121161e-05,
      "loss": 0.6448,
      "step": 2250
    },
    {
      "epoch": 0.37710662439512765,
      "grad_norm": 7.913095474243164,
      "learning_rate": 4.701340573561853e-05,
      "loss": 0.7612,
      "step": 2260
    },
    {
      "epoch": 0.37877523777740696,
      "grad_norm": 3.4856364727020264,
      "learning_rate": 4.6996436450025455e-05,
      "loss": 0.5559,
      "step": 2270
    },
    {
      "epoch": 0.3804438511596863,
      "grad_norm": 3.502786636352539,
      "learning_rate": 4.697946716443238e-05,
      "loss": 1.1246,
      "step": 2280
    },
    {
      "epoch": 0.3821124645419656,
      "grad_norm": 4.559396743774414,
      "learning_rate": 4.69624978788393e-05,
      "loss": 0.514,
      "step": 2290
    },
    {
      "epoch": 0.38378107792424493,
      "grad_norm": 10.608229637145996,
      "learning_rate": 4.6945528593246226e-05,
      "loss": 0.4996,
      "step": 2300
    },
    {
      "epoch": 0.3854496913065243,
      "grad_norm": 2.7624449729919434,
      "learning_rate": 4.692855930765315e-05,
      "loss": 0.4467,
      "step": 2310
    },
    {
      "epoch": 0.3871183046888036,
      "grad_norm": 2.5176689624786377,
      "learning_rate": 4.691159002206008e-05,
      "loss": 0.514,
      "step": 2320
    },
    {
      "epoch": 0.38878691807108295,
      "grad_norm": 29.607952117919922,
      "learning_rate": 4.6894620736467e-05,
      "loss": 0.8919,
      "step": 2330
    },
    {
      "epoch": 0.39045553145336226,
      "grad_norm": 7.312753200531006,
      "learning_rate": 4.687765145087392e-05,
      "loss": 0.6584,
      "step": 2340
    },
    {
      "epoch": 0.39212414483564156,
      "grad_norm": 10.338000297546387,
      "learning_rate": 4.686068216528084e-05,
      "loss": 0.8212,
      "step": 2350
    },
    {
      "epoch": 0.3937927582179209,
      "grad_norm": 7.501348972320557,
      "learning_rate": 4.684371287968777e-05,
      "loss": 0.984,
      "step": 2360
    },
    {
      "epoch": 0.39546137160020023,
      "grad_norm": 7.75062894821167,
      "learning_rate": 4.6826743594094694e-05,
      "loss": 0.6952,
      "step": 2370
    },
    {
      "epoch": 0.39712998498247953,
      "grad_norm": 13.856389045715332,
      "learning_rate": 4.680977430850161e-05,
      "loss": 0.8905,
      "step": 2380
    },
    {
      "epoch": 0.3987985983647589,
      "grad_norm": 4.53643798828125,
      "learning_rate": 4.679280502290854e-05,
      "loss": 0.8118,
      "step": 2390
    },
    {
      "epoch": 0.4004672117470382,
      "grad_norm": 5.516713619232178,
      "learning_rate": 4.677583573731546e-05,
      "loss": 0.7209,
      "step": 2400
    },
    {
      "epoch": 0.40213582512931756,
      "grad_norm": 8.12226390838623,
      "learning_rate": 4.6758866451722384e-05,
      "loss": 0.5662,
      "step": 2410
    },
    {
      "epoch": 0.40380443851159686,
      "grad_norm": 5.107851505279541,
      "learning_rate": 4.67418971661293e-05,
      "loss": 0.6007,
      "step": 2420
    },
    {
      "epoch": 0.40547305189387617,
      "grad_norm": 4.949314594268799,
      "learning_rate": 4.672492788053623e-05,
      "loss": 0.7865,
      "step": 2430
    },
    {
      "epoch": 0.40714166527615553,
      "grad_norm": 13.078946113586426,
      "learning_rate": 4.6707958594943154e-05,
      "loss": 0.8915,
      "step": 2440
    },
    {
      "epoch": 0.40881027865843483,
      "grad_norm": 10.92952823638916,
      "learning_rate": 4.669098930935008e-05,
      "loss": 0.9305,
      "step": 2450
    },
    {
      "epoch": 0.4104788920407142,
      "grad_norm": 9.040349960327148,
      "learning_rate": 4.6674020023757006e-05,
      "loss": 0.7678,
      "step": 2460
    },
    {
      "epoch": 0.4121475054229935,
      "grad_norm": 4.435794353485107,
      "learning_rate": 4.6657050738163925e-05,
      "loss": 0.8393,
      "step": 2470
    },
    {
      "epoch": 0.4138161188052728,
      "grad_norm": 5.322741985321045,
      "learning_rate": 4.664008145257085e-05,
      "loss": 0.6357,
      "step": 2480
    },
    {
      "epoch": 0.41548473218755216,
      "grad_norm": 8.003535270690918,
      "learning_rate": 4.662311216697777e-05,
      "loss": 0.6056,
      "step": 2490
    },
    {
      "epoch": 0.41715334556983147,
      "grad_norm": 1.591697096824646,
      "learning_rate": 4.6606142881384696e-05,
      "loss": 0.8547,
      "step": 2500
    },
    {
      "epoch": 0.4188219589521108,
      "grad_norm": 1.3259997367858887,
      "learning_rate": 4.658917359579162e-05,
      "loss": 0.6272,
      "step": 2510
    },
    {
      "epoch": 0.42049057233439013,
      "grad_norm": 13.520787239074707,
      "learning_rate": 4.657220431019854e-05,
      "loss": 0.7843,
      "step": 2520
    },
    {
      "epoch": 0.42215918571666944,
      "grad_norm": 5.95968770980835,
      "learning_rate": 4.655523502460547e-05,
      "loss": 0.9054,
      "step": 2530
    },
    {
      "epoch": 0.4238277990989488,
      "grad_norm": 5.7057108879089355,
      "learning_rate": 4.6538265739012386e-05,
      "loss": 0.9363,
      "step": 2540
    },
    {
      "epoch": 0.4254964124812281,
      "grad_norm": 9.836874961853027,
      "learning_rate": 4.652129645341931e-05,
      "loss": 0.9169,
      "step": 2550
    },
    {
      "epoch": 0.4271650258635074,
      "grad_norm": 8.053886413574219,
      "learning_rate": 4.650432716782624e-05,
      "loss": 0.8474,
      "step": 2560
    },
    {
      "epoch": 0.42883363924578677,
      "grad_norm": 8.77614688873291,
      "learning_rate": 4.6487357882233164e-05,
      "loss": 0.8068,
      "step": 2570
    },
    {
      "epoch": 0.4305022526280661,
      "grad_norm": 7.959442138671875,
      "learning_rate": 4.647038859664008e-05,
      "loss": 0.8546,
      "step": 2580
    },
    {
      "epoch": 0.4321708660103454,
      "grad_norm": 3.8034751415252686,
      "learning_rate": 4.645341931104701e-05,
      "loss": 0.5859,
      "step": 2590
    },
    {
      "epoch": 0.43383947939262474,
      "grad_norm": 9.123007774353027,
      "learning_rate": 4.6436450025453935e-05,
      "loss": 0.7763,
      "step": 2600
    },
    {
      "epoch": 0.43550809277490404,
      "grad_norm": 7.685604572296143,
      "learning_rate": 4.6419480739860854e-05,
      "loss": 0.891,
      "step": 2610
    },
    {
      "epoch": 0.4371767061571834,
      "grad_norm": 6.3122758865356445,
      "learning_rate": 4.640251145426778e-05,
      "loss": 0.6462,
      "step": 2620
    },
    {
      "epoch": 0.4388453195394627,
      "grad_norm": 4.7552385330200195,
      "learning_rate": 4.63855421686747e-05,
      "loss": 0.788,
      "step": 2630
    },
    {
      "epoch": 0.440513932921742,
      "grad_norm": 4.40580940246582,
      "learning_rate": 4.6368572883081625e-05,
      "loss": 0.7864,
      "step": 2640
    },
    {
      "epoch": 0.4421825463040214,
      "grad_norm": 3.5186431407928467,
      "learning_rate": 4.6351603597488544e-05,
      "loss": 0.6606,
      "step": 2650
    },
    {
      "epoch": 0.4438511596863007,
      "grad_norm": 6.737544536590576,
      "learning_rate": 4.633463431189547e-05,
      "loss": 0.6714,
      "step": 2660
    },
    {
      "epoch": 0.44551977306858,
      "grad_norm": 2.1617512702941895,
      "learning_rate": 4.6317665026302395e-05,
      "loss": 0.7032,
      "step": 2670
    },
    {
      "epoch": 0.44718838645085934,
      "grad_norm": 7.921445369720459,
      "learning_rate": 4.6300695740709314e-05,
      "loss": 0.7676,
      "step": 2680
    },
    {
      "epoch": 0.44885699983313865,
      "grad_norm": 7.745121955871582,
      "learning_rate": 4.628372645511625e-05,
      "loss": 0.521,
      "step": 2690
    },
    {
      "epoch": 0.450525613215418,
      "grad_norm": 5.564319133758545,
      "learning_rate": 4.6266757169523166e-05,
      "loss": 0.8596,
      "step": 2700
    },
    {
      "epoch": 0.4521942265976973,
      "grad_norm": 8.842630386352539,
      "learning_rate": 4.624978788393009e-05,
      "loss": 1.1088,
      "step": 2710
    },
    {
      "epoch": 0.4538628399799766,
      "grad_norm": 6.874146461486816,
      "learning_rate": 4.623281859833701e-05,
      "loss": 0.7688,
      "step": 2720
    },
    {
      "epoch": 0.455531453362256,
      "grad_norm": 6.731356143951416,
      "learning_rate": 4.621584931274394e-05,
      "loss": 1.2956,
      "step": 2730
    },
    {
      "epoch": 0.4572000667445353,
      "grad_norm": 5.926513195037842,
      "learning_rate": 4.6198880027150856e-05,
      "loss": 0.6151,
      "step": 2740
    },
    {
      "epoch": 0.45886868012681464,
      "grad_norm": 7.988247871398926,
      "learning_rate": 4.618191074155778e-05,
      "loss": 0.9961,
      "step": 2750
    },
    {
      "epoch": 0.46053729350909395,
      "grad_norm": 11.19940185546875,
      "learning_rate": 4.616494145596471e-05,
      "loss": 0.6395,
      "step": 2760
    },
    {
      "epoch": 0.46220590689137325,
      "grad_norm": 4.873228549957275,
      "learning_rate": 4.614797217037163e-05,
      "loss": 0.6646,
      "step": 2770
    },
    {
      "epoch": 0.4638745202736526,
      "grad_norm": 9.068425178527832,
      "learning_rate": 4.613100288477855e-05,
      "loss": 0.6746,
      "step": 2780
    },
    {
      "epoch": 0.4655431336559319,
      "grad_norm": 8.824785232543945,
      "learning_rate": 4.611403359918547e-05,
      "loss": 1.117,
      "step": 2790
    },
    {
      "epoch": 0.4672117470382112,
      "grad_norm": 10.9446439743042,
      "learning_rate": 4.60970643135924e-05,
      "loss": 0.9726,
      "step": 2800
    },
    {
      "epoch": 0.4688803604204906,
      "grad_norm": 6.258224010467529,
      "learning_rate": 4.6080095027999324e-05,
      "loss": 0.6232,
      "step": 2810
    },
    {
      "epoch": 0.4705489738027699,
      "grad_norm": 7.989435195922852,
      "learning_rate": 4.606312574240625e-05,
      "loss": 0.92,
      "step": 2820
    },
    {
      "epoch": 0.47221758718504925,
      "grad_norm": 10.53073501586914,
      "learning_rate": 4.604615645681317e-05,
      "loss": 1.0279,
      "step": 2830
    },
    {
      "epoch": 0.47388620056732855,
      "grad_norm": 12.071959495544434,
      "learning_rate": 4.6029187171220095e-05,
      "loss": 0.6907,
      "step": 2840
    },
    {
      "epoch": 0.47555481394960786,
      "grad_norm": 7.947457313537598,
      "learning_rate": 4.601221788562702e-05,
      "loss": 0.6597,
      "step": 2850
    },
    {
      "epoch": 0.4772234273318872,
      "grad_norm": 3.26193904876709,
      "learning_rate": 4.599524860003394e-05,
      "loss": 0.5111,
      "step": 2860
    },
    {
      "epoch": 0.4788920407141665,
      "grad_norm": 6.503660202026367,
      "learning_rate": 4.5978279314440865e-05,
      "loss": 0.7881,
      "step": 2870
    },
    {
      "epoch": 0.4805606540964458,
      "grad_norm": 5.391315937042236,
      "learning_rate": 4.5961310028847785e-05,
      "loss": 0.5728,
      "step": 2880
    },
    {
      "epoch": 0.4822292674787252,
      "grad_norm": 3.878417491912842,
      "learning_rate": 4.594434074325471e-05,
      "loss": 0.666,
      "step": 2890
    },
    {
      "epoch": 0.4838978808610045,
      "grad_norm": 3.0510120391845703,
      "learning_rate": 4.5927371457661636e-05,
      "loss": 0.6521,
      "step": 2900
    },
    {
      "epoch": 0.48556649424328385,
      "grad_norm": 6.52381706237793,
      "learning_rate": 4.5910402172068555e-05,
      "loss": 0.5154,
      "step": 2910
    },
    {
      "epoch": 0.48723510762556316,
      "grad_norm": 11.997550964355469,
      "learning_rate": 4.589343288647548e-05,
      "loss": 0.8797,
      "step": 2920
    },
    {
      "epoch": 0.48890372100784246,
      "grad_norm": 2.5449585914611816,
      "learning_rate": 4.587646360088241e-05,
      "loss": 0.4899,
      "step": 2930
    },
    {
      "epoch": 0.4905723343901218,
      "grad_norm": 2.677755355834961,
      "learning_rate": 4.585949431528933e-05,
      "loss": 0.5137,
      "step": 2940
    },
    {
      "epoch": 0.4922409477724011,
      "grad_norm": 5.397197723388672,
      "learning_rate": 4.584252502969625e-05,
      "loss": 0.9506,
      "step": 2950
    },
    {
      "epoch": 0.4939095611546805,
      "grad_norm": 3.6196203231811523,
      "learning_rate": 4.582555574410318e-05,
      "loss": 0.7175,
      "step": 2960
    },
    {
      "epoch": 0.4955781745369598,
      "grad_norm": 4.9687700271606445,
      "learning_rate": 4.58085864585101e-05,
      "loss": 0.7461,
      "step": 2970
    },
    {
      "epoch": 0.4972467879192391,
      "grad_norm": 10.7244873046875,
      "learning_rate": 4.579161717291702e-05,
      "loss": 0.5131,
      "step": 2980
    },
    {
      "epoch": 0.49891540130151846,
      "grad_norm": 4.382063388824463,
      "learning_rate": 4.577464788732395e-05,
      "loss": 0.6938,
      "step": 2990
    }
  ],
  "logging_steps": 10,
  "max_steps": 29965,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 2997,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.244045107257344e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
