{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 29965,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016686133822793258,
      "grad_norm": 3.79904842376709,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 2.2711,
      "step": 10
    },
    {
      "epoch": 0.0033372267645586516,
      "grad_norm": 7.182623386383057,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 2.1339,
      "step": 20
    },
    {
      "epoch": 0.005005840146837978,
      "grad_norm": 3.6051793098449707,
      "learning_rate": 3e-06,
      "loss": 1.34,
      "step": 30
    },
    {
      "epoch": 0.006674453529117303,
      "grad_norm": 10.527331352233887,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.6011,
      "step": 40
    },
    {
      "epoch": 0.00834306691139663,
      "grad_norm": 9.845771789550781,
      "learning_rate": 5e-06,
      "loss": 1.7923,
      "step": 50
    },
    {
      "epoch": 0.010011680293675955,
      "grad_norm": 5.402078151702881,
      "learning_rate": 6e-06,
      "loss": 1.4371,
      "step": 60
    },
    {
      "epoch": 0.011680293675955281,
      "grad_norm": 6.919439315795898,
      "learning_rate": 7.000000000000001e-06,
      "loss": 1.2766,
      "step": 70
    },
    {
      "epoch": 0.013348907058234607,
      "grad_norm": 3.590498447418213,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.7382,
      "step": 80
    },
    {
      "epoch": 0.015017520440513932,
      "grad_norm": 4.093526840209961,
      "learning_rate": 9e-06,
      "loss": 1.9864,
      "step": 90
    },
    {
      "epoch": 0.01668613382279326,
      "grad_norm": 3.692473888397217,
      "learning_rate": 1e-05,
      "loss": 2.0114,
      "step": 100
    },
    {
      "epoch": 0.018354747205072585,
      "grad_norm": 4.630216121673584,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.1169,
      "step": 110
    },
    {
      "epoch": 0.02002336058735191,
      "grad_norm": 5.28784704208374,
      "learning_rate": 1.2e-05,
      "loss": 2.0046,
      "step": 120
    },
    {
      "epoch": 0.021691973969631236,
      "grad_norm": 5.671827793121338,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 2.0312,
      "step": 130
    },
    {
      "epoch": 0.023360587351910562,
      "grad_norm": 8.2243013381958,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.5622,
      "step": 140
    },
    {
      "epoch": 0.025029200734189887,
      "grad_norm": 2.4041430950164795,
      "learning_rate": 1.5e-05,
      "loss": 1.461,
      "step": 150
    },
    {
      "epoch": 0.026697814116469213,
      "grad_norm": 4.042627334594727,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.7078,
      "step": 160
    },
    {
      "epoch": 0.02836642749874854,
      "grad_norm": 5.533896446228027,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 1.6739,
      "step": 170
    },
    {
      "epoch": 0.030035040881027864,
      "grad_norm": 6.103790760040283,
      "learning_rate": 1.8e-05,
      "loss": 1.2197,
      "step": 180
    },
    {
      "epoch": 0.03170365426330719,
      "grad_norm": 3.236377000808716,
      "learning_rate": 1.9e-05,
      "loss": 1.1634,
      "step": 190
    },
    {
      "epoch": 0.03337226764558652,
      "grad_norm": 3.6233606338500977,
      "learning_rate": 2e-05,
      "loss": 1.4596,
      "step": 200
    },
    {
      "epoch": 0.035040881027865844,
      "grad_norm": 2.819995403289795,
      "learning_rate": 2.1e-05,
      "loss": 0.883,
      "step": 210
    },
    {
      "epoch": 0.03670949441014517,
      "grad_norm": 3.301626443862915,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.9302,
      "step": 220
    },
    {
      "epoch": 0.038378107792424496,
      "grad_norm": 5.836217403411865,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 1.3534,
      "step": 230
    },
    {
      "epoch": 0.04004672117470382,
      "grad_norm": 2.4070804119110107,
      "learning_rate": 2.4e-05,
      "loss": 1.6678,
      "step": 240
    },
    {
      "epoch": 0.04171533455698315,
      "grad_norm": 5.188549041748047,
      "learning_rate": 2.5e-05,
      "loss": 1.5615,
      "step": 250
    },
    {
      "epoch": 0.04338394793926247,
      "grad_norm": 3.4946234226226807,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.1306,
      "step": 260
    },
    {
      "epoch": 0.0450525613215418,
      "grad_norm": 3.4348223209381104,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.9766,
      "step": 270
    },
    {
      "epoch": 0.046721174703821124,
      "grad_norm": 6.687024116516113,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.2911,
      "step": 280
    },
    {
      "epoch": 0.04838978808610045,
      "grad_norm": 8.801347732543945,
      "learning_rate": 2.9e-05,
      "loss": 1.1132,
      "step": 290
    },
    {
      "epoch": 0.050058401468379775,
      "grad_norm": 4.196038722991943,
      "learning_rate": 3e-05,
      "loss": 1.1248,
      "step": 300
    },
    {
      "epoch": 0.0517270148506591,
      "grad_norm": 2.8185057640075684,
      "learning_rate": 3.1e-05,
      "loss": 1.5625,
      "step": 310
    },
    {
      "epoch": 0.053395628232938426,
      "grad_norm": 7.821621417999268,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.1737,
      "step": 320
    },
    {
      "epoch": 0.05506424161521775,
      "grad_norm": 3.4825820922851562,
      "learning_rate": 3.3e-05,
      "loss": 1.2027,
      "step": 330
    },
    {
      "epoch": 0.05673285499749708,
      "grad_norm": 3.456397294998169,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.8523,
      "step": 340
    },
    {
      "epoch": 0.0584014683797764,
      "grad_norm": 6.144533634185791,
      "learning_rate": 3.5e-05,
      "loss": 1.1343,
      "step": 350
    },
    {
      "epoch": 0.06007008176205573,
      "grad_norm": 3.0288727283477783,
      "learning_rate": 3.6e-05,
      "loss": 1.0722,
      "step": 360
    },
    {
      "epoch": 0.06173869514433506,
      "grad_norm": 2.38199520111084,
      "learning_rate": 3.7e-05,
      "loss": 1.2691,
      "step": 370
    },
    {
      "epoch": 0.06340730852661439,
      "grad_norm": 9.471896171569824,
      "learning_rate": 3.8e-05,
      "loss": 1.1728,
      "step": 380
    },
    {
      "epoch": 0.0650759219088937,
      "grad_norm": 5.951301574707031,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.0393,
      "step": 390
    },
    {
      "epoch": 0.06674453529117304,
      "grad_norm": 4.838334560394287,
      "learning_rate": 4e-05,
      "loss": 0.6978,
      "step": 400
    },
    {
      "epoch": 0.06841314867345236,
      "grad_norm": 5.813498497009277,
      "learning_rate": 4.1e-05,
      "loss": 0.782,
      "step": 410
    },
    {
      "epoch": 0.07008176205573169,
      "grad_norm": 6.75109338760376,
      "learning_rate": 4.2e-05,
      "loss": 1.1449,
      "step": 420
    },
    {
      "epoch": 0.07175037543801101,
      "grad_norm": 1.5278546810150146,
      "learning_rate": 4.3e-05,
      "loss": 0.8661,
      "step": 430
    },
    {
      "epoch": 0.07341898882029034,
      "grad_norm": 4.583148002624512,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.058,
      "step": 440
    },
    {
      "epoch": 0.07508760220256966,
      "grad_norm": 7.415335178375244,
      "learning_rate": 4.5e-05,
      "loss": 1.0257,
      "step": 450
    },
    {
      "epoch": 0.07675621558484899,
      "grad_norm": 7.909468173980713,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.1085,
      "step": 460
    },
    {
      "epoch": 0.07842482896712831,
      "grad_norm": 3.7046473026275635,
      "learning_rate": 4.7e-05,
      "loss": 0.9718,
      "step": 470
    },
    {
      "epoch": 0.08009344234940764,
      "grad_norm": 5.0828118324279785,
      "learning_rate": 4.8e-05,
      "loss": 0.7885,
      "step": 480
    },
    {
      "epoch": 0.08176205573168698,
      "grad_norm": 1.0030765533447266,
      "learning_rate": 4.9e-05,
      "loss": 0.8161,
      "step": 490
    },
    {
      "epoch": 0.0834306691139663,
      "grad_norm": 14.221755027770996,
      "learning_rate": 5e-05,
      "loss": 0.8607,
      "step": 500
    },
    {
      "epoch": 0.08509928249624563,
      "grad_norm": 3.024747133255005,
      "learning_rate": 4.998303071440693e-05,
      "loss": 0.8952,
      "step": 510
    },
    {
      "epoch": 0.08676789587852494,
      "grad_norm": 8.669234275817871,
      "learning_rate": 4.996606142881385e-05,
      "loss": 0.782,
      "step": 520
    },
    {
      "epoch": 0.08843650926080428,
      "grad_norm": 5.924846649169922,
      "learning_rate": 4.994909214322077e-05,
      "loss": 1.2535,
      "step": 530
    },
    {
      "epoch": 0.0901051226430836,
      "grad_norm": 3.6464028358459473,
      "learning_rate": 4.993212285762769e-05,
      "loss": 0.8856,
      "step": 540
    },
    {
      "epoch": 0.09177373602536293,
      "grad_norm": 6.805990219116211,
      "learning_rate": 4.991515357203462e-05,
      "loss": 1.1457,
      "step": 550
    },
    {
      "epoch": 0.09344234940764225,
      "grad_norm": 5.023440837860107,
      "learning_rate": 4.989818428644154e-05,
      "loss": 0.6686,
      "step": 560
    },
    {
      "epoch": 0.09511096278992158,
      "grad_norm": 7.8002753257751465,
      "learning_rate": 4.988121500084847e-05,
      "loss": 1.3947,
      "step": 570
    },
    {
      "epoch": 0.0967795761722009,
      "grad_norm": 6.081882953643799,
      "learning_rate": 4.986424571525539e-05,
      "loss": 0.7415,
      "step": 580
    },
    {
      "epoch": 0.09844818955448023,
      "grad_norm": 3.987962007522583,
      "learning_rate": 4.9847276429662315e-05,
      "loss": 0.8556,
      "step": 590
    },
    {
      "epoch": 0.10011680293675955,
      "grad_norm": 8.489911079406738,
      "learning_rate": 4.983030714406924e-05,
      "loss": 0.7891,
      "step": 600
    },
    {
      "epoch": 0.10178541631903888,
      "grad_norm": 7.404081344604492,
      "learning_rate": 4.981333785847616e-05,
      "loss": 1.1191,
      "step": 610
    },
    {
      "epoch": 0.1034540297013182,
      "grad_norm": 4.346790313720703,
      "learning_rate": 4.9796368572883086e-05,
      "loss": 0.8707,
      "step": 620
    },
    {
      "epoch": 0.10512264308359753,
      "grad_norm": 7.410800933837891,
      "learning_rate": 4.9779399287290005e-05,
      "loss": 0.9092,
      "step": 630
    },
    {
      "epoch": 0.10679125646587685,
      "grad_norm": 2.9268417358398438,
      "learning_rate": 4.976243000169693e-05,
      "loss": 1.0369,
      "step": 640
    },
    {
      "epoch": 0.10845986984815618,
      "grad_norm": 4.379398822784424,
      "learning_rate": 4.974546071610386e-05,
      "loss": 0.8231,
      "step": 650
    },
    {
      "epoch": 0.1101284832304355,
      "grad_norm": 5.61354923248291,
      "learning_rate": 4.9728491430510776e-05,
      "loss": 0.9985,
      "step": 660
    },
    {
      "epoch": 0.11179709661271484,
      "grad_norm": 6.251413822174072,
      "learning_rate": 4.97115221449177e-05,
      "loss": 1.087,
      "step": 670
    },
    {
      "epoch": 0.11346570999499415,
      "grad_norm": 4.300239086151123,
      "learning_rate": 4.969455285932462e-05,
      "loss": 0.9326,
      "step": 680
    },
    {
      "epoch": 0.11513432337727349,
      "grad_norm": 1.7682503461837769,
      "learning_rate": 4.967758357373155e-05,
      "loss": 0.9457,
      "step": 690
    },
    {
      "epoch": 0.1168029367595528,
      "grad_norm": 6.0161309242248535,
      "learning_rate": 4.966061428813847e-05,
      "loss": 0.7711,
      "step": 700
    },
    {
      "epoch": 0.11847155014183214,
      "grad_norm": 7.376018047332764,
      "learning_rate": 4.96436450025454e-05,
      "loss": 0.7476,
      "step": 710
    },
    {
      "epoch": 0.12014016352411146,
      "grad_norm": 5.102439880371094,
      "learning_rate": 4.962667571695232e-05,
      "loss": 0.7983,
      "step": 720
    },
    {
      "epoch": 0.12180877690639079,
      "grad_norm": 5.592205047607422,
      "learning_rate": 4.960970643135924e-05,
      "loss": 0.7777,
      "step": 730
    },
    {
      "epoch": 0.12347739028867012,
      "grad_norm": 10.7653169631958,
      "learning_rate": 4.959273714576617e-05,
      "loss": 1.2737,
      "step": 740
    },
    {
      "epoch": 0.12514600367094944,
      "grad_norm": 4.526876926422119,
      "learning_rate": 4.957576786017309e-05,
      "loss": 1.1343,
      "step": 750
    },
    {
      "epoch": 0.12681461705322877,
      "grad_norm": 4.812863826751709,
      "learning_rate": 4.9558798574580014e-05,
      "loss": 0.9902,
      "step": 760
    },
    {
      "epoch": 0.1284832304355081,
      "grad_norm": 7.757281303405762,
      "learning_rate": 4.954182928898693e-05,
      "loss": 0.9167,
      "step": 770
    },
    {
      "epoch": 0.1301518438177874,
      "grad_norm": 6.603519916534424,
      "learning_rate": 4.952486000339386e-05,
      "loss": 0.975,
      "step": 780
    },
    {
      "epoch": 0.13182045720006674,
      "grad_norm": 7.927140712738037,
      "learning_rate": 4.950789071780078e-05,
      "loss": 1.0208,
      "step": 790
    },
    {
      "epoch": 0.13348907058234608,
      "grad_norm": 3.15924072265625,
      "learning_rate": 4.9490921432207704e-05,
      "loss": 0.8477,
      "step": 800
    },
    {
      "epoch": 0.1351576839646254,
      "grad_norm": 4.649066925048828,
      "learning_rate": 4.947395214661463e-05,
      "loss": 1.0416,
      "step": 810
    },
    {
      "epoch": 0.1368262973469047,
      "grad_norm": 13.645825386047363,
      "learning_rate": 4.9456982861021556e-05,
      "loss": 0.8851,
      "step": 820
    },
    {
      "epoch": 0.13849491072918405,
      "grad_norm": 1.7864187955856323,
      "learning_rate": 4.944001357542848e-05,
      "loss": 0.6225,
      "step": 830
    },
    {
      "epoch": 0.14016352411146338,
      "grad_norm": 6.079190731048584,
      "learning_rate": 4.94230442898354e-05,
      "loss": 0.8376,
      "step": 840
    },
    {
      "epoch": 0.1418321374937427,
      "grad_norm": 7.027696132659912,
      "learning_rate": 4.940607500424233e-05,
      "loss": 1.2121,
      "step": 850
    },
    {
      "epoch": 0.14350075087602202,
      "grad_norm": 6.695984840393066,
      "learning_rate": 4.9389105718649246e-05,
      "loss": 0.965,
      "step": 860
    },
    {
      "epoch": 0.14516936425830135,
      "grad_norm": 7.41980504989624,
      "learning_rate": 4.937213643305617e-05,
      "loss": 1.0054,
      "step": 870
    },
    {
      "epoch": 0.14683797764058068,
      "grad_norm": 6.862626075744629,
      "learning_rate": 4.935516714746309e-05,
      "loss": 0.9562,
      "step": 880
    },
    {
      "epoch": 0.14850659102286,
      "grad_norm": 6.646551609039307,
      "learning_rate": 4.933819786187002e-05,
      "loss": 0.868,
      "step": 890
    },
    {
      "epoch": 0.15017520440513932,
      "grad_norm": 5.551462650299072,
      "learning_rate": 4.932122857627694e-05,
      "loss": 0.7635,
      "step": 900
    },
    {
      "epoch": 0.15184381778741865,
      "grad_norm": 1.2757699489593506,
      "learning_rate": 4.930425929068386e-05,
      "loss": 0.8866,
      "step": 910
    },
    {
      "epoch": 0.15351243116969798,
      "grad_norm": 9.283583641052246,
      "learning_rate": 4.928729000509079e-05,
      "loss": 0.8781,
      "step": 920
    },
    {
      "epoch": 0.15518104455197732,
      "grad_norm": 8.223098754882812,
      "learning_rate": 4.9270320719497707e-05,
      "loss": 0.8231,
      "step": 930
    },
    {
      "epoch": 0.15684965793425662,
      "grad_norm": 6.533298015594482,
      "learning_rate": 4.925335143390464e-05,
      "loss": 0.885,
      "step": 940
    },
    {
      "epoch": 0.15851827131653595,
      "grad_norm": 10.064355850219727,
      "learning_rate": 4.923638214831156e-05,
      "loss": 1.0058,
      "step": 950
    },
    {
      "epoch": 0.16018688469881529,
      "grad_norm": 3.179572582244873,
      "learning_rate": 4.9219412862718484e-05,
      "loss": 1.0824,
      "step": 960
    },
    {
      "epoch": 0.16185549808109462,
      "grad_norm": 6.268158435821533,
      "learning_rate": 4.92024435771254e-05,
      "loss": 0.8083,
      "step": 970
    },
    {
      "epoch": 0.16352411146337395,
      "grad_norm": 7.978918552398682,
      "learning_rate": 4.918547429153233e-05,
      "loss": 0.8201,
      "step": 980
    },
    {
      "epoch": 0.16519272484565325,
      "grad_norm": 7.36130952835083,
      "learning_rate": 4.9168505005939255e-05,
      "loss": 0.8972,
      "step": 990
    },
    {
      "epoch": 0.1668613382279326,
      "grad_norm": 6.709752559661865,
      "learning_rate": 4.9151535720346174e-05,
      "loss": 1.0784,
      "step": 1000
    },
    {
      "epoch": 0.16852995161021192,
      "grad_norm": 3.5908749103546143,
      "learning_rate": 4.91345664347531e-05,
      "loss": 0.8012,
      "step": 1010
    },
    {
      "epoch": 0.17019856499249125,
      "grad_norm": 6.520646572113037,
      "learning_rate": 4.911759714916002e-05,
      "loss": 0.7217,
      "step": 1020
    },
    {
      "epoch": 0.17186717837477056,
      "grad_norm": 9.398969650268555,
      "learning_rate": 4.9100627863566945e-05,
      "loss": 0.6783,
      "step": 1030
    },
    {
      "epoch": 0.1735357917570499,
      "grad_norm": 4.233824253082275,
      "learning_rate": 4.908365857797387e-05,
      "loss": 0.7253,
      "step": 1040
    },
    {
      "epoch": 0.17520440513932922,
      "grad_norm": 7.398596286773682,
      "learning_rate": 4.906668929238079e-05,
      "loss": 0.7637,
      "step": 1050
    },
    {
      "epoch": 0.17687301852160855,
      "grad_norm": 6.739957809448242,
      "learning_rate": 4.9049720006787716e-05,
      "loss": 0.8031,
      "step": 1060
    },
    {
      "epoch": 0.17854163190388786,
      "grad_norm": 7.9486985206604,
      "learning_rate": 4.903275072119464e-05,
      "loss": 0.9621,
      "step": 1070
    },
    {
      "epoch": 0.1802102452861672,
      "grad_norm": 7.202178955078125,
      "learning_rate": 4.901578143560157e-05,
      "loss": 0.9566,
      "step": 1080
    },
    {
      "epoch": 0.18187885866844652,
      "grad_norm": 6.278532028198242,
      "learning_rate": 4.899881215000849e-05,
      "loss": 0.9441,
      "step": 1090
    },
    {
      "epoch": 0.18354747205072586,
      "grad_norm": 5.335439205169678,
      "learning_rate": 4.898184286441541e-05,
      "loss": 0.6441,
      "step": 1100
    },
    {
      "epoch": 0.18521608543300516,
      "grad_norm": 4.642460346221924,
      "learning_rate": 4.896487357882233e-05,
      "loss": 0.7104,
      "step": 1110
    },
    {
      "epoch": 0.1868846988152845,
      "grad_norm": 8.298638343811035,
      "learning_rate": 4.894790429322926e-05,
      "loss": 1.1014,
      "step": 1120
    },
    {
      "epoch": 0.18855331219756383,
      "grad_norm": 4.595211982727051,
      "learning_rate": 4.8930935007636183e-05,
      "loss": 0.7587,
      "step": 1130
    },
    {
      "epoch": 0.19022192557984316,
      "grad_norm": 3.740720748901367,
      "learning_rate": 4.89139657220431e-05,
      "loss": 0.9371,
      "step": 1140
    },
    {
      "epoch": 0.19189053896212246,
      "grad_norm": 9.767218589782715,
      "learning_rate": 4.889699643645003e-05,
      "loss": 0.6969,
      "step": 1150
    },
    {
      "epoch": 0.1935591523444018,
      "grad_norm": 7.882074356079102,
      "learning_rate": 4.888002715085695e-05,
      "loss": 0.4989,
      "step": 1160
    },
    {
      "epoch": 0.19522776572668113,
      "grad_norm": 8.713257789611816,
      "learning_rate": 4.886305786526387e-05,
      "loss": 0.6804,
      "step": 1170
    },
    {
      "epoch": 0.19689637910896046,
      "grad_norm": 6.494941711425781,
      "learning_rate": 4.884608857967079e-05,
      "loss": 0.6599,
      "step": 1180
    },
    {
      "epoch": 0.19856499249123977,
      "grad_norm": 5.320250511169434,
      "learning_rate": 4.8829119294077725e-05,
      "loss": 1.2213,
      "step": 1190
    },
    {
      "epoch": 0.2002336058735191,
      "grad_norm": 6.940951824188232,
      "learning_rate": 4.8812150008484644e-05,
      "loss": 1.151,
      "step": 1200
    },
    {
      "epoch": 0.20190221925579843,
      "grad_norm": 3.36543869972229,
      "learning_rate": 4.879518072289157e-05,
      "loss": 0.9777,
      "step": 1210
    },
    {
      "epoch": 0.20357083263807776,
      "grad_norm": 3.555760145187378,
      "learning_rate": 4.8778211437298496e-05,
      "loss": 0.9639,
      "step": 1220
    },
    {
      "epoch": 0.2052394460203571,
      "grad_norm": 7.245345592498779,
      "learning_rate": 4.8761242151705415e-05,
      "loss": 0.8724,
      "step": 1230
    },
    {
      "epoch": 0.2069080594026364,
      "grad_norm": 6.3500189781188965,
      "learning_rate": 4.874427286611234e-05,
      "loss": 0.7266,
      "step": 1240
    },
    {
      "epoch": 0.20857667278491573,
      "grad_norm": 7.472283363342285,
      "learning_rate": 4.872730358051926e-05,
      "loss": 0.7371,
      "step": 1250
    },
    {
      "epoch": 0.21024528616719507,
      "grad_norm": 7.047811985015869,
      "learning_rate": 4.8710334294926186e-05,
      "loss": 0.6164,
      "step": 1260
    },
    {
      "epoch": 0.2119138995494744,
      "grad_norm": 6.9337639808654785,
      "learning_rate": 4.869336500933311e-05,
      "loss": 0.7654,
      "step": 1270
    },
    {
      "epoch": 0.2135825129317537,
      "grad_norm": 15.973174095153809,
      "learning_rate": 4.867639572374003e-05,
      "loss": 1.0935,
      "step": 1280
    },
    {
      "epoch": 0.21525112631403304,
      "grad_norm": 8.758491516113281,
      "learning_rate": 4.865942643814696e-05,
      "loss": 0.7974,
      "step": 1290
    },
    {
      "epoch": 0.21691973969631237,
      "grad_norm": 1.8754631280899048,
      "learning_rate": 4.8642457152553876e-05,
      "loss": 0.5935,
      "step": 1300
    },
    {
      "epoch": 0.2185883530785917,
      "grad_norm": 7.009585857391357,
      "learning_rate": 4.862548786696081e-05,
      "loss": 1.0534,
      "step": 1310
    },
    {
      "epoch": 0.220256966460871,
      "grad_norm": 4.205259799957275,
      "learning_rate": 4.860851858136773e-05,
      "loss": 0.9154,
      "step": 1320
    },
    {
      "epoch": 0.22192557984315034,
      "grad_norm": 6.721474647521973,
      "learning_rate": 4.8591549295774653e-05,
      "loss": 0.6583,
      "step": 1330
    },
    {
      "epoch": 0.22359419322542967,
      "grad_norm": 3.5803515911102295,
      "learning_rate": 4.857458001018157e-05,
      "loss": 0.8278,
      "step": 1340
    },
    {
      "epoch": 0.225262806607709,
      "grad_norm": 4.124467849731445,
      "learning_rate": 4.85576107245885e-05,
      "loss": 0.5003,
      "step": 1350
    },
    {
      "epoch": 0.2269314199899883,
      "grad_norm": 10.031257629394531,
      "learning_rate": 4.8540641438995424e-05,
      "loss": 0.9419,
      "step": 1360
    },
    {
      "epoch": 0.22860003337226764,
      "grad_norm": 7.770595073699951,
      "learning_rate": 4.8523672153402343e-05,
      "loss": 0.9462,
      "step": 1370
    },
    {
      "epoch": 0.23026864675454697,
      "grad_norm": 6.804042339324951,
      "learning_rate": 4.850670286780927e-05,
      "loss": 0.862,
      "step": 1380
    },
    {
      "epoch": 0.2319372601368263,
      "grad_norm": 3.2381014823913574,
      "learning_rate": 4.848973358221619e-05,
      "loss": 0.8612,
      "step": 1390
    },
    {
      "epoch": 0.2336058735191056,
      "grad_norm": 5.524078845977783,
      "learning_rate": 4.8472764296623114e-05,
      "loss": 0.7656,
      "step": 1400
    },
    {
      "epoch": 0.23527448690138494,
      "grad_norm": 5.035533905029297,
      "learning_rate": 4.845579501103003e-05,
      "loss": 1.071,
      "step": 1410
    },
    {
      "epoch": 0.23694310028366428,
      "grad_norm": 9.536332130432129,
      "learning_rate": 4.843882572543696e-05,
      "loss": 0.6668,
      "step": 1420
    },
    {
      "epoch": 0.2386117136659436,
      "grad_norm": 5.131458282470703,
      "learning_rate": 4.8421856439843885e-05,
      "loss": 1.0141,
      "step": 1430
    },
    {
      "epoch": 0.2402803270482229,
      "grad_norm": 5.364581108093262,
      "learning_rate": 4.840488715425081e-05,
      "loss": 1.0744,
      "step": 1440
    },
    {
      "epoch": 0.24194894043050225,
      "grad_norm": 10.462005615234375,
      "learning_rate": 4.838791786865774e-05,
      "loss": 0.7962,
      "step": 1450
    },
    {
      "epoch": 0.24361755381278158,
      "grad_norm": 3.568871021270752,
      "learning_rate": 4.8370948583064656e-05,
      "loss": 0.8062,
      "step": 1460
    },
    {
      "epoch": 0.2452861671950609,
      "grad_norm": 5.848616600036621,
      "learning_rate": 4.835397929747158e-05,
      "loss": 0.8054,
      "step": 1470
    },
    {
      "epoch": 0.24695478057734024,
      "grad_norm": 2.287451982498169,
      "learning_rate": 4.83370100118785e-05,
      "loss": 0.7035,
      "step": 1480
    },
    {
      "epoch": 0.24862339395961955,
      "grad_norm": 7.200664043426514,
      "learning_rate": 4.832004072628543e-05,
      "loss": 0.8759,
      "step": 1490
    },
    {
      "epoch": 0.2502920073418989,
      "grad_norm": 4.867833614349365,
      "learning_rate": 4.8303071440692346e-05,
      "loss": 0.8476,
      "step": 1500
    },
    {
      "epoch": 0.2519606207241782,
      "grad_norm": 4.327005863189697,
      "learning_rate": 4.828610215509927e-05,
      "loss": 0.877,
      "step": 1510
    },
    {
      "epoch": 0.25362923410645755,
      "grad_norm": 7.988686561584473,
      "learning_rate": 4.82691328695062e-05,
      "loss": 0.7017,
      "step": 1520
    },
    {
      "epoch": 0.25529784748873685,
      "grad_norm": 3.5522189140319824,
      "learning_rate": 4.825216358391312e-05,
      "loss": 0.8189,
      "step": 1530
    },
    {
      "epoch": 0.2569664608710162,
      "grad_norm": 9.3996000289917,
      "learning_rate": 4.823519429832004e-05,
      "loss": 0.8931,
      "step": 1540
    },
    {
      "epoch": 0.2586350742532955,
      "grad_norm": 6.500752925872803,
      "learning_rate": 4.821822501272696e-05,
      "loss": 1.0194,
      "step": 1550
    },
    {
      "epoch": 0.2603036876355748,
      "grad_norm": 5.607722282409668,
      "learning_rate": 4.8201255727133894e-05,
      "loss": 0.841,
      "step": 1560
    },
    {
      "epoch": 0.2619723010178542,
      "grad_norm": 7.975852966308594,
      "learning_rate": 4.8184286441540813e-05,
      "loss": 0.9532,
      "step": 1570
    },
    {
      "epoch": 0.2636409144001335,
      "grad_norm": 4.294488906860352,
      "learning_rate": 4.816731715594774e-05,
      "loss": 0.5532,
      "step": 1580
    },
    {
      "epoch": 0.2653095277824128,
      "grad_norm": 5.220550537109375,
      "learning_rate": 4.815034787035466e-05,
      "loss": 1.0821,
      "step": 1590
    },
    {
      "epoch": 0.26697814116469215,
      "grad_norm": 10.952414512634277,
      "learning_rate": 4.8133378584761584e-05,
      "loss": 0.9575,
      "step": 1600
    },
    {
      "epoch": 0.26864675454697146,
      "grad_norm": 3.701032876968384,
      "learning_rate": 4.811640929916851e-05,
      "loss": 0.8681,
      "step": 1610
    },
    {
      "epoch": 0.2703153679292508,
      "grad_norm": 4.70545768737793,
      "learning_rate": 4.809944001357543e-05,
      "loss": 0.9569,
      "step": 1620
    },
    {
      "epoch": 0.2719839813115301,
      "grad_norm": 7.7636284828186035,
      "learning_rate": 4.8082470727982355e-05,
      "loss": 0.7812,
      "step": 1630
    },
    {
      "epoch": 0.2736525946938094,
      "grad_norm": 6.987236022949219,
      "learning_rate": 4.8065501442389274e-05,
      "loss": 0.7828,
      "step": 1640
    },
    {
      "epoch": 0.2753212080760888,
      "grad_norm": 4.573073863983154,
      "learning_rate": 4.80485321567962e-05,
      "loss": 0.9015,
      "step": 1650
    },
    {
      "epoch": 0.2769898214583681,
      "grad_norm": 3.060145854949951,
      "learning_rate": 4.8031562871203126e-05,
      "loss": 0.6581,
      "step": 1660
    },
    {
      "epoch": 0.2786584348406474,
      "grad_norm": 9.09227466583252,
      "learning_rate": 4.8014593585610045e-05,
      "loss": 0.9648,
      "step": 1670
    },
    {
      "epoch": 0.28032704822292676,
      "grad_norm": 7.815892219543457,
      "learning_rate": 4.799762430001697e-05,
      "loss": 0.8771,
      "step": 1680
    },
    {
      "epoch": 0.28199566160520606,
      "grad_norm": 8.1954984664917,
      "learning_rate": 4.79806550144239e-05,
      "loss": 0.4498,
      "step": 1690
    },
    {
      "epoch": 0.2836642749874854,
      "grad_norm": 4.973302364349365,
      "learning_rate": 4.796368572883082e-05,
      "loss": 0.9246,
      "step": 1700
    },
    {
      "epoch": 0.2853328883697647,
      "grad_norm": 8.766337394714355,
      "learning_rate": 4.794671644323774e-05,
      "loss": 0.5424,
      "step": 1710
    },
    {
      "epoch": 0.28700150175204403,
      "grad_norm": 5.705307960510254,
      "learning_rate": 4.792974715764467e-05,
      "loss": 1.1216,
      "step": 1720
    },
    {
      "epoch": 0.2886701151343234,
      "grad_norm": 5.839878082275391,
      "learning_rate": 4.791277787205159e-05,
      "loss": 0.5954,
      "step": 1730
    },
    {
      "epoch": 0.2903387285166027,
      "grad_norm": 6.5529465675354,
      "learning_rate": 4.789580858645851e-05,
      "loss": 0.8163,
      "step": 1740
    },
    {
      "epoch": 0.29200734189888206,
      "grad_norm": 8.933732986450195,
      "learning_rate": 4.787883930086544e-05,
      "loss": 0.8461,
      "step": 1750
    },
    {
      "epoch": 0.29367595528116136,
      "grad_norm": 4.6097187995910645,
      "learning_rate": 4.786187001527236e-05,
      "loss": 0.5946,
      "step": 1760
    },
    {
      "epoch": 0.29534456866344067,
      "grad_norm": 7.7182183265686035,
      "learning_rate": 4.7844900729679284e-05,
      "loss": 0.6555,
      "step": 1770
    },
    {
      "epoch": 0.29701318204572,
      "grad_norm": 2.280984878540039,
      "learning_rate": 4.78279314440862e-05,
      "loss": 0.7076,
      "step": 1780
    },
    {
      "epoch": 0.29868179542799933,
      "grad_norm": 8.5516357421875,
      "learning_rate": 4.781096215849313e-05,
      "loss": 0.9988,
      "step": 1790
    },
    {
      "epoch": 0.30035040881027864,
      "grad_norm": 4.809074878692627,
      "learning_rate": 4.779399287290005e-05,
      "loss": 0.8486,
      "step": 1800
    },
    {
      "epoch": 0.302019022192558,
      "grad_norm": 3.1991074085235596,
      "learning_rate": 4.777702358730698e-05,
      "loss": 0.6643,
      "step": 1810
    },
    {
      "epoch": 0.3036876355748373,
      "grad_norm": 7.109113693237305,
      "learning_rate": 4.77600543017139e-05,
      "loss": 0.7072,
      "step": 1820
    },
    {
      "epoch": 0.30535624895711666,
      "grad_norm": 5.751452922821045,
      "learning_rate": 4.7743085016120825e-05,
      "loss": 0.4635,
      "step": 1830
    },
    {
      "epoch": 0.30702486233939597,
      "grad_norm": 6.025784015655518,
      "learning_rate": 4.772611573052775e-05,
      "loss": 0.7978,
      "step": 1840
    },
    {
      "epoch": 0.30869347572167527,
      "grad_norm": 7.925596714019775,
      "learning_rate": 4.770914644493467e-05,
      "loss": 0.7224,
      "step": 1850
    },
    {
      "epoch": 0.31036208910395463,
      "grad_norm": 2.901942729949951,
      "learning_rate": 4.7692177159341596e-05,
      "loss": 0.8664,
      "step": 1860
    },
    {
      "epoch": 0.31203070248623394,
      "grad_norm": 4.568856239318848,
      "learning_rate": 4.7675207873748515e-05,
      "loss": 0.8431,
      "step": 1870
    },
    {
      "epoch": 0.31369931586851324,
      "grad_norm": 3.7095916271209717,
      "learning_rate": 4.765823858815544e-05,
      "loss": 0.6565,
      "step": 1880
    },
    {
      "epoch": 0.3153679292507926,
      "grad_norm": 6.966523170471191,
      "learning_rate": 4.764126930256237e-05,
      "loss": 0.6631,
      "step": 1890
    },
    {
      "epoch": 0.3170365426330719,
      "grad_norm": 3.690739631652832,
      "learning_rate": 4.7624300016969286e-05,
      "loss": 1.0129,
      "step": 1900
    },
    {
      "epoch": 0.31870515601535127,
      "grad_norm": 10.0293550491333,
      "learning_rate": 4.760733073137621e-05,
      "loss": 0.8941,
      "step": 1910
    },
    {
      "epoch": 0.32037376939763057,
      "grad_norm": 5.941582679748535,
      "learning_rate": 4.759036144578313e-05,
      "loss": 0.727,
      "step": 1920
    },
    {
      "epoch": 0.3220423827799099,
      "grad_norm": 7.233775615692139,
      "learning_rate": 4.757339216019006e-05,
      "loss": 0.8189,
      "step": 1930
    },
    {
      "epoch": 0.32371099616218924,
      "grad_norm": 6.904343605041504,
      "learning_rate": 4.755642287459698e-05,
      "loss": 0.9145,
      "step": 1940
    },
    {
      "epoch": 0.32537960954446854,
      "grad_norm": 7.455501079559326,
      "learning_rate": 4.753945358900391e-05,
      "loss": 0.639,
      "step": 1950
    },
    {
      "epoch": 0.3270482229267479,
      "grad_norm": 2.02082896232605,
      "learning_rate": 4.752248430341083e-05,
      "loss": 0.62,
      "step": 1960
    },
    {
      "epoch": 0.3287168363090272,
      "grad_norm": 0.8728573322296143,
      "learning_rate": 4.7505515017817754e-05,
      "loss": 0.4651,
      "step": 1970
    },
    {
      "epoch": 0.3303854496913065,
      "grad_norm": 6.1950836181640625,
      "learning_rate": 4.748854573222468e-05,
      "loss": 1.0393,
      "step": 1980
    },
    {
      "epoch": 0.33205406307358587,
      "grad_norm": 6.6434712409973145,
      "learning_rate": 4.74715764466316e-05,
      "loss": 0.5031,
      "step": 1990
    },
    {
      "epoch": 0.3337226764558652,
      "grad_norm": 5.445943355560303,
      "learning_rate": 4.7454607161038524e-05,
      "loss": 0.5834,
      "step": 2000
    },
    {
      "epoch": 0.3353912898381445,
      "grad_norm": 2.0814809799194336,
      "learning_rate": 4.7437637875445444e-05,
      "loss": 0.7365,
      "step": 2010
    },
    {
      "epoch": 0.33705990322042384,
      "grad_norm": 8.248190879821777,
      "learning_rate": 4.742066858985237e-05,
      "loss": 0.5451,
      "step": 2020
    },
    {
      "epoch": 0.33872851660270314,
      "grad_norm": 5.852273464202881,
      "learning_rate": 4.740369930425929e-05,
      "loss": 0.6407,
      "step": 2030
    },
    {
      "epoch": 0.3403971299849825,
      "grad_norm": 10.305346488952637,
      "learning_rate": 4.7386730018666214e-05,
      "loss": 0.7857,
      "step": 2040
    },
    {
      "epoch": 0.3420657433672618,
      "grad_norm": 3.9916467666625977,
      "learning_rate": 4.736976073307314e-05,
      "loss": 0.595,
      "step": 2050
    },
    {
      "epoch": 0.3437343567495411,
      "grad_norm": 7.858857154846191,
      "learning_rate": 4.7352791447480066e-05,
      "loss": 0.8684,
      "step": 2060
    },
    {
      "epoch": 0.3454029701318205,
      "grad_norm": 4.695647716522217,
      "learning_rate": 4.733582216188699e-05,
      "loss": 0.5155,
      "step": 2070
    },
    {
      "epoch": 0.3470715835140998,
      "grad_norm": 3.2922067642211914,
      "learning_rate": 4.731885287629391e-05,
      "loss": 0.9256,
      "step": 2080
    },
    {
      "epoch": 0.3487401968963791,
      "grad_norm": 8.713995933532715,
      "learning_rate": 4.730188359070084e-05,
      "loss": 0.5911,
      "step": 2090
    },
    {
      "epoch": 0.35040881027865844,
      "grad_norm": 9.679231643676758,
      "learning_rate": 4.7284914305107756e-05,
      "loss": 0.7517,
      "step": 2100
    },
    {
      "epoch": 0.35207742366093775,
      "grad_norm": 10.645646095275879,
      "learning_rate": 4.726794501951468e-05,
      "loss": 1.1755,
      "step": 2110
    },
    {
      "epoch": 0.3537460370432171,
      "grad_norm": 6.195444107055664,
      "learning_rate": 4.72509757339216e-05,
      "loss": 0.7128,
      "step": 2120
    },
    {
      "epoch": 0.3554146504254964,
      "grad_norm": 9.371164321899414,
      "learning_rate": 4.723400644832853e-05,
      "loss": 0.5542,
      "step": 2130
    },
    {
      "epoch": 0.3570832638077757,
      "grad_norm": 0.7653611302375793,
      "learning_rate": 4.721703716273545e-05,
      "loss": 0.6224,
      "step": 2140
    },
    {
      "epoch": 0.3587518771900551,
      "grad_norm": 7.059931755065918,
      "learning_rate": 4.720006787714237e-05,
      "loss": 0.7012,
      "step": 2150
    },
    {
      "epoch": 0.3604204905723344,
      "grad_norm": 7.114423751831055,
      "learning_rate": 4.71830985915493e-05,
      "loss": 0.8296,
      "step": 2160
    },
    {
      "epoch": 0.3620891039546137,
      "grad_norm": 8.467098236083984,
      "learning_rate": 4.716612930595622e-05,
      "loss": 1.0739,
      "step": 2170
    },
    {
      "epoch": 0.36375771733689305,
      "grad_norm": 8.871882438659668,
      "learning_rate": 4.714916002036314e-05,
      "loss": 0.6668,
      "step": 2180
    },
    {
      "epoch": 0.36542633071917235,
      "grad_norm": 5.060971260070801,
      "learning_rate": 4.713219073477007e-05,
      "loss": 0.8976,
      "step": 2190
    },
    {
      "epoch": 0.3670949441014517,
      "grad_norm": 4.836977005004883,
      "learning_rate": 4.7115221449176994e-05,
      "loss": 0.6857,
      "step": 2200
    },
    {
      "epoch": 0.368763557483731,
      "grad_norm": 1.0327324867248535,
      "learning_rate": 4.7098252163583914e-05,
      "loss": 0.5267,
      "step": 2210
    },
    {
      "epoch": 0.3704321708660103,
      "grad_norm": 11.744599342346191,
      "learning_rate": 4.708128287799084e-05,
      "loss": 0.8479,
      "step": 2220
    },
    {
      "epoch": 0.3721007842482897,
      "grad_norm": 2.2491724491119385,
      "learning_rate": 4.7064313592397765e-05,
      "loss": 0.5676,
      "step": 2230
    },
    {
      "epoch": 0.373769397630569,
      "grad_norm": 2.801563024520874,
      "learning_rate": 4.7047344306804684e-05,
      "loss": 0.7472,
      "step": 2240
    },
    {
      "epoch": 0.37543801101284835,
      "grad_norm": 4.369708061218262,
      "learning_rate": 4.703037502121161e-05,
      "loss": 0.6448,
      "step": 2250
    },
    {
      "epoch": 0.37710662439512765,
      "grad_norm": 7.913095474243164,
      "learning_rate": 4.701340573561853e-05,
      "loss": 0.7612,
      "step": 2260
    },
    {
      "epoch": 0.37877523777740696,
      "grad_norm": 3.4856364727020264,
      "learning_rate": 4.6996436450025455e-05,
      "loss": 0.5559,
      "step": 2270
    },
    {
      "epoch": 0.3804438511596863,
      "grad_norm": 3.502786636352539,
      "learning_rate": 4.697946716443238e-05,
      "loss": 1.1246,
      "step": 2280
    },
    {
      "epoch": 0.3821124645419656,
      "grad_norm": 4.559396743774414,
      "learning_rate": 4.69624978788393e-05,
      "loss": 0.514,
      "step": 2290
    },
    {
      "epoch": 0.38378107792424493,
      "grad_norm": 10.608229637145996,
      "learning_rate": 4.6945528593246226e-05,
      "loss": 0.4996,
      "step": 2300
    },
    {
      "epoch": 0.3854496913065243,
      "grad_norm": 2.7624449729919434,
      "learning_rate": 4.692855930765315e-05,
      "loss": 0.4467,
      "step": 2310
    },
    {
      "epoch": 0.3871183046888036,
      "grad_norm": 2.5176689624786377,
      "learning_rate": 4.691159002206008e-05,
      "loss": 0.514,
      "step": 2320
    },
    {
      "epoch": 0.38878691807108295,
      "grad_norm": 29.607952117919922,
      "learning_rate": 4.6894620736467e-05,
      "loss": 0.8919,
      "step": 2330
    },
    {
      "epoch": 0.39045553145336226,
      "grad_norm": 7.312753200531006,
      "learning_rate": 4.687765145087392e-05,
      "loss": 0.6584,
      "step": 2340
    },
    {
      "epoch": 0.39212414483564156,
      "grad_norm": 10.338000297546387,
      "learning_rate": 4.686068216528084e-05,
      "loss": 0.8212,
      "step": 2350
    },
    {
      "epoch": 0.3937927582179209,
      "grad_norm": 7.501348972320557,
      "learning_rate": 4.684371287968777e-05,
      "loss": 0.984,
      "step": 2360
    },
    {
      "epoch": 0.39546137160020023,
      "grad_norm": 7.75062894821167,
      "learning_rate": 4.6826743594094694e-05,
      "loss": 0.6952,
      "step": 2370
    },
    {
      "epoch": 0.39712998498247953,
      "grad_norm": 13.856389045715332,
      "learning_rate": 4.680977430850161e-05,
      "loss": 0.8905,
      "step": 2380
    },
    {
      "epoch": 0.3987985983647589,
      "grad_norm": 4.53643798828125,
      "learning_rate": 4.679280502290854e-05,
      "loss": 0.8118,
      "step": 2390
    },
    {
      "epoch": 0.4004672117470382,
      "grad_norm": 5.516713619232178,
      "learning_rate": 4.677583573731546e-05,
      "loss": 0.7209,
      "step": 2400
    },
    {
      "epoch": 0.40213582512931756,
      "grad_norm": 8.12226390838623,
      "learning_rate": 4.6758866451722384e-05,
      "loss": 0.5662,
      "step": 2410
    },
    {
      "epoch": 0.40380443851159686,
      "grad_norm": 5.107851505279541,
      "learning_rate": 4.67418971661293e-05,
      "loss": 0.6007,
      "step": 2420
    },
    {
      "epoch": 0.40547305189387617,
      "grad_norm": 4.949314594268799,
      "learning_rate": 4.672492788053623e-05,
      "loss": 0.7865,
      "step": 2430
    },
    {
      "epoch": 0.40714166527615553,
      "grad_norm": 13.078946113586426,
      "learning_rate": 4.6707958594943154e-05,
      "loss": 0.8915,
      "step": 2440
    },
    {
      "epoch": 0.40881027865843483,
      "grad_norm": 10.92952823638916,
      "learning_rate": 4.669098930935008e-05,
      "loss": 0.9305,
      "step": 2450
    },
    {
      "epoch": 0.4104788920407142,
      "grad_norm": 9.040349960327148,
      "learning_rate": 4.6674020023757006e-05,
      "loss": 0.7678,
      "step": 2460
    },
    {
      "epoch": 0.4121475054229935,
      "grad_norm": 4.435794353485107,
      "learning_rate": 4.6657050738163925e-05,
      "loss": 0.8393,
      "step": 2470
    },
    {
      "epoch": 0.4138161188052728,
      "grad_norm": 5.322741985321045,
      "learning_rate": 4.664008145257085e-05,
      "loss": 0.6357,
      "step": 2480
    },
    {
      "epoch": 0.41548473218755216,
      "grad_norm": 8.003535270690918,
      "learning_rate": 4.662311216697777e-05,
      "loss": 0.6056,
      "step": 2490
    },
    {
      "epoch": 0.41715334556983147,
      "grad_norm": 1.591697096824646,
      "learning_rate": 4.6606142881384696e-05,
      "loss": 0.8547,
      "step": 2500
    },
    {
      "epoch": 0.4188219589521108,
      "grad_norm": 1.3259997367858887,
      "learning_rate": 4.658917359579162e-05,
      "loss": 0.6272,
      "step": 2510
    },
    {
      "epoch": 0.42049057233439013,
      "grad_norm": 13.520787239074707,
      "learning_rate": 4.657220431019854e-05,
      "loss": 0.7843,
      "step": 2520
    },
    {
      "epoch": 0.42215918571666944,
      "grad_norm": 5.95968770980835,
      "learning_rate": 4.655523502460547e-05,
      "loss": 0.9054,
      "step": 2530
    },
    {
      "epoch": 0.4238277990989488,
      "grad_norm": 5.7057108879089355,
      "learning_rate": 4.6538265739012386e-05,
      "loss": 0.9363,
      "step": 2540
    },
    {
      "epoch": 0.4254964124812281,
      "grad_norm": 9.836874961853027,
      "learning_rate": 4.652129645341931e-05,
      "loss": 0.9169,
      "step": 2550
    },
    {
      "epoch": 0.4271650258635074,
      "grad_norm": 8.053886413574219,
      "learning_rate": 4.650432716782624e-05,
      "loss": 0.8474,
      "step": 2560
    },
    {
      "epoch": 0.42883363924578677,
      "grad_norm": 8.77614688873291,
      "learning_rate": 4.6487357882233164e-05,
      "loss": 0.8068,
      "step": 2570
    },
    {
      "epoch": 0.4305022526280661,
      "grad_norm": 7.959442138671875,
      "learning_rate": 4.647038859664008e-05,
      "loss": 0.8546,
      "step": 2580
    },
    {
      "epoch": 0.4321708660103454,
      "grad_norm": 3.8034751415252686,
      "learning_rate": 4.645341931104701e-05,
      "loss": 0.5859,
      "step": 2590
    },
    {
      "epoch": 0.43383947939262474,
      "grad_norm": 9.123007774353027,
      "learning_rate": 4.6436450025453935e-05,
      "loss": 0.7763,
      "step": 2600
    },
    {
      "epoch": 0.43550809277490404,
      "grad_norm": 7.685604572296143,
      "learning_rate": 4.6419480739860854e-05,
      "loss": 0.891,
      "step": 2610
    },
    {
      "epoch": 0.4371767061571834,
      "grad_norm": 6.3122758865356445,
      "learning_rate": 4.640251145426778e-05,
      "loss": 0.6462,
      "step": 2620
    },
    {
      "epoch": 0.4388453195394627,
      "grad_norm": 4.7552385330200195,
      "learning_rate": 4.63855421686747e-05,
      "loss": 0.788,
      "step": 2630
    },
    {
      "epoch": 0.440513932921742,
      "grad_norm": 4.40580940246582,
      "learning_rate": 4.6368572883081625e-05,
      "loss": 0.7864,
      "step": 2640
    },
    {
      "epoch": 0.4421825463040214,
      "grad_norm": 3.5186431407928467,
      "learning_rate": 4.6351603597488544e-05,
      "loss": 0.6606,
      "step": 2650
    },
    {
      "epoch": 0.4438511596863007,
      "grad_norm": 6.737544536590576,
      "learning_rate": 4.633463431189547e-05,
      "loss": 0.6714,
      "step": 2660
    },
    {
      "epoch": 0.44551977306858,
      "grad_norm": 2.1617512702941895,
      "learning_rate": 4.6317665026302395e-05,
      "loss": 0.7032,
      "step": 2670
    },
    {
      "epoch": 0.44718838645085934,
      "grad_norm": 7.921445369720459,
      "learning_rate": 4.6300695740709314e-05,
      "loss": 0.7676,
      "step": 2680
    },
    {
      "epoch": 0.44885699983313865,
      "grad_norm": 7.745121955871582,
      "learning_rate": 4.628372645511625e-05,
      "loss": 0.521,
      "step": 2690
    },
    {
      "epoch": 0.450525613215418,
      "grad_norm": 5.564319133758545,
      "learning_rate": 4.6266757169523166e-05,
      "loss": 0.8596,
      "step": 2700
    },
    {
      "epoch": 0.4521942265976973,
      "grad_norm": 8.842630386352539,
      "learning_rate": 4.624978788393009e-05,
      "loss": 1.1088,
      "step": 2710
    },
    {
      "epoch": 0.4538628399799766,
      "grad_norm": 6.874146461486816,
      "learning_rate": 4.623281859833701e-05,
      "loss": 0.7688,
      "step": 2720
    },
    {
      "epoch": 0.455531453362256,
      "grad_norm": 6.731356143951416,
      "learning_rate": 4.621584931274394e-05,
      "loss": 1.2956,
      "step": 2730
    },
    {
      "epoch": 0.4572000667445353,
      "grad_norm": 5.926513195037842,
      "learning_rate": 4.6198880027150856e-05,
      "loss": 0.6151,
      "step": 2740
    },
    {
      "epoch": 0.45886868012681464,
      "grad_norm": 7.988247871398926,
      "learning_rate": 4.618191074155778e-05,
      "loss": 0.9961,
      "step": 2750
    },
    {
      "epoch": 0.46053729350909395,
      "grad_norm": 11.19940185546875,
      "learning_rate": 4.616494145596471e-05,
      "loss": 0.6395,
      "step": 2760
    },
    {
      "epoch": 0.46220590689137325,
      "grad_norm": 4.873228549957275,
      "learning_rate": 4.614797217037163e-05,
      "loss": 0.6646,
      "step": 2770
    },
    {
      "epoch": 0.4638745202736526,
      "grad_norm": 9.068425178527832,
      "learning_rate": 4.613100288477855e-05,
      "loss": 0.6746,
      "step": 2780
    },
    {
      "epoch": 0.4655431336559319,
      "grad_norm": 8.824785232543945,
      "learning_rate": 4.611403359918547e-05,
      "loss": 1.117,
      "step": 2790
    },
    {
      "epoch": 0.4672117470382112,
      "grad_norm": 10.9446439743042,
      "learning_rate": 4.60970643135924e-05,
      "loss": 0.9726,
      "step": 2800
    },
    {
      "epoch": 0.4688803604204906,
      "grad_norm": 6.258224010467529,
      "learning_rate": 4.6080095027999324e-05,
      "loss": 0.6232,
      "step": 2810
    },
    {
      "epoch": 0.4705489738027699,
      "grad_norm": 7.989435195922852,
      "learning_rate": 4.606312574240625e-05,
      "loss": 0.92,
      "step": 2820
    },
    {
      "epoch": 0.47221758718504925,
      "grad_norm": 10.53073501586914,
      "learning_rate": 4.604615645681317e-05,
      "loss": 1.0279,
      "step": 2830
    },
    {
      "epoch": 0.47388620056732855,
      "grad_norm": 12.071959495544434,
      "learning_rate": 4.6029187171220095e-05,
      "loss": 0.6907,
      "step": 2840
    },
    {
      "epoch": 0.47555481394960786,
      "grad_norm": 7.947457313537598,
      "learning_rate": 4.601221788562702e-05,
      "loss": 0.6597,
      "step": 2850
    },
    {
      "epoch": 0.4772234273318872,
      "grad_norm": 3.26193904876709,
      "learning_rate": 4.599524860003394e-05,
      "loss": 0.5111,
      "step": 2860
    },
    {
      "epoch": 0.4788920407141665,
      "grad_norm": 6.503660202026367,
      "learning_rate": 4.5978279314440865e-05,
      "loss": 0.7881,
      "step": 2870
    },
    {
      "epoch": 0.4805606540964458,
      "grad_norm": 5.391315937042236,
      "learning_rate": 4.5961310028847785e-05,
      "loss": 0.5728,
      "step": 2880
    },
    {
      "epoch": 0.4822292674787252,
      "grad_norm": 3.878417491912842,
      "learning_rate": 4.594434074325471e-05,
      "loss": 0.666,
      "step": 2890
    },
    {
      "epoch": 0.4838978808610045,
      "grad_norm": 3.0510120391845703,
      "learning_rate": 4.5927371457661636e-05,
      "loss": 0.6521,
      "step": 2900
    },
    {
      "epoch": 0.48556649424328385,
      "grad_norm": 6.52381706237793,
      "learning_rate": 4.5910402172068555e-05,
      "loss": 0.5154,
      "step": 2910
    },
    {
      "epoch": 0.48723510762556316,
      "grad_norm": 11.997550964355469,
      "learning_rate": 4.589343288647548e-05,
      "loss": 0.8797,
      "step": 2920
    },
    {
      "epoch": 0.48890372100784246,
      "grad_norm": 2.5449585914611816,
      "learning_rate": 4.587646360088241e-05,
      "loss": 0.4899,
      "step": 2930
    },
    {
      "epoch": 0.4905723343901218,
      "grad_norm": 2.677755355834961,
      "learning_rate": 4.585949431528933e-05,
      "loss": 0.5137,
      "step": 2940
    },
    {
      "epoch": 0.4922409477724011,
      "grad_norm": 5.397197723388672,
      "learning_rate": 4.584252502969625e-05,
      "loss": 0.9506,
      "step": 2950
    },
    {
      "epoch": 0.4939095611546805,
      "grad_norm": 3.6196203231811523,
      "learning_rate": 4.582555574410318e-05,
      "loss": 0.7175,
      "step": 2960
    },
    {
      "epoch": 0.4955781745369598,
      "grad_norm": 4.9687700271606445,
      "learning_rate": 4.58085864585101e-05,
      "loss": 0.7461,
      "step": 2970
    },
    {
      "epoch": 0.4972467879192391,
      "grad_norm": 10.7244873046875,
      "learning_rate": 4.579161717291702e-05,
      "loss": 0.5131,
      "step": 2980
    },
    {
      "epoch": 0.49891540130151846,
      "grad_norm": 4.382063388824463,
      "learning_rate": 4.577464788732395e-05,
      "loss": 0.6938,
      "step": 2990
    },
    {
      "epoch": 0.5005840146837978,
      "grad_norm": 15.06298542022705,
      "learning_rate": 4.575767860173087e-05,
      "loss": 0.7685,
      "step": 3000
    },
    {
      "epoch": 0.5022526280660771,
      "grad_norm": 11.215258598327637,
      "learning_rate": 4.5740709316137794e-05,
      "loss": 0.6438,
      "step": 3010
    },
    {
      "epoch": 0.5039212414483564,
      "grad_norm": 4.09038782119751,
      "learning_rate": 4.572374003054471e-05,
      "loss": 0.7529,
      "step": 3020
    },
    {
      "epoch": 0.5055898548306358,
      "grad_norm": 5.056797027587891,
      "learning_rate": 4.570677074495164e-05,
      "loss": 0.8101,
      "step": 3030
    },
    {
      "epoch": 0.5072584682129151,
      "grad_norm": 1.333560585975647,
      "learning_rate": 4.568980145935856e-05,
      "loss": 0.7968,
      "step": 3040
    },
    {
      "epoch": 0.5089270815951944,
      "grad_norm": 8.449666023254395,
      "learning_rate": 4.5672832173765484e-05,
      "loss": 0.8319,
      "step": 3050
    },
    {
      "epoch": 0.5105956949774737,
      "grad_norm": 2.4076712131500244,
      "learning_rate": 4.565586288817241e-05,
      "loss": 0.5137,
      "step": 3060
    },
    {
      "epoch": 0.512264308359753,
      "grad_norm": 6.741187572479248,
      "learning_rate": 4.5638893602579335e-05,
      "loss": 0.5956,
      "step": 3070
    },
    {
      "epoch": 0.5139329217420324,
      "grad_norm": 12.210086822509766,
      "learning_rate": 4.562192431698626e-05,
      "loss": 0.8891,
      "step": 3080
    },
    {
      "epoch": 0.5156015351243117,
      "grad_norm": 2.90934681892395,
      "learning_rate": 4.560495503139318e-05,
      "loss": 0.4891,
      "step": 3090
    },
    {
      "epoch": 0.517270148506591,
      "grad_norm": 6.5355048179626465,
      "learning_rate": 4.5587985745800106e-05,
      "loss": 0.7645,
      "step": 3100
    },
    {
      "epoch": 0.5189387618888703,
      "grad_norm": 5.370847225189209,
      "learning_rate": 4.5571016460207025e-05,
      "loss": 0.7511,
      "step": 3110
    },
    {
      "epoch": 0.5206073752711496,
      "grad_norm": 7.274075984954834,
      "learning_rate": 4.555404717461395e-05,
      "loss": 0.6446,
      "step": 3120
    },
    {
      "epoch": 0.522275988653429,
      "grad_norm": 1.7459408044815063,
      "learning_rate": 4.553707788902088e-05,
      "loss": 0.8842,
      "step": 3130
    },
    {
      "epoch": 0.5239446020357084,
      "grad_norm": 7.273232936859131,
      "learning_rate": 4.5520108603427796e-05,
      "loss": 0.5839,
      "step": 3140
    },
    {
      "epoch": 0.5256132154179877,
      "grad_norm": 5.582218170166016,
      "learning_rate": 4.550313931783472e-05,
      "loss": 0.5211,
      "step": 3150
    },
    {
      "epoch": 0.527281828800267,
      "grad_norm": 6.476041316986084,
      "learning_rate": 4.548617003224164e-05,
      "loss": 0.5596,
      "step": 3160
    },
    {
      "epoch": 0.5289504421825463,
      "grad_norm": 10.81589412689209,
      "learning_rate": 4.546920074664857e-05,
      "loss": 0.7506,
      "step": 3170
    },
    {
      "epoch": 0.5306190555648256,
      "grad_norm": 8.805999755859375,
      "learning_rate": 4.545223146105549e-05,
      "loss": 0.8487,
      "step": 3180
    },
    {
      "epoch": 0.532287668947105,
      "grad_norm": 10.624868392944336,
      "learning_rate": 4.543526217546242e-05,
      "loss": 0.8112,
      "step": 3190
    },
    {
      "epoch": 0.5339562823293843,
      "grad_norm": 4.965054988861084,
      "learning_rate": 4.541829288986934e-05,
      "loss": 0.6627,
      "step": 3200
    },
    {
      "epoch": 0.5356248957116636,
      "grad_norm": 13.000005722045898,
      "learning_rate": 4.5401323604276264e-05,
      "loss": 0.8799,
      "step": 3210
    },
    {
      "epoch": 0.5372935090939429,
      "grad_norm": 12.053877830505371,
      "learning_rate": 4.538435431868319e-05,
      "loss": 0.6146,
      "step": 3220
    },
    {
      "epoch": 0.5389621224762222,
      "grad_norm": 6.057259559631348,
      "learning_rate": 4.536738503309011e-05,
      "loss": 0.698,
      "step": 3230
    },
    {
      "epoch": 0.5406307358585016,
      "grad_norm": 6.611588001251221,
      "learning_rate": 4.5350415747497035e-05,
      "loss": 0.7164,
      "step": 3240
    },
    {
      "epoch": 0.5422993492407809,
      "grad_norm": 6.577651023864746,
      "learning_rate": 4.5333446461903954e-05,
      "loss": 0.6433,
      "step": 3250
    },
    {
      "epoch": 0.5439679626230602,
      "grad_norm": 12.571124076843262,
      "learning_rate": 4.531647717631088e-05,
      "loss": 0.6316,
      "step": 3260
    },
    {
      "epoch": 0.5456365760053395,
      "grad_norm": 5.339183330535889,
      "learning_rate": 4.52995078907178e-05,
      "loss": 0.6235,
      "step": 3270
    },
    {
      "epoch": 0.5473051893876189,
      "grad_norm": 4.490739345550537,
      "learning_rate": 4.5282538605124725e-05,
      "loss": 0.6044,
      "step": 3280
    },
    {
      "epoch": 0.5489738027698983,
      "grad_norm": 6.58143949508667,
      "learning_rate": 4.526556931953165e-05,
      "loss": 0.9361,
      "step": 3290
    },
    {
      "epoch": 0.5506424161521776,
      "grad_norm": 2.867375373840332,
      "learning_rate": 4.524860003393857e-05,
      "loss": 0.5557,
      "step": 3300
    },
    {
      "epoch": 0.5523110295344569,
      "grad_norm": 4.522762775421143,
      "learning_rate": 4.52316307483455e-05,
      "loss": 0.7428,
      "step": 3310
    },
    {
      "epoch": 0.5539796429167362,
      "grad_norm": 7.414405345916748,
      "learning_rate": 4.521466146275242e-05,
      "loss": 0.5488,
      "step": 3320
    },
    {
      "epoch": 0.5556482562990155,
      "grad_norm": 0.2303873896598816,
      "learning_rate": 4.519769217715935e-05,
      "loss": 0.5146,
      "step": 3330
    },
    {
      "epoch": 0.5573168696812948,
      "grad_norm": 9.322689056396484,
      "learning_rate": 4.5180722891566266e-05,
      "loss": 0.5328,
      "step": 3340
    },
    {
      "epoch": 0.5589854830635742,
      "grad_norm": 6.545580863952637,
      "learning_rate": 4.516375360597319e-05,
      "loss": 0.5253,
      "step": 3350
    },
    {
      "epoch": 0.5606540964458535,
      "grad_norm": 5.050734996795654,
      "learning_rate": 4.514678432038011e-05,
      "loss": 0.8012,
      "step": 3360
    },
    {
      "epoch": 0.5623227098281328,
      "grad_norm": 2.5080630779266357,
      "learning_rate": 4.512981503478704e-05,
      "loss": 0.4401,
      "step": 3370
    },
    {
      "epoch": 0.5639913232104121,
      "grad_norm": 5.957943916320801,
      "learning_rate": 4.511284574919396e-05,
      "loss": 0.8909,
      "step": 3380
    },
    {
      "epoch": 0.5656599365926914,
      "grad_norm": 3.8780250549316406,
      "learning_rate": 4.509587646360088e-05,
      "loss": 0.85,
      "step": 3390
    },
    {
      "epoch": 0.5673285499749708,
      "grad_norm": 11.064157485961914,
      "learning_rate": 4.507890717800781e-05,
      "loss": 0.7281,
      "step": 3400
    },
    {
      "epoch": 0.5689971633572501,
      "grad_norm": 7.285651683807373,
      "learning_rate": 4.506193789241473e-05,
      "loss": 0.7347,
      "step": 3410
    },
    {
      "epoch": 0.5706657767395295,
      "grad_norm": 11.072522163391113,
      "learning_rate": 4.504496860682165e-05,
      "loss": 0.8291,
      "step": 3420
    },
    {
      "epoch": 0.5723343901218088,
      "grad_norm": 7.517305374145508,
      "learning_rate": 4.502799932122858e-05,
      "loss": 0.5967,
      "step": 3430
    },
    {
      "epoch": 0.5740030035040881,
      "grad_norm": 6.676129341125488,
      "learning_rate": 4.5011030035635505e-05,
      "loss": 0.6264,
      "step": 3440
    },
    {
      "epoch": 0.5756716168863675,
      "grad_norm": 2.1607940196990967,
      "learning_rate": 4.4994060750042424e-05,
      "loss": 0.8075,
      "step": 3450
    },
    {
      "epoch": 0.5773402302686468,
      "grad_norm": 8.994595527648926,
      "learning_rate": 4.497709146444935e-05,
      "loss": 0.8443,
      "step": 3460
    },
    {
      "epoch": 0.5790088436509261,
      "grad_norm": 4.227181434631348,
      "learning_rate": 4.4960122178856276e-05,
      "loss": 0.4253,
      "step": 3470
    },
    {
      "epoch": 0.5806774570332054,
      "grad_norm": 6.6641998291015625,
      "learning_rate": 4.4943152893263195e-05,
      "loss": 0.9796,
      "step": 3480
    },
    {
      "epoch": 0.5823460704154847,
      "grad_norm": 10.859420776367188,
      "learning_rate": 4.492618360767012e-05,
      "loss": 1.1375,
      "step": 3490
    },
    {
      "epoch": 0.5840146837977641,
      "grad_norm": 2.8551836013793945,
      "learning_rate": 4.490921432207704e-05,
      "loss": 0.7302,
      "step": 3500
    },
    {
      "epoch": 0.5856832971800434,
      "grad_norm": 6.613987445831299,
      "learning_rate": 4.4892245036483966e-05,
      "loss": 0.6564,
      "step": 3510
    },
    {
      "epoch": 0.5873519105623227,
      "grad_norm": 11.335299491882324,
      "learning_rate": 4.487527575089089e-05,
      "loss": 0.6665,
      "step": 3520
    },
    {
      "epoch": 0.589020523944602,
      "grad_norm": 8.634321212768555,
      "learning_rate": 4.485830646529781e-05,
      "loss": 0.4841,
      "step": 3530
    },
    {
      "epoch": 0.5906891373268813,
      "grad_norm": 5.651169300079346,
      "learning_rate": 4.4841337179704736e-05,
      "loss": 0.885,
      "step": 3540
    },
    {
      "epoch": 0.5923577507091606,
      "grad_norm": 8.284029006958008,
      "learning_rate": 4.4824367894111655e-05,
      "loss": 0.733,
      "step": 3550
    },
    {
      "epoch": 0.59402636409144,
      "grad_norm": 4.870278835296631,
      "learning_rate": 4.480739860851859e-05,
      "loss": 0.5902,
      "step": 3560
    },
    {
      "epoch": 0.5956949774737194,
      "grad_norm": 1.6221606731414795,
      "learning_rate": 4.479042932292551e-05,
      "loss": 0.7004,
      "step": 3570
    },
    {
      "epoch": 0.5973635908559987,
      "grad_norm": 5.988095283508301,
      "learning_rate": 4.477346003733243e-05,
      "loss": 0.5431,
      "step": 3580
    },
    {
      "epoch": 0.599032204238278,
      "grad_norm": 12.33525562286377,
      "learning_rate": 4.475649075173935e-05,
      "loss": 0.7791,
      "step": 3590
    },
    {
      "epoch": 0.6007008176205573,
      "grad_norm": 3.564730644226074,
      "learning_rate": 4.473952146614628e-05,
      "loss": 0.3783,
      "step": 3600
    },
    {
      "epoch": 0.6023694310028367,
      "grad_norm": 1.3369828462600708,
      "learning_rate": 4.4722552180553204e-05,
      "loss": 0.504,
      "step": 3610
    },
    {
      "epoch": 0.604038044385116,
      "grad_norm": 9.598024368286133,
      "learning_rate": 4.470558289496012e-05,
      "loss": 0.7133,
      "step": 3620
    },
    {
      "epoch": 0.6057066577673953,
      "grad_norm": 2.0081145763397217,
      "learning_rate": 4.468861360936705e-05,
      "loss": 0.6167,
      "step": 3630
    },
    {
      "epoch": 0.6073752711496746,
      "grad_norm": 11.848610877990723,
      "learning_rate": 4.467164432377397e-05,
      "loss": 0.9504,
      "step": 3640
    },
    {
      "epoch": 0.6090438845319539,
      "grad_norm": 7.685434818267822,
      "learning_rate": 4.4654675038180894e-05,
      "loss": 1.0392,
      "step": 3650
    },
    {
      "epoch": 0.6107124979142333,
      "grad_norm": 2.3584744930267334,
      "learning_rate": 4.463770575258781e-05,
      "loss": 0.7798,
      "step": 3660
    },
    {
      "epoch": 0.6123811112965126,
      "grad_norm": 2.1608529090881348,
      "learning_rate": 4.462073646699474e-05,
      "loss": 0.772,
      "step": 3670
    },
    {
      "epoch": 0.6140497246787919,
      "grad_norm": 6.198031425476074,
      "learning_rate": 4.4603767181401665e-05,
      "loss": 0.8374,
      "step": 3680
    },
    {
      "epoch": 0.6157183380610712,
      "grad_norm": 1.9841444492340088,
      "learning_rate": 4.458679789580859e-05,
      "loss": 0.5618,
      "step": 3690
    },
    {
      "epoch": 0.6173869514433505,
      "grad_norm": 4.772614479064941,
      "learning_rate": 4.4569828610215516e-05,
      "loss": 0.6631,
      "step": 3700
    },
    {
      "epoch": 0.61905556482563,
      "grad_norm": 7.690817832946777,
      "learning_rate": 4.4552859324622436e-05,
      "loss": 0.9196,
      "step": 3710
    },
    {
      "epoch": 0.6207241782079093,
      "grad_norm": 4.0594329833984375,
      "learning_rate": 4.453589003902936e-05,
      "loss": 0.8049,
      "step": 3720
    },
    {
      "epoch": 0.6223927915901886,
      "grad_norm": 11.17619514465332,
      "learning_rate": 4.451892075343628e-05,
      "loss": 0.7519,
      "step": 3730
    },
    {
      "epoch": 0.6240614049724679,
      "grad_norm": 3.2281079292297363,
      "learning_rate": 4.4501951467843206e-05,
      "loss": 0.4604,
      "step": 3740
    },
    {
      "epoch": 0.6257300183547472,
      "grad_norm": 11.126716613769531,
      "learning_rate": 4.448498218225013e-05,
      "loss": 0.7375,
      "step": 3750
    },
    {
      "epoch": 0.6273986317370265,
      "grad_norm": 10.761882781982422,
      "learning_rate": 4.446801289665705e-05,
      "loss": 0.5572,
      "step": 3760
    },
    {
      "epoch": 0.6290672451193059,
      "grad_norm": 6.086820125579834,
      "learning_rate": 4.445104361106398e-05,
      "loss": 0.6703,
      "step": 3770
    },
    {
      "epoch": 0.6307358585015852,
      "grad_norm": 3.4272842407226562,
      "learning_rate": 4.4434074325470896e-05,
      "loss": 0.7716,
      "step": 3780
    },
    {
      "epoch": 0.6324044718838645,
      "grad_norm": 6.36560583114624,
      "learning_rate": 4.441710503987782e-05,
      "loss": 0.6836,
      "step": 3790
    },
    {
      "epoch": 0.6340730852661438,
      "grad_norm": 5.820964813232422,
      "learning_rate": 4.440013575428474e-05,
      "loss": 0.9726,
      "step": 3800
    },
    {
      "epoch": 0.6357416986484231,
      "grad_norm": 4.31635856628418,
      "learning_rate": 4.4383166468691674e-05,
      "loss": 0.6364,
      "step": 3810
    },
    {
      "epoch": 0.6374103120307025,
      "grad_norm": 5.013918399810791,
      "learning_rate": 4.436619718309859e-05,
      "loss": 0.6099,
      "step": 3820
    },
    {
      "epoch": 0.6390789254129818,
      "grad_norm": 11.187727928161621,
      "learning_rate": 4.434922789750552e-05,
      "loss": 0.6747,
      "step": 3830
    },
    {
      "epoch": 0.6407475387952611,
      "grad_norm": 4.538517475128174,
      "learning_rate": 4.4332258611912445e-05,
      "loss": 0.8539,
      "step": 3840
    },
    {
      "epoch": 0.6424161521775404,
      "grad_norm": 11.29455280303955,
      "learning_rate": 4.4315289326319364e-05,
      "loss": 0.6492,
      "step": 3850
    },
    {
      "epoch": 0.6440847655598197,
      "grad_norm": 1.6599870920181274,
      "learning_rate": 4.429832004072629e-05,
      "loss": 0.4763,
      "step": 3860
    },
    {
      "epoch": 0.6457533789420992,
      "grad_norm": 6.313650131225586,
      "learning_rate": 4.428135075513321e-05,
      "loss": 0.5956,
      "step": 3870
    },
    {
      "epoch": 0.6474219923243785,
      "grad_norm": 5.534331798553467,
      "learning_rate": 4.4264381469540135e-05,
      "loss": 0.764,
      "step": 3880
    },
    {
      "epoch": 0.6490906057066578,
      "grad_norm": 9.088793754577637,
      "learning_rate": 4.4247412183947054e-05,
      "loss": 0.5761,
      "step": 3890
    },
    {
      "epoch": 0.6507592190889371,
      "grad_norm": 8.311659812927246,
      "learning_rate": 4.423044289835398e-05,
      "loss": 0.7424,
      "step": 3900
    },
    {
      "epoch": 0.6524278324712164,
      "grad_norm": 7.157122611999512,
      "learning_rate": 4.4213473612760906e-05,
      "loss": 0.6607,
      "step": 3910
    },
    {
      "epoch": 0.6540964458534958,
      "grad_norm": 0.7818027138710022,
      "learning_rate": 4.4196504327167825e-05,
      "loss": 0.6013,
      "step": 3920
    },
    {
      "epoch": 0.6557650592357751,
      "grad_norm": 7.288153171539307,
      "learning_rate": 4.417953504157476e-05,
      "loss": 0.6452,
      "step": 3930
    },
    {
      "epoch": 0.6574336726180544,
      "grad_norm": 6.079833030700684,
      "learning_rate": 4.4162565755981676e-05,
      "loss": 0.848,
      "step": 3940
    },
    {
      "epoch": 0.6591022860003337,
      "grad_norm": 4.627569675445557,
      "learning_rate": 4.41455964703886e-05,
      "loss": 0.8348,
      "step": 3950
    },
    {
      "epoch": 0.660770899382613,
      "grad_norm": 3.3344967365264893,
      "learning_rate": 4.412862718479552e-05,
      "loss": 0.5287,
      "step": 3960
    },
    {
      "epoch": 0.6624395127648923,
      "grad_norm": 3.169508218765259,
      "learning_rate": 4.411165789920245e-05,
      "loss": 0.6444,
      "step": 3970
    },
    {
      "epoch": 0.6641081261471717,
      "grad_norm": 11.020730972290039,
      "learning_rate": 4.4094688613609366e-05,
      "loss": 1.0034,
      "step": 3980
    },
    {
      "epoch": 0.665776739529451,
      "grad_norm": 3.730940341949463,
      "learning_rate": 4.407771932801629e-05,
      "loss": 0.6404,
      "step": 3990
    },
    {
      "epoch": 0.6674453529117303,
      "grad_norm": 6.057884216308594,
      "learning_rate": 4.406075004242322e-05,
      "loss": 0.9318,
      "step": 4000
    },
    {
      "epoch": 0.6691139662940097,
      "grad_norm": 5.110137462615967,
      "learning_rate": 4.404378075683014e-05,
      "loss": 0.5163,
      "step": 4010
    },
    {
      "epoch": 0.670782579676289,
      "grad_norm": 6.410297393798828,
      "learning_rate": 4.402681147123706e-05,
      "loss": 0.8918,
      "step": 4020
    },
    {
      "epoch": 0.6724511930585684,
      "grad_norm": 1.6978691816329956,
      "learning_rate": 4.400984218564398e-05,
      "loss": 0.6854,
      "step": 4030
    },
    {
      "epoch": 0.6741198064408477,
      "grad_norm": 5.275461673736572,
      "learning_rate": 4.399287290005091e-05,
      "loss": 0.6092,
      "step": 4040
    },
    {
      "epoch": 0.675788419823127,
      "grad_norm": 7.0557684898376465,
      "learning_rate": 4.3975903614457834e-05,
      "loss": 1.0875,
      "step": 4050
    },
    {
      "epoch": 0.6774570332054063,
      "grad_norm": 6.935187339782715,
      "learning_rate": 4.395893432886476e-05,
      "loss": 0.5459,
      "step": 4060
    },
    {
      "epoch": 0.6791256465876856,
      "grad_norm": 6.831294536590576,
      "learning_rate": 4.394196504327168e-05,
      "loss": 0.7378,
      "step": 4070
    },
    {
      "epoch": 0.680794259969965,
      "grad_norm": 9.966114044189453,
      "learning_rate": 4.3924995757678605e-05,
      "loss": 0.8799,
      "step": 4080
    },
    {
      "epoch": 0.6824628733522443,
      "grad_norm": 9.209259986877441,
      "learning_rate": 4.390802647208553e-05,
      "loss": 0.888,
      "step": 4090
    },
    {
      "epoch": 0.6841314867345236,
      "grad_norm": 5.225367069244385,
      "learning_rate": 4.389105718649245e-05,
      "loss": 0.396,
      "step": 4100
    },
    {
      "epoch": 0.6858001001168029,
      "grad_norm": 1.920923113822937,
      "learning_rate": 4.3874087900899376e-05,
      "loss": 0.6196,
      "step": 4110
    },
    {
      "epoch": 0.6874687134990822,
      "grad_norm": 7.026232719421387,
      "learning_rate": 4.3857118615306295e-05,
      "loss": 0.6714,
      "step": 4120
    },
    {
      "epoch": 0.6891373268813615,
      "grad_norm": 13.045815467834473,
      "learning_rate": 4.384014932971322e-05,
      "loss": 0.8097,
      "step": 4130
    },
    {
      "epoch": 0.690805940263641,
      "grad_norm": 2.4661405086517334,
      "learning_rate": 4.3823180044120147e-05,
      "loss": 0.7737,
      "step": 4140
    },
    {
      "epoch": 0.6924745536459203,
      "grad_norm": 5.793625354766846,
      "learning_rate": 4.3806210758527066e-05,
      "loss": 0.4437,
      "step": 4150
    },
    {
      "epoch": 0.6941431670281996,
      "grad_norm": 8.876808166503906,
      "learning_rate": 4.378924147293399e-05,
      "loss": 0.4919,
      "step": 4160
    },
    {
      "epoch": 0.6958117804104789,
      "grad_norm": 7.983793258666992,
      "learning_rate": 4.377227218734091e-05,
      "loss": 0.8774,
      "step": 4170
    },
    {
      "epoch": 0.6974803937927582,
      "grad_norm": 6.038628101348877,
      "learning_rate": 4.375530290174784e-05,
      "loss": 0.6683,
      "step": 4180
    },
    {
      "epoch": 0.6991490071750376,
      "grad_norm": 5.2195305824279785,
      "learning_rate": 4.373833361615476e-05,
      "loss": 0.629,
      "step": 4190
    },
    {
      "epoch": 0.7008176205573169,
      "grad_norm": 4.324680805206299,
      "learning_rate": 4.372136433056169e-05,
      "loss": 0.5957,
      "step": 4200
    },
    {
      "epoch": 0.7024862339395962,
      "grad_norm": 6.242452144622803,
      "learning_rate": 4.370439504496861e-05,
      "loss": 0.8208,
      "step": 4210
    },
    {
      "epoch": 0.7041548473218755,
      "grad_norm": 4.399996757507324,
      "learning_rate": 4.368742575937553e-05,
      "loss": 0.5191,
      "step": 4220
    },
    {
      "epoch": 0.7058234607041548,
      "grad_norm": 9.633098602294922,
      "learning_rate": 4.367045647378246e-05,
      "loss": 0.6857,
      "step": 4230
    },
    {
      "epoch": 0.7074920740864342,
      "grad_norm": 7.2886552810668945,
      "learning_rate": 4.365348718818938e-05,
      "loss": 0.5933,
      "step": 4240
    },
    {
      "epoch": 0.7091606874687135,
      "grad_norm": 11.15596866607666,
      "learning_rate": 4.3636517902596304e-05,
      "loss": 0.5833,
      "step": 4250
    },
    {
      "epoch": 0.7108293008509928,
      "grad_norm": 6.6126179695129395,
      "learning_rate": 4.361954861700322e-05,
      "loss": 0.7148,
      "step": 4260
    },
    {
      "epoch": 0.7124979142332721,
      "grad_norm": 7.988368988037109,
      "learning_rate": 4.360257933141015e-05,
      "loss": 0.6919,
      "step": 4270
    },
    {
      "epoch": 0.7141665276155514,
      "grad_norm": 4.636739730834961,
      "learning_rate": 4.358561004581707e-05,
      "loss": 0.5343,
      "step": 4280
    },
    {
      "epoch": 0.7158351409978309,
      "grad_norm": 2.458247661590576,
      "learning_rate": 4.3568640760223994e-05,
      "loss": 0.7939,
      "step": 4290
    },
    {
      "epoch": 0.7175037543801102,
      "grad_norm": 9.745562553405762,
      "learning_rate": 4.355167147463092e-05,
      "loss": 0.9542,
      "step": 4300
    },
    {
      "epoch": 0.7191723677623895,
      "grad_norm": 8.619933128356934,
      "learning_rate": 4.3534702189037846e-05,
      "loss": 0.5411,
      "step": 4310
    },
    {
      "epoch": 0.7208409811446688,
      "grad_norm": 4.163063049316406,
      "learning_rate": 4.351773290344477e-05,
      "loss": 0.4796,
      "step": 4320
    },
    {
      "epoch": 0.7225095945269481,
      "grad_norm": 4.938095569610596,
      "learning_rate": 4.350076361785169e-05,
      "loss": 0.5654,
      "step": 4330
    },
    {
      "epoch": 0.7241782079092274,
      "grad_norm": 1.0958913564682007,
      "learning_rate": 4.3483794332258617e-05,
      "loss": 0.7761,
      "step": 4340
    },
    {
      "epoch": 0.7258468212915068,
      "grad_norm": 4.2065253257751465,
      "learning_rate": 4.3466825046665536e-05,
      "loss": 0.6785,
      "step": 4350
    },
    {
      "epoch": 0.7275154346737861,
      "grad_norm": 8.917095184326172,
      "learning_rate": 4.344985576107246e-05,
      "loss": 0.5883,
      "step": 4360
    },
    {
      "epoch": 0.7291840480560654,
      "grad_norm": 2.5580339431762695,
      "learning_rate": 4.343288647547938e-05,
      "loss": 0.6331,
      "step": 4370
    },
    {
      "epoch": 0.7308526614383447,
      "grad_norm": 6.877384662628174,
      "learning_rate": 4.3415917189886307e-05,
      "loss": 0.5563,
      "step": 4380
    },
    {
      "epoch": 0.732521274820624,
      "grad_norm": 6.859411716461182,
      "learning_rate": 4.339894790429323e-05,
      "loss": 0.7242,
      "step": 4390
    },
    {
      "epoch": 0.7341898882029034,
      "grad_norm": 2.2898499965667725,
      "learning_rate": 4.338197861870015e-05,
      "loss": 0.5761,
      "step": 4400
    },
    {
      "epoch": 0.7358585015851827,
      "grad_norm": 2.8775618076324463,
      "learning_rate": 4.336500933310708e-05,
      "loss": 0.4545,
      "step": 4410
    },
    {
      "epoch": 0.737527114967462,
      "grad_norm": 5.726140975952148,
      "learning_rate": 4.3348040047513996e-05,
      "loss": 0.3783,
      "step": 4420
    },
    {
      "epoch": 0.7391957283497413,
      "grad_norm": 9.304436683654785,
      "learning_rate": 4.333107076192093e-05,
      "loss": 0.8976,
      "step": 4430
    },
    {
      "epoch": 0.7408643417320206,
      "grad_norm": 9.143195152282715,
      "learning_rate": 4.331410147632785e-05,
      "loss": 0.6382,
      "step": 4440
    },
    {
      "epoch": 0.7425329551143001,
      "grad_norm": 6.717785358428955,
      "learning_rate": 4.3297132190734774e-05,
      "loss": 0.8884,
      "step": 4450
    },
    {
      "epoch": 0.7442015684965794,
      "grad_norm": 7.7591633796691895,
      "learning_rate": 4.32801629051417e-05,
      "loss": 0.4417,
      "step": 4460
    },
    {
      "epoch": 0.7458701818788587,
      "grad_norm": 14.082252502441406,
      "learning_rate": 4.326319361954862e-05,
      "loss": 0.5161,
      "step": 4470
    },
    {
      "epoch": 0.747538795261138,
      "grad_norm": 5.869783878326416,
      "learning_rate": 4.3246224333955545e-05,
      "loss": 0.6385,
      "step": 4480
    },
    {
      "epoch": 0.7492074086434173,
      "grad_norm": 6.809282302856445,
      "learning_rate": 4.3229255048362464e-05,
      "loss": 0.8744,
      "step": 4490
    },
    {
      "epoch": 0.7508760220256967,
      "grad_norm": 7.007767677307129,
      "learning_rate": 4.321228576276939e-05,
      "loss": 0.7173,
      "step": 4500
    },
    {
      "epoch": 0.752544635407976,
      "grad_norm": 5.034497261047363,
      "learning_rate": 4.319531647717631e-05,
      "loss": 0.7064,
      "step": 4510
    },
    {
      "epoch": 0.7542132487902553,
      "grad_norm": 6.769698619842529,
      "learning_rate": 4.3178347191583235e-05,
      "loss": 0.5916,
      "step": 4520
    },
    {
      "epoch": 0.7558818621725346,
      "grad_norm": 7.849212169647217,
      "learning_rate": 4.316137790599016e-05,
      "loss": 0.9192,
      "step": 4530
    },
    {
      "epoch": 0.7575504755548139,
      "grad_norm": 5.8093976974487305,
      "learning_rate": 4.314440862039708e-05,
      "loss": 0.8365,
      "step": 4540
    },
    {
      "epoch": 0.7592190889370932,
      "grad_norm": 8.804215431213379,
      "learning_rate": 4.312743933480401e-05,
      "loss": 0.5565,
      "step": 4550
    },
    {
      "epoch": 0.7608877023193726,
      "grad_norm": 3.0501999855041504,
      "learning_rate": 4.311047004921093e-05,
      "loss": 0.4349,
      "step": 4560
    },
    {
      "epoch": 0.7625563157016519,
      "grad_norm": 6.776899814605713,
      "learning_rate": 4.309350076361786e-05,
      "loss": 0.8511,
      "step": 4570
    },
    {
      "epoch": 0.7642249290839312,
      "grad_norm": 2.5118634700775146,
      "learning_rate": 4.3076531478024777e-05,
      "loss": 0.7892,
      "step": 4580
    },
    {
      "epoch": 0.7658935424662106,
      "grad_norm": 11.172088623046875,
      "learning_rate": 4.30595621924317e-05,
      "loss": 0.7853,
      "step": 4590
    },
    {
      "epoch": 0.7675621558484899,
      "grad_norm": 2.560894012451172,
      "learning_rate": 4.304259290683862e-05,
      "loss": 0.5424,
      "step": 4600
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 4.666998863220215,
      "learning_rate": 4.302562362124555e-05,
      "loss": 0.5385,
      "step": 4610
    },
    {
      "epoch": 0.7708993826130486,
      "grad_norm": 5.732120513916016,
      "learning_rate": 4.300865433565247e-05,
      "loss": 0.5812,
      "step": 4620
    },
    {
      "epoch": 0.7725679959953279,
      "grad_norm": 5.363624572753906,
      "learning_rate": 4.299168505005939e-05,
      "loss": 0.8825,
      "step": 4630
    },
    {
      "epoch": 0.7742366093776072,
      "grad_norm": 1.8749109506607056,
      "learning_rate": 4.297471576446632e-05,
      "loss": 0.7005,
      "step": 4640
    },
    {
      "epoch": 0.7759052227598865,
      "grad_norm": 7.661915302276611,
      "learning_rate": 4.295774647887324e-05,
      "loss": 0.9213,
      "step": 4650
    },
    {
      "epoch": 0.7775738361421659,
      "grad_norm": 1.9717713594436646,
      "learning_rate": 4.294077719328016e-05,
      "loss": 0.7454,
      "step": 4660
    },
    {
      "epoch": 0.7792424495244452,
      "grad_norm": 5.182392597198486,
      "learning_rate": 4.292380790768709e-05,
      "loss": 0.6689,
      "step": 4670
    },
    {
      "epoch": 0.7809110629067245,
      "grad_norm": 5.68388032913208,
      "learning_rate": 4.2906838622094015e-05,
      "loss": 0.6146,
      "step": 4680
    },
    {
      "epoch": 0.7825796762890038,
      "grad_norm": 6.913420677185059,
      "learning_rate": 4.2889869336500934e-05,
      "loss": 0.4854,
      "step": 4690
    },
    {
      "epoch": 0.7842482896712831,
      "grad_norm": 10.628379821777344,
      "learning_rate": 4.287290005090786e-05,
      "loss": 0.4944,
      "step": 4700
    },
    {
      "epoch": 0.7859169030535625,
      "grad_norm": 16.107650756835938,
      "learning_rate": 4.2855930765314786e-05,
      "loss": 0.7472,
      "step": 4710
    },
    {
      "epoch": 0.7875855164358418,
      "grad_norm": 4.507907390594482,
      "learning_rate": 4.2838961479721705e-05,
      "loss": 0.6286,
      "step": 4720
    },
    {
      "epoch": 0.7892541298181212,
      "grad_norm": 6.041508674621582,
      "learning_rate": 4.282199219412863e-05,
      "loss": 0.6341,
      "step": 4730
    },
    {
      "epoch": 0.7909227432004005,
      "grad_norm": 6.264540672302246,
      "learning_rate": 4.280502290853555e-05,
      "loss": 0.5388,
      "step": 4740
    },
    {
      "epoch": 0.7925913565826798,
      "grad_norm": 3.2984392642974854,
      "learning_rate": 4.2788053622942476e-05,
      "loss": 0.3909,
      "step": 4750
    },
    {
      "epoch": 0.7942599699649591,
      "grad_norm": 11.084415435791016,
      "learning_rate": 4.27710843373494e-05,
      "loss": 0.618,
      "step": 4760
    },
    {
      "epoch": 0.7959285833472385,
      "grad_norm": 2.163936138153076,
      "learning_rate": 4.275411505175632e-05,
      "loss": 0.588,
      "step": 4770
    },
    {
      "epoch": 0.7975971967295178,
      "grad_norm": 8.102314949035645,
      "learning_rate": 4.273714576616325e-05,
      "loss": 0.5813,
      "step": 4780
    },
    {
      "epoch": 0.7992658101117971,
      "grad_norm": 5.020988464355469,
      "learning_rate": 4.2720176480570166e-05,
      "loss": 0.7614,
      "step": 4790
    },
    {
      "epoch": 0.8009344234940764,
      "grad_norm": 11.799640655517578,
      "learning_rate": 4.27032071949771e-05,
      "loss": 0.5358,
      "step": 4800
    },
    {
      "epoch": 0.8026030368763557,
      "grad_norm": 2.727512836456299,
      "learning_rate": 4.268623790938402e-05,
      "loss": 0.7992,
      "step": 4810
    },
    {
      "epoch": 0.8042716502586351,
      "grad_norm": 5.246251583099365,
      "learning_rate": 4.266926862379094e-05,
      "loss": 0.8267,
      "step": 4820
    },
    {
      "epoch": 0.8059402636409144,
      "grad_norm": 2.2390408515930176,
      "learning_rate": 4.265229933819786e-05,
      "loss": 0.6939,
      "step": 4830
    },
    {
      "epoch": 0.8076088770231937,
      "grad_norm": 12.144538879394531,
      "learning_rate": 4.263533005260479e-05,
      "loss": 1.2697,
      "step": 4840
    },
    {
      "epoch": 0.809277490405473,
      "grad_norm": 16.213382720947266,
      "learning_rate": 4.2618360767011714e-05,
      "loss": 0.2932,
      "step": 4850
    },
    {
      "epoch": 0.8109461037877523,
      "grad_norm": 1.259371280670166,
      "learning_rate": 4.260139148141863e-05,
      "loss": 0.577,
      "step": 4860
    },
    {
      "epoch": 0.8126147171700318,
      "grad_norm": 7.75519323348999,
      "learning_rate": 4.258442219582556e-05,
      "loss": 0.6545,
      "step": 4870
    },
    {
      "epoch": 0.8142833305523111,
      "grad_norm": 5.786462306976318,
      "learning_rate": 4.256745291023248e-05,
      "loss": 0.7855,
      "step": 4880
    },
    {
      "epoch": 0.8159519439345904,
      "grad_norm": 5.113633155822754,
      "learning_rate": 4.2550483624639404e-05,
      "loss": 0.706,
      "step": 4890
    },
    {
      "epoch": 0.8176205573168697,
      "grad_norm": 19.406774520874023,
      "learning_rate": 4.253351433904632e-05,
      "loss": 0.614,
      "step": 4900
    },
    {
      "epoch": 0.819289170699149,
      "grad_norm": 9.980238914489746,
      "learning_rate": 4.251654505345325e-05,
      "loss": 0.7607,
      "step": 4910
    },
    {
      "epoch": 0.8209577840814284,
      "grad_norm": 6.8825578689575195,
      "learning_rate": 4.2499575767860175e-05,
      "loss": 0.6379,
      "step": 4920
    },
    {
      "epoch": 0.8226263974637077,
      "grad_norm": 4.099124431610107,
      "learning_rate": 4.24826064822671e-05,
      "loss": 0.5603,
      "step": 4930
    },
    {
      "epoch": 0.824295010845987,
      "grad_norm": 8.896454811096191,
      "learning_rate": 4.246563719667403e-05,
      "loss": 0.6675,
      "step": 4940
    },
    {
      "epoch": 0.8259636242282663,
      "grad_norm": 3.420372486114502,
      "learning_rate": 4.2448667911080946e-05,
      "loss": 0.6642,
      "step": 4950
    },
    {
      "epoch": 0.8276322376105456,
      "grad_norm": 4.121123313903809,
      "learning_rate": 4.243169862548787e-05,
      "loss": 0.5786,
      "step": 4960
    },
    {
      "epoch": 0.8293008509928249,
      "grad_norm": 11.6819486618042,
      "learning_rate": 4.241472933989479e-05,
      "loss": 0.6982,
      "step": 4970
    },
    {
      "epoch": 0.8309694643751043,
      "grad_norm": 3.4534575939178467,
      "learning_rate": 4.239776005430172e-05,
      "loss": 0.7891,
      "step": 4980
    },
    {
      "epoch": 0.8326380777573836,
      "grad_norm": 8.859320640563965,
      "learning_rate": 4.2380790768708636e-05,
      "loss": 1.2304,
      "step": 4990
    },
    {
      "epoch": 0.8343066911396629,
      "grad_norm": 6.556605815887451,
      "learning_rate": 4.236382148311556e-05,
      "loss": 0.8358,
      "step": 5000
    },
    {
      "epoch": 0.8359753045219422,
      "grad_norm": 4.322836399078369,
      "learning_rate": 4.234685219752249e-05,
      "loss": 0.8217,
      "step": 5010
    },
    {
      "epoch": 0.8376439179042215,
      "grad_norm": 3.1503679752349854,
      "learning_rate": 4.232988291192941e-05,
      "loss": 0.6249,
      "step": 5020
    },
    {
      "epoch": 0.839312531286501,
      "grad_norm": 2.255263566970825,
      "learning_rate": 4.231291362633633e-05,
      "loss": 0.5876,
      "step": 5030
    },
    {
      "epoch": 0.8409811446687803,
      "grad_norm": 5.652220249176025,
      "learning_rate": 4.229594434074325e-05,
      "loss": 0.543,
      "step": 5040
    },
    {
      "epoch": 0.8426497580510596,
      "grad_norm": 11.563413619995117,
      "learning_rate": 4.2278975055150184e-05,
      "loss": 0.8175,
      "step": 5050
    },
    {
      "epoch": 0.8443183714333389,
      "grad_norm": 8.309548377990723,
      "learning_rate": 4.22620057695571e-05,
      "loss": 0.7823,
      "step": 5060
    },
    {
      "epoch": 0.8459869848156182,
      "grad_norm": 5.631826877593994,
      "learning_rate": 4.224503648396403e-05,
      "loss": 0.9053,
      "step": 5070
    },
    {
      "epoch": 0.8476555981978976,
      "grad_norm": 3.193244457244873,
      "learning_rate": 4.2228067198370955e-05,
      "loss": 0.6664,
      "step": 5080
    },
    {
      "epoch": 0.8493242115801769,
      "grad_norm": 5.531043529510498,
      "learning_rate": 4.2211097912777874e-05,
      "loss": 0.6202,
      "step": 5090
    },
    {
      "epoch": 0.8509928249624562,
      "grad_norm": 5.683650016784668,
      "learning_rate": 4.21941286271848e-05,
      "loss": 0.5661,
      "step": 5100
    },
    {
      "epoch": 0.8526614383447355,
      "grad_norm": 4.842739105224609,
      "learning_rate": 4.217715934159172e-05,
      "loss": 0.6468,
      "step": 5110
    },
    {
      "epoch": 0.8543300517270148,
      "grad_norm": 1.5164592266082764,
      "learning_rate": 4.2160190055998645e-05,
      "loss": 0.4893,
      "step": 5120
    },
    {
      "epoch": 0.8559986651092941,
      "grad_norm": 6.801939010620117,
      "learning_rate": 4.2143220770405564e-05,
      "loss": 0.7387,
      "step": 5130
    },
    {
      "epoch": 0.8576672784915735,
      "grad_norm": 3.8182663917541504,
      "learning_rate": 4.212625148481249e-05,
      "loss": 0.7449,
      "step": 5140
    },
    {
      "epoch": 0.8593358918738528,
      "grad_norm": 8.898698806762695,
      "learning_rate": 4.2109282199219416e-05,
      "loss": 0.5247,
      "step": 5150
    },
    {
      "epoch": 0.8610045052561321,
      "grad_norm": 1.3317350149154663,
      "learning_rate": 4.2092312913626335e-05,
      "loss": 0.5463,
      "step": 5160
    },
    {
      "epoch": 0.8626731186384115,
      "grad_norm": 6.228333950042725,
      "learning_rate": 4.207534362803326e-05,
      "loss": 0.8868,
      "step": 5170
    },
    {
      "epoch": 0.8643417320206908,
      "grad_norm": 9.815524101257324,
      "learning_rate": 4.205837434244019e-05,
      "loss": 0.7263,
      "step": 5180
    },
    {
      "epoch": 0.8660103454029702,
      "grad_norm": 7.110081672668457,
      "learning_rate": 4.204140505684711e-05,
      "loss": 0.8358,
      "step": 5190
    },
    {
      "epoch": 0.8676789587852495,
      "grad_norm": 4.962923049926758,
      "learning_rate": 4.202443577125403e-05,
      "loss": 0.7162,
      "step": 5200
    },
    {
      "epoch": 0.8693475721675288,
      "grad_norm": 2.563223361968994,
      "learning_rate": 4.200746648566096e-05,
      "loss": 0.6345,
      "step": 5210
    },
    {
      "epoch": 0.8710161855498081,
      "grad_norm": 3.371091365814209,
      "learning_rate": 4.199049720006788e-05,
      "loss": 0.744,
      "step": 5220
    },
    {
      "epoch": 0.8726847989320874,
      "grad_norm": 4.976936340332031,
      "learning_rate": 4.19735279144748e-05,
      "loss": 0.5955,
      "step": 5230
    },
    {
      "epoch": 0.8743534123143668,
      "grad_norm": 6.893229961395264,
      "learning_rate": 4.195655862888173e-05,
      "loss": 0.4491,
      "step": 5240
    },
    {
      "epoch": 0.8760220256966461,
      "grad_norm": 7.070240020751953,
      "learning_rate": 4.193958934328865e-05,
      "loss": 0.5659,
      "step": 5250
    },
    {
      "epoch": 0.8776906390789254,
      "grad_norm": 3.339160442352295,
      "learning_rate": 4.1922620057695573e-05,
      "loss": 0.6215,
      "step": 5260
    },
    {
      "epoch": 0.8793592524612047,
      "grad_norm": 2.8599579334259033,
      "learning_rate": 4.190565077210249e-05,
      "loss": 0.6877,
      "step": 5270
    },
    {
      "epoch": 0.881027865843484,
      "grad_norm": 5.412785530090332,
      "learning_rate": 4.188868148650942e-05,
      "loss": 0.7017,
      "step": 5280
    },
    {
      "epoch": 0.8826964792257634,
      "grad_norm": 8.722503662109375,
      "learning_rate": 4.1871712200916344e-05,
      "loss": 0.646,
      "step": 5290
    },
    {
      "epoch": 0.8843650926080427,
      "grad_norm": 4.836069107055664,
      "learning_rate": 4.185474291532327e-05,
      "loss": 0.5328,
      "step": 5300
    },
    {
      "epoch": 0.886033705990322,
      "grad_norm": 9.418333053588867,
      "learning_rate": 4.183777362973019e-05,
      "loss": 0.9033,
      "step": 5310
    },
    {
      "epoch": 0.8877023193726014,
      "grad_norm": 7.081607818603516,
      "learning_rate": 4.1820804344137115e-05,
      "loss": 0.7894,
      "step": 5320
    },
    {
      "epoch": 0.8893709327548807,
      "grad_norm": 4.9945526123046875,
      "learning_rate": 4.180383505854404e-05,
      "loss": 0.5195,
      "step": 5330
    },
    {
      "epoch": 0.89103954613716,
      "grad_norm": 6.5386176109313965,
      "learning_rate": 4.178686577295096e-05,
      "loss": 0.6565,
      "step": 5340
    },
    {
      "epoch": 0.8927081595194394,
      "grad_norm": 6.849684715270996,
      "learning_rate": 4.1769896487357886e-05,
      "loss": 0.5634,
      "step": 5350
    },
    {
      "epoch": 0.8943767729017187,
      "grad_norm": 13.75334358215332,
      "learning_rate": 4.1752927201764805e-05,
      "loss": 0.7763,
      "step": 5360
    },
    {
      "epoch": 0.896045386283998,
      "grad_norm": 6.718748092651367,
      "learning_rate": 4.173595791617173e-05,
      "loss": 0.9289,
      "step": 5370
    },
    {
      "epoch": 0.8977139996662773,
      "grad_norm": 12.123237609863281,
      "learning_rate": 4.171898863057866e-05,
      "loss": 0.5061,
      "step": 5380
    },
    {
      "epoch": 0.8993826130485566,
      "grad_norm": 10.28067684173584,
      "learning_rate": 4.1702019344985576e-05,
      "loss": 0.8026,
      "step": 5390
    },
    {
      "epoch": 0.901051226430836,
      "grad_norm": 8.775866508483887,
      "learning_rate": 4.16850500593925e-05,
      "loss": 0.7988,
      "step": 5400
    },
    {
      "epoch": 0.9027198398131153,
      "grad_norm": 7.310545444488525,
      "learning_rate": 4.166808077379942e-05,
      "loss": 0.5305,
      "step": 5410
    },
    {
      "epoch": 0.9043884531953946,
      "grad_norm": 2.226510763168335,
      "learning_rate": 4.165111148820635e-05,
      "loss": 0.7124,
      "step": 5420
    },
    {
      "epoch": 0.9060570665776739,
      "grad_norm": 7.282177448272705,
      "learning_rate": 4.163414220261327e-05,
      "loss": 0.4127,
      "step": 5430
    },
    {
      "epoch": 0.9077256799599532,
      "grad_norm": 10.734892845153809,
      "learning_rate": 4.16171729170202e-05,
      "loss": 0.6631,
      "step": 5440
    },
    {
      "epoch": 0.9093942933422327,
      "grad_norm": 11.756941795349121,
      "learning_rate": 4.160020363142712e-05,
      "loss": 0.5773,
      "step": 5450
    },
    {
      "epoch": 0.911062906724512,
      "grad_norm": 3.166810989379883,
      "learning_rate": 4.1583234345834043e-05,
      "loss": 0.6495,
      "step": 5460
    },
    {
      "epoch": 0.9127315201067913,
      "grad_norm": 4.299709320068359,
      "learning_rate": 4.156626506024097e-05,
      "loss": 0.6716,
      "step": 5470
    },
    {
      "epoch": 0.9144001334890706,
      "grad_norm": 4.141909599304199,
      "learning_rate": 4.154929577464789e-05,
      "loss": 0.7565,
      "step": 5480
    },
    {
      "epoch": 0.9160687468713499,
      "grad_norm": 9.989823341369629,
      "learning_rate": 4.1532326489054814e-05,
      "loss": 0.6365,
      "step": 5490
    },
    {
      "epoch": 0.9177373602536293,
      "grad_norm": 7.672375679016113,
      "learning_rate": 4.1515357203461733e-05,
      "loss": 0.8757,
      "step": 5500
    },
    {
      "epoch": 0.9194059736359086,
      "grad_norm": 6.755795478820801,
      "learning_rate": 4.149838791786866e-05,
      "loss": 0.673,
      "step": 5510
    },
    {
      "epoch": 0.9210745870181879,
      "grad_norm": 4.3422346115112305,
      "learning_rate": 4.148141863227558e-05,
      "loss": 0.5129,
      "step": 5520
    },
    {
      "epoch": 0.9227432004004672,
      "grad_norm": 4.82142972946167,
      "learning_rate": 4.1464449346682504e-05,
      "loss": 0.4887,
      "step": 5530
    },
    {
      "epoch": 0.9244118137827465,
      "grad_norm": 3.747398853302002,
      "learning_rate": 4.144748006108943e-05,
      "loss": 0.4191,
      "step": 5540
    },
    {
      "epoch": 0.9260804271650258,
      "grad_norm": 6.317661285400391,
      "learning_rate": 4.1430510775496356e-05,
      "loss": 0.6077,
      "step": 5550
    },
    {
      "epoch": 0.9277490405473052,
      "grad_norm": 8.354905128479004,
      "learning_rate": 4.141354148990328e-05,
      "loss": 0.5112,
      "step": 5560
    },
    {
      "epoch": 0.9294176539295845,
      "grad_norm": 4.525085926055908,
      "learning_rate": 4.13965722043102e-05,
      "loss": 0.6139,
      "step": 5570
    },
    {
      "epoch": 0.9310862673118638,
      "grad_norm": 8.851008415222168,
      "learning_rate": 4.137960291871713e-05,
      "loss": 0.6961,
      "step": 5580
    },
    {
      "epoch": 0.9327548806941431,
      "grad_norm": 1.5388422012329102,
      "learning_rate": 4.1362633633124046e-05,
      "loss": 0.437,
      "step": 5590
    },
    {
      "epoch": 0.9344234940764224,
      "grad_norm": 12.384132385253906,
      "learning_rate": 4.134566434753097e-05,
      "loss": 0.6892,
      "step": 5600
    },
    {
      "epoch": 0.9360921074587019,
      "grad_norm": 5.3065714836120605,
      "learning_rate": 4.132869506193789e-05,
      "loss": 0.6548,
      "step": 5610
    },
    {
      "epoch": 0.9377607208409812,
      "grad_norm": 8.042698860168457,
      "learning_rate": 4.131172577634482e-05,
      "loss": 0.8408,
      "step": 5620
    },
    {
      "epoch": 0.9394293342232605,
      "grad_norm": 1.0562492609024048,
      "learning_rate": 4.129475649075174e-05,
      "loss": 0.6352,
      "step": 5630
    },
    {
      "epoch": 0.9410979476055398,
      "grad_norm": 5.534671306610107,
      "learning_rate": 4.127778720515866e-05,
      "loss": 0.6412,
      "step": 5640
    },
    {
      "epoch": 0.9427665609878191,
      "grad_norm": 7.715068817138672,
      "learning_rate": 4.126081791956559e-05,
      "loss": 0.6922,
      "step": 5650
    },
    {
      "epoch": 0.9444351743700985,
      "grad_norm": 3.7539029121398926,
      "learning_rate": 4.124384863397251e-05,
      "loss": 0.6979,
      "step": 5660
    },
    {
      "epoch": 0.9461037877523778,
      "grad_norm": 6.7038350105285645,
      "learning_rate": 4.122687934837943e-05,
      "loss": 0.838,
      "step": 5670
    },
    {
      "epoch": 0.9477724011346571,
      "grad_norm": 1.6392005681991577,
      "learning_rate": 4.120991006278636e-05,
      "loss": 0.4468,
      "step": 5680
    },
    {
      "epoch": 0.9494410145169364,
      "grad_norm": 3.4735050201416016,
      "learning_rate": 4.1192940777193284e-05,
      "loss": 0.8158,
      "step": 5690
    },
    {
      "epoch": 0.9511096278992157,
      "grad_norm": 8.991637229919434,
      "learning_rate": 4.117597149160021e-05,
      "loss": 0.8683,
      "step": 5700
    },
    {
      "epoch": 0.9527782412814951,
      "grad_norm": 7.031501770019531,
      "learning_rate": 4.115900220600713e-05,
      "loss": 0.7253,
      "step": 5710
    },
    {
      "epoch": 0.9544468546637744,
      "grad_norm": 7.107928276062012,
      "learning_rate": 4.1142032920414055e-05,
      "loss": 0.5106,
      "step": 5720
    },
    {
      "epoch": 0.9561154680460537,
      "grad_norm": 7.524052143096924,
      "learning_rate": 4.1125063634820974e-05,
      "loss": 0.52,
      "step": 5730
    },
    {
      "epoch": 0.957784081428333,
      "grad_norm": 3.145481586456299,
      "learning_rate": 4.11080943492279e-05,
      "loss": 0.5373,
      "step": 5740
    },
    {
      "epoch": 0.9594526948106123,
      "grad_norm": 2.202536106109619,
      "learning_rate": 4.109112506363482e-05,
      "loss": 0.4803,
      "step": 5750
    },
    {
      "epoch": 0.9611213081928917,
      "grad_norm": 11.449325561523438,
      "learning_rate": 4.1074155778041745e-05,
      "loss": 0.5958,
      "step": 5760
    },
    {
      "epoch": 0.9627899215751711,
      "grad_norm": 6.558907985687256,
      "learning_rate": 4.105718649244867e-05,
      "loss": 0.7473,
      "step": 5770
    },
    {
      "epoch": 0.9644585349574504,
      "grad_norm": 3.457144021987915,
      "learning_rate": 4.104021720685559e-05,
      "loss": 0.8943,
      "step": 5780
    },
    {
      "epoch": 0.9661271483397297,
      "grad_norm": 4.960923194885254,
      "learning_rate": 4.1023247921262516e-05,
      "loss": 0.7275,
      "step": 5790
    },
    {
      "epoch": 0.967795761722009,
      "grad_norm": 4.399231433868408,
      "learning_rate": 4.100627863566944e-05,
      "loss": 1.1137,
      "step": 5800
    },
    {
      "epoch": 0.9694643751042883,
      "grad_norm": 7.2484517097473145,
      "learning_rate": 4.098930935007637e-05,
      "loss": 0.6629,
      "step": 5810
    },
    {
      "epoch": 0.9711329884865677,
      "grad_norm": 6.6189961433410645,
      "learning_rate": 4.097234006448329e-05,
      "loss": 0.6317,
      "step": 5820
    },
    {
      "epoch": 0.972801601868847,
      "grad_norm": 14.54278564453125,
      "learning_rate": 4.095537077889021e-05,
      "loss": 0.9016,
      "step": 5830
    },
    {
      "epoch": 0.9744702152511263,
      "grad_norm": 7.602426528930664,
      "learning_rate": 4.093840149329713e-05,
      "loss": 0.4989,
      "step": 5840
    },
    {
      "epoch": 0.9761388286334056,
      "grad_norm": 1.0486760139465332,
      "learning_rate": 4.092143220770406e-05,
      "loss": 0.5774,
      "step": 5850
    },
    {
      "epoch": 0.9778074420156849,
      "grad_norm": 3.768935203552246,
      "learning_rate": 4.0904462922110984e-05,
      "loss": 0.3698,
      "step": 5860
    },
    {
      "epoch": 0.9794760553979643,
      "grad_norm": 7.159753322601318,
      "learning_rate": 4.08874936365179e-05,
      "loss": 0.6508,
      "step": 5870
    },
    {
      "epoch": 0.9811446687802436,
      "grad_norm": 7.644687175750732,
      "learning_rate": 4.087052435092483e-05,
      "loss": 0.4545,
      "step": 5880
    },
    {
      "epoch": 0.982813282162523,
      "grad_norm": 9.999479293823242,
      "learning_rate": 4.085355506533175e-05,
      "loss": 0.6054,
      "step": 5890
    },
    {
      "epoch": 0.9844818955448023,
      "grad_norm": 5.687373161315918,
      "learning_rate": 4.0836585779738674e-05,
      "loss": 0.7532,
      "step": 5900
    },
    {
      "epoch": 0.9861505089270816,
      "grad_norm": 4.953110218048096,
      "learning_rate": 4.08196164941456e-05,
      "loss": 0.5566,
      "step": 5910
    },
    {
      "epoch": 0.987819122309361,
      "grad_norm": 2.7320237159729004,
      "learning_rate": 4.0802647208552525e-05,
      "loss": 0.614,
      "step": 5920
    },
    {
      "epoch": 0.9894877356916403,
      "grad_norm": 7.805532932281494,
      "learning_rate": 4.0785677922959444e-05,
      "loss": 0.5588,
      "step": 5930
    },
    {
      "epoch": 0.9911563490739196,
      "grad_norm": 7.908514976501465,
      "learning_rate": 4.076870863736637e-05,
      "loss": 0.8623,
      "step": 5940
    },
    {
      "epoch": 0.9928249624561989,
      "grad_norm": 1.8355716466903687,
      "learning_rate": 4.0751739351773296e-05,
      "loss": 0.4996,
      "step": 5950
    },
    {
      "epoch": 0.9944935758384782,
      "grad_norm": 2.027433156967163,
      "learning_rate": 4.0734770066180215e-05,
      "loss": 0.8997,
      "step": 5960
    },
    {
      "epoch": 0.9961621892207575,
      "grad_norm": 8.070856094360352,
      "learning_rate": 4.071780078058714e-05,
      "loss": 0.8567,
      "step": 5970
    },
    {
      "epoch": 0.9978308026030369,
      "grad_norm": 5.994241714477539,
      "learning_rate": 4.070083149499406e-05,
      "loss": 0.9064,
      "step": 5980
    },
    {
      "epoch": 0.9994994159853162,
      "grad_norm": 4.204895496368408,
      "learning_rate": 4.0683862209400986e-05,
      "loss": 0.703,
      "step": 5990
    },
    {
      "epoch": 1.0011680293675955,
      "grad_norm": 6.452363967895508,
      "learning_rate": 4.066689292380791e-05,
      "loss": 0.931,
      "step": 6000
    },
    {
      "epoch": 1.002836642749875,
      "grad_norm": 9.70814323425293,
      "learning_rate": 4.064992363821483e-05,
      "loss": 0.887,
      "step": 6010
    },
    {
      "epoch": 1.0045052561321541,
      "grad_norm": 5.757652759552002,
      "learning_rate": 4.063295435262176e-05,
      "loss": 0.6966,
      "step": 6020
    },
    {
      "epoch": 1.0061738695144335,
      "grad_norm": 11.107064247131348,
      "learning_rate": 4.0615985067028676e-05,
      "loss": 0.6586,
      "step": 6030
    },
    {
      "epoch": 1.0078424828967127,
      "grad_norm": 2.1242623329162598,
      "learning_rate": 4.05990157814356e-05,
      "loss": 0.598,
      "step": 6040
    },
    {
      "epoch": 1.0095110962789922,
      "grad_norm": 8.732366561889648,
      "learning_rate": 4.058204649584253e-05,
      "loss": 0.8618,
      "step": 6050
    },
    {
      "epoch": 1.0111797096612716,
      "grad_norm": 4.514962196350098,
      "learning_rate": 4.0565077210249454e-05,
      "loss": 0.5061,
      "step": 6060
    },
    {
      "epoch": 1.0128483230435508,
      "grad_norm": 3.3237650394439697,
      "learning_rate": 4.054810792465637e-05,
      "loss": 0.6074,
      "step": 6070
    },
    {
      "epoch": 1.0145169364258302,
      "grad_norm": 7.82145881652832,
      "learning_rate": 4.05311386390633e-05,
      "loss": 0.7204,
      "step": 6080
    },
    {
      "epoch": 1.0161855498081094,
      "grad_norm": 1.5880180597305298,
      "learning_rate": 4.0514169353470224e-05,
      "loss": 0.4037,
      "step": 6090
    },
    {
      "epoch": 1.0178541631903888,
      "grad_norm": 2.071082353591919,
      "learning_rate": 4.0497200067877144e-05,
      "loss": 0.6935,
      "step": 6100
    },
    {
      "epoch": 1.0195227765726682,
      "grad_norm": 10.974601745605469,
      "learning_rate": 4.048023078228407e-05,
      "loss": 0.7285,
      "step": 6110
    },
    {
      "epoch": 1.0211913899549474,
      "grad_norm": 15.488637924194336,
      "learning_rate": 4.046326149669099e-05,
      "loss": 0.7752,
      "step": 6120
    },
    {
      "epoch": 1.0228600033372268,
      "grad_norm": 4.307401657104492,
      "learning_rate": 4.0446292211097914e-05,
      "loss": 0.7469,
      "step": 6130
    },
    {
      "epoch": 1.024528616719506,
      "grad_norm": 2.5659019947052,
      "learning_rate": 4.0429322925504834e-05,
      "loss": 0.6405,
      "step": 6140
    },
    {
      "epoch": 1.0261972301017854,
      "grad_norm": 6.318736553192139,
      "learning_rate": 4.041235363991176e-05,
      "loss": 0.4995,
      "step": 6150
    },
    {
      "epoch": 1.0278658434840648,
      "grad_norm": 9.108660697937012,
      "learning_rate": 4.0395384354318685e-05,
      "loss": 0.5685,
      "step": 6160
    },
    {
      "epoch": 1.029534456866344,
      "grad_norm": 1.4804381132125854,
      "learning_rate": 4.037841506872561e-05,
      "loss": 0.5341,
      "step": 6170
    },
    {
      "epoch": 1.0312030702486235,
      "grad_norm": 9.271636009216309,
      "learning_rate": 4.036144578313254e-05,
      "loss": 0.6538,
      "step": 6180
    },
    {
      "epoch": 1.0328716836309026,
      "grad_norm": 4.025053977966309,
      "learning_rate": 4.0344476497539456e-05,
      "loss": 0.6924,
      "step": 6190
    },
    {
      "epoch": 1.034540297013182,
      "grad_norm": 4.319014549255371,
      "learning_rate": 4.032750721194638e-05,
      "loss": 0.6863,
      "step": 6200
    },
    {
      "epoch": 1.0362089103954615,
      "grad_norm": 6.248849868774414,
      "learning_rate": 4.03105379263533e-05,
      "loss": 0.776,
      "step": 6210
    },
    {
      "epoch": 1.0378775237777407,
      "grad_norm": 4.492367744445801,
      "learning_rate": 4.029356864076023e-05,
      "loss": 0.8395,
      "step": 6220
    },
    {
      "epoch": 1.03954613716002,
      "grad_norm": 4.884897708892822,
      "learning_rate": 4.0276599355167146e-05,
      "loss": 0.4691,
      "step": 6230
    },
    {
      "epoch": 1.0412147505422993,
      "grad_norm": 5.811783313751221,
      "learning_rate": 4.025963006957407e-05,
      "loss": 0.652,
      "step": 6240
    },
    {
      "epoch": 1.0428833639245787,
      "grad_norm": 9.772992134094238,
      "learning_rate": 4.0242660783981e-05,
      "loss": 0.7405,
      "step": 6250
    },
    {
      "epoch": 1.044551977306858,
      "grad_norm": 4.007981777191162,
      "learning_rate": 4.022569149838792e-05,
      "loss": 0.45,
      "step": 6260
    },
    {
      "epoch": 1.0462205906891373,
      "grad_norm": 3.125521421432495,
      "learning_rate": 4.020872221279484e-05,
      "loss": 0.4395,
      "step": 6270
    },
    {
      "epoch": 1.0478892040714167,
      "grad_norm": 12.306452751159668,
      "learning_rate": 4.019175292720176e-05,
      "loss": 0.7185,
      "step": 6280
    },
    {
      "epoch": 1.049557817453696,
      "grad_norm": 3.59104323387146,
      "learning_rate": 4.017478364160869e-05,
      "loss": 0.453,
      "step": 6290
    },
    {
      "epoch": 1.0512264308359753,
      "grad_norm": 5.61136531829834,
      "learning_rate": 4.0157814356015614e-05,
      "loss": 0.6339,
      "step": 6300
    },
    {
      "epoch": 1.0528950442182545,
      "grad_norm": 2.623913049697876,
      "learning_rate": 4.014084507042254e-05,
      "loss": 0.4613,
      "step": 6310
    },
    {
      "epoch": 1.054563657600534,
      "grad_norm": 9.31950569152832,
      "learning_rate": 4.0123875784829465e-05,
      "loss": 0.6252,
      "step": 6320
    },
    {
      "epoch": 1.0562322709828134,
      "grad_norm": 9.423600196838379,
      "learning_rate": 4.0106906499236384e-05,
      "loss": 0.6655,
      "step": 6330
    },
    {
      "epoch": 1.0579008843650926,
      "grad_norm": 6.726747512817383,
      "learning_rate": 4.008993721364331e-05,
      "loss": 0.6147,
      "step": 6340
    },
    {
      "epoch": 1.059569497747372,
      "grad_norm": 6.676293849945068,
      "learning_rate": 4.007296792805023e-05,
      "loss": 0.501,
      "step": 6350
    },
    {
      "epoch": 1.0612381111296512,
      "grad_norm": 6.032887935638428,
      "learning_rate": 4.0055998642457155e-05,
      "loss": 0.3644,
      "step": 6360
    },
    {
      "epoch": 1.0629067245119306,
      "grad_norm": 4.237717151641846,
      "learning_rate": 4.0039029356864074e-05,
      "loss": 0.4959,
      "step": 6370
    },
    {
      "epoch": 1.06457533789421,
      "grad_norm": 5.539376258850098,
      "learning_rate": 4.0022060071271e-05,
      "loss": 0.4417,
      "step": 6380
    },
    {
      "epoch": 1.0662439512764892,
      "grad_norm": 3.3651347160339355,
      "learning_rate": 4.0005090785677926e-05,
      "loss": 0.5681,
      "step": 6390
    },
    {
      "epoch": 1.0679125646587686,
      "grad_norm": 3.22143816947937,
      "learning_rate": 3.9988121500084845e-05,
      "loss": 0.5609,
      "step": 6400
    },
    {
      "epoch": 1.0695811780410478,
      "grad_norm": 13.483207702636719,
      "learning_rate": 3.997115221449177e-05,
      "loss": 0.4526,
      "step": 6410
    },
    {
      "epoch": 1.0712497914233272,
      "grad_norm": 5.522721290588379,
      "learning_rate": 3.99541829288987e-05,
      "loss": 0.6213,
      "step": 6420
    },
    {
      "epoch": 1.0729184048056066,
      "grad_norm": 8.329171180725098,
      "learning_rate": 3.993721364330562e-05,
      "loss": 0.4773,
      "step": 6430
    },
    {
      "epoch": 1.0745870181878858,
      "grad_norm": 6.116387844085693,
      "learning_rate": 3.992024435771254e-05,
      "loss": 0.5477,
      "step": 6440
    },
    {
      "epoch": 1.0762556315701652,
      "grad_norm": 6.06419038772583,
      "learning_rate": 3.990327507211947e-05,
      "loss": 0.645,
      "step": 6450
    },
    {
      "epoch": 1.0779242449524444,
      "grad_norm": 4.918543815612793,
      "learning_rate": 3.988630578652639e-05,
      "loss": 0.8897,
      "step": 6460
    },
    {
      "epoch": 1.0795928583347238,
      "grad_norm": 6.925195693969727,
      "learning_rate": 3.986933650093331e-05,
      "loss": 0.7725,
      "step": 6470
    },
    {
      "epoch": 1.0812614717170033,
      "grad_norm": 5.18811559677124,
      "learning_rate": 3.985236721534024e-05,
      "loss": 0.6733,
      "step": 6480
    },
    {
      "epoch": 1.0829300850992825,
      "grad_norm": 7.084288597106934,
      "learning_rate": 3.983539792974716e-05,
      "loss": 0.4155,
      "step": 6490
    },
    {
      "epoch": 1.0845986984815619,
      "grad_norm": 3.4874751567840576,
      "learning_rate": 3.9818428644154084e-05,
      "loss": 0.4371,
      "step": 6500
    },
    {
      "epoch": 1.086267311863841,
      "grad_norm": 4.360889911651611,
      "learning_rate": 3.9801459358561e-05,
      "loss": 0.6662,
      "step": 6510
    },
    {
      "epoch": 1.0879359252461205,
      "grad_norm": 13.805707931518555,
      "learning_rate": 3.978449007296793e-05,
      "loss": 0.5468,
      "step": 6520
    },
    {
      "epoch": 1.0896045386284,
      "grad_norm": 8.457906723022461,
      "learning_rate": 3.9767520787374855e-05,
      "loss": 0.6934,
      "step": 6530
    },
    {
      "epoch": 1.091273152010679,
      "grad_norm": 9.740283966064453,
      "learning_rate": 3.9750551501781774e-05,
      "loss": 0.629,
      "step": 6540
    },
    {
      "epoch": 1.0929417653929585,
      "grad_norm": 1.554426670074463,
      "learning_rate": 3.97335822161887e-05,
      "loss": 0.2695,
      "step": 6550
    },
    {
      "epoch": 1.0946103787752377,
      "grad_norm": 7.647530555725098,
      "learning_rate": 3.9716612930595625e-05,
      "loss": 0.7094,
      "step": 6560
    },
    {
      "epoch": 1.0962789921575171,
      "grad_norm": 1.3183813095092773,
      "learning_rate": 3.969964364500255e-05,
      "loss": 0.3295,
      "step": 6570
    },
    {
      "epoch": 1.0979476055397965,
      "grad_norm": 8.293034553527832,
      "learning_rate": 3.968267435940947e-05,
      "loss": 0.8624,
      "step": 6580
    },
    {
      "epoch": 1.0996162189220757,
      "grad_norm": 13.888615608215332,
      "learning_rate": 3.9665705073816396e-05,
      "loss": 0.3953,
      "step": 6590
    },
    {
      "epoch": 1.1012848323043551,
      "grad_norm": 10.42340087890625,
      "learning_rate": 3.9648735788223315e-05,
      "loss": 0.3288,
      "step": 6600
    },
    {
      "epoch": 1.1029534456866343,
      "grad_norm": 4.7607741355896,
      "learning_rate": 3.963176650263024e-05,
      "loss": 0.347,
      "step": 6610
    },
    {
      "epoch": 1.1046220590689138,
      "grad_norm": 4.208438396453857,
      "learning_rate": 3.961479721703717e-05,
      "loss": 0.812,
      "step": 6620
    },
    {
      "epoch": 1.106290672451193,
      "grad_norm": 5.72502326965332,
      "learning_rate": 3.9597827931444086e-05,
      "loss": 0.5614,
      "step": 6630
    },
    {
      "epoch": 1.1079592858334724,
      "grad_norm": 8.286643981933594,
      "learning_rate": 3.958085864585101e-05,
      "loss": 0.4737,
      "step": 6640
    },
    {
      "epoch": 1.1096278992157518,
      "grad_norm": 5.74937105178833,
      "learning_rate": 3.956388936025793e-05,
      "loss": 0.9574,
      "step": 6650
    },
    {
      "epoch": 1.111296512598031,
      "grad_norm": 5.976428985595703,
      "learning_rate": 3.954692007466486e-05,
      "loss": 0.5012,
      "step": 6660
    },
    {
      "epoch": 1.1129651259803104,
      "grad_norm": 5.311002254486084,
      "learning_rate": 3.952995078907178e-05,
      "loss": 0.742,
      "step": 6670
    },
    {
      "epoch": 1.1146337393625898,
      "grad_norm": 1.499950885772705,
      "learning_rate": 3.951298150347871e-05,
      "loss": 0.6304,
      "step": 6680
    },
    {
      "epoch": 1.116302352744869,
      "grad_norm": 4.021117210388184,
      "learning_rate": 3.949601221788563e-05,
      "loss": 0.4069,
      "step": 6690
    },
    {
      "epoch": 1.1179709661271484,
      "grad_norm": 6.643870830535889,
      "learning_rate": 3.9479042932292554e-05,
      "loss": 0.5674,
      "step": 6700
    },
    {
      "epoch": 1.1196395795094276,
      "grad_norm": 8.012467384338379,
      "learning_rate": 3.946207364669948e-05,
      "loss": 0.5107,
      "step": 6710
    },
    {
      "epoch": 1.121308192891707,
      "grad_norm": 4.6667866706848145,
      "learning_rate": 3.94451043611064e-05,
      "loss": 0.6275,
      "step": 6720
    },
    {
      "epoch": 1.1229768062739862,
      "grad_norm": 11.827342987060547,
      "learning_rate": 3.9428135075513325e-05,
      "loss": 0.7092,
      "step": 6730
    },
    {
      "epoch": 1.1246454196562656,
      "grad_norm": 2.769144058227539,
      "learning_rate": 3.9411165789920244e-05,
      "loss": 0.6011,
      "step": 6740
    },
    {
      "epoch": 1.126314033038545,
      "grad_norm": 12.51241683959961,
      "learning_rate": 3.939419650432717e-05,
      "loss": 0.6983,
      "step": 6750
    },
    {
      "epoch": 1.1279826464208242,
      "grad_norm": 0.9313949346542358,
      "learning_rate": 3.937722721873409e-05,
      "loss": 0.5012,
      "step": 6760
    },
    {
      "epoch": 1.1296512598031037,
      "grad_norm": 11.29928970336914,
      "learning_rate": 3.9360257933141015e-05,
      "loss": 0.5676,
      "step": 6770
    },
    {
      "epoch": 1.1313198731853829,
      "grad_norm": 1.6237843036651611,
      "learning_rate": 3.934328864754794e-05,
      "loss": 0.6212,
      "step": 6780
    },
    {
      "epoch": 1.1329884865676623,
      "grad_norm": 10.124720573425293,
      "learning_rate": 3.932631936195486e-05,
      "loss": 0.9343,
      "step": 6790
    },
    {
      "epoch": 1.1346570999499417,
      "grad_norm": 2.8833553791046143,
      "learning_rate": 3.930935007636179e-05,
      "loss": 0.3764,
      "step": 6800
    },
    {
      "epoch": 1.1363257133322209,
      "grad_norm": 7.623715877532959,
      "learning_rate": 3.929238079076871e-05,
      "loss": 0.3045,
      "step": 6810
    },
    {
      "epoch": 1.1379943267145003,
      "grad_norm": 5.706605434417725,
      "learning_rate": 3.927541150517564e-05,
      "loss": 0.8366,
      "step": 6820
    },
    {
      "epoch": 1.1396629400967795,
      "grad_norm": 7.017123222351074,
      "learning_rate": 3.9258442219582556e-05,
      "loss": 0.7341,
      "step": 6830
    },
    {
      "epoch": 1.141331553479059,
      "grad_norm": 6.041262626647949,
      "learning_rate": 3.924147293398948e-05,
      "loss": 0.7808,
      "step": 6840
    },
    {
      "epoch": 1.1430001668613383,
      "grad_norm": 5.1493988037109375,
      "learning_rate": 3.92245036483964e-05,
      "loss": 0.5102,
      "step": 6850
    },
    {
      "epoch": 1.1446687802436175,
      "grad_norm": 3.4423091411590576,
      "learning_rate": 3.920753436280333e-05,
      "loss": 0.7236,
      "step": 6860
    },
    {
      "epoch": 1.146337393625897,
      "grad_norm": 11.081779479980469,
      "learning_rate": 3.919056507721025e-05,
      "loss": 0.7033,
      "step": 6870
    },
    {
      "epoch": 1.1480060070081761,
      "grad_norm": 2.1486687660217285,
      "learning_rate": 3.917359579161717e-05,
      "loss": 0.4139,
      "step": 6880
    },
    {
      "epoch": 1.1496746203904555,
      "grad_norm": 4.132455825805664,
      "learning_rate": 3.91566265060241e-05,
      "loss": 0.4711,
      "step": 6890
    },
    {
      "epoch": 1.151343233772735,
      "grad_norm": 2.089820623397827,
      "learning_rate": 3.913965722043102e-05,
      "loss": 0.6067,
      "step": 6900
    },
    {
      "epoch": 1.1530118471550141,
      "grad_norm": 3.971978187561035,
      "learning_rate": 3.912268793483794e-05,
      "loss": 0.8773,
      "step": 6910
    },
    {
      "epoch": 1.1546804605372936,
      "grad_norm": 7.959862232208252,
      "learning_rate": 3.910571864924487e-05,
      "loss": 0.6127,
      "step": 6920
    },
    {
      "epoch": 1.1563490739195728,
      "grad_norm": 3.8342251777648926,
      "learning_rate": 3.9088749363651795e-05,
      "loss": 0.3562,
      "step": 6930
    },
    {
      "epoch": 1.1580176873018522,
      "grad_norm": 7.878370761871338,
      "learning_rate": 3.907178007805872e-05,
      "loss": 0.6654,
      "step": 6940
    },
    {
      "epoch": 1.1596863006841316,
      "grad_norm": 3.6111323833465576,
      "learning_rate": 3.905481079246564e-05,
      "loss": 0.6343,
      "step": 6950
    },
    {
      "epoch": 1.1613549140664108,
      "grad_norm": 3.9259631633758545,
      "learning_rate": 3.9037841506872566e-05,
      "loss": 0.9279,
      "step": 6960
    },
    {
      "epoch": 1.1630235274486902,
      "grad_norm": 2.123358726501465,
      "learning_rate": 3.9020872221279485e-05,
      "loss": 0.5184,
      "step": 6970
    },
    {
      "epoch": 1.1646921408309694,
      "grad_norm": 4.260928153991699,
      "learning_rate": 3.900390293568641e-05,
      "loss": 0.5713,
      "step": 6980
    },
    {
      "epoch": 1.1663607542132488,
      "grad_norm": 2.5002646446228027,
      "learning_rate": 3.898693365009333e-05,
      "loss": 0.6652,
      "step": 6990
    },
    {
      "epoch": 1.168029367595528,
      "grad_norm": 6.687923908233643,
      "learning_rate": 3.8969964364500255e-05,
      "loss": 0.6513,
      "step": 7000
    },
    {
      "epoch": 1.1696979809778074,
      "grad_norm": 4.537695407867432,
      "learning_rate": 3.895299507890718e-05,
      "loss": 0.7158,
      "step": 7010
    },
    {
      "epoch": 1.1713665943600868,
      "grad_norm": 4.466524124145508,
      "learning_rate": 3.89360257933141e-05,
      "loss": 0.6415,
      "step": 7020
    },
    {
      "epoch": 1.173035207742366,
      "grad_norm": 6.4244160652160645,
      "learning_rate": 3.8919056507721026e-05,
      "loss": 0.6071,
      "step": 7030
    },
    {
      "epoch": 1.1747038211246454,
      "grad_norm": 4.207466125488281,
      "learning_rate": 3.8902087222127945e-05,
      "loss": 0.5935,
      "step": 7040
    },
    {
      "epoch": 1.1763724345069249,
      "grad_norm": 6.414242267608643,
      "learning_rate": 3.888511793653488e-05,
      "loss": 0.6637,
      "step": 7050
    },
    {
      "epoch": 1.178041047889204,
      "grad_norm": 6.024798393249512,
      "learning_rate": 3.88681486509418e-05,
      "loss": 0.5791,
      "step": 7060
    },
    {
      "epoch": 1.1797096612714835,
      "grad_norm": 9.773534774780273,
      "learning_rate": 3.885117936534872e-05,
      "loss": 0.9924,
      "step": 7070
    },
    {
      "epoch": 1.1813782746537627,
      "grad_norm": 2.3335888385772705,
      "learning_rate": 3.883421007975564e-05,
      "loss": 0.7044,
      "step": 7080
    },
    {
      "epoch": 1.183046888036042,
      "grad_norm": 4.184549331665039,
      "learning_rate": 3.881724079416257e-05,
      "loss": 0.614,
      "step": 7090
    },
    {
      "epoch": 1.1847155014183213,
      "grad_norm": 7.5106682777404785,
      "learning_rate": 3.8800271508569494e-05,
      "loss": 0.5843,
      "step": 7100
    },
    {
      "epoch": 1.1863841148006007,
      "grad_norm": 13.612321853637695,
      "learning_rate": 3.878330222297641e-05,
      "loss": 0.6738,
      "step": 7110
    },
    {
      "epoch": 1.18805272818288,
      "grad_norm": 4.663409233093262,
      "learning_rate": 3.876633293738334e-05,
      "loss": 0.6024,
      "step": 7120
    },
    {
      "epoch": 1.1897213415651593,
      "grad_norm": 4.475833415985107,
      "learning_rate": 3.874936365179026e-05,
      "loss": 0.6286,
      "step": 7130
    },
    {
      "epoch": 1.1913899549474387,
      "grad_norm": 4.284053325653076,
      "learning_rate": 3.8732394366197184e-05,
      "loss": 0.6573,
      "step": 7140
    },
    {
      "epoch": 1.1930585683297181,
      "grad_norm": 5.725481033325195,
      "learning_rate": 3.871542508060411e-05,
      "loss": 0.7229,
      "step": 7150
    },
    {
      "epoch": 1.1947271817119973,
      "grad_norm": 13.67314338684082,
      "learning_rate": 3.869845579501103e-05,
      "loss": 0.4251,
      "step": 7160
    },
    {
      "epoch": 1.1963957950942767,
      "grad_norm": 13.898173332214355,
      "learning_rate": 3.8681486509417955e-05,
      "loss": 0.7939,
      "step": 7170
    },
    {
      "epoch": 1.198064408476556,
      "grad_norm": 1.361419439315796,
      "learning_rate": 3.866451722382488e-05,
      "loss": 0.5103,
      "step": 7180
    },
    {
      "epoch": 1.1997330218588353,
      "grad_norm": 4.8054656982421875,
      "learning_rate": 3.8647547938231806e-05,
      "loss": 0.6348,
      "step": 7190
    },
    {
      "epoch": 1.2014016352411145,
      "grad_norm": 5.2540788650512695,
      "learning_rate": 3.8630578652638726e-05,
      "loss": 0.4898,
      "step": 7200
    },
    {
      "epoch": 1.203070248623394,
      "grad_norm": 2.849172592163086,
      "learning_rate": 3.861360936704565e-05,
      "loss": 0.5371,
      "step": 7210
    },
    {
      "epoch": 1.2047388620056734,
      "grad_norm": 4.929891586303711,
      "learning_rate": 3.859664008145257e-05,
      "loss": 0.6686,
      "step": 7220
    },
    {
      "epoch": 1.2064074753879526,
      "grad_norm": 6.0635666847229,
      "learning_rate": 3.8579670795859496e-05,
      "loss": 0.6934,
      "step": 7230
    },
    {
      "epoch": 1.208076088770232,
      "grad_norm": 2.5255846977233887,
      "learning_rate": 3.856270151026642e-05,
      "loss": 0.4409,
      "step": 7240
    },
    {
      "epoch": 1.2097447021525112,
      "grad_norm": 5.538783073425293,
      "learning_rate": 3.854573222467334e-05,
      "loss": 0.3345,
      "step": 7250
    },
    {
      "epoch": 1.2114133155347906,
      "grad_norm": 7.002161979675293,
      "learning_rate": 3.852876293908027e-05,
      "loss": 0.6045,
      "step": 7260
    },
    {
      "epoch": 1.21308192891707,
      "grad_norm": 4.476808071136475,
      "learning_rate": 3.8511793653487186e-05,
      "loss": 0.5204,
      "step": 7270
    },
    {
      "epoch": 1.2147505422993492,
      "grad_norm": 8.805429458618164,
      "learning_rate": 3.849482436789411e-05,
      "loss": 0.6887,
      "step": 7280
    },
    {
      "epoch": 1.2164191556816286,
      "grad_norm": 5.047526836395264,
      "learning_rate": 3.847785508230103e-05,
      "loss": 0.8827,
      "step": 7290
    },
    {
      "epoch": 1.2180877690639078,
      "grad_norm": 7.352685928344727,
      "learning_rate": 3.8460885796707964e-05,
      "loss": 0.8534,
      "step": 7300
    },
    {
      "epoch": 1.2197563824461872,
      "grad_norm": 3.025263547897339,
      "learning_rate": 3.844391651111488e-05,
      "loss": 0.523,
      "step": 7310
    },
    {
      "epoch": 1.2214249958284666,
      "grad_norm": 6.818612575531006,
      "learning_rate": 3.842694722552181e-05,
      "loss": 0.6219,
      "step": 7320
    },
    {
      "epoch": 1.2230936092107458,
      "grad_norm": 9.185415267944336,
      "learning_rate": 3.8409977939928735e-05,
      "loss": 0.6748,
      "step": 7330
    },
    {
      "epoch": 1.2247622225930253,
      "grad_norm": 3.5129475593566895,
      "learning_rate": 3.8393008654335654e-05,
      "loss": 0.6282,
      "step": 7340
    },
    {
      "epoch": 1.2264308359753044,
      "grad_norm": 11.081511497497559,
      "learning_rate": 3.837603936874258e-05,
      "loss": 0.6812,
      "step": 7350
    },
    {
      "epoch": 1.2280994493575839,
      "grad_norm": 6.4841203689575195,
      "learning_rate": 3.83590700831495e-05,
      "loss": 0.45,
      "step": 7360
    },
    {
      "epoch": 1.229768062739863,
      "grad_norm": 3.3309600353240967,
      "learning_rate": 3.8342100797556425e-05,
      "loss": 0.7833,
      "step": 7370
    },
    {
      "epoch": 1.2314366761221425,
      "grad_norm": 2.5241854190826416,
      "learning_rate": 3.8325131511963344e-05,
      "loss": 0.7309,
      "step": 7380
    },
    {
      "epoch": 1.2331052895044219,
      "grad_norm": 5.774630546569824,
      "learning_rate": 3.830816222637027e-05,
      "loss": 0.6851,
      "step": 7390
    },
    {
      "epoch": 1.234773902886701,
      "grad_norm": 6.315774440765381,
      "learning_rate": 3.8291192940777196e-05,
      "loss": 0.5114,
      "step": 7400
    },
    {
      "epoch": 1.2364425162689805,
      "grad_norm": 0.8638496994972229,
      "learning_rate": 3.8274223655184115e-05,
      "loss": 0.286,
      "step": 7410
    },
    {
      "epoch": 1.23811112965126,
      "grad_norm": 2.164592742919922,
      "learning_rate": 3.825725436959105e-05,
      "loss": 0.6558,
      "step": 7420
    },
    {
      "epoch": 1.239779743033539,
      "grad_norm": 12.363561630249023,
      "learning_rate": 3.8240285083997966e-05,
      "loss": 0.5599,
      "step": 7430
    },
    {
      "epoch": 1.2414483564158185,
      "grad_norm": 11.58707046508789,
      "learning_rate": 3.822331579840489e-05,
      "loss": 0.7871,
      "step": 7440
    },
    {
      "epoch": 1.2431169697980977,
      "grad_norm": 1.8665372133255005,
      "learning_rate": 3.820634651281181e-05,
      "loss": 0.6223,
      "step": 7450
    },
    {
      "epoch": 1.2447855831803771,
      "grad_norm": 8.121451377868652,
      "learning_rate": 3.818937722721874e-05,
      "loss": 0.77,
      "step": 7460
    },
    {
      "epoch": 1.2464541965626563,
      "grad_norm": 2.7941555976867676,
      "learning_rate": 3.8172407941625656e-05,
      "loss": 0.7129,
      "step": 7470
    },
    {
      "epoch": 1.2481228099449357,
      "grad_norm": 3.195108413696289,
      "learning_rate": 3.815543865603258e-05,
      "loss": 0.349,
      "step": 7480
    },
    {
      "epoch": 1.2497914233272152,
      "grad_norm": 17.298004150390625,
      "learning_rate": 3.813846937043951e-05,
      "loss": 0.6278,
      "step": 7490
    },
    {
      "epoch": 1.2514600367094944,
      "grad_norm": 8.329365730285645,
      "learning_rate": 3.812150008484643e-05,
      "loss": 0.5518,
      "step": 7500
    },
    {
      "epoch": 1.2531286500917738,
      "grad_norm": 1.833101749420166,
      "learning_rate": 3.810453079925335e-05,
      "loss": 0.56,
      "step": 7510
    },
    {
      "epoch": 1.2547972634740532,
      "grad_norm": 6.475449085235596,
      "learning_rate": 3.808756151366027e-05,
      "loss": 0.4566,
      "step": 7520
    },
    {
      "epoch": 1.2564658768563324,
      "grad_norm": 5.771843910217285,
      "learning_rate": 3.80705922280672e-05,
      "loss": 0.5559,
      "step": 7530
    },
    {
      "epoch": 1.2581344902386118,
      "grad_norm": 4.5495829582214355,
      "learning_rate": 3.8053622942474124e-05,
      "loss": 0.6315,
      "step": 7540
    },
    {
      "epoch": 1.259803103620891,
      "grad_norm": 8.522125244140625,
      "learning_rate": 3.803665365688105e-05,
      "loss": 0.7748,
      "step": 7550
    },
    {
      "epoch": 1.2614717170031704,
      "grad_norm": 4.631547927856445,
      "learning_rate": 3.8019684371287976e-05,
      "loss": 0.3197,
      "step": 7560
    },
    {
      "epoch": 1.2631403303854496,
      "grad_norm": 7.650500297546387,
      "learning_rate": 3.8002715085694895e-05,
      "loss": 0.6475,
      "step": 7570
    },
    {
      "epoch": 1.264808943767729,
      "grad_norm": 13.482539176940918,
      "learning_rate": 3.798574580010182e-05,
      "loss": 0.6315,
      "step": 7580
    },
    {
      "epoch": 1.2664775571500084,
      "grad_norm": 4.588191509246826,
      "learning_rate": 3.796877651450874e-05,
      "loss": 0.3671,
      "step": 7590
    },
    {
      "epoch": 1.2681461705322876,
      "grad_norm": 10.101158142089844,
      "learning_rate": 3.7951807228915666e-05,
      "loss": 0.5769,
      "step": 7600
    },
    {
      "epoch": 1.269814783914567,
      "grad_norm": 3.9936962127685547,
      "learning_rate": 3.7934837943322585e-05,
      "loss": 0.458,
      "step": 7610
    },
    {
      "epoch": 1.2714833972968465,
      "grad_norm": 3.9121673107147217,
      "learning_rate": 3.791786865772951e-05,
      "loss": 0.6818,
      "step": 7620
    },
    {
      "epoch": 1.2731520106791256,
      "grad_norm": 9.749631881713867,
      "learning_rate": 3.7900899372136436e-05,
      "loss": 0.6443,
      "step": 7630
    },
    {
      "epoch": 1.2748206240614048,
      "grad_norm": 7.5914106369018555,
      "learning_rate": 3.7883930086543356e-05,
      "loss": 0.3959,
      "step": 7640
    },
    {
      "epoch": 1.2764892374436843,
      "grad_norm": 11.602721214294434,
      "learning_rate": 3.786696080095028e-05,
      "loss": 0.6351,
      "step": 7650
    },
    {
      "epoch": 1.2781578508259637,
      "grad_norm": 3.3062291145324707,
      "learning_rate": 3.78499915153572e-05,
      "loss": 0.6808,
      "step": 7660
    },
    {
      "epoch": 1.2798264642082429,
      "grad_norm": 4.635564804077148,
      "learning_rate": 3.783302222976413e-05,
      "loss": 0.371,
      "step": 7670
    },
    {
      "epoch": 1.2814950775905223,
      "grad_norm": 10.582955360412598,
      "learning_rate": 3.781605294417105e-05,
      "loss": 0.8261,
      "step": 7680
    },
    {
      "epoch": 1.2831636909728017,
      "grad_norm": 7.625622272491455,
      "learning_rate": 3.779908365857798e-05,
      "loss": 0.556,
      "step": 7690
    },
    {
      "epoch": 1.284832304355081,
      "grad_norm": 6.874505996704102,
      "learning_rate": 3.77821143729849e-05,
      "loss": 0.3626,
      "step": 7700
    },
    {
      "epoch": 1.2865009177373603,
      "grad_norm": 8.740799903869629,
      "learning_rate": 3.776514508739182e-05,
      "loss": 0.5824,
      "step": 7710
    },
    {
      "epoch": 1.2881695311196395,
      "grad_norm": 3.273411273956299,
      "learning_rate": 3.774817580179875e-05,
      "loss": 0.4401,
      "step": 7720
    },
    {
      "epoch": 1.289838144501919,
      "grad_norm": 0.4924328327178955,
      "learning_rate": 3.773120651620567e-05,
      "loss": 0.3119,
      "step": 7730
    },
    {
      "epoch": 1.291506757884198,
      "grad_norm": 7.795819282531738,
      "learning_rate": 3.7714237230612594e-05,
      "loss": 0.3903,
      "step": 7740
    },
    {
      "epoch": 1.2931753712664775,
      "grad_norm": 9.588044166564941,
      "learning_rate": 3.769726794501951e-05,
      "loss": 0.7107,
      "step": 7750
    },
    {
      "epoch": 1.294843984648757,
      "grad_norm": 10.001688957214355,
      "learning_rate": 3.768029865942644e-05,
      "loss": 0.6432,
      "step": 7760
    },
    {
      "epoch": 1.2965125980310361,
      "grad_norm": 6.772157669067383,
      "learning_rate": 3.7663329373833365e-05,
      "loss": 0.7262,
      "step": 7770
    },
    {
      "epoch": 1.2981812114133156,
      "grad_norm": 3.6538097858428955,
      "learning_rate": 3.7646360088240284e-05,
      "loss": 0.5256,
      "step": 7780
    },
    {
      "epoch": 1.299849824795595,
      "grad_norm": 5.768121242523193,
      "learning_rate": 3.762939080264721e-05,
      "loss": 0.5644,
      "step": 7790
    },
    {
      "epoch": 1.3015184381778742,
      "grad_norm": 13.318739891052246,
      "learning_rate": 3.7612421517054136e-05,
      "loss": 0.6593,
      "step": 7800
    },
    {
      "epoch": 1.3031870515601536,
      "grad_norm": 5.541025638580322,
      "learning_rate": 3.759545223146106e-05,
      "loss": 0.8271,
      "step": 7810
    },
    {
      "epoch": 1.3048556649424328,
      "grad_norm": 9.579255104064941,
      "learning_rate": 3.757848294586798e-05,
      "loss": 0.6477,
      "step": 7820
    },
    {
      "epoch": 1.3065242783247122,
      "grad_norm": 11.795647621154785,
      "learning_rate": 3.7561513660274907e-05,
      "loss": 0.9737,
      "step": 7830
    },
    {
      "epoch": 1.3081928917069914,
      "grad_norm": 2.279179096221924,
      "learning_rate": 3.7544544374681826e-05,
      "loss": 0.6849,
      "step": 7840
    },
    {
      "epoch": 1.3098615050892708,
      "grad_norm": 7.8024001121521,
      "learning_rate": 3.752757508908875e-05,
      "loss": 0.5806,
      "step": 7850
    },
    {
      "epoch": 1.3115301184715502,
      "grad_norm": 7.579118251800537,
      "learning_rate": 3.751060580349568e-05,
      "loss": 0.8034,
      "step": 7860
    },
    {
      "epoch": 1.3131987318538294,
      "grad_norm": 13.442233085632324,
      "learning_rate": 3.7493636517902596e-05,
      "loss": 0.6743,
      "step": 7870
    },
    {
      "epoch": 1.3148673452361088,
      "grad_norm": 5.20002555847168,
      "learning_rate": 3.747666723230952e-05,
      "loss": 0.7391,
      "step": 7880
    },
    {
      "epoch": 1.3165359586183882,
      "grad_norm": 2.428025722503662,
      "learning_rate": 3.745969794671644e-05,
      "loss": 0.7265,
      "step": 7890
    },
    {
      "epoch": 1.3182045720006674,
      "grad_norm": 6.385347843170166,
      "learning_rate": 3.744272866112337e-05,
      "loss": 0.7209,
      "step": 7900
    },
    {
      "epoch": 1.3198731853829468,
      "grad_norm": 2.2544970512390137,
      "learning_rate": 3.7425759375530286e-05,
      "loss": 0.741,
      "step": 7910
    },
    {
      "epoch": 1.321541798765226,
      "grad_norm": 8.091168403625488,
      "learning_rate": 3.740879008993722e-05,
      "loss": 0.5508,
      "step": 7920
    },
    {
      "epoch": 1.3232104121475055,
      "grad_norm": 12.237728118896484,
      "learning_rate": 3.739182080434414e-05,
      "loss": 0.7017,
      "step": 7930
    },
    {
      "epoch": 1.3248790255297846,
      "grad_norm": 11.917459487915039,
      "learning_rate": 3.7374851518751064e-05,
      "loss": 0.5603,
      "step": 7940
    },
    {
      "epoch": 1.326547638912064,
      "grad_norm": 9.66199779510498,
      "learning_rate": 3.735788223315799e-05,
      "loss": 0.5768,
      "step": 7950
    },
    {
      "epoch": 1.3282162522943435,
      "grad_norm": 10.093448638916016,
      "learning_rate": 3.734091294756491e-05,
      "loss": 0.5563,
      "step": 7960
    },
    {
      "epoch": 1.3298848656766227,
      "grad_norm": 13.51915454864502,
      "learning_rate": 3.7323943661971835e-05,
      "loss": 0.6352,
      "step": 7970
    },
    {
      "epoch": 1.331553479058902,
      "grad_norm": 7.836437702178955,
      "learning_rate": 3.7306974376378754e-05,
      "loss": 0.7038,
      "step": 7980
    },
    {
      "epoch": 1.3332220924411815,
      "grad_norm": 2.0940239429473877,
      "learning_rate": 3.729000509078568e-05,
      "loss": 0.4805,
      "step": 7990
    },
    {
      "epoch": 1.3348907058234607,
      "grad_norm": 8.304146766662598,
      "learning_rate": 3.72730358051926e-05,
      "loss": 0.6514,
      "step": 8000
    },
    {
      "epoch": 1.33655931920574,
      "grad_norm": 6.8462700843811035,
      "learning_rate": 3.7256066519599525e-05,
      "loss": 0.4883,
      "step": 8010
    },
    {
      "epoch": 1.3382279325880193,
      "grad_norm": 6.753251075744629,
      "learning_rate": 3.723909723400645e-05,
      "loss": 0.6013,
      "step": 8020
    },
    {
      "epoch": 1.3398965459702987,
      "grad_norm": 9.019292831420898,
      "learning_rate": 3.722212794841337e-05,
      "loss": 0.5467,
      "step": 8030
    },
    {
      "epoch": 1.341565159352578,
      "grad_norm": 2.985377550125122,
      "learning_rate": 3.72051586628203e-05,
      "loss": 0.4265,
      "step": 8040
    },
    {
      "epoch": 1.3432337727348573,
      "grad_norm": 7.614914894104004,
      "learning_rate": 3.718818937722722e-05,
      "loss": 0.8302,
      "step": 8050
    },
    {
      "epoch": 1.3449023861171367,
      "grad_norm": 13.341880798339844,
      "learning_rate": 3.717122009163415e-05,
      "loss": 0.514,
      "step": 8060
    },
    {
      "epoch": 1.346570999499416,
      "grad_norm": 8.190046310424805,
      "learning_rate": 3.7154250806041067e-05,
      "loss": 0.512,
      "step": 8070
    },
    {
      "epoch": 1.3482396128816954,
      "grad_norm": 1.753519892692566,
      "learning_rate": 3.713728152044799e-05,
      "loss": 0.4132,
      "step": 8080
    },
    {
      "epoch": 1.3499082262639748,
      "grad_norm": 6.777633190155029,
      "learning_rate": 3.712031223485491e-05,
      "loss": 0.5098,
      "step": 8090
    },
    {
      "epoch": 1.351576839646254,
      "grad_norm": 1.5742789506912231,
      "learning_rate": 3.710334294926184e-05,
      "loss": 0.4568,
      "step": 8100
    },
    {
      "epoch": 1.3532454530285332,
      "grad_norm": 6.631460666656494,
      "learning_rate": 3.708637366366876e-05,
      "loss": 0.8981,
      "step": 8110
    },
    {
      "epoch": 1.3549140664108126,
      "grad_norm": 8.45129108428955,
      "learning_rate": 3.706940437807568e-05,
      "loss": 0.5943,
      "step": 8120
    },
    {
      "epoch": 1.356582679793092,
      "grad_norm": 8.750185012817383,
      "learning_rate": 3.705243509248261e-05,
      "loss": 0.7544,
      "step": 8130
    },
    {
      "epoch": 1.3582512931753712,
      "grad_norm": 2.135101318359375,
      "learning_rate": 3.703546580688953e-05,
      "loss": 0.4592,
      "step": 8140
    },
    {
      "epoch": 1.3599199065576506,
      "grad_norm": 6.255336284637451,
      "learning_rate": 3.701849652129645e-05,
      "loss": 0.4187,
      "step": 8150
    },
    {
      "epoch": 1.36158851993993,
      "grad_norm": 9.981610298156738,
      "learning_rate": 3.700152723570338e-05,
      "loss": 0.3746,
      "step": 8160
    },
    {
      "epoch": 1.3632571333222092,
      "grad_norm": 16.658870697021484,
      "learning_rate": 3.6984557950110305e-05,
      "loss": 0.6653,
      "step": 8170
    },
    {
      "epoch": 1.3649257467044886,
      "grad_norm": 4.10536003112793,
      "learning_rate": 3.696758866451723e-05,
      "loss": 0.4929,
      "step": 8180
    },
    {
      "epoch": 1.3665943600867678,
      "grad_norm": 2.1825737953186035,
      "learning_rate": 3.695061937892415e-05,
      "loss": 0.7245,
      "step": 8190
    },
    {
      "epoch": 1.3682629734690472,
      "grad_norm": 4.6542887687683105,
      "learning_rate": 3.6933650093331076e-05,
      "loss": 0.3141,
      "step": 8200
    },
    {
      "epoch": 1.3699315868513264,
      "grad_norm": 2.5679078102111816,
      "learning_rate": 3.6916680807737995e-05,
      "loss": 0.2909,
      "step": 8210
    },
    {
      "epoch": 1.3716002002336058,
      "grad_norm": 3.962031126022339,
      "learning_rate": 3.689971152214492e-05,
      "loss": 0.7369,
      "step": 8220
    },
    {
      "epoch": 1.3732688136158853,
      "grad_norm": 2.8475043773651123,
      "learning_rate": 3.688274223655184e-05,
      "loss": 0.6928,
      "step": 8230
    },
    {
      "epoch": 1.3749374269981645,
      "grad_norm": 1.721266269683838,
      "learning_rate": 3.6865772950958766e-05,
      "loss": 0.364,
      "step": 8240
    },
    {
      "epoch": 1.3766060403804439,
      "grad_norm": 5.527297496795654,
      "learning_rate": 3.684880366536569e-05,
      "loss": 0.8081,
      "step": 8250
    },
    {
      "epoch": 1.3782746537627233,
      "grad_norm": 1.045560598373413,
      "learning_rate": 3.683183437977261e-05,
      "loss": 0.6271,
      "step": 8260
    },
    {
      "epoch": 1.3799432671450025,
      "grad_norm": 0.5547844171524048,
      "learning_rate": 3.6814865094179537e-05,
      "loss": 0.5378,
      "step": 8270
    },
    {
      "epoch": 1.381611880527282,
      "grad_norm": 5.3306193351745605,
      "learning_rate": 3.6797895808586456e-05,
      "loss": 0.5351,
      "step": 8280
    },
    {
      "epoch": 1.383280493909561,
      "grad_norm": 9.672178268432617,
      "learning_rate": 3.678092652299339e-05,
      "loss": 0.5828,
      "step": 8290
    },
    {
      "epoch": 1.3849491072918405,
      "grad_norm": 7.3099565505981445,
      "learning_rate": 3.676395723740031e-05,
      "loss": 0.4388,
      "step": 8300
    },
    {
      "epoch": 1.3866177206741197,
      "grad_norm": 2.433317184448242,
      "learning_rate": 3.674698795180723e-05,
      "loss": 0.5929,
      "step": 8310
    },
    {
      "epoch": 1.3882863340563991,
      "grad_norm": 0.6896153092384338,
      "learning_rate": 3.673001866621415e-05,
      "loss": 0.4468,
      "step": 8320
    },
    {
      "epoch": 1.3899549474386785,
      "grad_norm": 4.204913139343262,
      "learning_rate": 3.671304938062108e-05,
      "loss": 0.6717,
      "step": 8330
    },
    {
      "epoch": 1.3916235608209577,
      "grad_norm": 6.5474700927734375,
      "learning_rate": 3.6696080095028004e-05,
      "loss": 0.6468,
      "step": 8340
    },
    {
      "epoch": 1.3932921742032371,
      "grad_norm": 8.005317687988281,
      "learning_rate": 3.667911080943492e-05,
      "loss": 0.5842,
      "step": 8350
    },
    {
      "epoch": 1.3949607875855166,
      "grad_norm": 7.647856712341309,
      "learning_rate": 3.666214152384185e-05,
      "loss": 0.6756,
      "step": 8360
    },
    {
      "epoch": 1.3966294009677958,
      "grad_norm": 2.175602674484253,
      "learning_rate": 3.664517223824877e-05,
      "loss": 0.3956,
      "step": 8370
    },
    {
      "epoch": 1.3982980143500752,
      "grad_norm": 6.359965801239014,
      "learning_rate": 3.6628202952655694e-05,
      "loss": 0.5003,
      "step": 8380
    },
    {
      "epoch": 1.3999666277323544,
      "grad_norm": 3.372567653656006,
      "learning_rate": 3.661123366706262e-05,
      "loss": 0.4507,
      "step": 8390
    },
    {
      "epoch": 1.4016352411146338,
      "grad_norm": 16.47830581665039,
      "learning_rate": 3.659426438146954e-05,
      "loss": 0.6194,
      "step": 8400
    },
    {
      "epoch": 1.403303854496913,
      "grad_norm": 1.495778203010559,
      "learning_rate": 3.6577295095876465e-05,
      "loss": 0.2704,
      "step": 8410
    },
    {
      "epoch": 1.4049724678791924,
      "grad_norm": 9.975116729736328,
      "learning_rate": 3.656032581028339e-05,
      "loss": 0.8876,
      "step": 8420
    },
    {
      "epoch": 1.4066410812614718,
      "grad_norm": 14.122232437133789,
      "learning_rate": 3.654335652469032e-05,
      "loss": 0.7669,
      "step": 8430
    },
    {
      "epoch": 1.408309694643751,
      "grad_norm": 7.062209606170654,
      "learning_rate": 3.6526387239097236e-05,
      "loss": 0.671,
      "step": 8440
    },
    {
      "epoch": 1.4099783080260304,
      "grad_norm": 2.917647361755371,
      "learning_rate": 3.650941795350416e-05,
      "loss": 0.6398,
      "step": 8450
    },
    {
      "epoch": 1.4116469214083098,
      "grad_norm": 7.829510688781738,
      "learning_rate": 3.649244866791108e-05,
      "loss": 0.8576,
      "step": 8460
    },
    {
      "epoch": 1.413315534790589,
      "grad_norm": 4.823068618774414,
      "learning_rate": 3.647547938231801e-05,
      "loss": 0.6135,
      "step": 8470
    },
    {
      "epoch": 1.4149841481728682,
      "grad_norm": 8.451234817504883,
      "learning_rate": 3.645851009672493e-05,
      "loss": 0.6189,
      "step": 8480
    },
    {
      "epoch": 1.4166527615551476,
      "grad_norm": 3.8112740516662598,
      "learning_rate": 3.644154081113185e-05,
      "loss": 0.5998,
      "step": 8490
    },
    {
      "epoch": 1.418321374937427,
      "grad_norm": 3.848320245742798,
      "learning_rate": 3.642457152553878e-05,
      "loss": 0.4653,
      "step": 8500
    },
    {
      "epoch": 1.4199899883197062,
      "grad_norm": 2.963008165359497,
      "learning_rate": 3.6407602239945697e-05,
      "loss": 0.548,
      "step": 8510
    },
    {
      "epoch": 1.4216586017019857,
      "grad_norm": 10.881186485290527,
      "learning_rate": 3.639063295435262e-05,
      "loss": 0.6429,
      "step": 8520
    },
    {
      "epoch": 1.423327215084265,
      "grad_norm": 3.599104881286621,
      "learning_rate": 3.637366366875954e-05,
      "loss": 0.5464,
      "step": 8530
    },
    {
      "epoch": 1.4249958284665443,
      "grad_norm": 9.541318893432617,
      "learning_rate": 3.6356694383166474e-05,
      "loss": 0.7911,
      "step": 8540
    },
    {
      "epoch": 1.4266644418488237,
      "grad_norm": 6.3017754554748535,
      "learning_rate": 3.633972509757339e-05,
      "loss": 0.4189,
      "step": 8550
    },
    {
      "epoch": 1.4283330552311029,
      "grad_norm": 1.2507991790771484,
      "learning_rate": 3.632275581198032e-05,
      "loss": 0.4063,
      "step": 8560
    },
    {
      "epoch": 1.4300016686133823,
      "grad_norm": 2.9394829273223877,
      "learning_rate": 3.6305786526387245e-05,
      "loss": 0.6289,
      "step": 8570
    },
    {
      "epoch": 1.4316702819956615,
      "grad_norm": 4.23877477645874,
      "learning_rate": 3.6288817240794164e-05,
      "loss": 0.4755,
      "step": 8580
    },
    {
      "epoch": 1.433338895377941,
      "grad_norm": 8.077390670776367,
      "learning_rate": 3.627184795520109e-05,
      "loss": 0.934,
      "step": 8590
    },
    {
      "epoch": 1.4350075087602203,
      "grad_norm": 4.571857452392578,
      "learning_rate": 3.625487866960801e-05,
      "loss": 0.7734,
      "step": 8600
    },
    {
      "epoch": 1.4366761221424995,
      "grad_norm": 2.3911595344543457,
      "learning_rate": 3.6237909384014935e-05,
      "loss": 0.621,
      "step": 8610
    },
    {
      "epoch": 1.438344735524779,
      "grad_norm": 8.429826736450195,
      "learning_rate": 3.6220940098421854e-05,
      "loss": 0.4299,
      "step": 8620
    },
    {
      "epoch": 1.4400133489070583,
      "grad_norm": 0.2355324625968933,
      "learning_rate": 3.620397081282878e-05,
      "loss": 0.5512,
      "step": 8630
    },
    {
      "epoch": 1.4416819622893375,
      "grad_norm": 0.20896077156066895,
      "learning_rate": 3.6187001527235706e-05,
      "loss": 0.7157,
      "step": 8640
    },
    {
      "epoch": 1.443350575671617,
      "grad_norm": 2.1638805866241455,
      "learning_rate": 3.6170032241642625e-05,
      "loss": 0.7044,
      "step": 8650
    },
    {
      "epoch": 1.4450191890538961,
      "grad_norm": 0.45499613881111145,
      "learning_rate": 3.615306295604955e-05,
      "loss": 0.5754,
      "step": 8660
    },
    {
      "epoch": 1.4466878024361756,
      "grad_norm": 2.344085693359375,
      "learning_rate": 3.613609367045648e-05,
      "loss": 0.4615,
      "step": 8670
    },
    {
      "epoch": 1.4483564158184548,
      "grad_norm": 10.528299331665039,
      "learning_rate": 3.61191243848634e-05,
      "loss": 0.6595,
      "step": 8680
    },
    {
      "epoch": 1.4500250292007342,
      "grad_norm": 3.1200692653656006,
      "learning_rate": 3.610215509927032e-05,
      "loss": 0.4724,
      "step": 8690
    },
    {
      "epoch": 1.4516936425830136,
      "grad_norm": 1.9261642694473267,
      "learning_rate": 3.608518581367725e-05,
      "loss": 0.4826,
      "step": 8700
    },
    {
      "epoch": 1.4533622559652928,
      "grad_norm": 2.7581088542938232,
      "learning_rate": 3.606821652808417e-05,
      "loss": 0.5593,
      "step": 8710
    },
    {
      "epoch": 1.4550308693475722,
      "grad_norm": 4.674509525299072,
      "learning_rate": 3.605124724249109e-05,
      "loss": 0.7866,
      "step": 8720
    },
    {
      "epoch": 1.4566994827298516,
      "grad_norm": 2.207087755203247,
      "learning_rate": 3.603427795689802e-05,
      "loss": 0.3152,
      "step": 8730
    },
    {
      "epoch": 1.4583680961121308,
      "grad_norm": 7.067447185516357,
      "learning_rate": 3.601730867130494e-05,
      "loss": 0.5547,
      "step": 8740
    },
    {
      "epoch": 1.4600367094944102,
      "grad_norm": 5.352639675140381,
      "learning_rate": 3.600033938571186e-05,
      "loss": 0.5014,
      "step": 8750
    },
    {
      "epoch": 1.4617053228766894,
      "grad_norm": 6.7113776206970215,
      "learning_rate": 3.598337010011878e-05,
      "loss": 0.4107,
      "step": 8760
    },
    {
      "epoch": 1.4633739362589688,
      "grad_norm": 5.325056076049805,
      "learning_rate": 3.596640081452571e-05,
      "loss": 0.5999,
      "step": 8770
    },
    {
      "epoch": 1.465042549641248,
      "grad_norm": 13.123467445373535,
      "learning_rate": 3.5949431528932634e-05,
      "loss": 0.4492,
      "step": 8780
    },
    {
      "epoch": 1.4667111630235274,
      "grad_norm": 14.622159004211426,
      "learning_rate": 3.593246224333956e-05,
      "loss": 0.7847,
      "step": 8790
    },
    {
      "epoch": 1.4683797764058069,
      "grad_norm": 7.665839672088623,
      "learning_rate": 3.5915492957746486e-05,
      "loss": 0.4038,
      "step": 8800
    },
    {
      "epoch": 1.470048389788086,
      "grad_norm": 5.293795108795166,
      "learning_rate": 3.5898523672153405e-05,
      "loss": 0.3976,
      "step": 8810
    },
    {
      "epoch": 1.4717170031703655,
      "grad_norm": 6.494593620300293,
      "learning_rate": 3.588155438656033e-05,
      "loss": 1.123,
      "step": 8820
    },
    {
      "epoch": 1.4733856165526449,
      "grad_norm": 11.537860870361328,
      "learning_rate": 3.586458510096725e-05,
      "loss": 0.6504,
      "step": 8830
    },
    {
      "epoch": 1.475054229934924,
      "grad_norm": 6.701302528381348,
      "learning_rate": 3.5847615815374176e-05,
      "loss": 0.3605,
      "step": 8840
    },
    {
      "epoch": 1.4767228433172033,
      "grad_norm": 4.830488681793213,
      "learning_rate": 3.5830646529781095e-05,
      "loss": 0.6734,
      "step": 8850
    },
    {
      "epoch": 1.4783914566994827,
      "grad_norm": 12.909965515136719,
      "learning_rate": 3.581367724418802e-05,
      "loss": 0.7639,
      "step": 8860
    },
    {
      "epoch": 1.480060070081762,
      "grad_norm": 5.302270889282227,
      "learning_rate": 3.579670795859495e-05,
      "loss": 0.3523,
      "step": 8870
    },
    {
      "epoch": 1.4817286834640413,
      "grad_norm": 4.859670639038086,
      "learning_rate": 3.5779738673001866e-05,
      "loss": 0.5114,
      "step": 8880
    },
    {
      "epoch": 1.4833972968463207,
      "grad_norm": 1.6431915760040283,
      "learning_rate": 3.576276938740879e-05,
      "loss": 0.7205,
      "step": 8890
    },
    {
      "epoch": 1.4850659102286001,
      "grad_norm": 10.54200267791748,
      "learning_rate": 3.574580010181571e-05,
      "loss": 0.7555,
      "step": 8900
    },
    {
      "epoch": 1.4867345236108793,
      "grad_norm": 5.857269763946533,
      "learning_rate": 3.5728830816222643e-05,
      "loss": 0.6069,
      "step": 8910
    },
    {
      "epoch": 1.4884031369931587,
      "grad_norm": 1.636991024017334,
      "learning_rate": 3.571186153062956e-05,
      "loss": 0.5636,
      "step": 8920
    },
    {
      "epoch": 1.490071750375438,
      "grad_norm": 3.3499999046325684,
      "learning_rate": 3.569489224503649e-05,
      "loss": 0.5385,
      "step": 8930
    },
    {
      "epoch": 1.4917403637577173,
      "grad_norm": 8.017191886901855,
      "learning_rate": 3.567792295944341e-05,
      "loss": 0.7461,
      "step": 8940
    },
    {
      "epoch": 1.4934089771399965,
      "grad_norm": 1.8825867176055908,
      "learning_rate": 3.5660953673850333e-05,
      "loss": 0.4254,
      "step": 8950
    },
    {
      "epoch": 1.495077590522276,
      "grad_norm": 2.048374891281128,
      "learning_rate": 3.564398438825726e-05,
      "loss": 0.4685,
      "step": 8960
    },
    {
      "epoch": 1.4967462039045554,
      "grad_norm": 2.8978655338287354,
      "learning_rate": 3.562701510266418e-05,
      "loss": 0.7504,
      "step": 8970
    },
    {
      "epoch": 1.4984148172868346,
      "grad_norm": 7.818826675415039,
      "learning_rate": 3.5610045817071104e-05,
      "loss": 0.5056,
      "step": 8980
    },
    {
      "epoch": 1.500083430669114,
      "grad_norm": 7.561398029327393,
      "learning_rate": 3.559307653147802e-05,
      "loss": 1.1149,
      "step": 8990
    },
    {
      "epoch": 1.5017520440513934,
      "grad_norm": 10.520286560058594,
      "learning_rate": 3.557610724588495e-05,
      "loss": 0.5653,
      "step": 9000
    },
    {
      "epoch": 1.5034206574336726,
      "grad_norm": 8.104022979736328,
      "learning_rate": 3.5559137960291875e-05,
      "loss": 0.7954,
      "step": 9010
    },
    {
      "epoch": 1.5050892708159518,
      "grad_norm": 9.481030464172363,
      "learning_rate": 3.5542168674698794e-05,
      "loss": 0.8822,
      "step": 9020
    },
    {
      "epoch": 1.5067578841982314,
      "grad_norm": 8.938942909240723,
      "learning_rate": 3.552519938910572e-05,
      "loss": 0.4576,
      "step": 9030
    },
    {
      "epoch": 1.5084264975805106,
      "grad_norm": 8.497920989990234,
      "learning_rate": 3.5508230103512646e-05,
      "loss": 0.7087,
      "step": 9040
    },
    {
      "epoch": 1.5100951109627898,
      "grad_norm": 9.812793731689453,
      "learning_rate": 3.549126081791957e-05,
      "loss": 0.6088,
      "step": 9050
    },
    {
      "epoch": 1.5117637243450692,
      "grad_norm": 6.086471080780029,
      "learning_rate": 3.547429153232649e-05,
      "loss": 0.5514,
      "step": 9060
    },
    {
      "epoch": 1.5134323377273486,
      "grad_norm": 9.153331756591797,
      "learning_rate": 3.545732224673342e-05,
      "loss": 0.4952,
      "step": 9070
    },
    {
      "epoch": 1.5151009511096278,
      "grad_norm": 5.758989334106445,
      "learning_rate": 3.5440352961140336e-05,
      "loss": 0.7389,
      "step": 9080
    },
    {
      "epoch": 1.5167695644919073,
      "grad_norm": 2.66257643699646,
      "learning_rate": 3.542338367554726e-05,
      "loss": 0.5803,
      "step": 9090
    },
    {
      "epoch": 1.5184381778741867,
      "grad_norm": 10.907795906066895,
      "learning_rate": 3.540641438995419e-05,
      "loss": 0.4923,
      "step": 9100
    },
    {
      "epoch": 1.5201067912564659,
      "grad_norm": 2.555349349975586,
      "learning_rate": 3.538944510436111e-05,
      "loss": 0.4725,
      "step": 9110
    },
    {
      "epoch": 1.521775404638745,
      "grad_norm": 7.693441867828369,
      "learning_rate": 3.537247581876803e-05,
      "loss": 0.6068,
      "step": 9120
    },
    {
      "epoch": 1.5234440180210247,
      "grad_norm": 2.082005023956299,
      "learning_rate": 3.535550653317495e-05,
      "loss": 0.4121,
      "step": 9130
    },
    {
      "epoch": 1.5251126314033039,
      "grad_norm": 15.415262222290039,
      "learning_rate": 3.533853724758188e-05,
      "loss": 0.6638,
      "step": 9140
    },
    {
      "epoch": 1.526781244785583,
      "grad_norm": 9.114809036254883,
      "learning_rate": 3.53215679619888e-05,
      "loss": 0.5005,
      "step": 9150
    },
    {
      "epoch": 1.5284498581678625,
      "grad_norm": 7.473019599914551,
      "learning_rate": 3.530459867639573e-05,
      "loss": 0.6167,
      "step": 9160
    },
    {
      "epoch": 1.530118471550142,
      "grad_norm": 6.288372993469238,
      "learning_rate": 3.528762939080265e-05,
      "loss": 0.6754,
      "step": 9170
    },
    {
      "epoch": 1.531787084932421,
      "grad_norm": 7.160839557647705,
      "learning_rate": 3.5270660105209574e-05,
      "loss": 0.6735,
      "step": 9180
    },
    {
      "epoch": 1.5334556983147005,
      "grad_norm": 11.754270553588867,
      "learning_rate": 3.52536908196165e-05,
      "loss": 0.846,
      "step": 9190
    },
    {
      "epoch": 1.53512431169698,
      "grad_norm": 2.1646759510040283,
      "learning_rate": 3.523672153402342e-05,
      "loss": 0.4835,
      "step": 9200
    },
    {
      "epoch": 1.5367929250792591,
      "grad_norm": 11.230558395385742,
      "learning_rate": 3.5219752248430345e-05,
      "loss": 0.7658,
      "step": 9210
    },
    {
      "epoch": 1.5384615384615383,
      "grad_norm": 9.761209487915039,
      "learning_rate": 3.5202782962837264e-05,
      "loss": 0.5661,
      "step": 9220
    },
    {
      "epoch": 1.5401301518438177,
      "grad_norm": 3.306755542755127,
      "learning_rate": 3.518581367724419e-05,
      "loss": 0.9067,
      "step": 9230
    },
    {
      "epoch": 1.5417987652260972,
      "grad_norm": 8.133557319641113,
      "learning_rate": 3.516884439165111e-05,
      "loss": 0.6443,
      "step": 9240
    },
    {
      "epoch": 1.5434673786083764,
      "grad_norm": 0.8230580687522888,
      "learning_rate": 3.5151875106058035e-05,
      "loss": 0.4863,
      "step": 9250
    },
    {
      "epoch": 1.5451359919906558,
      "grad_norm": 1.753133773803711,
      "learning_rate": 3.513490582046496e-05,
      "loss": 0.4006,
      "step": 9260
    },
    {
      "epoch": 1.5468046053729352,
      "grad_norm": 5.1278076171875,
      "learning_rate": 3.511793653487188e-05,
      "loss": 0.7843,
      "step": 9270
    },
    {
      "epoch": 1.5484732187552144,
      "grad_norm": 3.756078004837036,
      "learning_rate": 3.5100967249278806e-05,
      "loss": 0.4679,
      "step": 9280
    },
    {
      "epoch": 1.5501418321374938,
      "grad_norm": 7.179980278015137,
      "learning_rate": 3.508399796368573e-05,
      "loss": 0.5506,
      "step": 9290
    },
    {
      "epoch": 1.5518104455197732,
      "grad_norm": 3.9825713634490967,
      "learning_rate": 3.506702867809266e-05,
      "loss": 0.4378,
      "step": 9300
    },
    {
      "epoch": 1.5534790589020524,
      "grad_norm": 13.064250946044922,
      "learning_rate": 3.505005939249958e-05,
      "loss": 0.5837,
      "step": 9310
    },
    {
      "epoch": 1.5551476722843316,
      "grad_norm": 7.398890495300293,
      "learning_rate": 3.50330901069065e-05,
      "loss": 0.486,
      "step": 9320
    },
    {
      "epoch": 1.556816285666611,
      "grad_norm": 4.799051284790039,
      "learning_rate": 3.501612082131342e-05,
      "loss": 0.5619,
      "step": 9330
    },
    {
      "epoch": 1.5584848990488904,
      "grad_norm": 14.43273639678955,
      "learning_rate": 3.499915153572035e-05,
      "loss": 1.0759,
      "step": 9340
    },
    {
      "epoch": 1.5601535124311696,
      "grad_norm": 5.13259744644165,
      "learning_rate": 3.4982182250127274e-05,
      "loss": 0.6511,
      "step": 9350
    },
    {
      "epoch": 1.561822125813449,
      "grad_norm": 7.059608459472656,
      "learning_rate": 3.496521296453419e-05,
      "loss": 0.5606,
      "step": 9360
    },
    {
      "epoch": 1.5634907391957285,
      "grad_norm": 6.378010272979736,
      "learning_rate": 3.494824367894112e-05,
      "loss": 0.683,
      "step": 9370
    },
    {
      "epoch": 1.5651593525780076,
      "grad_norm": 3.5665369033813477,
      "learning_rate": 3.493127439334804e-05,
      "loss": 0.6094,
      "step": 9380
    },
    {
      "epoch": 1.5668279659602868,
      "grad_norm": 1.5860958099365234,
      "learning_rate": 3.4914305107754963e-05,
      "loss": 0.427,
      "step": 9390
    },
    {
      "epoch": 1.5684965793425665,
      "grad_norm": 5.109947204589844,
      "learning_rate": 3.489733582216189e-05,
      "loss": 0.4213,
      "step": 9400
    },
    {
      "epoch": 1.5701651927248457,
      "grad_norm": 1.8259072303771973,
      "learning_rate": 3.4880366536568815e-05,
      "loss": 0.362,
      "step": 9410
    },
    {
      "epoch": 1.5718338061071249,
      "grad_norm": 18.623748779296875,
      "learning_rate": 3.486339725097574e-05,
      "loss": 0.7913,
      "step": 9420
    },
    {
      "epoch": 1.5735024194894043,
      "grad_norm": 11.023067474365234,
      "learning_rate": 3.484642796538266e-05,
      "loss": 0.6347,
      "step": 9430
    },
    {
      "epoch": 1.5751710328716837,
      "grad_norm": 2.9334003925323486,
      "learning_rate": 3.4829458679789586e-05,
      "loss": 0.2842,
      "step": 9440
    },
    {
      "epoch": 1.576839646253963,
      "grad_norm": 5.806861877441406,
      "learning_rate": 3.4812489394196505e-05,
      "loss": 0.5537,
      "step": 9450
    },
    {
      "epoch": 1.5785082596362423,
      "grad_norm": 9.087989807128906,
      "learning_rate": 3.479552010860343e-05,
      "loss": 0.6679,
      "step": 9460
    },
    {
      "epoch": 1.5801768730185217,
      "grad_norm": 11.637462615966797,
      "learning_rate": 3.477855082301035e-05,
      "loss": 0.8277,
      "step": 9470
    },
    {
      "epoch": 1.581845486400801,
      "grad_norm": 4.4658613204956055,
      "learning_rate": 3.4761581537417276e-05,
      "loss": 0.4299,
      "step": 9480
    },
    {
      "epoch": 1.58351409978308,
      "grad_norm": 3.6021037101745605,
      "learning_rate": 3.47446122518242e-05,
      "loss": 0.7409,
      "step": 9490
    },
    {
      "epoch": 1.5851827131653597,
      "grad_norm": 18.027408599853516,
      "learning_rate": 3.472764296623112e-05,
      "loss": 0.4571,
      "step": 9500
    },
    {
      "epoch": 1.586851326547639,
      "grad_norm": 1.8270260095596313,
      "learning_rate": 3.471067368063805e-05,
      "loss": 0.5758,
      "step": 9510
    },
    {
      "epoch": 1.5885199399299181,
      "grad_norm": 13.867746353149414,
      "learning_rate": 3.4693704395044966e-05,
      "loss": 0.9703,
      "step": 9520
    },
    {
      "epoch": 1.5901885533121976,
      "grad_norm": 5.515559196472168,
      "learning_rate": 3.467673510945189e-05,
      "loss": 0.5562,
      "step": 9530
    },
    {
      "epoch": 1.591857166694477,
      "grad_norm": 8.074597358703613,
      "learning_rate": 3.465976582385882e-05,
      "loss": 0.552,
      "step": 9540
    },
    {
      "epoch": 1.5935257800767562,
      "grad_norm": 3.245556116104126,
      "learning_rate": 3.4642796538265744e-05,
      "loss": 0.4115,
      "step": 9550
    },
    {
      "epoch": 1.5951943934590356,
      "grad_norm": 7.180743217468262,
      "learning_rate": 3.462582725267266e-05,
      "loss": 0.3446,
      "step": 9560
    },
    {
      "epoch": 1.596863006841315,
      "grad_norm": 3.1166152954101562,
      "learning_rate": 3.460885796707959e-05,
      "loss": 0.4909,
      "step": 9570
    },
    {
      "epoch": 1.5985316202235942,
      "grad_norm": 3.1264803409576416,
      "learning_rate": 3.4591888681486514e-05,
      "loss": 0.4043,
      "step": 9580
    },
    {
      "epoch": 1.6002002336058734,
      "grad_norm": 2.704465627670288,
      "learning_rate": 3.4574919395893434e-05,
      "loss": 0.5719,
      "step": 9590
    },
    {
      "epoch": 1.6018688469881528,
      "grad_norm": 6.734618663787842,
      "learning_rate": 3.455795011030036e-05,
      "loss": 0.3655,
      "step": 9600
    },
    {
      "epoch": 1.6035374603704322,
      "grad_norm": 19.894163131713867,
      "learning_rate": 3.454098082470728e-05,
      "loss": 0.7325,
      "step": 9610
    },
    {
      "epoch": 1.6052060737527114,
      "grad_norm": 17.621639251708984,
      "learning_rate": 3.4524011539114204e-05,
      "loss": 0.7323,
      "step": 9620
    },
    {
      "epoch": 1.6068746871349908,
      "grad_norm": 3.924262285232544,
      "learning_rate": 3.450704225352113e-05,
      "loss": 0.472,
      "step": 9630
    },
    {
      "epoch": 1.6085433005172702,
      "grad_norm": 1.0274114608764648,
      "learning_rate": 3.449007296792805e-05,
      "loss": 0.6769,
      "step": 9640
    },
    {
      "epoch": 1.6102119138995494,
      "grad_norm": 5.660332202911377,
      "learning_rate": 3.4473103682334975e-05,
      "loss": 0.7765,
      "step": 9650
    },
    {
      "epoch": 1.6118805272818288,
      "grad_norm": 14.007878303527832,
      "learning_rate": 3.44561343967419e-05,
      "loss": 0.4574,
      "step": 9660
    },
    {
      "epoch": 1.6135491406641083,
      "grad_norm": 7.205118656158447,
      "learning_rate": 3.443916511114883e-05,
      "loss": 0.4035,
      "step": 9670
    },
    {
      "epoch": 1.6152177540463875,
      "grad_norm": 6.361394882202148,
      "learning_rate": 3.4422195825555746e-05,
      "loss": 0.6438,
      "step": 9680
    },
    {
      "epoch": 1.6168863674286666,
      "grad_norm": 14.966527938842773,
      "learning_rate": 3.440522653996267e-05,
      "loss": 0.4889,
      "step": 9690
    },
    {
      "epoch": 1.618554980810946,
      "grad_norm": 5.250165939331055,
      "learning_rate": 3.438825725436959e-05,
      "loss": 0.666,
      "step": 9700
    },
    {
      "epoch": 1.6202235941932255,
      "grad_norm": 8.938180923461914,
      "learning_rate": 3.437128796877652e-05,
      "loss": 0.4731,
      "step": 9710
    },
    {
      "epoch": 1.6218922075755047,
      "grad_norm": 8.169857025146484,
      "learning_rate": 3.435431868318344e-05,
      "loss": 0.7051,
      "step": 9720
    },
    {
      "epoch": 1.623560820957784,
      "grad_norm": 10.785101890563965,
      "learning_rate": 3.433734939759036e-05,
      "loss": 0.4039,
      "step": 9730
    },
    {
      "epoch": 1.6252294343400635,
      "grad_norm": 3.9373369216918945,
      "learning_rate": 3.432038011199729e-05,
      "loss": 0.7625,
      "step": 9740
    },
    {
      "epoch": 1.6268980477223427,
      "grad_norm": 7.149449825286865,
      "learning_rate": 3.430341082640421e-05,
      "loss": 0.3511,
      "step": 9750
    },
    {
      "epoch": 1.628566661104622,
      "grad_norm": 7.514533042907715,
      "learning_rate": 3.428644154081113e-05,
      "loss": 0.4162,
      "step": 9760
    },
    {
      "epoch": 1.6302352744869015,
      "grad_norm": 5.902365207672119,
      "learning_rate": 3.426947225521805e-05,
      "loss": 0.8348,
      "step": 9770
    },
    {
      "epoch": 1.6319038878691807,
      "grad_norm": 2.0656137466430664,
      "learning_rate": 3.425250296962498e-05,
      "loss": 0.7096,
      "step": 9780
    },
    {
      "epoch": 1.63357250125146,
      "grad_norm": 6.309934616088867,
      "learning_rate": 3.4235533684031904e-05,
      "loss": 0.4474,
      "step": 9790
    },
    {
      "epoch": 1.6352411146337393,
      "grad_norm": 10.064696311950684,
      "learning_rate": 3.421856439843883e-05,
      "loss": 0.5423,
      "step": 9800
    },
    {
      "epoch": 1.6369097280160188,
      "grad_norm": 10.24215030670166,
      "learning_rate": 3.4201595112845755e-05,
      "loss": 0.6311,
      "step": 9810
    },
    {
      "epoch": 1.638578341398298,
      "grad_norm": 4.138203144073486,
      "learning_rate": 3.4184625827252674e-05,
      "loss": 0.4247,
      "step": 9820
    },
    {
      "epoch": 1.6402469547805774,
      "grad_norm": 10.005592346191406,
      "learning_rate": 3.41676565416596e-05,
      "loss": 0.7278,
      "step": 9830
    },
    {
      "epoch": 1.6419155681628568,
      "grad_norm": 7.158360481262207,
      "learning_rate": 3.415068725606652e-05,
      "loss": 0.6432,
      "step": 9840
    },
    {
      "epoch": 1.643584181545136,
      "grad_norm": 4.675595283508301,
      "learning_rate": 3.4133717970473445e-05,
      "loss": 0.6148,
      "step": 9850
    },
    {
      "epoch": 1.6452527949274152,
      "grad_norm": 6.164094924926758,
      "learning_rate": 3.4116748684880364e-05,
      "loss": 0.6232,
      "step": 9860
    },
    {
      "epoch": 1.6469214083096948,
      "grad_norm": 1.5634784698486328,
      "learning_rate": 3.409977939928729e-05,
      "loss": 0.6197,
      "step": 9870
    },
    {
      "epoch": 1.648590021691974,
      "grad_norm": 1.806812047958374,
      "learning_rate": 3.4082810113694216e-05,
      "loss": 0.4845,
      "step": 9880
    },
    {
      "epoch": 1.6502586350742532,
      "grad_norm": 6.735447406768799,
      "learning_rate": 3.4065840828101135e-05,
      "loss": 0.8746,
      "step": 9890
    },
    {
      "epoch": 1.6519272484565326,
      "grad_norm": 8.531285285949707,
      "learning_rate": 3.404887154250806e-05,
      "loss": 0.5879,
      "step": 9900
    },
    {
      "epoch": 1.653595861838812,
      "grad_norm": 9.248482704162598,
      "learning_rate": 3.403190225691499e-05,
      "loss": 0.5309,
      "step": 9910
    },
    {
      "epoch": 1.6552644752210912,
      "grad_norm": 2.408804416656494,
      "learning_rate": 3.401493297132191e-05,
      "loss": 0.4771,
      "step": 9920
    },
    {
      "epoch": 1.6569330886033706,
      "grad_norm": 8.26290225982666,
      "learning_rate": 3.399796368572883e-05,
      "loss": 0.823,
      "step": 9930
    },
    {
      "epoch": 1.65860170198565,
      "grad_norm": 6.296304225921631,
      "learning_rate": 3.398099440013576e-05,
      "loss": 0.7747,
      "step": 9940
    },
    {
      "epoch": 1.6602703153679292,
      "grad_norm": 8.775835990905762,
      "learning_rate": 3.396402511454268e-05,
      "loss": 0.491,
      "step": 9950
    },
    {
      "epoch": 1.6619389287502084,
      "grad_norm": 1.6027535200119019,
      "learning_rate": 3.39470558289496e-05,
      "loss": 0.4454,
      "step": 9960
    },
    {
      "epoch": 1.6636075421324878,
      "grad_norm": 14.157684326171875,
      "learning_rate": 3.393008654335653e-05,
      "loss": 0.4172,
      "step": 9970
    },
    {
      "epoch": 1.6652761555147673,
      "grad_norm": 8.069867134094238,
      "learning_rate": 3.391311725776345e-05,
      "loss": 0.4945,
      "step": 9980
    },
    {
      "epoch": 1.6669447688970465,
      "grad_norm": 12.302104949951172,
      "learning_rate": 3.3896147972170374e-05,
      "loss": 0.6476,
      "step": 9990
    },
    {
      "epoch": 1.6686133822793259,
      "grad_norm": 6.0034284591674805,
      "learning_rate": 3.387917868657729e-05,
      "loss": 0.4655,
      "step": 10000
    },
    {
      "epoch": 1.6702819956616053,
      "grad_norm": 8.328264236450195,
      "learning_rate": 3.386220940098422e-05,
      "loss": 0.5171,
      "step": 10010
    },
    {
      "epoch": 1.6719506090438845,
      "grad_norm": 4.555424213409424,
      "learning_rate": 3.3845240115391144e-05,
      "loss": 0.3967,
      "step": 10020
    },
    {
      "epoch": 1.673619222426164,
      "grad_norm": 9.465559005737305,
      "learning_rate": 3.3828270829798064e-05,
      "loss": 0.4324,
      "step": 10030
    },
    {
      "epoch": 1.6752878358084433,
      "grad_norm": 5.456212043762207,
      "learning_rate": 3.3811301544204996e-05,
      "loss": 0.7829,
      "step": 10040
    },
    {
      "epoch": 1.6769564491907225,
      "grad_norm": 2.1916537284851074,
      "learning_rate": 3.3794332258611915e-05,
      "loss": 0.5685,
      "step": 10050
    },
    {
      "epoch": 1.6786250625730017,
      "grad_norm": 11.575387954711914,
      "learning_rate": 3.377736297301884e-05,
      "loss": 0.4366,
      "step": 10060
    },
    {
      "epoch": 1.6802936759552811,
      "grad_norm": 8.13235092163086,
      "learning_rate": 3.376039368742576e-05,
      "loss": 0.8482,
      "step": 10070
    },
    {
      "epoch": 1.6819622893375605,
      "grad_norm": 0.4870733618736267,
      "learning_rate": 3.3743424401832686e-05,
      "loss": 0.3918,
      "step": 10080
    },
    {
      "epoch": 1.6836309027198397,
      "grad_norm": 10.175559043884277,
      "learning_rate": 3.3726455116239605e-05,
      "loss": 0.68,
      "step": 10090
    },
    {
      "epoch": 1.6852995161021191,
      "grad_norm": 7.519979953765869,
      "learning_rate": 3.370948583064653e-05,
      "loss": 0.8566,
      "step": 10100
    },
    {
      "epoch": 1.6869681294843986,
      "grad_norm": 10.077585220336914,
      "learning_rate": 3.369251654505346e-05,
      "loss": 0.4376,
      "step": 10110
    },
    {
      "epoch": 1.6886367428666778,
      "grad_norm": 8.665776252746582,
      "learning_rate": 3.3675547259460376e-05,
      "loss": 0.6863,
      "step": 10120
    },
    {
      "epoch": 1.6903053562489572,
      "grad_norm": 4.25504732131958,
      "learning_rate": 3.36585779738673e-05,
      "loss": 0.447,
      "step": 10130
    },
    {
      "epoch": 1.6919739696312366,
      "grad_norm": 0.2539052367210388,
      "learning_rate": 3.364160868827422e-05,
      "loss": 0.5548,
      "step": 10140
    },
    {
      "epoch": 1.6936425830135158,
      "grad_norm": 6.704615116119385,
      "learning_rate": 3.362463940268115e-05,
      "loss": 0.7202,
      "step": 10150
    },
    {
      "epoch": 1.695311196395795,
      "grad_norm": 7.806873321533203,
      "learning_rate": 3.360767011708807e-05,
      "loss": 0.6836,
      "step": 10160
    },
    {
      "epoch": 1.6969798097780744,
      "grad_norm": 1.4467766284942627,
      "learning_rate": 3.3590700831495e-05,
      "loss": 0.5286,
      "step": 10170
    },
    {
      "epoch": 1.6986484231603538,
      "grad_norm": 3.8107166290283203,
      "learning_rate": 3.357373154590192e-05,
      "loss": 0.5786,
      "step": 10180
    },
    {
      "epoch": 1.700317036542633,
      "grad_norm": 8.94092082977295,
      "learning_rate": 3.3556762260308844e-05,
      "loss": 0.6227,
      "step": 10190
    },
    {
      "epoch": 1.7019856499249124,
      "grad_norm": 4.2686309814453125,
      "learning_rate": 3.353979297471577e-05,
      "loss": 0.4964,
      "step": 10200
    },
    {
      "epoch": 1.7036542633071918,
      "grad_norm": 5.216062068939209,
      "learning_rate": 3.352282368912269e-05,
      "loss": 0.9956,
      "step": 10210
    },
    {
      "epoch": 1.705322876689471,
      "grad_norm": 7.093140125274658,
      "learning_rate": 3.3505854403529615e-05,
      "loss": 0.513,
      "step": 10220
    },
    {
      "epoch": 1.7069914900717502,
      "grad_norm": 5.905189514160156,
      "learning_rate": 3.3488885117936534e-05,
      "loss": 0.5001,
      "step": 10230
    },
    {
      "epoch": 1.7086601034540299,
      "grad_norm": 7.339599132537842,
      "learning_rate": 3.347191583234346e-05,
      "loss": 0.4455,
      "step": 10240
    },
    {
      "epoch": 1.710328716836309,
      "grad_norm": 5.459413528442383,
      "learning_rate": 3.3454946546750385e-05,
      "loss": 0.3505,
      "step": 10250
    },
    {
      "epoch": 1.7119973302185882,
      "grad_norm": 1.7064919471740723,
      "learning_rate": 3.3437977261157304e-05,
      "loss": 0.336,
      "step": 10260
    },
    {
      "epoch": 1.7136659436008677,
      "grad_norm": 10.208683967590332,
      "learning_rate": 3.342100797556423e-05,
      "loss": 0.8065,
      "step": 10270
    },
    {
      "epoch": 1.715334556983147,
      "grad_norm": 0.45303332805633545,
      "learning_rate": 3.340403868997115e-05,
      "loss": 0.485,
      "step": 10280
    },
    {
      "epoch": 1.7170031703654263,
      "grad_norm": 5.371586799621582,
      "learning_rate": 3.338706940437808e-05,
      "loss": 0.4955,
      "step": 10290
    },
    {
      "epoch": 1.7186717837477057,
      "grad_norm": 14.911267280578613,
      "learning_rate": 3.3370100118785e-05,
      "loss": 0.6952,
      "step": 10300
    },
    {
      "epoch": 1.720340397129985,
      "grad_norm": 1.1155827045440674,
      "learning_rate": 3.335313083319193e-05,
      "loss": 0.6677,
      "step": 10310
    },
    {
      "epoch": 1.7220090105122643,
      "grad_norm": 3.93982195854187,
      "learning_rate": 3.3336161547598846e-05,
      "loss": 0.6769,
      "step": 10320
    },
    {
      "epoch": 1.7236776238945435,
      "grad_norm": 1.6334038972854614,
      "learning_rate": 3.331919226200577e-05,
      "loss": 0.8063,
      "step": 10330
    },
    {
      "epoch": 1.7253462372768231,
      "grad_norm": 1.9405429363250732,
      "learning_rate": 3.33022229764127e-05,
      "loss": 0.8705,
      "step": 10340
    },
    {
      "epoch": 1.7270148506591023,
      "grad_norm": 5.319823265075684,
      "learning_rate": 3.328525369081962e-05,
      "loss": 0.5349,
      "step": 10350
    },
    {
      "epoch": 1.7286834640413815,
      "grad_norm": 7.546411514282227,
      "learning_rate": 3.326828440522654e-05,
      "loss": 0.5349,
      "step": 10360
    },
    {
      "epoch": 1.730352077423661,
      "grad_norm": 8.086259841918945,
      "learning_rate": 3.325131511963346e-05,
      "loss": 0.4279,
      "step": 10370
    },
    {
      "epoch": 1.7320206908059403,
      "grad_norm": 2.5271189212799072,
      "learning_rate": 3.323434583404039e-05,
      "loss": 0.558,
      "step": 10380
    },
    {
      "epoch": 1.7336893041882195,
      "grad_norm": 3.8749277591705322,
      "learning_rate": 3.321737654844731e-05,
      "loss": 0.8812,
      "step": 10390
    },
    {
      "epoch": 1.735357917570499,
      "grad_norm": 6.739394187927246,
      "learning_rate": 3.320040726285423e-05,
      "loss": 0.6771,
      "step": 10400
    },
    {
      "epoch": 1.7370265309527784,
      "grad_norm": 9.835336685180664,
      "learning_rate": 3.318343797726116e-05,
      "loss": 0.776,
      "step": 10410
    },
    {
      "epoch": 1.7386951443350576,
      "grad_norm": 6.164978504180908,
      "learning_rate": 3.3166468691668085e-05,
      "loss": 0.5325,
      "step": 10420
    },
    {
      "epoch": 1.7403637577173368,
      "grad_norm": 10.558032035827637,
      "learning_rate": 3.314949940607501e-05,
      "loss": 0.458,
      "step": 10430
    },
    {
      "epoch": 1.7420323710996162,
      "grad_norm": 1.3320008516311646,
      "learning_rate": 3.313253012048193e-05,
      "loss": 0.5137,
      "step": 10440
    },
    {
      "epoch": 1.7437009844818956,
      "grad_norm": 8.498662948608398,
      "learning_rate": 3.3115560834888855e-05,
      "loss": 0.6199,
      "step": 10450
    },
    {
      "epoch": 1.7453695978641748,
      "grad_norm": 8.61320972442627,
      "learning_rate": 3.3098591549295775e-05,
      "loss": 0.6661,
      "step": 10460
    },
    {
      "epoch": 1.7470382112464542,
      "grad_norm": 5.039820194244385,
      "learning_rate": 3.30816222637027e-05,
      "loss": 0.5044,
      "step": 10470
    },
    {
      "epoch": 1.7487068246287336,
      "grad_norm": 7.160680770874023,
      "learning_rate": 3.306465297810962e-05,
      "loss": 0.4391,
      "step": 10480
    },
    {
      "epoch": 1.7503754380110128,
      "grad_norm": 11.10680866241455,
      "learning_rate": 3.3047683692516545e-05,
      "loss": 0.7103,
      "step": 10490
    },
    {
      "epoch": 1.7520440513932922,
      "grad_norm": 9.883527755737305,
      "learning_rate": 3.303071440692347e-05,
      "loss": 0.5019,
      "step": 10500
    },
    {
      "epoch": 1.7537126647755716,
      "grad_norm": 7.379860877990723,
      "learning_rate": 3.301374512133039e-05,
      "loss": 0.5657,
      "step": 10510
    },
    {
      "epoch": 1.7553812781578508,
      "grad_norm": 15.058272361755371,
      "learning_rate": 3.2996775835737316e-05,
      "loss": 0.6599,
      "step": 10520
    },
    {
      "epoch": 1.75704989154013,
      "grad_norm": 1.5763709545135498,
      "learning_rate": 3.297980655014424e-05,
      "loss": 0.5614,
      "step": 10530
    },
    {
      "epoch": 1.7587185049224094,
      "grad_norm": 3.910569906234741,
      "learning_rate": 3.296283726455117e-05,
      "loss": 0.7069,
      "step": 10540
    },
    {
      "epoch": 1.7603871183046889,
      "grad_norm": 7.270357131958008,
      "learning_rate": 3.294586797895809e-05,
      "loss": 0.3316,
      "step": 10550
    },
    {
      "epoch": 1.762055731686968,
      "grad_norm": 13.864198684692383,
      "learning_rate": 3.292889869336501e-05,
      "loss": 0.6985,
      "step": 10560
    },
    {
      "epoch": 1.7637243450692475,
      "grad_norm": 4.435797214508057,
      "learning_rate": 3.291192940777193e-05,
      "loss": 0.5355,
      "step": 10570
    },
    {
      "epoch": 1.7653929584515269,
      "grad_norm": 9.68209171295166,
      "learning_rate": 3.289496012217886e-05,
      "loss": 0.5913,
      "step": 10580
    },
    {
      "epoch": 1.767061571833806,
      "grad_norm": 16.739978790283203,
      "learning_rate": 3.2877990836585784e-05,
      "loss": 0.6345,
      "step": 10590
    },
    {
      "epoch": 1.7687301852160853,
      "grad_norm": 4.130638122558594,
      "learning_rate": 3.28610215509927e-05,
      "loss": 0.5921,
      "step": 10600
    },
    {
      "epoch": 1.770398798598365,
      "grad_norm": 13.422857284545898,
      "learning_rate": 3.284405226539963e-05,
      "loss": 0.6877,
      "step": 10610
    },
    {
      "epoch": 1.772067411980644,
      "grad_norm": 11.048580169677734,
      "learning_rate": 3.282708297980655e-05,
      "loss": 0.5803,
      "step": 10620
    },
    {
      "epoch": 1.7737360253629233,
      "grad_norm": 4.975961685180664,
      "learning_rate": 3.2810113694213474e-05,
      "loss": 1.0338,
      "step": 10630
    },
    {
      "epoch": 1.7754046387452027,
      "grad_norm": 1.387017846107483,
      "learning_rate": 3.27931444086204e-05,
      "loss": 0.6117,
      "step": 10640
    },
    {
      "epoch": 1.7770732521274821,
      "grad_norm": 10.155404090881348,
      "learning_rate": 3.277617512302732e-05,
      "loss": 0.3057,
      "step": 10650
    },
    {
      "epoch": 1.7787418655097613,
      "grad_norm": 9.089507102966309,
      "learning_rate": 3.275920583743425e-05,
      "loss": 0.5072,
      "step": 10660
    },
    {
      "epoch": 1.7804104788920407,
      "grad_norm": 3.144770622253418,
      "learning_rate": 3.274223655184117e-05,
      "loss": 0.6008,
      "step": 10670
    },
    {
      "epoch": 1.7820790922743202,
      "grad_norm": 12.173644065856934,
      "learning_rate": 3.2725267266248096e-05,
      "loss": 0.4466,
      "step": 10680
    },
    {
      "epoch": 1.7837477056565993,
      "grad_norm": 7.076869010925293,
      "learning_rate": 3.2708297980655015e-05,
      "loss": 0.62,
      "step": 10690
    },
    {
      "epoch": 1.7854163190388785,
      "grad_norm": 9.217889785766602,
      "learning_rate": 3.269132869506194e-05,
      "loss": 0.5966,
      "step": 10700
    },
    {
      "epoch": 1.7870849324211582,
      "grad_norm": 1.9038982391357422,
      "learning_rate": 3.267435940946886e-05,
      "loss": 0.3942,
      "step": 10710
    },
    {
      "epoch": 1.7887535458034374,
      "grad_norm": 11.270748138427734,
      "learning_rate": 3.2657390123875786e-05,
      "loss": 0.6842,
      "step": 10720
    },
    {
      "epoch": 1.7904221591857166,
      "grad_norm": 2.9055447578430176,
      "learning_rate": 3.264042083828271e-05,
      "loss": 0.5642,
      "step": 10730
    },
    {
      "epoch": 1.792090772567996,
      "grad_norm": 5.878037452697754,
      "learning_rate": 3.262345155268963e-05,
      "loss": 0.4799,
      "step": 10740
    },
    {
      "epoch": 1.7937593859502754,
      "grad_norm": 1.6430087089538574,
      "learning_rate": 3.260648226709656e-05,
      "loss": 0.7068,
      "step": 10750
    },
    {
      "epoch": 1.7954279993325546,
      "grad_norm": 3.177004337310791,
      "learning_rate": 3.2589512981503476e-05,
      "loss": 0.6962,
      "step": 10760
    },
    {
      "epoch": 1.797096612714834,
      "grad_norm": 3.186316967010498,
      "learning_rate": 3.25725436959104e-05,
      "loss": 0.397,
      "step": 10770
    },
    {
      "epoch": 1.7987652260971134,
      "grad_norm": 1.0612130165100098,
      "learning_rate": 3.255557441031733e-05,
      "loss": 0.6258,
      "step": 10780
    },
    {
      "epoch": 1.8004338394793926,
      "grad_norm": 10.110121726989746,
      "learning_rate": 3.2538605124724254e-05,
      "loss": 0.6516,
      "step": 10790
    },
    {
      "epoch": 1.8021024528616718,
      "grad_norm": 0.8251810669898987,
      "learning_rate": 3.252163583913117e-05,
      "loss": 0.4628,
      "step": 10800
    },
    {
      "epoch": 1.8037710662439512,
      "grad_norm": 5.150258541107178,
      "learning_rate": 3.25046665535381e-05,
      "loss": 0.4753,
      "step": 10810
    },
    {
      "epoch": 1.8054396796262306,
      "grad_norm": 4.0074920654296875,
      "learning_rate": 3.2487697267945025e-05,
      "loss": 0.4662,
      "step": 10820
    },
    {
      "epoch": 1.8071082930085098,
      "grad_norm": 3.0421319007873535,
      "learning_rate": 3.2470727982351944e-05,
      "loss": 0.6621,
      "step": 10830
    },
    {
      "epoch": 1.8087769063907893,
      "grad_norm": 0.7683529257774353,
      "learning_rate": 3.245375869675887e-05,
      "loss": 0.4321,
      "step": 10840
    },
    {
      "epoch": 1.8104455197730687,
      "grad_norm": 2.7492711544036865,
      "learning_rate": 3.243678941116579e-05,
      "loss": 0.7065,
      "step": 10850
    },
    {
      "epoch": 1.8121141331553479,
      "grad_norm": 5.861680030822754,
      "learning_rate": 3.2419820125572715e-05,
      "loss": 0.6307,
      "step": 10860
    },
    {
      "epoch": 1.8137827465376273,
      "grad_norm": 6.611598014831543,
      "learning_rate": 3.2402850839979634e-05,
      "loss": 0.5151,
      "step": 10870
    },
    {
      "epoch": 1.8154513599199067,
      "grad_norm": 5.6307268142700195,
      "learning_rate": 3.238588155438656e-05,
      "loss": 0.5046,
      "step": 10880
    },
    {
      "epoch": 1.8171199733021859,
      "grad_norm": 12.733263969421387,
      "learning_rate": 3.2368912268793485e-05,
      "loss": 0.5777,
      "step": 10890
    },
    {
      "epoch": 1.818788586684465,
      "grad_norm": 7.524905204772949,
      "learning_rate": 3.2351942983200405e-05,
      "loss": 0.5385,
      "step": 10900
    },
    {
      "epoch": 1.8204572000667445,
      "grad_norm": 12.156197547912598,
      "learning_rate": 3.233497369760734e-05,
      "loss": 0.6401,
      "step": 10910
    },
    {
      "epoch": 1.822125813449024,
      "grad_norm": 7.838683128356934,
      "learning_rate": 3.2318004412014256e-05,
      "loss": 0.8871,
      "step": 10920
    },
    {
      "epoch": 1.823794426831303,
      "grad_norm": 5.020012378692627,
      "learning_rate": 3.230103512642118e-05,
      "loss": 0.6045,
      "step": 10930
    },
    {
      "epoch": 1.8254630402135825,
      "grad_norm": 0.3396381139755249,
      "learning_rate": 3.22840658408281e-05,
      "loss": 0.4246,
      "step": 10940
    },
    {
      "epoch": 1.827131653595862,
      "grad_norm": 7.901393413543701,
      "learning_rate": 3.226709655523503e-05,
      "loss": 0.6673,
      "step": 10950
    },
    {
      "epoch": 1.8288002669781411,
      "grad_norm": 5.437432765960693,
      "learning_rate": 3.225012726964195e-05,
      "loss": 0.4441,
      "step": 10960
    },
    {
      "epoch": 1.8304688803604203,
      "grad_norm": 9.994062423706055,
      "learning_rate": 3.223315798404887e-05,
      "loss": 0.6864,
      "step": 10970
    },
    {
      "epoch": 1.8321374937427,
      "grad_norm": 6.089557647705078,
      "learning_rate": 3.22161886984558e-05,
      "loss": 0.6012,
      "step": 10980
    },
    {
      "epoch": 1.8338061071249792,
      "grad_norm": 0.9742010235786438,
      "learning_rate": 3.219921941286272e-05,
      "loss": 0.6539,
      "step": 10990
    },
    {
      "epoch": 1.8354747205072584,
      "grad_norm": 0.9143402576446533,
      "learning_rate": 3.218225012726964e-05,
      "loss": 0.5677,
      "step": 11000
    },
    {
      "epoch": 1.8371433338895378,
      "grad_norm": 13.350502967834473,
      "learning_rate": 3.216528084167656e-05,
      "loss": 0.8112,
      "step": 11010
    },
    {
      "epoch": 1.8388119472718172,
      "grad_norm": 6.972437858581543,
      "learning_rate": 3.214831155608349e-05,
      "loss": 0.596,
      "step": 11020
    },
    {
      "epoch": 1.8404805606540964,
      "grad_norm": 10.994546890258789,
      "learning_rate": 3.2131342270490414e-05,
      "loss": 0.5629,
      "step": 11030
    },
    {
      "epoch": 1.8421491740363758,
      "grad_norm": 10.201017379760742,
      "learning_rate": 3.211437298489734e-05,
      "loss": 0.7729,
      "step": 11040
    },
    {
      "epoch": 1.8438177874186552,
      "grad_norm": 7.5085530281066895,
      "learning_rate": 3.2097403699304266e-05,
      "loss": 0.8053,
      "step": 11050
    },
    {
      "epoch": 1.8454864008009344,
      "grad_norm": 9.653270721435547,
      "learning_rate": 3.2080434413711185e-05,
      "loss": 0.3794,
      "step": 11060
    },
    {
      "epoch": 1.8471550141832136,
      "grad_norm": 0.6389364004135132,
      "learning_rate": 3.206346512811811e-05,
      "loss": 0.363,
      "step": 11070
    },
    {
      "epoch": 1.8488236275654932,
      "grad_norm": 4.516025543212891,
      "learning_rate": 3.204649584252503e-05,
      "loss": 0.6252,
      "step": 11080
    },
    {
      "epoch": 1.8504922409477724,
      "grad_norm": 0.5527119636535645,
      "learning_rate": 3.2029526556931956e-05,
      "loss": 0.4839,
      "step": 11090
    },
    {
      "epoch": 1.8521608543300516,
      "grad_norm": 3.7188079357147217,
      "learning_rate": 3.2012557271338875e-05,
      "loss": 0.4381,
      "step": 11100
    },
    {
      "epoch": 1.853829467712331,
      "grad_norm": 3.263822555541992,
      "learning_rate": 3.19955879857458e-05,
      "loss": 0.5079,
      "step": 11110
    },
    {
      "epoch": 1.8554980810946105,
      "grad_norm": 2.927082061767578,
      "learning_rate": 3.1978618700152726e-05,
      "loss": 0.3719,
      "step": 11120
    },
    {
      "epoch": 1.8571666944768896,
      "grad_norm": 5.318258285522461,
      "learning_rate": 3.1961649414559645e-05,
      "loss": 0.6272,
      "step": 11130
    },
    {
      "epoch": 1.858835307859169,
      "grad_norm": 11.67852783203125,
      "learning_rate": 3.194468012896657e-05,
      "loss": 0.6091,
      "step": 11140
    },
    {
      "epoch": 1.8605039212414485,
      "grad_norm": 4.896939754486084,
      "learning_rate": 3.192771084337349e-05,
      "loss": 0.4004,
      "step": 11150
    },
    {
      "epoch": 1.8621725346237277,
      "grad_norm": 0.5973196029663086,
      "learning_rate": 3.191074155778042e-05,
      "loss": 0.4897,
      "step": 11160
    },
    {
      "epoch": 1.8638411480060069,
      "grad_norm": 9.12716293334961,
      "learning_rate": 3.189377227218734e-05,
      "loss": 0.4832,
      "step": 11170
    },
    {
      "epoch": 1.8655097613882863,
      "grad_norm": 7.2383904457092285,
      "learning_rate": 3.187680298659427e-05,
      "loss": 0.2619,
      "step": 11180
    },
    {
      "epoch": 1.8671783747705657,
      "grad_norm": 4.951515197753906,
      "learning_rate": 3.185983370100119e-05,
      "loss": 0.4571,
      "step": 11190
    },
    {
      "epoch": 1.868846988152845,
      "grad_norm": 8.563172340393066,
      "learning_rate": 3.184286441540811e-05,
      "loss": 0.6392,
      "step": 11200
    },
    {
      "epoch": 1.8705156015351243,
      "grad_norm": 9.74447250366211,
      "learning_rate": 3.182589512981504e-05,
      "loss": 0.6079,
      "step": 11210
    },
    {
      "epoch": 1.8721842149174037,
      "grad_norm": 16.878835678100586,
      "learning_rate": 3.180892584422196e-05,
      "loss": 0.4575,
      "step": 11220
    },
    {
      "epoch": 1.873852828299683,
      "grad_norm": 5.3040008544921875,
      "learning_rate": 3.1791956558628884e-05,
      "loss": 0.6637,
      "step": 11230
    },
    {
      "epoch": 1.8755214416819623,
      "grad_norm": 7.29168701171875,
      "learning_rate": 3.17749872730358e-05,
      "loss": 0.5337,
      "step": 11240
    },
    {
      "epoch": 1.8771900550642417,
      "grad_norm": 8.342411041259766,
      "learning_rate": 3.175801798744273e-05,
      "loss": 0.7521,
      "step": 11250
    },
    {
      "epoch": 1.878858668446521,
      "grad_norm": 14.964503288269043,
      "learning_rate": 3.1741048701849655e-05,
      "loss": 0.4066,
      "step": 11260
    },
    {
      "epoch": 1.8805272818288001,
      "grad_norm": 5.132977485656738,
      "learning_rate": 3.1724079416256574e-05,
      "loss": 0.6916,
      "step": 11270
    },
    {
      "epoch": 1.8821958952110796,
      "grad_norm": 4.267861366271973,
      "learning_rate": 3.1707110130663506e-05,
      "loss": 0.5152,
      "step": 11280
    },
    {
      "epoch": 1.883864508593359,
      "grad_norm": 3.3968148231506348,
      "learning_rate": 3.1690140845070426e-05,
      "loss": 0.6236,
      "step": 11290
    },
    {
      "epoch": 1.8855331219756382,
      "grad_norm": 5.2629570960998535,
      "learning_rate": 3.167317155947735e-05,
      "loss": 0.6447,
      "step": 11300
    },
    {
      "epoch": 1.8872017353579176,
      "grad_norm": 5.754486083984375,
      "learning_rate": 3.165620227388427e-05,
      "loss": 0.5522,
      "step": 11310
    },
    {
      "epoch": 1.888870348740197,
      "grad_norm": 4.311862945556641,
      "learning_rate": 3.1639232988291196e-05,
      "loss": 0.585,
      "step": 11320
    },
    {
      "epoch": 1.8905389621224762,
      "grad_norm": 4.566871643066406,
      "learning_rate": 3.1622263702698116e-05,
      "loss": 0.8009,
      "step": 11330
    },
    {
      "epoch": 1.8922075755047556,
      "grad_norm": 5.624439239501953,
      "learning_rate": 3.160529441710504e-05,
      "loss": 0.6126,
      "step": 11340
    },
    {
      "epoch": 1.893876188887035,
      "grad_norm": 8.596899032592773,
      "learning_rate": 3.158832513151197e-05,
      "loss": 0.4831,
      "step": 11350
    },
    {
      "epoch": 1.8955448022693142,
      "grad_norm": 10.288007736206055,
      "learning_rate": 3.1571355845918886e-05,
      "loss": 0.4761,
      "step": 11360
    },
    {
      "epoch": 1.8972134156515934,
      "grad_norm": 6.186624050140381,
      "learning_rate": 3.155438656032581e-05,
      "loss": 0.8307,
      "step": 11370
    },
    {
      "epoch": 1.8988820290338728,
      "grad_norm": 6.426663398742676,
      "learning_rate": 3.153741727473273e-05,
      "loss": 0.6038,
      "step": 11380
    },
    {
      "epoch": 1.9005506424161522,
      "grad_norm": 0.8136530518531799,
      "learning_rate": 3.152044798913966e-05,
      "loss": 0.6117,
      "step": 11390
    },
    {
      "epoch": 1.9022192557984314,
      "grad_norm": 3.5329484939575195,
      "learning_rate": 3.1503478703546576e-05,
      "loss": 0.5556,
      "step": 11400
    },
    {
      "epoch": 1.9038878691807108,
      "grad_norm": 7.959613800048828,
      "learning_rate": 3.148650941795351e-05,
      "loss": 0.5365,
      "step": 11410
    },
    {
      "epoch": 1.9055564825629903,
      "grad_norm": 1.387656569480896,
      "learning_rate": 3.146954013236043e-05,
      "loss": 0.6792,
      "step": 11420
    },
    {
      "epoch": 1.9072250959452695,
      "grad_norm": 6.122289180755615,
      "learning_rate": 3.1452570846767354e-05,
      "loss": 0.6186,
      "step": 11430
    },
    {
      "epoch": 1.9088937093275486,
      "grad_norm": 2.7878494262695312,
      "learning_rate": 3.143560156117428e-05,
      "loss": 0.5085,
      "step": 11440
    },
    {
      "epoch": 1.9105623227098283,
      "grad_norm": 5.101213455200195,
      "learning_rate": 3.14186322755812e-05,
      "loss": 0.6894,
      "step": 11450
    },
    {
      "epoch": 1.9122309360921075,
      "grad_norm": 6.578903675079346,
      "learning_rate": 3.1401662989988125e-05,
      "loss": 0.4731,
      "step": 11460
    },
    {
      "epoch": 1.9138995494743867,
      "grad_norm": 6.3538007736206055,
      "learning_rate": 3.1384693704395044e-05,
      "loss": 0.4421,
      "step": 11470
    },
    {
      "epoch": 1.915568162856666,
      "grad_norm": 10.396081924438477,
      "learning_rate": 3.136772441880197e-05,
      "loss": 0.4329,
      "step": 11480
    },
    {
      "epoch": 1.9172367762389455,
      "grad_norm": 9.749968528747559,
      "learning_rate": 3.135075513320889e-05,
      "loss": 0.4967,
      "step": 11490
    },
    {
      "epoch": 1.9189053896212247,
      "grad_norm": 0.720585286617279,
      "learning_rate": 3.1333785847615815e-05,
      "loss": 0.39,
      "step": 11500
    },
    {
      "epoch": 1.9205740030035041,
      "grad_norm": 9.793452262878418,
      "learning_rate": 3.131681656202274e-05,
      "loss": 0.8769,
      "step": 11510
    },
    {
      "epoch": 1.9222426163857835,
      "grad_norm": 6.595872402191162,
      "learning_rate": 3.129984727642966e-05,
      "loss": 0.682,
      "step": 11520
    },
    {
      "epoch": 1.9239112297680627,
      "grad_norm": 8.885978698730469,
      "learning_rate": 3.128287799083659e-05,
      "loss": 0.6056,
      "step": 11530
    },
    {
      "epoch": 1.925579843150342,
      "grad_norm": 11.935713768005371,
      "learning_rate": 3.126590870524351e-05,
      "loss": 0.6897,
      "step": 11540
    },
    {
      "epoch": 1.9272484565326216,
      "grad_norm": 10.548831939697266,
      "learning_rate": 3.124893941965044e-05,
      "loss": 0.4227,
      "step": 11550
    },
    {
      "epoch": 1.9289170699149008,
      "grad_norm": 1.4791288375854492,
      "learning_rate": 3.1231970134057356e-05,
      "loss": 0.3104,
      "step": 11560
    },
    {
      "epoch": 1.93058568329718,
      "grad_norm": 3.1260805130004883,
      "learning_rate": 3.121500084846428e-05,
      "loss": 0.6085,
      "step": 11570
    },
    {
      "epoch": 1.9322542966794594,
      "grad_norm": 8.880208969116211,
      "learning_rate": 3.119803156287121e-05,
      "loss": 0.538,
      "step": 11580
    },
    {
      "epoch": 1.9339229100617388,
      "grad_norm": 0.8617621660232544,
      "learning_rate": 3.118106227727813e-05,
      "loss": 0.5741,
      "step": 11590
    },
    {
      "epoch": 1.935591523444018,
      "grad_norm": 12.775101661682129,
      "learning_rate": 3.116409299168505e-05,
      "loss": 0.5784,
      "step": 11600
    },
    {
      "epoch": 1.9372601368262974,
      "grad_norm": 8.807821273803711,
      "learning_rate": 3.114712370609197e-05,
      "loss": 0.7057,
      "step": 11610
    },
    {
      "epoch": 1.9389287502085768,
      "grad_norm": 8.331503868103027,
      "learning_rate": 3.11301544204989e-05,
      "loss": 0.5435,
      "step": 11620
    },
    {
      "epoch": 1.940597363590856,
      "grad_norm": 1.2829078435897827,
      "learning_rate": 3.111318513490582e-05,
      "loss": 0.4695,
      "step": 11630
    },
    {
      "epoch": 1.9422659769731352,
      "grad_norm": 4.042067050933838,
      "learning_rate": 3.109621584931274e-05,
      "loss": 0.5055,
      "step": 11640
    },
    {
      "epoch": 1.9439345903554146,
      "grad_norm": 4.46281099319458,
      "learning_rate": 3.107924656371967e-05,
      "loss": 0.6485,
      "step": 11650
    },
    {
      "epoch": 1.945603203737694,
      "grad_norm": 1.5449819564819336,
      "learning_rate": 3.1062277278126595e-05,
      "loss": 0.5728,
      "step": 11660
    },
    {
      "epoch": 1.9472718171199732,
      "grad_norm": 4.187109470367432,
      "learning_rate": 3.104530799253352e-05,
      "loss": 0.3793,
      "step": 11670
    },
    {
      "epoch": 1.9489404305022526,
      "grad_norm": 17.017940521240234,
      "learning_rate": 3.102833870694044e-05,
      "loss": 0.6161,
      "step": 11680
    },
    {
      "epoch": 1.950609043884532,
      "grad_norm": 1.8691128492355347,
      "learning_rate": 3.1011369421347366e-05,
      "loss": 0.4978,
      "step": 11690
    },
    {
      "epoch": 1.9522776572668112,
      "grad_norm": 7.943981170654297,
      "learning_rate": 3.0994400135754285e-05,
      "loss": 0.4734,
      "step": 11700
    },
    {
      "epoch": 1.9539462706490907,
      "grad_norm": 6.97990083694458,
      "learning_rate": 3.097743085016121e-05,
      "loss": 0.4744,
      "step": 11710
    },
    {
      "epoch": 1.95561488403137,
      "grad_norm": 6.21304988861084,
      "learning_rate": 3.096046156456813e-05,
      "loss": 0.4604,
      "step": 11720
    },
    {
      "epoch": 1.9572834974136493,
      "grad_norm": 7.487623691558838,
      "learning_rate": 3.0943492278975056e-05,
      "loss": 0.6934,
      "step": 11730
    },
    {
      "epoch": 1.9589521107959285,
      "grad_norm": 10.930161476135254,
      "learning_rate": 3.092652299338198e-05,
      "loss": 0.7604,
      "step": 11740
    },
    {
      "epoch": 1.9606207241782079,
      "grad_norm": 1.0326964855194092,
      "learning_rate": 3.09095537077889e-05,
      "loss": 0.2681,
      "step": 11750
    },
    {
      "epoch": 1.9622893375604873,
      "grad_norm": 11.920121192932129,
      "learning_rate": 3.0892584422195826e-05,
      "loss": 0.7866,
      "step": 11760
    },
    {
      "epoch": 1.9639579509427665,
      "grad_norm": 10.528475761413574,
      "learning_rate": 3.0875615136602746e-05,
      "loss": 0.438,
      "step": 11770
    },
    {
      "epoch": 1.965626564325046,
      "grad_norm": 3.993576765060425,
      "learning_rate": 3.085864585100968e-05,
      "loss": 0.4942,
      "step": 11780
    },
    {
      "epoch": 1.9672951777073253,
      "grad_norm": 9.122810363769531,
      "learning_rate": 3.08416765654166e-05,
      "loss": 0.461,
      "step": 11790
    },
    {
      "epoch": 1.9689637910896045,
      "grad_norm": 8.41453742980957,
      "learning_rate": 3.082470727982352e-05,
      "loss": 0.3495,
      "step": 11800
    },
    {
      "epoch": 1.9706324044718837,
      "grad_norm": 7.185608386993408,
      "learning_rate": 3.080773799423044e-05,
      "loss": 0.4807,
      "step": 11810
    },
    {
      "epoch": 1.9723010178541633,
      "grad_norm": 10.972938537597656,
      "learning_rate": 3.079076870863737e-05,
      "loss": 0.9336,
      "step": 11820
    },
    {
      "epoch": 1.9739696312364425,
      "grad_norm": 3.127551555633545,
      "learning_rate": 3.0773799423044294e-05,
      "loss": 0.3119,
      "step": 11830
    },
    {
      "epoch": 1.9756382446187217,
      "grad_norm": 6.248111724853516,
      "learning_rate": 3.075683013745121e-05,
      "loss": 0.5762,
      "step": 11840
    },
    {
      "epoch": 1.9773068580010011,
      "grad_norm": 6.568592548370361,
      "learning_rate": 3.073986085185814e-05,
      "loss": 0.7642,
      "step": 11850
    },
    {
      "epoch": 1.9789754713832806,
      "grad_norm": 12.768522262573242,
      "learning_rate": 3.072289156626506e-05,
      "loss": 0.4833,
      "step": 11860
    },
    {
      "epoch": 1.9806440847655598,
      "grad_norm": 8.61013412475586,
      "learning_rate": 3.0705922280671984e-05,
      "loss": 0.4242,
      "step": 11870
    },
    {
      "epoch": 1.9823126981478392,
      "grad_norm": 1.0347942113876343,
      "learning_rate": 3.068895299507891e-05,
      "loss": 0.5865,
      "step": 11880
    },
    {
      "epoch": 1.9839813115301186,
      "grad_norm": 10.497627258300781,
      "learning_rate": 3.067198370948583e-05,
      "loss": 0.5033,
      "step": 11890
    },
    {
      "epoch": 1.9856499249123978,
      "grad_norm": 2.715622901916504,
      "learning_rate": 3.065501442389276e-05,
      "loss": 0.3635,
      "step": 11900
    },
    {
      "epoch": 1.987318538294677,
      "grad_norm": 12.932367324829102,
      "learning_rate": 3.063804513829968e-05,
      "loss": 0.6352,
      "step": 11910
    },
    {
      "epoch": 1.9889871516769566,
      "grad_norm": 1.9335442781448364,
      "learning_rate": 3.0621075852706607e-05,
      "loss": 0.3778,
      "step": 11920
    },
    {
      "epoch": 1.9906557650592358,
      "grad_norm": 10.496332168579102,
      "learning_rate": 3.0604106567113526e-05,
      "loss": 0.7397,
      "step": 11930
    },
    {
      "epoch": 1.992324378441515,
      "grad_norm": 9.492559432983398,
      "learning_rate": 3.058713728152045e-05,
      "loss": 0.5012,
      "step": 11940
    },
    {
      "epoch": 1.9939929918237944,
      "grad_norm": 4.908839702606201,
      "learning_rate": 3.057016799592737e-05,
      "loss": 0.3917,
      "step": 11950
    },
    {
      "epoch": 1.9956616052060738,
      "grad_norm": 6.538492679595947,
      "learning_rate": 3.0553198710334297e-05,
      "loss": 0.6639,
      "step": 11960
    },
    {
      "epoch": 1.997330218588353,
      "grad_norm": 4.734493732452393,
      "learning_rate": 3.053622942474122e-05,
      "loss": 0.6073,
      "step": 11970
    },
    {
      "epoch": 1.9989988319706324,
      "grad_norm": 5.847544193267822,
      "learning_rate": 3.051926013914814e-05,
      "loss": 0.5506,
      "step": 11980
    },
    {
      "epoch": 2.000667445352912,
      "grad_norm": 3.656233310699463,
      "learning_rate": 3.0502290853555064e-05,
      "loss": 0.6955,
      "step": 11990
    },
    {
      "epoch": 2.002336058735191,
      "grad_norm": 7.540619850158691,
      "learning_rate": 3.048532156796199e-05,
      "loss": 0.3495,
      "step": 12000
    },
    {
      "epoch": 2.0040046721174702,
      "grad_norm": 6.719699859619141,
      "learning_rate": 3.0468352282368912e-05,
      "loss": 0.5206,
      "step": 12010
    },
    {
      "epoch": 2.00567328549975,
      "grad_norm": 4.37650728225708,
      "learning_rate": 3.0451382996775835e-05,
      "loss": 0.5715,
      "step": 12020
    },
    {
      "epoch": 2.007341898882029,
      "grad_norm": 4.422008514404297,
      "learning_rate": 3.0434413711182764e-05,
      "loss": 0.4083,
      "step": 12030
    },
    {
      "epoch": 2.0090105122643083,
      "grad_norm": 10.741729736328125,
      "learning_rate": 3.0417444425589687e-05,
      "loss": 0.5939,
      "step": 12040
    },
    {
      "epoch": 2.010679125646588,
      "grad_norm": 8.938575744628906,
      "learning_rate": 3.040047513999661e-05,
      "loss": 0.6847,
      "step": 12050
    },
    {
      "epoch": 2.012347739028867,
      "grad_norm": 4.586263179779053,
      "learning_rate": 3.038350585440353e-05,
      "loss": 0.4663,
      "step": 12060
    },
    {
      "epoch": 2.0140163524111463,
      "grad_norm": 7.086064338684082,
      "learning_rate": 3.0366536568810454e-05,
      "loss": 0.6129,
      "step": 12070
    },
    {
      "epoch": 2.0156849657934255,
      "grad_norm": 3.0381317138671875,
      "learning_rate": 3.034956728321738e-05,
      "loss": 0.6335,
      "step": 12080
    },
    {
      "epoch": 2.017353579175705,
      "grad_norm": 4.102874279022217,
      "learning_rate": 3.0332597997624302e-05,
      "loss": 0.3789,
      "step": 12090
    },
    {
      "epoch": 2.0190221925579843,
      "grad_norm": 1.653260350227356,
      "learning_rate": 3.0315628712031225e-05,
      "loss": 0.4595,
      "step": 12100
    },
    {
      "epoch": 2.0206908059402635,
      "grad_norm": 7.681581974029541,
      "learning_rate": 3.0298659426438147e-05,
      "loss": 0.4069,
      "step": 12110
    },
    {
      "epoch": 2.022359419322543,
      "grad_norm": 4.243295192718506,
      "learning_rate": 3.028169014084507e-05,
      "loss": 0.581,
      "step": 12120
    },
    {
      "epoch": 2.0240280327048223,
      "grad_norm": 0.35029134154319763,
      "learning_rate": 3.0264720855251992e-05,
      "loss": 0.6699,
      "step": 12130
    },
    {
      "epoch": 2.0256966460871015,
      "grad_norm": 6.087726593017578,
      "learning_rate": 3.0247751569658915e-05,
      "loss": 0.4583,
      "step": 12140
    },
    {
      "epoch": 2.0273652594693807,
      "grad_norm": 7.99487829208374,
      "learning_rate": 3.0230782284065844e-05,
      "loss": 0.5356,
      "step": 12150
    },
    {
      "epoch": 2.0290338728516604,
      "grad_norm": 19.553234100341797,
      "learning_rate": 3.0213812998472767e-05,
      "loss": 0.663,
      "step": 12160
    },
    {
      "epoch": 2.0307024862339396,
      "grad_norm": 15.015917778015137,
      "learning_rate": 3.0196843712879692e-05,
      "loss": 0.5251,
      "step": 12170
    },
    {
      "epoch": 2.0323710996162188,
      "grad_norm": 14.860518455505371,
      "learning_rate": 3.0179874427286615e-05,
      "loss": 0.538,
      "step": 12180
    },
    {
      "epoch": 2.0340397129984984,
      "grad_norm": 10.012701034545898,
      "learning_rate": 3.0162905141693537e-05,
      "loss": 0.7961,
      "step": 12190
    },
    {
      "epoch": 2.0357083263807776,
      "grad_norm": 7.70063591003418,
      "learning_rate": 3.014593585610046e-05,
      "loss": 0.4188,
      "step": 12200
    },
    {
      "epoch": 2.037376939763057,
      "grad_norm": 5.291234493255615,
      "learning_rate": 3.0128966570507382e-05,
      "loss": 0.8423,
      "step": 12210
    },
    {
      "epoch": 2.0390455531453364,
      "grad_norm": 0.9538934826850891,
      "learning_rate": 3.0111997284914305e-05,
      "loss": 0.7358,
      "step": 12220
    },
    {
      "epoch": 2.0407141665276156,
      "grad_norm": 4.357987403869629,
      "learning_rate": 3.009502799932123e-05,
      "loss": 0.5681,
      "step": 12230
    },
    {
      "epoch": 2.042382779909895,
      "grad_norm": 13.693408966064453,
      "learning_rate": 3.0078058713728153e-05,
      "loss": 0.5558,
      "step": 12240
    },
    {
      "epoch": 2.044051393292174,
      "grad_norm": 6.740083694458008,
      "learning_rate": 3.0061089428135076e-05,
      "loss": 0.5777,
      "step": 12250
    },
    {
      "epoch": 2.0457200066744536,
      "grad_norm": 7.99412202835083,
      "learning_rate": 3.0044120142541998e-05,
      "loss": 0.4226,
      "step": 12260
    },
    {
      "epoch": 2.047388620056733,
      "grad_norm": 3.7204415798187256,
      "learning_rate": 3.002715085694892e-05,
      "loss": 0.4644,
      "step": 12270
    },
    {
      "epoch": 2.049057233439012,
      "grad_norm": 10.582141876220703,
      "learning_rate": 3.001018157135585e-05,
      "loss": 0.4853,
      "step": 12280
    },
    {
      "epoch": 2.0507258468212917,
      "grad_norm": 9.746149063110352,
      "learning_rate": 2.9993212285762772e-05,
      "loss": 0.3847,
      "step": 12290
    },
    {
      "epoch": 2.052394460203571,
      "grad_norm": 13.522899627685547,
      "learning_rate": 2.9976243000169695e-05,
      "loss": 0.3982,
      "step": 12300
    },
    {
      "epoch": 2.05406307358585,
      "grad_norm": 5.7668046951293945,
      "learning_rate": 2.9959273714576617e-05,
      "loss": 0.338,
      "step": 12310
    },
    {
      "epoch": 2.0557316869681297,
      "grad_norm": 6.3579912185668945,
      "learning_rate": 2.9942304428983543e-05,
      "loss": 0.3756,
      "step": 12320
    },
    {
      "epoch": 2.057400300350409,
      "grad_norm": 1.4249862432479858,
      "learning_rate": 2.9925335143390466e-05,
      "loss": 0.4636,
      "step": 12330
    },
    {
      "epoch": 2.059068913732688,
      "grad_norm": 6.857154369354248,
      "learning_rate": 2.9908365857797388e-05,
      "loss": 0.4674,
      "step": 12340
    },
    {
      "epoch": 2.0607375271149673,
      "grad_norm": 1.3785401582717896,
      "learning_rate": 2.989139657220431e-05,
      "loss": 0.4943,
      "step": 12350
    },
    {
      "epoch": 2.062406140497247,
      "grad_norm": 8.89223861694336,
      "learning_rate": 2.9874427286611233e-05,
      "loss": 0.5299,
      "step": 12360
    },
    {
      "epoch": 2.064074753879526,
      "grad_norm": 3.889099597930908,
      "learning_rate": 2.9857458001018156e-05,
      "loss": 0.5589,
      "step": 12370
    },
    {
      "epoch": 2.0657433672618053,
      "grad_norm": 2.692795515060425,
      "learning_rate": 2.984048871542508e-05,
      "loss": 0.9343,
      "step": 12380
    },
    {
      "epoch": 2.067411980644085,
      "grad_norm": 1.1588386297225952,
      "learning_rate": 2.9823519429832004e-05,
      "loss": 0.5586,
      "step": 12390
    },
    {
      "epoch": 2.069080594026364,
      "grad_norm": 7.8674750328063965,
      "learning_rate": 2.980655014423893e-05,
      "loss": 0.484,
      "step": 12400
    },
    {
      "epoch": 2.0707492074086433,
      "grad_norm": 1.4806269407272339,
      "learning_rate": 2.9789580858645856e-05,
      "loss": 0.4164,
      "step": 12410
    },
    {
      "epoch": 2.072417820790923,
      "grad_norm": 15.640493392944336,
      "learning_rate": 2.977261157305278e-05,
      "loss": 0.5886,
      "step": 12420
    },
    {
      "epoch": 2.074086434173202,
      "grad_norm": 9.192948341369629,
      "learning_rate": 2.97556422874597e-05,
      "loss": 0.702,
      "step": 12430
    },
    {
      "epoch": 2.0757550475554813,
      "grad_norm": 8.320338249206543,
      "learning_rate": 2.9738673001866623e-05,
      "loss": 0.8481,
      "step": 12440
    },
    {
      "epoch": 2.0774236609377605,
      "grad_norm": 5.786459445953369,
      "learning_rate": 2.9721703716273546e-05,
      "loss": 0.5195,
      "step": 12450
    },
    {
      "epoch": 2.07909227432004,
      "grad_norm": 4.137303829193115,
      "learning_rate": 2.9704734430680468e-05,
      "loss": 0.7377,
      "step": 12460
    },
    {
      "epoch": 2.0807608877023194,
      "grad_norm": 1.7403210401535034,
      "learning_rate": 2.9687765145087394e-05,
      "loss": 0.431,
      "step": 12470
    },
    {
      "epoch": 2.0824295010845986,
      "grad_norm": 3.6532537937164307,
      "learning_rate": 2.9670795859494317e-05,
      "loss": 0.4636,
      "step": 12480
    },
    {
      "epoch": 2.084098114466878,
      "grad_norm": 0.8545393943786621,
      "learning_rate": 2.965382657390124e-05,
      "loss": 0.4733,
      "step": 12490
    },
    {
      "epoch": 2.0857667278491574,
      "grad_norm": 10.500234603881836,
      "learning_rate": 2.963685728830816e-05,
      "loss": 0.4495,
      "step": 12500
    },
    {
      "epoch": 2.0874353412314366,
      "grad_norm": 6.946829795837402,
      "learning_rate": 2.9619888002715084e-05,
      "loss": 0.2455,
      "step": 12510
    },
    {
      "epoch": 2.089103954613716,
      "grad_norm": 10.221985816955566,
      "learning_rate": 2.9602918717122007e-05,
      "loss": 0.5495,
      "step": 12520
    },
    {
      "epoch": 2.0907725679959954,
      "grad_norm": 10.66140079498291,
      "learning_rate": 2.9585949431528936e-05,
      "loss": 0.3738,
      "step": 12530
    },
    {
      "epoch": 2.0924411813782746,
      "grad_norm": 6.3389177322387695,
      "learning_rate": 2.956898014593586e-05,
      "loss": 0.3899,
      "step": 12540
    },
    {
      "epoch": 2.094109794760554,
      "grad_norm": 1.6589007377624512,
      "learning_rate": 2.9552010860342784e-05,
      "loss": 0.6496,
      "step": 12550
    },
    {
      "epoch": 2.0957784081428334,
      "grad_norm": 9.849089622497559,
      "learning_rate": 2.9535041574749707e-05,
      "loss": 0.8932,
      "step": 12560
    },
    {
      "epoch": 2.0974470215251126,
      "grad_norm": 7.120684623718262,
      "learning_rate": 2.951807228915663e-05,
      "loss": 0.477,
      "step": 12570
    },
    {
      "epoch": 2.099115634907392,
      "grad_norm": 11.444722175598145,
      "learning_rate": 2.950110300356355e-05,
      "loss": 0.6905,
      "step": 12580
    },
    {
      "epoch": 2.1007842482896715,
      "grad_norm": 4.231425762176514,
      "learning_rate": 2.9484133717970474e-05,
      "loss": 0.4447,
      "step": 12590
    },
    {
      "epoch": 2.1024528616719507,
      "grad_norm": 5.696352005004883,
      "learning_rate": 2.9467164432377397e-05,
      "loss": 0.5398,
      "step": 12600
    },
    {
      "epoch": 2.10412147505423,
      "grad_norm": 3.5514798164367676,
      "learning_rate": 2.945019514678432e-05,
      "loss": 0.4802,
      "step": 12610
    },
    {
      "epoch": 2.105790088436509,
      "grad_norm": 1.5908422470092773,
      "learning_rate": 2.9433225861191245e-05,
      "loss": 0.3968,
      "step": 12620
    },
    {
      "epoch": 2.1074587018187887,
      "grad_norm": 10.419943809509277,
      "learning_rate": 2.9416256575598167e-05,
      "loss": 0.5811,
      "step": 12630
    },
    {
      "epoch": 2.109127315201068,
      "grad_norm": 5.435500144958496,
      "learning_rate": 2.939928729000509e-05,
      "loss": 0.6109,
      "step": 12640
    },
    {
      "epoch": 2.110795928583347,
      "grad_norm": 6.436064720153809,
      "learning_rate": 2.938231800441202e-05,
      "loss": 0.4067,
      "step": 12650
    },
    {
      "epoch": 2.1124645419656267,
      "grad_norm": 1.5710006952285767,
      "learning_rate": 2.9365348718818942e-05,
      "loss": 0.3871,
      "step": 12660
    },
    {
      "epoch": 2.114133155347906,
      "grad_norm": 0.9056005477905273,
      "learning_rate": 2.9348379433225864e-05,
      "loss": 0.3814,
      "step": 12670
    },
    {
      "epoch": 2.115801768730185,
      "grad_norm": 3.527066230773926,
      "learning_rate": 2.9331410147632787e-05,
      "loss": 0.5436,
      "step": 12680
    },
    {
      "epoch": 2.1174703821124647,
      "grad_norm": 6.737740993499756,
      "learning_rate": 2.931444086203971e-05,
      "loss": 0.5535,
      "step": 12690
    },
    {
      "epoch": 2.119138995494744,
      "grad_norm": 13.733390808105469,
      "learning_rate": 2.9297471576446635e-05,
      "loss": 0.5044,
      "step": 12700
    },
    {
      "epoch": 2.120807608877023,
      "grad_norm": 2.3150222301483154,
      "learning_rate": 2.9280502290853558e-05,
      "loss": 0.7062,
      "step": 12710
    },
    {
      "epoch": 2.1224762222593023,
      "grad_norm": 7.311789035797119,
      "learning_rate": 2.926353300526048e-05,
      "loss": 0.5543,
      "step": 12720
    },
    {
      "epoch": 2.124144835641582,
      "grad_norm": 6.234816074371338,
      "learning_rate": 2.9246563719667403e-05,
      "loss": 0.5136,
      "step": 12730
    },
    {
      "epoch": 2.125813449023861,
      "grad_norm": 2.717360734939575,
      "learning_rate": 2.9229594434074325e-05,
      "loss": 0.4859,
      "step": 12740
    },
    {
      "epoch": 2.1274820624061404,
      "grad_norm": 9.480264663696289,
      "learning_rate": 2.9212625148481247e-05,
      "loss": 0.7995,
      "step": 12750
    },
    {
      "epoch": 2.12915067578842,
      "grad_norm": 2.9084832668304443,
      "learning_rate": 2.919565586288817e-05,
      "loss": 0.3789,
      "step": 12760
    },
    {
      "epoch": 2.130819289170699,
      "grad_norm": 7.807698726654053,
      "learning_rate": 2.9178686577295096e-05,
      "loss": 0.4867,
      "step": 12770
    },
    {
      "epoch": 2.1324879025529784,
      "grad_norm": 4.789455890655518,
      "learning_rate": 2.9161717291702022e-05,
      "loss": 0.3474,
      "step": 12780
    },
    {
      "epoch": 2.134156515935258,
      "grad_norm": 0.97650545835495,
      "learning_rate": 2.9144748006108948e-05,
      "loss": 0.4278,
      "step": 12790
    },
    {
      "epoch": 2.135825129317537,
      "grad_norm": 2.711272716522217,
      "learning_rate": 2.912777872051587e-05,
      "loss": 0.4764,
      "step": 12800
    },
    {
      "epoch": 2.1374937426998164,
      "grad_norm": 9.148513793945312,
      "learning_rate": 2.9110809434922793e-05,
      "loss": 0.4655,
      "step": 12810
    },
    {
      "epoch": 2.1391623560820956,
      "grad_norm": 1.172139048576355,
      "learning_rate": 2.9093840149329715e-05,
      "loss": 0.7616,
      "step": 12820
    },
    {
      "epoch": 2.1408309694643752,
      "grad_norm": 5.869009494781494,
      "learning_rate": 2.9076870863736638e-05,
      "loss": 0.4977,
      "step": 12830
    },
    {
      "epoch": 2.1424995828466544,
      "grad_norm": 4.221654891967773,
      "learning_rate": 2.905990157814356e-05,
      "loss": 0.4293,
      "step": 12840
    },
    {
      "epoch": 2.1441681962289336,
      "grad_norm": 10.422656059265137,
      "learning_rate": 2.9042932292550486e-05,
      "loss": 0.5325,
      "step": 12850
    },
    {
      "epoch": 2.1458368096112133,
      "grad_norm": 10.91253662109375,
      "learning_rate": 2.902596300695741e-05,
      "loss": 0.4442,
      "step": 12860
    },
    {
      "epoch": 2.1475054229934925,
      "grad_norm": 1.8430544137954712,
      "learning_rate": 2.900899372136433e-05,
      "loss": 0.2767,
      "step": 12870
    },
    {
      "epoch": 2.1491740363757716,
      "grad_norm": 3.2155070304870605,
      "learning_rate": 2.8992024435771253e-05,
      "loss": 0.7218,
      "step": 12880
    },
    {
      "epoch": 2.150842649758051,
      "grad_norm": 10.82689094543457,
      "learning_rate": 2.8975055150178176e-05,
      "loss": 0.5186,
      "step": 12890
    },
    {
      "epoch": 2.1525112631403305,
      "grad_norm": 13.060395240783691,
      "learning_rate": 2.8958085864585105e-05,
      "loss": 0.6032,
      "step": 12900
    },
    {
      "epoch": 2.1541798765226097,
      "grad_norm": 5.103481769561768,
      "learning_rate": 2.8941116578992028e-05,
      "loss": 0.5916,
      "step": 12910
    },
    {
      "epoch": 2.155848489904889,
      "grad_norm": 2.019801139831543,
      "learning_rate": 2.892414729339895e-05,
      "loss": 0.3231,
      "step": 12920
    },
    {
      "epoch": 2.1575171032871685,
      "grad_norm": 3.637540578842163,
      "learning_rate": 2.8907178007805873e-05,
      "loss": 0.6351,
      "step": 12930
    },
    {
      "epoch": 2.1591857166694477,
      "grad_norm": 6.709490776062012,
      "learning_rate": 2.88902087222128e-05,
      "loss": 0.5977,
      "step": 12940
    },
    {
      "epoch": 2.160854330051727,
      "grad_norm": 8.298423767089844,
      "learning_rate": 2.887323943661972e-05,
      "loss": 0.4528,
      "step": 12950
    },
    {
      "epoch": 2.1625229434340065,
      "grad_norm": 3.162914752960205,
      "learning_rate": 2.8856270151026643e-05,
      "loss": 0.6431,
      "step": 12960
    },
    {
      "epoch": 2.1641915568162857,
      "grad_norm": 4.120605945587158,
      "learning_rate": 2.8839300865433566e-05,
      "loss": 0.6353,
      "step": 12970
    },
    {
      "epoch": 2.165860170198565,
      "grad_norm": 12.940481185913086,
      "learning_rate": 2.882233157984049e-05,
      "loss": 0.5146,
      "step": 12980
    },
    {
      "epoch": 2.1675287835808446,
      "grad_norm": 2.7246739864349365,
      "learning_rate": 2.880536229424741e-05,
      "loss": 0.4305,
      "step": 12990
    },
    {
      "epoch": 2.1691973969631237,
      "grad_norm": 1.1362152099609375,
      "learning_rate": 2.8788393008654337e-05,
      "loss": 0.7868,
      "step": 13000
    },
    {
      "epoch": 2.170866010345403,
      "grad_norm": 4.970401287078857,
      "learning_rate": 2.877142372306126e-05,
      "loss": 0.5964,
      "step": 13010
    },
    {
      "epoch": 2.172534623727682,
      "grad_norm": 1.8331283330917358,
      "learning_rate": 2.8754454437468182e-05,
      "loss": 0.4568,
      "step": 13020
    },
    {
      "epoch": 2.1742032371099618,
      "grad_norm": 7.600140571594238,
      "learning_rate": 2.873748515187511e-05,
      "loss": 0.6412,
      "step": 13030
    },
    {
      "epoch": 2.175871850492241,
      "grad_norm": 6.49030876159668,
      "learning_rate": 2.8720515866282033e-05,
      "loss": 0.5284,
      "step": 13040
    },
    {
      "epoch": 2.17754046387452,
      "grad_norm": 12.079925537109375,
      "learning_rate": 2.8703546580688956e-05,
      "loss": 0.4784,
      "step": 13050
    },
    {
      "epoch": 2.1792090772568,
      "grad_norm": 3.1388189792633057,
      "learning_rate": 2.868657729509588e-05,
      "loss": 0.5296,
      "step": 13060
    },
    {
      "epoch": 2.180877690639079,
      "grad_norm": 7.171360969543457,
      "learning_rate": 2.86696080095028e-05,
      "loss": 0.501,
      "step": 13070
    },
    {
      "epoch": 2.182546304021358,
      "grad_norm": 2.903075695037842,
      "learning_rate": 2.8652638723909723e-05,
      "loss": 0.3182,
      "step": 13080
    },
    {
      "epoch": 2.1842149174036374,
      "grad_norm": 5.190382957458496,
      "learning_rate": 2.863566943831665e-05,
      "loss": 0.5311,
      "step": 13090
    },
    {
      "epoch": 2.185883530785917,
      "grad_norm": 8.679228782653809,
      "learning_rate": 2.8618700152723572e-05,
      "loss": 0.4008,
      "step": 13100
    },
    {
      "epoch": 2.187552144168196,
      "grad_norm": 2.4199836254119873,
      "learning_rate": 2.8601730867130494e-05,
      "loss": 0.5175,
      "step": 13110
    },
    {
      "epoch": 2.1892207575504754,
      "grad_norm": 3.0048866271972656,
      "learning_rate": 2.8584761581537417e-05,
      "loss": 0.526,
      "step": 13120
    },
    {
      "epoch": 2.190889370932755,
      "grad_norm": 6.48457145690918,
      "learning_rate": 2.856779229594434e-05,
      "loss": 0.3963,
      "step": 13130
    },
    {
      "epoch": 2.1925579843150342,
      "grad_norm": 6.437535762786865,
      "learning_rate": 2.8550823010351262e-05,
      "loss": 0.3553,
      "step": 13140
    },
    {
      "epoch": 2.1942265976973134,
      "grad_norm": 2.339966297149658,
      "learning_rate": 2.853385372475819e-05,
      "loss": 0.4541,
      "step": 13150
    },
    {
      "epoch": 2.195895211079593,
      "grad_norm": 11.770095825195312,
      "learning_rate": 2.8516884439165113e-05,
      "loss": 0.5482,
      "step": 13160
    },
    {
      "epoch": 2.1975638244618723,
      "grad_norm": 4.126591682434082,
      "learning_rate": 2.849991515357204e-05,
      "loss": 0.269,
      "step": 13170
    },
    {
      "epoch": 2.1992324378441515,
      "grad_norm": 2.0386154651641846,
      "learning_rate": 2.8482945867978962e-05,
      "loss": 0.5041,
      "step": 13180
    },
    {
      "epoch": 2.2009010512264306,
      "grad_norm": 5.194934368133545,
      "learning_rate": 2.8465976582385884e-05,
      "loss": 0.7724,
      "step": 13190
    },
    {
      "epoch": 2.2025696646087103,
      "grad_norm": 5.929370403289795,
      "learning_rate": 2.8449007296792807e-05,
      "loss": 0.3169,
      "step": 13200
    },
    {
      "epoch": 2.2042382779909895,
      "grad_norm": 4.292011737823486,
      "learning_rate": 2.843203801119973e-05,
      "loss": 0.5352,
      "step": 13210
    },
    {
      "epoch": 2.2059068913732687,
      "grad_norm": 1.6832318305969238,
      "learning_rate": 2.8415068725606652e-05,
      "loss": 0.4769,
      "step": 13220
    },
    {
      "epoch": 2.2075755047555483,
      "grad_norm": 4.6774139404296875,
      "learning_rate": 2.8398099440013574e-05,
      "loss": 0.3571,
      "step": 13230
    },
    {
      "epoch": 2.2092441181378275,
      "grad_norm": 0.8410805463790894,
      "learning_rate": 2.83811301544205e-05,
      "loss": 0.3762,
      "step": 13240
    },
    {
      "epoch": 2.2109127315201067,
      "grad_norm": 9.830373764038086,
      "learning_rate": 2.8364160868827423e-05,
      "loss": 0.3128,
      "step": 13250
    },
    {
      "epoch": 2.212581344902386,
      "grad_norm": 4.924778938293457,
      "learning_rate": 2.8347191583234345e-05,
      "loss": 0.5139,
      "step": 13260
    },
    {
      "epoch": 2.2142499582846655,
      "grad_norm": 1.2371395826339722,
      "learning_rate": 2.8330222297641268e-05,
      "loss": 0.6135,
      "step": 13270
    },
    {
      "epoch": 2.2159185716669447,
      "grad_norm": 8.88076114654541,
      "learning_rate": 2.8313253012048197e-05,
      "loss": 0.6983,
      "step": 13280
    },
    {
      "epoch": 2.217587185049224,
      "grad_norm": 9.418289184570312,
      "learning_rate": 2.829628372645512e-05,
      "loss": 0.4874,
      "step": 13290
    },
    {
      "epoch": 2.2192557984315036,
      "grad_norm": 1.2204797267913818,
      "learning_rate": 2.8279314440862042e-05,
      "loss": 0.6484,
      "step": 13300
    },
    {
      "epoch": 2.2209244118137828,
      "grad_norm": 8.434146881103516,
      "learning_rate": 2.8262345155268964e-05,
      "loss": 0.5336,
      "step": 13310
    },
    {
      "epoch": 2.222593025196062,
      "grad_norm": 10.143244743347168,
      "learning_rate": 2.824537586967589e-05,
      "loss": 0.644,
      "step": 13320
    },
    {
      "epoch": 2.2242616385783416,
      "grad_norm": 12.904918670654297,
      "learning_rate": 2.8228406584082813e-05,
      "loss": 0.5091,
      "step": 13330
    },
    {
      "epoch": 2.2259302519606208,
      "grad_norm": 10.921339988708496,
      "learning_rate": 2.8211437298489735e-05,
      "loss": 0.6932,
      "step": 13340
    },
    {
      "epoch": 2.2275988653429,
      "grad_norm": 7.824341297149658,
      "learning_rate": 2.8194468012896658e-05,
      "loss": 0.582,
      "step": 13350
    },
    {
      "epoch": 2.2292674787251796,
      "grad_norm": 6.41585636138916,
      "learning_rate": 2.817749872730358e-05,
      "loss": 0.4768,
      "step": 13360
    },
    {
      "epoch": 2.230936092107459,
      "grad_norm": 2.735159397125244,
      "learning_rate": 2.8160529441710503e-05,
      "loss": 0.5172,
      "step": 13370
    },
    {
      "epoch": 2.232604705489738,
      "grad_norm": 7.337742328643799,
      "learning_rate": 2.8143560156117425e-05,
      "loss": 0.3858,
      "step": 13380
    },
    {
      "epoch": 2.234273318872017,
      "grad_norm": 3.4040939807891846,
      "learning_rate": 2.812659087052435e-05,
      "loss": 0.2563,
      "step": 13390
    },
    {
      "epoch": 2.235941932254297,
      "grad_norm": 10.228240966796875,
      "learning_rate": 2.8109621584931277e-05,
      "loss": 0.4231,
      "step": 13400
    },
    {
      "epoch": 2.237610545636576,
      "grad_norm": 5.563318729400635,
      "learning_rate": 2.8092652299338203e-05,
      "loss": 0.5893,
      "step": 13410
    },
    {
      "epoch": 2.239279159018855,
      "grad_norm": 10.337251663208008,
      "learning_rate": 2.8075683013745125e-05,
      "loss": 0.4024,
      "step": 13420
    },
    {
      "epoch": 2.240947772401135,
      "grad_norm": 1.1530741453170776,
      "learning_rate": 2.8058713728152048e-05,
      "loss": 0.4062,
      "step": 13430
    },
    {
      "epoch": 2.242616385783414,
      "grad_norm": 10.079024314880371,
      "learning_rate": 2.804174444255897e-05,
      "loss": 0.6917,
      "step": 13440
    },
    {
      "epoch": 2.2442849991656932,
      "grad_norm": 8.627216339111328,
      "learning_rate": 2.8024775156965893e-05,
      "loss": 0.4201,
      "step": 13450
    },
    {
      "epoch": 2.2459536125479724,
      "grad_norm": 5.335139274597168,
      "learning_rate": 2.8007805871372815e-05,
      "loss": 0.325,
      "step": 13460
    },
    {
      "epoch": 2.247622225930252,
      "grad_norm": 2.225618362426758,
      "learning_rate": 2.799083658577974e-05,
      "loss": 0.3293,
      "step": 13470
    },
    {
      "epoch": 2.2492908393125313,
      "grad_norm": 1.7753230333328247,
      "learning_rate": 2.7973867300186664e-05,
      "loss": 0.4549,
      "step": 13480
    },
    {
      "epoch": 2.2509594526948105,
      "grad_norm": 19.180471420288086,
      "learning_rate": 2.7956898014593586e-05,
      "loss": 0.5312,
      "step": 13490
    },
    {
      "epoch": 2.25262806607709,
      "grad_norm": 7.465324878692627,
      "learning_rate": 2.793992872900051e-05,
      "loss": 0.4008,
      "step": 13500
    },
    {
      "epoch": 2.2542966794593693,
      "grad_norm": 8.322196006774902,
      "learning_rate": 2.792295944340743e-05,
      "loss": 0.4705,
      "step": 13510
    },
    {
      "epoch": 2.2559652928416485,
      "grad_norm": 2.858254909515381,
      "learning_rate": 2.790599015781436e-05,
      "loss": 0.7672,
      "step": 13520
    },
    {
      "epoch": 2.257633906223928,
      "grad_norm": 3.1629180908203125,
      "learning_rate": 2.7889020872221283e-05,
      "loss": 0.4558,
      "step": 13530
    },
    {
      "epoch": 2.2593025196062073,
      "grad_norm": 4.030550003051758,
      "learning_rate": 2.7872051586628205e-05,
      "loss": 0.6191,
      "step": 13540
    },
    {
      "epoch": 2.2609711329884865,
      "grad_norm": 10.517328262329102,
      "learning_rate": 2.7855082301035128e-05,
      "loss": 0.4625,
      "step": 13550
    },
    {
      "epoch": 2.2626397463707657,
      "grad_norm": 2.135347843170166,
      "learning_rate": 2.7838113015442054e-05,
      "loss": 0.5435,
      "step": 13560
    },
    {
      "epoch": 2.2643083597530453,
      "grad_norm": 4.862833499908447,
      "learning_rate": 2.7821143729848976e-05,
      "loss": 0.4581,
      "step": 13570
    },
    {
      "epoch": 2.2659769731353245,
      "grad_norm": 6.84574556350708,
      "learning_rate": 2.78041744442559e-05,
      "loss": 0.5261,
      "step": 13580
    },
    {
      "epoch": 2.2676455865176037,
      "grad_norm": 15.721399307250977,
      "learning_rate": 2.778720515866282e-05,
      "loss": 0.6093,
      "step": 13590
    },
    {
      "epoch": 2.2693141998998834,
      "grad_norm": 2.9687376022338867,
      "learning_rate": 2.7770235873069744e-05,
      "loss": 0.5485,
      "step": 13600
    },
    {
      "epoch": 2.2709828132821626,
      "grad_norm": 5.083893775939941,
      "learning_rate": 2.7753266587476666e-05,
      "loss": 0.3441,
      "step": 13610
    },
    {
      "epoch": 2.2726514266644418,
      "grad_norm": 0.4541105031967163,
      "learning_rate": 2.7736297301883592e-05,
      "loss": 0.4773,
      "step": 13620
    },
    {
      "epoch": 2.274320040046721,
      "grad_norm": 9.963207244873047,
      "learning_rate": 2.7719328016290514e-05,
      "loss": 0.5137,
      "step": 13630
    },
    {
      "epoch": 2.2759886534290006,
      "grad_norm": 10.280560493469238,
      "learning_rate": 2.7702358730697437e-05,
      "loss": 0.54,
      "step": 13640
    },
    {
      "epoch": 2.27765726681128,
      "grad_norm": 14.807251930236816,
      "learning_rate": 2.7685389445104366e-05,
      "loss": 0.5698,
      "step": 13650
    },
    {
      "epoch": 2.279325880193559,
      "grad_norm": 8.25794792175293,
      "learning_rate": 2.766842015951129e-05,
      "loss": 0.3997,
      "step": 13660
    },
    {
      "epoch": 2.2809944935758386,
      "grad_norm": 10.646844863891602,
      "learning_rate": 2.765145087391821e-05,
      "loss": 0.7307,
      "step": 13670
    },
    {
      "epoch": 2.282663106958118,
      "grad_norm": 4.251768112182617,
      "learning_rate": 2.7634481588325134e-05,
      "loss": 0.5461,
      "step": 13680
    },
    {
      "epoch": 2.284331720340397,
      "grad_norm": 4.759134292602539,
      "learning_rate": 2.7617512302732056e-05,
      "loss": 0.3784,
      "step": 13690
    },
    {
      "epoch": 2.2860003337226766,
      "grad_norm": 9.836586952209473,
      "learning_rate": 2.760054301713898e-05,
      "loss": 0.5128,
      "step": 13700
    },
    {
      "epoch": 2.287668947104956,
      "grad_norm": 9.351521492004395,
      "learning_rate": 2.7583573731545904e-05,
      "loss": 0.3768,
      "step": 13710
    },
    {
      "epoch": 2.289337560487235,
      "grad_norm": 3.6652004718780518,
      "learning_rate": 2.7566604445952827e-05,
      "loss": 0.6278,
      "step": 13720
    },
    {
      "epoch": 2.2910061738695147,
      "grad_norm": 6.103109836578369,
      "learning_rate": 2.754963516035975e-05,
      "loss": 0.4892,
      "step": 13730
    },
    {
      "epoch": 2.292674787251794,
      "grad_norm": 11.029582977294922,
      "learning_rate": 2.7532665874766672e-05,
      "loss": 0.4198,
      "step": 13740
    },
    {
      "epoch": 2.294343400634073,
      "grad_norm": 8.3753662109375,
      "learning_rate": 2.7515696589173594e-05,
      "loss": 1.0209,
      "step": 13750
    },
    {
      "epoch": 2.2960120140163522,
      "grad_norm": 3.1848976612091064,
      "learning_rate": 2.7498727303580517e-05,
      "loss": 0.3916,
      "step": 13760
    },
    {
      "epoch": 2.297680627398632,
      "grad_norm": 12.65826416015625,
      "learning_rate": 2.7481758017987446e-05,
      "loss": 0.6761,
      "step": 13770
    },
    {
      "epoch": 2.299349240780911,
      "grad_norm": 5.044442176818848,
      "learning_rate": 2.746478873239437e-05,
      "loss": 0.3089,
      "step": 13780
    },
    {
      "epoch": 2.3010178541631903,
      "grad_norm": 8.814764976501465,
      "learning_rate": 2.7447819446801294e-05,
      "loss": 0.4645,
      "step": 13790
    },
    {
      "epoch": 2.30268646754547,
      "grad_norm": 12.346200942993164,
      "learning_rate": 2.7430850161208217e-05,
      "loss": 0.6876,
      "step": 13800
    },
    {
      "epoch": 2.304355080927749,
      "grad_norm": 9.363348960876465,
      "learning_rate": 2.741388087561514e-05,
      "loss": 0.4114,
      "step": 13810
    },
    {
      "epoch": 2.3060236943100283,
      "grad_norm": 6.339417457580566,
      "learning_rate": 2.7396911590022062e-05,
      "loss": 0.5824,
      "step": 13820
    },
    {
      "epoch": 2.3076923076923075,
      "grad_norm": 1.413108229637146,
      "learning_rate": 2.7379942304428984e-05,
      "loss": 0.62,
      "step": 13830
    },
    {
      "epoch": 2.309360921074587,
      "grad_norm": 3.0935795307159424,
      "learning_rate": 2.7362973018835907e-05,
      "loss": 0.4602,
      "step": 13840
    },
    {
      "epoch": 2.3110295344568663,
      "grad_norm": 5.093170166015625,
      "learning_rate": 2.734600373324283e-05,
      "loss": 0.3843,
      "step": 13850
    },
    {
      "epoch": 2.3126981478391455,
      "grad_norm": 2.305204391479492,
      "learning_rate": 2.7329034447649755e-05,
      "loss": 0.645,
      "step": 13860
    },
    {
      "epoch": 2.314366761221425,
      "grad_norm": 1.0741926431655884,
      "learning_rate": 2.7312065162056678e-05,
      "loss": 0.5107,
      "step": 13870
    },
    {
      "epoch": 2.3160353746037043,
      "grad_norm": 11.032587051391602,
      "learning_rate": 2.72950958764636e-05,
      "loss": 0.4718,
      "step": 13880
    },
    {
      "epoch": 2.3177039879859835,
      "grad_norm": 9.186116218566895,
      "learning_rate": 2.7278126590870523e-05,
      "loss": 0.434,
      "step": 13890
    },
    {
      "epoch": 2.319372601368263,
      "grad_norm": 5.332423686981201,
      "learning_rate": 2.7261157305277452e-05,
      "loss": 0.2657,
      "step": 13900
    },
    {
      "epoch": 2.3210412147505424,
      "grad_norm": 2.013514280319214,
      "learning_rate": 2.7244188019684374e-05,
      "loss": 0.4249,
      "step": 13910
    },
    {
      "epoch": 2.3227098281328216,
      "grad_norm": 3.371216058731079,
      "learning_rate": 2.7227218734091297e-05,
      "loss": 0.5923,
      "step": 13920
    },
    {
      "epoch": 2.3243784415151008,
      "grad_norm": 5.954972743988037,
      "learning_rate": 2.721024944849822e-05,
      "loss": 0.7007,
      "step": 13930
    },
    {
      "epoch": 2.3260470548973804,
      "grad_norm": 3.5891213417053223,
      "learning_rate": 2.7193280162905145e-05,
      "loss": 0.5125,
      "step": 13940
    },
    {
      "epoch": 2.3277156682796596,
      "grad_norm": 5.69428014755249,
      "learning_rate": 2.7176310877312068e-05,
      "loss": 0.4972,
      "step": 13950
    },
    {
      "epoch": 2.329384281661939,
      "grad_norm": 2.0404610633850098,
      "learning_rate": 2.715934159171899e-05,
      "loss": 0.6031,
      "step": 13960
    },
    {
      "epoch": 2.3310528950442184,
      "grad_norm": 3.26879620552063,
      "learning_rate": 2.7142372306125913e-05,
      "loss": 0.3575,
      "step": 13970
    },
    {
      "epoch": 2.3327215084264976,
      "grad_norm": 4.002285957336426,
      "learning_rate": 2.7125403020532835e-05,
      "loss": 0.4275,
      "step": 13980
    },
    {
      "epoch": 2.334390121808777,
      "grad_norm": 8.953630447387695,
      "learning_rate": 2.7108433734939758e-05,
      "loss": 0.428,
      "step": 13990
    },
    {
      "epoch": 2.336058735191056,
      "grad_norm": 11.360264778137207,
      "learning_rate": 2.709146444934668e-05,
      "loss": 0.4947,
      "step": 14000
    },
    {
      "epoch": 2.3377273485733356,
      "grad_norm": 2.4051787853240967,
      "learning_rate": 2.7074495163753606e-05,
      "loss": 0.4013,
      "step": 14010
    },
    {
      "epoch": 2.339395961955615,
      "grad_norm": 11.869847297668457,
      "learning_rate": 2.7057525878160532e-05,
      "loss": 0.3602,
      "step": 14020
    },
    {
      "epoch": 2.341064575337894,
      "grad_norm": 14.245484352111816,
      "learning_rate": 2.7040556592567458e-05,
      "loss": 0.4715,
      "step": 14030
    },
    {
      "epoch": 2.3427331887201737,
      "grad_norm": 13.203465461730957,
      "learning_rate": 2.702358730697438e-05,
      "loss": 0.6368,
      "step": 14040
    },
    {
      "epoch": 2.344401802102453,
      "grad_norm": 3.7135093212127686,
      "learning_rate": 2.7006618021381303e-05,
      "loss": 0.5107,
      "step": 14050
    },
    {
      "epoch": 2.346070415484732,
      "grad_norm": 2.1235883235931396,
      "learning_rate": 2.6989648735788225e-05,
      "loss": 0.452,
      "step": 14060
    },
    {
      "epoch": 2.3477390288670117,
      "grad_norm": 5.834864616394043,
      "learning_rate": 2.6972679450195148e-05,
      "loss": 0.9911,
      "step": 14070
    },
    {
      "epoch": 2.349407642249291,
      "grad_norm": 9.394811630249023,
      "learning_rate": 2.695571016460207e-05,
      "loss": 0.4846,
      "step": 14080
    },
    {
      "epoch": 2.35107625563157,
      "grad_norm": 6.146573543548584,
      "learning_rate": 2.6938740879008996e-05,
      "loss": 0.6658,
      "step": 14090
    },
    {
      "epoch": 2.3527448690138497,
      "grad_norm": 2.034334897994995,
      "learning_rate": 2.692177159341592e-05,
      "loss": 0.4207,
      "step": 14100
    },
    {
      "epoch": 2.354413482396129,
      "grad_norm": 4.17302131652832,
      "learning_rate": 2.690480230782284e-05,
      "loss": 0.2135,
      "step": 14110
    },
    {
      "epoch": 2.356082095778408,
      "grad_norm": 1.2188276052474976,
      "learning_rate": 2.6887833022229764e-05,
      "loss": 0.572,
      "step": 14120
    },
    {
      "epoch": 2.3577507091606873,
      "grad_norm": 0.7023659944534302,
      "learning_rate": 2.6870863736636686e-05,
      "loss": 0.2978,
      "step": 14130
    },
    {
      "epoch": 2.359419322542967,
      "grad_norm": 7.847061634063721,
      "learning_rate": 2.685389445104361e-05,
      "loss": 0.6712,
      "step": 14140
    },
    {
      "epoch": 2.361087935925246,
      "grad_norm": 6.2581305503845215,
      "learning_rate": 2.6836925165450538e-05,
      "loss": 0.732,
      "step": 14150
    },
    {
      "epoch": 2.3627565493075253,
      "grad_norm": 12.405592918395996,
      "learning_rate": 2.681995587985746e-05,
      "loss": 0.4932,
      "step": 14160
    },
    {
      "epoch": 2.364425162689805,
      "grad_norm": 5.8058271408081055,
      "learning_rate": 2.6802986594264383e-05,
      "loss": 0.5809,
      "step": 14170
    },
    {
      "epoch": 2.366093776072084,
      "grad_norm": 8.006143569946289,
      "learning_rate": 2.678601730867131e-05,
      "loss": 0.6618,
      "step": 14180
    },
    {
      "epoch": 2.3677623894543633,
      "grad_norm": 5.870664119720459,
      "learning_rate": 2.676904802307823e-05,
      "loss": 0.5565,
      "step": 14190
    },
    {
      "epoch": 2.3694310028366425,
      "grad_norm": 6.595209121704102,
      "learning_rate": 2.6752078737485154e-05,
      "loss": 0.4624,
      "step": 14200
    },
    {
      "epoch": 2.371099616218922,
      "grad_norm": 7.069930553436279,
      "learning_rate": 2.6735109451892076e-05,
      "loss": 0.5616,
      "step": 14210
    },
    {
      "epoch": 2.3727682296012014,
      "grad_norm": 4.934022426605225,
      "learning_rate": 2.6718140166299e-05,
      "loss": 0.6597,
      "step": 14220
    },
    {
      "epoch": 2.3744368429834806,
      "grad_norm": 2.8722667694091797,
      "learning_rate": 2.670117088070592e-05,
      "loss": 0.2925,
      "step": 14230
    },
    {
      "epoch": 2.37610545636576,
      "grad_norm": 6.489469528198242,
      "learning_rate": 2.6684201595112847e-05,
      "loss": 0.2403,
      "step": 14240
    },
    {
      "epoch": 2.3777740697480394,
      "grad_norm": 7.010463237762451,
      "learning_rate": 2.666723230951977e-05,
      "loss": 0.6156,
      "step": 14250
    },
    {
      "epoch": 2.3794426831303186,
      "grad_norm": 7.969442844390869,
      "learning_rate": 2.6650263023926692e-05,
      "loss": 0.479,
      "step": 14260
    },
    {
      "epoch": 2.3811112965125982,
      "grad_norm": 6.775581359863281,
      "learning_rate": 2.663329373833362e-05,
      "loss": 0.8058,
      "step": 14270
    },
    {
      "epoch": 2.3827799098948774,
      "grad_norm": 10.343745231628418,
      "learning_rate": 2.6616324452740544e-05,
      "loss": 0.768,
      "step": 14280
    },
    {
      "epoch": 2.3844485232771566,
      "grad_norm": 13.889986991882324,
      "learning_rate": 2.6599355167147466e-05,
      "loss": 0.6649,
      "step": 14290
    },
    {
      "epoch": 2.3861171366594363,
      "grad_norm": 4.058918476104736,
      "learning_rate": 2.658238588155439e-05,
      "loss": 0.4513,
      "step": 14300
    },
    {
      "epoch": 2.3877857500417154,
      "grad_norm": 3.0692429542541504,
      "learning_rate": 2.656541659596131e-05,
      "loss": 0.7322,
      "step": 14310
    },
    {
      "epoch": 2.3894543634239946,
      "grad_norm": 8.126846313476562,
      "learning_rate": 2.6548447310368234e-05,
      "loss": 0.4857,
      "step": 14320
    },
    {
      "epoch": 2.391122976806274,
      "grad_norm": 5.346311092376709,
      "learning_rate": 2.653147802477516e-05,
      "loss": 0.5438,
      "step": 14330
    },
    {
      "epoch": 2.3927915901885535,
      "grad_norm": 8.192163467407227,
      "learning_rate": 2.6514508739182082e-05,
      "loss": 0.3981,
      "step": 14340
    },
    {
      "epoch": 2.3944602035708327,
      "grad_norm": 9.77946662902832,
      "learning_rate": 2.6497539453589005e-05,
      "loss": 0.6053,
      "step": 14350
    },
    {
      "epoch": 2.396128816953112,
      "grad_norm": 7.38552713394165,
      "learning_rate": 2.6480570167995927e-05,
      "loss": 0.7322,
      "step": 14360
    },
    {
      "epoch": 2.397797430335391,
      "grad_norm": 4.431093215942383,
      "learning_rate": 2.646360088240285e-05,
      "loss": 0.4806,
      "step": 14370
    },
    {
      "epoch": 2.3994660437176707,
      "grad_norm": 7.803155422210693,
      "learning_rate": 2.6446631596809772e-05,
      "loss": 0.5576,
      "step": 14380
    },
    {
      "epoch": 2.40113465709995,
      "grad_norm": 1.8105309009552002,
      "learning_rate": 2.6429662311216698e-05,
      "loss": 0.3917,
      "step": 14390
    },
    {
      "epoch": 2.402803270482229,
      "grad_norm": 1.659758448600769,
      "learning_rate": 2.6412693025623624e-05,
      "loss": 0.3793,
      "step": 14400
    },
    {
      "epoch": 2.4044718838645087,
      "grad_norm": 8.72061538696289,
      "learning_rate": 2.639572374003055e-05,
      "loss": 0.7891,
      "step": 14410
    },
    {
      "epoch": 2.406140497246788,
      "grad_norm": 6.405839920043945,
      "learning_rate": 2.6378754454437472e-05,
      "loss": 0.4317,
      "step": 14420
    },
    {
      "epoch": 2.407809110629067,
      "grad_norm": 7.838187217712402,
      "learning_rate": 2.6361785168844395e-05,
      "loss": 0.355,
      "step": 14430
    },
    {
      "epoch": 2.4094777240113467,
      "grad_norm": 2.017415761947632,
      "learning_rate": 2.6344815883251317e-05,
      "loss": 0.5076,
      "step": 14440
    },
    {
      "epoch": 2.411146337393626,
      "grad_norm": 9.653995513916016,
      "learning_rate": 2.632784659765824e-05,
      "loss": 0.6292,
      "step": 14450
    },
    {
      "epoch": 2.412814950775905,
      "grad_norm": 4.4011688232421875,
      "learning_rate": 2.6310877312065162e-05,
      "loss": 0.3739,
      "step": 14460
    },
    {
      "epoch": 2.4144835641581848,
      "grad_norm": 2.529811143875122,
      "learning_rate": 2.6293908026472085e-05,
      "loss": 0.5479,
      "step": 14470
    },
    {
      "epoch": 2.416152177540464,
      "grad_norm": 2.3393197059631348,
      "learning_rate": 2.627693874087901e-05,
      "loss": 0.502,
      "step": 14480
    },
    {
      "epoch": 2.417820790922743,
      "grad_norm": 3.537388324737549,
      "learning_rate": 2.6259969455285933e-05,
      "loss": 0.4265,
      "step": 14490
    },
    {
      "epoch": 2.4194894043050224,
      "grad_norm": 8.723657608032227,
      "learning_rate": 2.6243000169692855e-05,
      "loss": 0.6017,
      "step": 14500
    },
    {
      "epoch": 2.421158017687302,
      "grad_norm": 8.501808166503906,
      "learning_rate": 2.6226030884099778e-05,
      "loss": 0.5396,
      "step": 14510
    },
    {
      "epoch": 2.422826631069581,
      "grad_norm": 3.483475685119629,
      "learning_rate": 2.6209061598506707e-05,
      "loss": 0.3606,
      "step": 14520
    },
    {
      "epoch": 2.4244952444518604,
      "grad_norm": 12.019657135009766,
      "learning_rate": 2.619209231291363e-05,
      "loss": 0.3995,
      "step": 14530
    },
    {
      "epoch": 2.42616385783414,
      "grad_norm": 5.933603286743164,
      "learning_rate": 2.6175123027320552e-05,
      "loss": 0.4967,
      "step": 14540
    },
    {
      "epoch": 2.427832471216419,
      "grad_norm": 18.012819290161133,
      "learning_rate": 2.6158153741727475e-05,
      "loss": 0.5732,
      "step": 14550
    },
    {
      "epoch": 2.4295010845986984,
      "grad_norm": 3.4072864055633545,
      "learning_rate": 2.61411844561344e-05,
      "loss": 0.408,
      "step": 14560
    },
    {
      "epoch": 2.4311696979809776,
      "grad_norm": 4.635274887084961,
      "learning_rate": 2.6124215170541323e-05,
      "loss": 0.2747,
      "step": 14570
    },
    {
      "epoch": 2.4328383113632572,
      "grad_norm": 2.687227487564087,
      "learning_rate": 2.6107245884948245e-05,
      "loss": 0.3333,
      "step": 14580
    },
    {
      "epoch": 2.4345069247455364,
      "grad_norm": 18.858657836914062,
      "learning_rate": 2.6090276599355168e-05,
      "loss": 0.5529,
      "step": 14590
    },
    {
      "epoch": 2.4361755381278156,
      "grad_norm": 0.7332319021224976,
      "learning_rate": 2.607330731376209e-05,
      "loss": 0.5308,
      "step": 14600
    },
    {
      "epoch": 2.4378441515100953,
      "grad_norm": 7.7477006912231445,
      "learning_rate": 2.6056338028169013e-05,
      "loss": 0.3134,
      "step": 14610
    },
    {
      "epoch": 2.4395127648923745,
      "grad_norm": 9.052059173583984,
      "learning_rate": 2.6039368742575935e-05,
      "loss": 0.6308,
      "step": 14620
    },
    {
      "epoch": 2.4411813782746536,
      "grad_norm": 6.49456787109375,
      "learning_rate": 2.602239945698286e-05,
      "loss": 0.7321,
      "step": 14630
    },
    {
      "epoch": 2.4428499916569333,
      "grad_norm": 7.9369587898254395,
      "learning_rate": 2.6005430171389784e-05,
      "loss": 0.6046,
      "step": 14640
    },
    {
      "epoch": 2.4445186050392125,
      "grad_norm": 1.922403335571289,
      "learning_rate": 2.5988460885796713e-05,
      "loss": 0.7074,
      "step": 14650
    },
    {
      "epoch": 2.4461872184214917,
      "grad_norm": 4.197954177856445,
      "learning_rate": 2.5971491600203635e-05,
      "loss": 0.3582,
      "step": 14660
    },
    {
      "epoch": 2.4478558318037713,
      "grad_norm": 8.403165817260742,
      "learning_rate": 2.5954522314610558e-05,
      "loss": 0.4864,
      "step": 14670
    },
    {
      "epoch": 2.4495244451860505,
      "grad_norm": 2.181483268737793,
      "learning_rate": 2.593755302901748e-05,
      "loss": 0.4648,
      "step": 14680
    },
    {
      "epoch": 2.4511930585683297,
      "grad_norm": 6.9971513748168945,
      "learning_rate": 2.5920583743424403e-05,
      "loss": 0.5694,
      "step": 14690
    },
    {
      "epoch": 2.452861671950609,
      "grad_norm": 6.43251895904541,
      "learning_rate": 2.5903614457831325e-05,
      "loss": 0.4111,
      "step": 14700
    },
    {
      "epoch": 2.4545302853328885,
      "grad_norm": 16.407045364379883,
      "learning_rate": 2.588664517223825e-05,
      "loss": 0.6676,
      "step": 14710
    },
    {
      "epoch": 2.4561988987151677,
      "grad_norm": 1.7210001945495605,
      "learning_rate": 2.5869675886645174e-05,
      "loss": 0.434,
      "step": 14720
    },
    {
      "epoch": 2.457867512097447,
      "grad_norm": 0.4929768443107605,
      "learning_rate": 2.5852706601052096e-05,
      "loss": 0.2279,
      "step": 14730
    },
    {
      "epoch": 2.459536125479726,
      "grad_norm": 8.608128547668457,
      "learning_rate": 2.583573731545902e-05,
      "loss": 0.5461,
      "step": 14740
    },
    {
      "epoch": 2.4612047388620057,
      "grad_norm": 7.115819931030273,
      "learning_rate": 2.581876802986594e-05,
      "loss": 0.8618,
      "step": 14750
    },
    {
      "epoch": 2.462873352244285,
      "grad_norm": 8.662942886352539,
      "learning_rate": 2.5801798744272864e-05,
      "loss": 0.6797,
      "step": 14760
    },
    {
      "epoch": 2.464541965626564,
      "grad_norm": 6.446753978729248,
      "learning_rate": 2.5784829458679793e-05,
      "loss": 0.4068,
      "step": 14770
    },
    {
      "epoch": 2.4662105790088438,
      "grad_norm": 4.421238899230957,
      "learning_rate": 2.5767860173086715e-05,
      "loss": 0.4625,
      "step": 14780
    },
    {
      "epoch": 2.467879192391123,
      "grad_norm": 7.028229713439941,
      "learning_rate": 2.5750890887493638e-05,
      "loss": 0.5823,
      "step": 14790
    },
    {
      "epoch": 2.469547805773402,
      "grad_norm": 7.2907280921936035,
      "learning_rate": 2.5733921601900564e-05,
      "loss": 0.6092,
      "step": 14800
    },
    {
      "epoch": 2.471216419155682,
      "grad_norm": 7.384166240692139,
      "learning_rate": 2.5716952316307486e-05,
      "loss": 0.4499,
      "step": 14810
    },
    {
      "epoch": 2.472885032537961,
      "grad_norm": 9.709456443786621,
      "learning_rate": 2.569998303071441e-05,
      "loss": 0.3586,
      "step": 14820
    },
    {
      "epoch": 2.47455364592024,
      "grad_norm": 2.334451198577881,
      "learning_rate": 2.568301374512133e-05,
      "loss": 0.5206,
      "step": 14830
    },
    {
      "epoch": 2.47622225930252,
      "grad_norm": 9.822071075439453,
      "learning_rate": 2.5666044459528254e-05,
      "loss": 0.3631,
      "step": 14840
    },
    {
      "epoch": 2.477890872684799,
      "grad_norm": 3.1229913234710693,
      "learning_rate": 2.5649075173935176e-05,
      "loss": 0.4449,
      "step": 14850
    },
    {
      "epoch": 2.479559486067078,
      "grad_norm": 4.262580871582031,
      "learning_rate": 2.5632105888342102e-05,
      "loss": 0.7467,
      "step": 14860
    },
    {
      "epoch": 2.4812280994493574,
      "grad_norm": 19.502531051635742,
      "learning_rate": 2.5615136602749025e-05,
      "loss": 0.5041,
      "step": 14870
    },
    {
      "epoch": 2.482896712831637,
      "grad_norm": 4.00022554397583,
      "learning_rate": 2.5598167317155947e-05,
      "loss": 0.6007,
      "step": 14880
    },
    {
      "epoch": 2.4845653262139162,
      "grad_norm": 5.99591588973999,
      "learning_rate": 2.5581198031562876e-05,
      "loss": 0.449,
      "step": 14890
    },
    {
      "epoch": 2.4862339395961954,
      "grad_norm": 9.254003524780273,
      "learning_rate": 2.55642287459698e-05,
      "loss": 0.4249,
      "step": 14900
    },
    {
      "epoch": 2.487902552978475,
      "grad_norm": 7.9046854972839355,
      "learning_rate": 2.554725946037672e-05,
      "loss": 0.5581,
      "step": 14910
    },
    {
      "epoch": 2.4895711663607543,
      "grad_norm": 3.846017837524414,
      "learning_rate": 2.5530290174783644e-05,
      "loss": 0.3793,
      "step": 14920
    },
    {
      "epoch": 2.4912397797430335,
      "grad_norm": 3.1959333419799805,
      "learning_rate": 2.5513320889190566e-05,
      "loss": 0.4345,
      "step": 14930
    },
    {
      "epoch": 2.4929083931253126,
      "grad_norm": 7.03842306137085,
      "learning_rate": 2.549635160359749e-05,
      "loss": 0.4015,
      "step": 14940
    },
    {
      "epoch": 2.4945770065075923,
      "grad_norm": 7.201101779937744,
      "learning_rate": 2.5479382318004415e-05,
      "loss": 0.5556,
      "step": 14950
    },
    {
      "epoch": 2.4962456198898715,
      "grad_norm": 0.2116396427154541,
      "learning_rate": 2.5462413032411337e-05,
      "loss": 0.4001,
      "step": 14960
    },
    {
      "epoch": 2.4979142332721507,
      "grad_norm": 14.903180122375488,
      "learning_rate": 2.544544374681826e-05,
      "loss": 0.5378,
      "step": 14970
    },
    {
      "epoch": 2.4995828466544303,
      "grad_norm": 15.223832130432129,
      "learning_rate": 2.5428474461225182e-05,
      "loss": 0.4911,
      "step": 14980
    },
    {
      "epoch": 2.5012514600367095,
      "grad_norm": 7.36738920211792,
      "learning_rate": 2.5411505175632105e-05,
      "loss": 0.4498,
      "step": 14990
    },
    {
      "epoch": 2.5029200734189887,
      "grad_norm": 3.4269583225250244,
      "learning_rate": 2.5394535890039027e-05,
      "loss": 0.7025,
      "step": 15000
    },
    {
      "epoch": 2.5045886868012683,
      "grad_norm": 3.4296202659606934,
      "learning_rate": 2.5377566604445953e-05,
      "loss": 0.3559,
      "step": 15010
    },
    {
      "epoch": 2.5062573001835475,
      "grad_norm": 2.241110324859619,
      "learning_rate": 2.536059731885288e-05,
      "loss": 0.3928,
      "step": 15020
    },
    {
      "epoch": 2.5079259135658267,
      "grad_norm": 7.751580238342285,
      "learning_rate": 2.53436280332598e-05,
      "loss": 0.5738,
      "step": 15030
    },
    {
      "epoch": 2.5095945269481064,
      "grad_norm": 5.655984878540039,
      "learning_rate": 2.5326658747666727e-05,
      "loss": 0.3181,
      "step": 15040
    },
    {
      "epoch": 2.5112631403303856,
      "grad_norm": 8.554976463317871,
      "learning_rate": 2.530968946207365e-05,
      "loss": 0.4382,
      "step": 15050
    },
    {
      "epoch": 2.5129317537126648,
      "grad_norm": 9.021954536437988,
      "learning_rate": 2.5292720176480572e-05,
      "loss": 0.5309,
      "step": 15060
    },
    {
      "epoch": 2.514600367094944,
      "grad_norm": 8.887527465820312,
      "learning_rate": 2.5275750890887495e-05,
      "loss": 0.6685,
      "step": 15070
    },
    {
      "epoch": 2.5162689804772236,
      "grad_norm": 16.303281784057617,
      "learning_rate": 2.5258781605294417e-05,
      "loss": 0.4966,
      "step": 15080
    },
    {
      "epoch": 2.5179375938595028,
      "grad_norm": 1.1034396886825562,
      "learning_rate": 2.524181231970134e-05,
      "loss": 0.4716,
      "step": 15090
    },
    {
      "epoch": 2.519606207241782,
      "grad_norm": 8.047897338867188,
      "learning_rate": 2.5224843034108266e-05,
      "loss": 0.6273,
      "step": 15100
    },
    {
      "epoch": 2.521274820624061,
      "grad_norm": 2.95052170753479,
      "learning_rate": 2.5207873748515188e-05,
      "loss": 0.4134,
      "step": 15110
    },
    {
      "epoch": 2.522943434006341,
      "grad_norm": 4.26331901550293,
      "learning_rate": 2.519090446292211e-05,
      "loss": 0.4431,
      "step": 15120
    },
    {
      "epoch": 2.52461204738862,
      "grad_norm": 12.052351951599121,
      "learning_rate": 2.5173935177329033e-05,
      "loss": 0.4557,
      "step": 15130
    },
    {
      "epoch": 2.526280660770899,
      "grad_norm": 0.7764163017272949,
      "learning_rate": 2.5156965891735962e-05,
      "loss": 0.5654,
      "step": 15140
    },
    {
      "epoch": 2.527949274153179,
      "grad_norm": 10.260351181030273,
      "learning_rate": 2.5139996606142885e-05,
      "loss": 0.6901,
      "step": 15150
    },
    {
      "epoch": 2.529617887535458,
      "grad_norm": 8.584959030151367,
      "learning_rate": 2.5123027320549807e-05,
      "loss": 0.6017,
      "step": 15160
    },
    {
      "epoch": 2.531286500917737,
      "grad_norm": 5.683115005493164,
      "learning_rate": 2.510605803495673e-05,
      "loss": 0.5982,
      "step": 15170
    },
    {
      "epoch": 2.532955114300017,
      "grad_norm": 6.220432281494141,
      "learning_rate": 2.5089088749363656e-05,
      "loss": 0.7533,
      "step": 15180
    },
    {
      "epoch": 2.534623727682296,
      "grad_norm": 2.0618109703063965,
      "learning_rate": 2.5072119463770578e-05,
      "loss": 0.4716,
      "step": 15190
    },
    {
      "epoch": 2.5362923410645752,
      "grad_norm": 10.426201820373535,
      "learning_rate": 2.50551501781775e-05,
      "loss": 0.49,
      "step": 15200
    },
    {
      "epoch": 2.537960954446855,
      "grad_norm": 3.155909776687622,
      "learning_rate": 2.5038180892584423e-05,
      "loss": 0.2083,
      "step": 15210
    },
    {
      "epoch": 2.539629567829134,
      "grad_norm": 9.4338960647583,
      "learning_rate": 2.5021211606991346e-05,
      "loss": 0.3537,
      "step": 15220
    },
    {
      "epoch": 2.5412981812114133,
      "grad_norm": 7.261816024780273,
      "learning_rate": 2.5004242321398268e-05,
      "loss": 0.595,
      "step": 15230
    },
    {
      "epoch": 2.542966794593693,
      "grad_norm": 6.897231578826904,
      "learning_rate": 2.4987273035805194e-05,
      "loss": 0.4566,
      "step": 15240
    },
    {
      "epoch": 2.544635407975972,
      "grad_norm": 7.295185565948486,
      "learning_rate": 2.4970303750212116e-05,
      "loss": 0.6429,
      "step": 15250
    },
    {
      "epoch": 2.5463040213582513,
      "grad_norm": 8.198369026184082,
      "learning_rate": 2.4953334464619042e-05,
      "loss": 0.6034,
      "step": 15260
    },
    {
      "epoch": 2.5479726347405305,
      "grad_norm": 8.56847095489502,
      "learning_rate": 2.4936365179025965e-05,
      "loss": 0.4595,
      "step": 15270
    },
    {
      "epoch": 2.5496412481228097,
      "grad_norm": 4.204850673675537,
      "learning_rate": 2.4919395893432887e-05,
      "loss": 0.6357,
      "step": 15280
    },
    {
      "epoch": 2.5513098615050893,
      "grad_norm": 4.348998546600342,
      "learning_rate": 2.490242660783981e-05,
      "loss": 0.4663,
      "step": 15290
    },
    {
      "epoch": 2.5529784748873685,
      "grad_norm": 4.7734785079956055,
      "learning_rate": 2.4885457322246736e-05,
      "loss": 0.2534,
      "step": 15300
    },
    {
      "epoch": 2.5546470882696477,
      "grad_norm": 9.427574157714844,
      "learning_rate": 2.4868488036653658e-05,
      "loss": 0.3996,
      "step": 15310
    },
    {
      "epoch": 2.5563157016519273,
      "grad_norm": 8.22690200805664,
      "learning_rate": 2.485151875106058e-05,
      "loss": 0.619,
      "step": 15320
    },
    {
      "epoch": 2.5579843150342065,
      "grad_norm": 8.526528358459473,
      "learning_rate": 2.4834549465467506e-05,
      "loss": 0.5331,
      "step": 15330
    },
    {
      "epoch": 2.5596529284164857,
      "grad_norm": 3.125581979751587,
      "learning_rate": 2.481758017987443e-05,
      "loss": 0.7437,
      "step": 15340
    },
    {
      "epoch": 2.5613215417987654,
      "grad_norm": 1.0665662288665771,
      "learning_rate": 2.480061089428135e-05,
      "loss": 0.5506,
      "step": 15350
    },
    {
      "epoch": 2.5629901551810446,
      "grad_norm": 9.536271095275879,
      "learning_rate": 2.4783641608688277e-05,
      "loss": 0.4808,
      "step": 15360
    },
    {
      "epoch": 2.5646587685633238,
      "grad_norm": 1.9769169092178345,
      "learning_rate": 2.47666723230952e-05,
      "loss": 0.5953,
      "step": 15370
    },
    {
      "epoch": 2.5663273819456034,
      "grad_norm": 9.612380027770996,
      "learning_rate": 2.4749703037502122e-05,
      "loss": 0.3735,
      "step": 15380
    },
    {
      "epoch": 2.5679959953278826,
      "grad_norm": 6.54282808303833,
      "learning_rate": 2.4732733751909045e-05,
      "loss": 0.4228,
      "step": 15390
    },
    {
      "epoch": 2.569664608710162,
      "grad_norm": 6.660163879394531,
      "learning_rate": 2.4715764466315967e-05,
      "loss": 0.445,
      "step": 15400
    },
    {
      "epoch": 2.5713332220924414,
      "grad_norm": 8.271379470825195,
      "learning_rate": 2.4698795180722893e-05,
      "loss": 0.4305,
      "step": 15410
    },
    {
      "epoch": 2.5730018354747206,
      "grad_norm": 2.3926968574523926,
      "learning_rate": 2.468182589512982e-05,
      "loss": 0.4407,
      "step": 15420
    },
    {
      "epoch": 2.574670448857,
      "grad_norm": 11.009907722473145,
      "learning_rate": 2.466485660953674e-05,
      "loss": 0.4015,
      "step": 15430
    },
    {
      "epoch": 2.576339062239279,
      "grad_norm": 4.9444499015808105,
      "learning_rate": 2.4647887323943664e-05,
      "loss": 0.4208,
      "step": 15440
    },
    {
      "epoch": 2.5780076756215586,
      "grad_norm": 4.815204620361328,
      "learning_rate": 2.4630918038350586e-05,
      "loss": 0.4756,
      "step": 15450
    },
    {
      "epoch": 2.579676289003838,
      "grad_norm": 7.593155860900879,
      "learning_rate": 2.461394875275751e-05,
      "loss": 0.435,
      "step": 15460
    },
    {
      "epoch": 2.581344902386117,
      "grad_norm": 1.7389898300170898,
      "learning_rate": 2.459697946716443e-05,
      "loss": 0.325,
      "step": 15470
    },
    {
      "epoch": 2.583013515768396,
      "grad_norm": 4.392370223999023,
      "learning_rate": 2.4580010181571357e-05,
      "loss": 0.6498,
      "step": 15480
    },
    {
      "epoch": 2.584682129150676,
      "grad_norm": 7.5693359375,
      "learning_rate": 2.4563040895978283e-05,
      "loss": 0.4716,
      "step": 15490
    },
    {
      "epoch": 2.586350742532955,
      "grad_norm": 2.369636058807373,
      "learning_rate": 2.4546071610385206e-05,
      "loss": 0.4537,
      "step": 15500
    },
    {
      "epoch": 2.5880193559152342,
      "grad_norm": 6.514257907867432,
      "learning_rate": 2.4529102324792128e-05,
      "loss": 0.4468,
      "step": 15510
    },
    {
      "epoch": 2.589687969297514,
      "grad_norm": 4.052573204040527,
      "learning_rate": 2.451213303919905e-05,
      "loss": 0.3225,
      "step": 15520
    },
    {
      "epoch": 2.591356582679793,
      "grad_norm": 8.853678703308105,
      "learning_rate": 2.4495163753605973e-05,
      "loss": 0.6668,
      "step": 15530
    },
    {
      "epoch": 2.5930251960620723,
      "grad_norm": 1.7853190898895264,
      "learning_rate": 2.4478194468012896e-05,
      "loss": 0.35,
      "step": 15540
    },
    {
      "epoch": 2.594693809444352,
      "grad_norm": 4.256961345672607,
      "learning_rate": 2.446122518241982e-05,
      "loss": 0.3467,
      "step": 15550
    },
    {
      "epoch": 2.596362422826631,
      "grad_norm": 7.635354042053223,
      "learning_rate": 2.4444255896826744e-05,
      "loss": 0.6371,
      "step": 15560
    },
    {
      "epoch": 2.5980310362089103,
      "grad_norm": 7.450419902801514,
      "learning_rate": 2.442728661123367e-05,
      "loss": 0.4454,
      "step": 15570
    },
    {
      "epoch": 2.59969964959119,
      "grad_norm": 8.105981826782227,
      "learning_rate": 2.4410317325640592e-05,
      "loss": 0.491,
      "step": 15580
    },
    {
      "epoch": 2.601368262973469,
      "grad_norm": 4.4786834716796875,
      "learning_rate": 2.4393348040047515e-05,
      "loss": 0.4479,
      "step": 15590
    },
    {
      "epoch": 2.6030368763557483,
      "grad_norm": 3.1717007160186768,
      "learning_rate": 2.4376378754454437e-05,
      "loss": 0.3937,
      "step": 15600
    },
    {
      "epoch": 2.604705489738028,
      "grad_norm": 2.240975856781006,
      "learning_rate": 2.4359409468861363e-05,
      "loss": 0.3493,
      "step": 15610
    },
    {
      "epoch": 2.606374103120307,
      "grad_norm": 8.208760261535645,
      "learning_rate": 2.4342440183268286e-05,
      "loss": 0.4973,
      "step": 15620
    },
    {
      "epoch": 2.6080427165025863,
      "grad_norm": 16.980100631713867,
      "learning_rate": 2.4325470897675208e-05,
      "loss": 0.7778,
      "step": 15630
    },
    {
      "epoch": 2.6097113298848655,
      "grad_norm": 20.751314163208008,
      "learning_rate": 2.4308501612082134e-05,
      "loss": 0.6939,
      "step": 15640
    },
    {
      "epoch": 2.6113799432671447,
      "grad_norm": 5.391786098480225,
      "learning_rate": 2.4291532326489057e-05,
      "loss": 0.3154,
      "step": 15650
    },
    {
      "epoch": 2.6130485566494244,
      "grad_norm": 1.1535004377365112,
      "learning_rate": 2.427456304089598e-05,
      "loss": 0.3798,
      "step": 15660
    },
    {
      "epoch": 2.6147171700317036,
      "grad_norm": 12.628144264221191,
      "learning_rate": 2.4257593755302905e-05,
      "loss": 0.6061,
      "step": 15670
    },
    {
      "epoch": 2.6163857834139828,
      "grad_norm": 1.5969280004501343,
      "learning_rate": 2.4240624469709827e-05,
      "loss": 0.3176,
      "step": 15680
    },
    {
      "epoch": 2.6180543967962624,
      "grad_norm": 1.7331598997116089,
      "learning_rate": 2.422365518411675e-05,
      "loss": 0.4766,
      "step": 15690
    },
    {
      "epoch": 2.6197230101785416,
      "grad_norm": 14.770472526550293,
      "learning_rate": 2.4206685898523672e-05,
      "loss": 0.4123,
      "step": 15700
    },
    {
      "epoch": 2.621391623560821,
      "grad_norm": 12.754800796508789,
      "learning_rate": 2.4189716612930595e-05,
      "loss": 0.5735,
      "step": 15710
    },
    {
      "epoch": 2.6230602369431004,
      "grad_norm": 9.444220542907715,
      "learning_rate": 2.417274732733752e-05,
      "loss": 0.4792,
      "step": 15720
    },
    {
      "epoch": 2.6247288503253796,
      "grad_norm": 7.7417473793029785,
      "learning_rate": 2.4155778041744443e-05,
      "loss": 0.5845,
      "step": 15730
    },
    {
      "epoch": 2.626397463707659,
      "grad_norm": 5.156935214996338,
      "learning_rate": 2.413880875615137e-05,
      "loss": 0.3512,
      "step": 15740
    },
    {
      "epoch": 2.6280660770899384,
      "grad_norm": 9.475017547607422,
      "learning_rate": 2.412183947055829e-05,
      "loss": 0.4167,
      "step": 15750
    },
    {
      "epoch": 2.6297346904722176,
      "grad_norm": 0.8005192279815674,
      "learning_rate": 2.4104870184965214e-05,
      "loss": 0.4489,
      "step": 15760
    },
    {
      "epoch": 2.631403303854497,
      "grad_norm": 5.693264961242676,
      "learning_rate": 2.4087900899372137e-05,
      "loss": 0.7878,
      "step": 15770
    },
    {
      "epoch": 2.6330719172367765,
      "grad_norm": 6.49540901184082,
      "learning_rate": 2.407093161377906e-05,
      "loss": 0.4048,
      "step": 15780
    },
    {
      "epoch": 2.6347405306190557,
      "grad_norm": 1.4408389329910278,
      "learning_rate": 2.4053962328185985e-05,
      "loss": 0.308,
      "step": 15790
    },
    {
      "epoch": 2.636409144001335,
      "grad_norm": 6.056171894073486,
      "learning_rate": 2.403699304259291e-05,
      "loss": 0.5954,
      "step": 15800
    },
    {
      "epoch": 2.638077757383614,
      "grad_norm": 11.90584659576416,
      "learning_rate": 2.4020023756999833e-05,
      "loss": 0.6132,
      "step": 15810
    },
    {
      "epoch": 2.6397463707658937,
      "grad_norm": 11.191725730895996,
      "learning_rate": 2.4003054471406756e-05,
      "loss": 0.3677,
      "step": 15820
    },
    {
      "epoch": 2.641414984148173,
      "grad_norm": 7.369121551513672,
      "learning_rate": 2.3986085185813678e-05,
      "loss": 0.5515,
      "step": 15830
    },
    {
      "epoch": 2.643083597530452,
      "grad_norm": 9.519935607910156,
      "learning_rate": 2.39691159002206e-05,
      "loss": 0.6293,
      "step": 15840
    },
    {
      "epoch": 2.6447522109127313,
      "grad_norm": 7.4998884201049805,
      "learning_rate": 2.3952146614627523e-05,
      "loss": 0.453,
      "step": 15850
    },
    {
      "epoch": 2.646420824295011,
      "grad_norm": 13.941529273986816,
      "learning_rate": 2.393517732903445e-05,
      "loss": 0.4977,
      "step": 15860
    },
    {
      "epoch": 2.64808943767729,
      "grad_norm": 8.772058486938477,
      "learning_rate": 2.391820804344137e-05,
      "loss": 0.546,
      "step": 15870
    },
    {
      "epoch": 2.6497580510595693,
      "grad_norm": 6.424615383148193,
      "learning_rate": 2.3901238757848297e-05,
      "loss": 0.5319,
      "step": 15880
    },
    {
      "epoch": 2.651426664441849,
      "grad_norm": 2.297041654586792,
      "learning_rate": 2.388426947225522e-05,
      "loss": 0.3779,
      "step": 15890
    },
    {
      "epoch": 2.653095277824128,
      "grad_norm": 0.8433332443237305,
      "learning_rate": 2.3867300186662142e-05,
      "loss": 0.4927,
      "step": 15900
    },
    {
      "epoch": 2.6547638912064073,
      "grad_norm": 4.465776443481445,
      "learning_rate": 2.3850330901069065e-05,
      "loss": 0.3326,
      "step": 15910
    },
    {
      "epoch": 2.656432504588687,
      "grad_norm": 11.51516342163086,
      "learning_rate": 2.383336161547599e-05,
      "loss": 0.5558,
      "step": 15920
    },
    {
      "epoch": 2.658101117970966,
      "grad_norm": 5.565394401550293,
      "learning_rate": 2.3816392329882913e-05,
      "loss": 0.8029,
      "step": 15930
    },
    {
      "epoch": 2.6597697313532453,
      "grad_norm": 3.036064386367798,
      "learning_rate": 2.3799423044289836e-05,
      "loss": 0.4536,
      "step": 15940
    },
    {
      "epoch": 2.661438344735525,
      "grad_norm": 7.147538185119629,
      "learning_rate": 2.378245375869676e-05,
      "loss": 0.4171,
      "step": 15950
    },
    {
      "epoch": 2.663106958117804,
      "grad_norm": 9.233628273010254,
      "learning_rate": 2.3765484473103684e-05,
      "loss": 0.5789,
      "step": 15960
    },
    {
      "epoch": 2.6647755715000834,
      "grad_norm": 12.95391845703125,
      "learning_rate": 2.3748515187510607e-05,
      "loss": 0.6499,
      "step": 15970
    },
    {
      "epoch": 2.666444184882363,
      "grad_norm": 4.983005523681641,
      "learning_rate": 2.373154590191753e-05,
      "loss": 0.5872,
      "step": 15980
    },
    {
      "epoch": 2.668112798264642,
      "grad_norm": 7.7999267578125,
      "learning_rate": 2.3714576616324455e-05,
      "loss": 0.4176,
      "step": 15990
    },
    {
      "epoch": 2.6697814116469214,
      "grad_norm": 9.207221984863281,
      "learning_rate": 2.3697607330731377e-05,
      "loss": 0.5011,
      "step": 16000
    },
    {
      "epoch": 2.6714500250292006,
      "grad_norm": 19.564205169677734,
      "learning_rate": 2.36806380451383e-05,
      "loss": 0.5342,
      "step": 16010
    },
    {
      "epoch": 2.67311863841148,
      "grad_norm": 6.201437473297119,
      "learning_rate": 2.3663668759545222e-05,
      "loss": 0.5993,
      "step": 16020
    },
    {
      "epoch": 2.6747872517937594,
      "grad_norm": 9.013176918029785,
      "learning_rate": 2.3646699473952148e-05,
      "loss": 0.5417,
      "step": 16030
    },
    {
      "epoch": 2.6764558651760386,
      "grad_norm": 7.9046831130981445,
      "learning_rate": 2.362973018835907e-05,
      "loss": 0.3494,
      "step": 16040
    },
    {
      "epoch": 2.678124478558318,
      "grad_norm": 0.9932793378829956,
      "learning_rate": 2.3612760902765997e-05,
      "loss": 0.394,
      "step": 16050
    },
    {
      "epoch": 2.6797930919405974,
      "grad_norm": 17.466035842895508,
      "learning_rate": 2.359579161717292e-05,
      "loss": 0.628,
      "step": 16060
    },
    {
      "epoch": 2.6814617053228766,
      "grad_norm": 8.800607681274414,
      "learning_rate": 2.357882233157984e-05,
      "loss": 0.4821,
      "step": 16070
    },
    {
      "epoch": 2.683130318705156,
      "grad_norm": 6.289823532104492,
      "learning_rate": 2.3561853045986764e-05,
      "loss": 0.5261,
      "step": 16080
    },
    {
      "epoch": 2.6847989320874355,
      "grad_norm": 5.39602518081665,
      "learning_rate": 2.3544883760393687e-05,
      "loss": 0.4168,
      "step": 16090
    },
    {
      "epoch": 2.6864675454697147,
      "grad_norm": 7.627829551696777,
      "learning_rate": 2.3527914474800612e-05,
      "loss": 0.6654,
      "step": 16100
    },
    {
      "epoch": 2.688136158851994,
      "grad_norm": 3.1361985206604004,
      "learning_rate": 2.351094518920754e-05,
      "loss": 0.4394,
      "step": 16110
    },
    {
      "epoch": 2.6898047722342735,
      "grad_norm": 7.086116790771484,
      "learning_rate": 2.349397590361446e-05,
      "loss": 0.3481,
      "step": 16120
    },
    {
      "epoch": 2.6914733856165527,
      "grad_norm": 8.18869686126709,
      "learning_rate": 2.3477006618021383e-05,
      "loss": 0.5182,
      "step": 16130
    },
    {
      "epoch": 2.693141998998832,
      "grad_norm": 2.8167574405670166,
      "learning_rate": 2.3460037332428306e-05,
      "loss": 0.4665,
      "step": 16140
    },
    {
      "epoch": 2.6948106123811115,
      "grad_norm": 12.290226936340332,
      "learning_rate": 2.3443068046835228e-05,
      "loss": 0.5622,
      "step": 16150
    },
    {
      "epoch": 2.6964792257633907,
      "grad_norm": 13.429605484008789,
      "learning_rate": 2.342609876124215e-05,
      "loss": 0.5071,
      "step": 16160
    },
    {
      "epoch": 2.69814783914567,
      "grad_norm": 13.207232475280762,
      "learning_rate": 2.3409129475649077e-05,
      "loss": 0.5043,
      "step": 16170
    },
    {
      "epoch": 2.6998164525279496,
      "grad_norm": 10.010942459106445,
      "learning_rate": 2.3392160190056e-05,
      "loss": 0.3584,
      "step": 16180
    },
    {
      "epoch": 2.7014850659102287,
      "grad_norm": 15.000743865966797,
      "learning_rate": 2.3375190904462925e-05,
      "loss": 0.4077,
      "step": 16190
    },
    {
      "epoch": 2.703153679292508,
      "grad_norm": 5.725346565246582,
      "learning_rate": 2.3358221618869847e-05,
      "loss": 0.4969,
      "step": 16200
    },
    {
      "epoch": 2.704822292674787,
      "grad_norm": 14.402826309204102,
      "learning_rate": 2.334125233327677e-05,
      "loss": 0.2715,
      "step": 16210
    },
    {
      "epoch": 2.7064909060570663,
      "grad_norm": 5.194535732269287,
      "learning_rate": 2.3324283047683692e-05,
      "loss": 0.826,
      "step": 16220
    },
    {
      "epoch": 2.708159519439346,
      "grad_norm": 13.897866249084473,
      "learning_rate": 2.330731376209062e-05,
      "loss": 0.5492,
      "step": 16230
    },
    {
      "epoch": 2.709828132821625,
      "grad_norm": 10.139904022216797,
      "learning_rate": 2.329034447649754e-05,
      "loss": 0.5894,
      "step": 16240
    },
    {
      "epoch": 2.7114967462039044,
      "grad_norm": 5.264289855957031,
      "learning_rate": 2.3273375190904463e-05,
      "loss": 0.4108,
      "step": 16250
    },
    {
      "epoch": 2.713165359586184,
      "grad_norm": 5.509305953979492,
      "learning_rate": 2.325640590531139e-05,
      "loss": 0.3022,
      "step": 16260
    },
    {
      "epoch": 2.714833972968463,
      "grad_norm": 5.9717888832092285,
      "learning_rate": 2.323943661971831e-05,
      "loss": 0.2543,
      "step": 16270
    },
    {
      "epoch": 2.7165025863507424,
      "grad_norm": 8.332247734069824,
      "learning_rate": 2.3222467334125234e-05,
      "loss": 0.6049,
      "step": 16280
    },
    {
      "epoch": 2.718171199733022,
      "grad_norm": 7.572310447692871,
      "learning_rate": 2.3205498048532157e-05,
      "loss": 0.4505,
      "step": 16290
    },
    {
      "epoch": 2.719839813115301,
      "grad_norm": 3.882415771484375,
      "learning_rate": 2.3188528762939082e-05,
      "loss": 0.4384,
      "step": 16300
    },
    {
      "epoch": 2.7215084264975804,
      "grad_norm": 0.8695495128631592,
      "learning_rate": 2.3171559477346005e-05,
      "loss": 0.3604,
      "step": 16310
    },
    {
      "epoch": 2.72317703987986,
      "grad_norm": 9.142533302307129,
      "learning_rate": 2.3154590191752927e-05,
      "loss": 0.3773,
      "step": 16320
    },
    {
      "epoch": 2.7248456532621392,
      "grad_norm": 5.9795002937316895,
      "learning_rate": 2.313762090615985e-05,
      "loss": 0.5239,
      "step": 16330
    },
    {
      "epoch": 2.7265142666444184,
      "grad_norm": 9.237393379211426,
      "learning_rate": 2.3120651620566776e-05,
      "loss": 0.4683,
      "step": 16340
    },
    {
      "epoch": 2.728182880026698,
      "grad_norm": 7.595658779144287,
      "learning_rate": 2.31036823349737e-05,
      "loss": 0.7487,
      "step": 16350
    },
    {
      "epoch": 2.7298514934089773,
      "grad_norm": 7.754763603210449,
      "learning_rate": 2.3086713049380624e-05,
      "loss": 0.3818,
      "step": 16360
    },
    {
      "epoch": 2.7315201067912565,
      "grad_norm": 5.9284257888793945,
      "learning_rate": 2.3069743763787547e-05,
      "loss": 0.3181,
      "step": 16370
    },
    {
      "epoch": 2.7331887201735356,
      "grad_norm": 2.297250747680664,
      "learning_rate": 2.305277447819447e-05,
      "loss": 0.3708,
      "step": 16380
    },
    {
      "epoch": 2.7348573335558153,
      "grad_norm": 6.978992462158203,
      "learning_rate": 2.303580519260139e-05,
      "loss": 0.5309,
      "step": 16390
    },
    {
      "epoch": 2.7365259469380945,
      "grad_norm": 10.17501163482666,
      "learning_rate": 2.3018835907008314e-05,
      "loss": 0.5854,
      "step": 16400
    },
    {
      "epoch": 2.7381945603203737,
      "grad_norm": 7.794633388519287,
      "learning_rate": 2.300186662141524e-05,
      "loss": 0.5947,
      "step": 16410
    },
    {
      "epoch": 2.739863173702653,
      "grad_norm": 5.303548812866211,
      "learning_rate": 2.2984897335822166e-05,
      "loss": 0.5286,
      "step": 16420
    },
    {
      "epoch": 2.7415317870849325,
      "grad_norm": 10.234528541564941,
      "learning_rate": 2.296792805022909e-05,
      "loss": 0.5657,
      "step": 16430
    },
    {
      "epoch": 2.7432004004672117,
      "grad_norm": 0.6919832229614258,
      "learning_rate": 2.295095876463601e-05,
      "loss": 0.5852,
      "step": 16440
    },
    {
      "epoch": 2.744869013849491,
      "grad_norm": 11.879711151123047,
      "learning_rate": 2.2933989479042933e-05,
      "loss": 0.708,
      "step": 16450
    },
    {
      "epoch": 2.7465376272317705,
      "grad_norm": 0.6940742135047913,
      "learning_rate": 2.2917020193449856e-05,
      "loss": 0.3499,
      "step": 16460
    },
    {
      "epoch": 2.7482062406140497,
      "grad_norm": 2.1743276119232178,
      "learning_rate": 2.290005090785678e-05,
      "loss": 0.3693,
      "step": 16470
    },
    {
      "epoch": 2.749874853996329,
      "grad_norm": 6.490678787231445,
      "learning_rate": 2.2883081622263704e-05,
      "loss": 0.6184,
      "step": 16480
    },
    {
      "epoch": 2.7515434673786086,
      "grad_norm": 3.3665456771850586,
      "learning_rate": 2.2866112336670627e-05,
      "loss": 0.475,
      "step": 16490
    },
    {
      "epoch": 2.7532120807608877,
      "grad_norm": 5.041553020477295,
      "learning_rate": 2.2849143051077553e-05,
      "loss": 0.5086,
      "step": 16500
    },
    {
      "epoch": 2.754880694143167,
      "grad_norm": 9.500812530517578,
      "learning_rate": 2.2832173765484475e-05,
      "loss": 0.5899,
      "step": 16510
    },
    {
      "epoch": 2.7565493075254466,
      "grad_norm": 6.550084590911865,
      "learning_rate": 2.2815204479891398e-05,
      "loss": 0.655,
      "step": 16520
    },
    {
      "epoch": 2.7582179209077258,
      "grad_norm": 4.453820705413818,
      "learning_rate": 2.279823519429832e-05,
      "loss": 0.5413,
      "step": 16530
    },
    {
      "epoch": 2.759886534290005,
      "grad_norm": 3.5420618057250977,
      "learning_rate": 2.2781265908705242e-05,
      "loss": 0.3171,
      "step": 16540
    },
    {
      "epoch": 2.7615551476722846,
      "grad_norm": 2.6278014183044434,
      "learning_rate": 2.276429662311217e-05,
      "loss": 0.4723,
      "step": 16550
    },
    {
      "epoch": 2.763223761054564,
      "grad_norm": 6.553165912628174,
      "learning_rate": 2.274732733751909e-05,
      "loss": 0.5675,
      "step": 16560
    },
    {
      "epoch": 2.764892374436843,
      "grad_norm": 5.08262300491333,
      "learning_rate": 2.2730358051926017e-05,
      "loss": 0.6298,
      "step": 16570
    },
    {
      "epoch": 2.766560987819122,
      "grad_norm": 4.2965898513793945,
      "learning_rate": 2.271338876633294e-05,
      "loss": 0.3529,
      "step": 16580
    },
    {
      "epoch": 2.7682296012014014,
      "grad_norm": 2.4759745597839355,
      "learning_rate": 2.2696419480739862e-05,
      "loss": 0.4659,
      "step": 16590
    },
    {
      "epoch": 2.769898214583681,
      "grad_norm": 12.277318000793457,
      "learning_rate": 2.2679450195146784e-05,
      "loss": 0.6033,
      "step": 16600
    },
    {
      "epoch": 2.77156682796596,
      "grad_norm": 0.6295731663703918,
      "learning_rate": 2.266248090955371e-05,
      "loss": 0.7991,
      "step": 16610
    },
    {
      "epoch": 2.7732354413482394,
      "grad_norm": 4.898409366607666,
      "learning_rate": 2.2645511623960633e-05,
      "loss": 0.5017,
      "step": 16620
    },
    {
      "epoch": 2.774904054730519,
      "grad_norm": 1.068556547164917,
      "learning_rate": 2.2628542338367555e-05,
      "loss": 0.5494,
      "step": 16630
    },
    {
      "epoch": 2.7765726681127982,
      "grad_norm": 12.21273136138916,
      "learning_rate": 2.2611573052774478e-05,
      "loss": 0.4204,
      "step": 16640
    },
    {
      "epoch": 2.7782412814950774,
      "grad_norm": 1.4288420677185059,
      "learning_rate": 2.2594603767181403e-05,
      "loss": 0.3951,
      "step": 16650
    },
    {
      "epoch": 2.779909894877357,
      "grad_norm": 2.166409492492676,
      "learning_rate": 2.2577634481588326e-05,
      "loss": 0.5148,
      "step": 16660
    },
    {
      "epoch": 2.7815785082596363,
      "grad_norm": 2.7928531169891357,
      "learning_rate": 2.2560665195995252e-05,
      "loss": 0.4255,
      "step": 16670
    },
    {
      "epoch": 2.7832471216419155,
      "grad_norm": 7.917448043823242,
      "learning_rate": 2.2543695910402174e-05,
      "loss": 0.6318,
      "step": 16680
    },
    {
      "epoch": 2.784915735024195,
      "grad_norm": 7.5218825340271,
      "learning_rate": 2.2526726624809097e-05,
      "loss": 0.6153,
      "step": 16690
    },
    {
      "epoch": 2.7865843484064743,
      "grad_norm": 6.381222248077393,
      "learning_rate": 2.250975733921602e-05,
      "loss": 0.6614,
      "step": 16700
    },
    {
      "epoch": 2.7882529617887535,
      "grad_norm": 6.117195129394531,
      "learning_rate": 2.2492788053622942e-05,
      "loss": 0.496,
      "step": 16710
    },
    {
      "epoch": 2.789921575171033,
      "grad_norm": 7.089505672454834,
      "learning_rate": 2.2475818768029868e-05,
      "loss": 0.7196,
      "step": 16720
    },
    {
      "epoch": 2.7915901885533123,
      "grad_norm": 12.273040771484375,
      "learning_rate": 2.2458849482436793e-05,
      "loss": 0.3064,
      "step": 16730
    },
    {
      "epoch": 2.7932588019355915,
      "grad_norm": 3.9008941650390625,
      "learning_rate": 2.2441880196843716e-05,
      "loss": 0.5463,
      "step": 16740
    },
    {
      "epoch": 2.7949274153178707,
      "grad_norm": 4.70382833480835,
      "learning_rate": 2.242491091125064e-05,
      "loss": 0.4218,
      "step": 16750
    },
    {
      "epoch": 2.7965960287001503,
      "grad_norm": 4.766720294952393,
      "learning_rate": 2.240794162565756e-05,
      "loss": 0.716,
      "step": 16760
    },
    {
      "epoch": 2.7982646420824295,
      "grad_norm": 1.554964303970337,
      "learning_rate": 2.2390972340064483e-05,
      "loss": 0.3565,
      "step": 16770
    },
    {
      "epoch": 2.7999332554647087,
      "grad_norm": 2.7582602500915527,
      "learning_rate": 2.2374003054471406e-05,
      "loss": 0.5456,
      "step": 16780
    },
    {
      "epoch": 2.801601868846988,
      "grad_norm": 8.998564720153809,
      "learning_rate": 2.2357033768878332e-05,
      "loss": 0.805,
      "step": 16790
    },
    {
      "epoch": 2.8032704822292676,
      "grad_norm": 3.6569344997406006,
      "learning_rate": 2.2340064483285254e-05,
      "loss": 0.4722,
      "step": 16800
    },
    {
      "epoch": 2.8049390956115468,
      "grad_norm": 0.8385048508644104,
      "learning_rate": 2.232309519769218e-05,
      "loss": 0.422,
      "step": 16810
    },
    {
      "epoch": 2.806607708993826,
      "grad_norm": 7.351850509643555,
      "learning_rate": 2.2306125912099103e-05,
      "loss": 0.5369,
      "step": 16820
    },
    {
      "epoch": 2.8082763223761056,
      "grad_norm": 5.551637649536133,
      "learning_rate": 2.2289156626506025e-05,
      "loss": 0.2831,
      "step": 16830
    },
    {
      "epoch": 2.8099449357583848,
      "grad_norm": 14.499505043029785,
      "learning_rate": 2.2272187340912948e-05,
      "loss": 0.6384,
      "step": 16840
    },
    {
      "epoch": 2.811613549140664,
      "grad_norm": 13.879106521606445,
      "learning_rate": 2.225521805531987e-05,
      "loss": 0.5113,
      "step": 16850
    },
    {
      "epoch": 2.8132821625229436,
      "grad_norm": 4.712796688079834,
      "learning_rate": 2.2238248769726796e-05,
      "loss": 0.3827,
      "step": 16860
    },
    {
      "epoch": 2.814950775905223,
      "grad_norm": 0.549705445766449,
      "learning_rate": 2.222127948413372e-05,
      "loss": 0.476,
      "step": 16870
    },
    {
      "epoch": 2.816619389287502,
      "grad_norm": 9.875107765197754,
      "learning_rate": 2.2204310198540644e-05,
      "loss": 0.5492,
      "step": 16880
    },
    {
      "epoch": 2.8182880026697816,
      "grad_norm": 6.697668075561523,
      "learning_rate": 2.2187340912947567e-05,
      "loss": 0.4947,
      "step": 16890
    },
    {
      "epoch": 2.819956616052061,
      "grad_norm": 3.6056408882141113,
      "learning_rate": 2.217037162735449e-05,
      "loss": 0.6353,
      "step": 16900
    },
    {
      "epoch": 2.82162522943434,
      "grad_norm": 7.3399658203125,
      "learning_rate": 2.2153402341761412e-05,
      "loss": 0.4532,
      "step": 16910
    },
    {
      "epoch": 2.8232938428166197,
      "grad_norm": 24.16758918762207,
      "learning_rate": 2.2136433056168338e-05,
      "loss": 0.7031,
      "step": 16920
    },
    {
      "epoch": 2.824962456198899,
      "grad_norm": 9.965164184570312,
      "learning_rate": 2.211946377057526e-05,
      "loss": 0.4068,
      "step": 16930
    },
    {
      "epoch": 2.826631069581178,
      "grad_norm": 9.369966506958008,
      "learning_rate": 2.2102494484982183e-05,
      "loss": 0.4406,
      "step": 16940
    },
    {
      "epoch": 2.8282996829634572,
      "grad_norm": 10.399845123291016,
      "learning_rate": 2.2085525199389105e-05,
      "loss": 0.736,
      "step": 16950
    },
    {
      "epoch": 2.8299682963457364,
      "grad_norm": 10.656510353088379,
      "learning_rate": 2.206855591379603e-05,
      "loss": 0.8702,
      "step": 16960
    },
    {
      "epoch": 2.831636909728016,
      "grad_norm": 3.7875161170959473,
      "learning_rate": 2.2051586628202953e-05,
      "loss": 0.5845,
      "step": 16970
    },
    {
      "epoch": 2.8333055231102953,
      "grad_norm": 4.911840915679932,
      "learning_rate": 2.203461734260988e-05,
      "loss": 0.5202,
      "step": 16980
    },
    {
      "epoch": 2.8349741364925745,
      "grad_norm": 6.395911693572998,
      "learning_rate": 2.2017648057016802e-05,
      "loss": 0.567,
      "step": 16990
    },
    {
      "epoch": 2.836642749874854,
      "grad_norm": 4.336467266082764,
      "learning_rate": 2.2000678771423724e-05,
      "loss": 0.5545,
      "step": 17000
    },
    {
      "epoch": 2.8383113632571333,
      "grad_norm": 2.0689032077789307,
      "learning_rate": 2.1983709485830647e-05,
      "loss": 0.1901,
      "step": 17010
    },
    {
      "epoch": 2.8399799766394125,
      "grad_norm": 15.860489845275879,
      "learning_rate": 2.196674020023757e-05,
      "loss": 0.7594,
      "step": 17020
    },
    {
      "epoch": 2.841648590021692,
      "grad_norm": 10.57407283782959,
      "learning_rate": 2.1949770914644495e-05,
      "loss": 0.5232,
      "step": 17030
    },
    {
      "epoch": 2.8433172034039713,
      "grad_norm": 1.438622236251831,
      "learning_rate": 2.193280162905142e-05,
      "loss": 0.3566,
      "step": 17040
    },
    {
      "epoch": 2.8449858167862505,
      "grad_norm": 10.288118362426758,
      "learning_rate": 2.1915832343458343e-05,
      "loss": 0.6233,
      "step": 17050
    },
    {
      "epoch": 2.84665443016853,
      "grad_norm": 2.430983066558838,
      "learning_rate": 2.1898863057865266e-05,
      "loss": 0.3905,
      "step": 17060
    },
    {
      "epoch": 2.8483230435508093,
      "grad_norm": 1.8745077848434448,
      "learning_rate": 2.188189377227219e-05,
      "loss": 0.422,
      "step": 17070
    },
    {
      "epoch": 2.8499916569330885,
      "grad_norm": 10.554431915283203,
      "learning_rate": 2.186492448667911e-05,
      "loss": 0.4857,
      "step": 17080
    },
    {
      "epoch": 2.851660270315368,
      "grad_norm": 7.943004131317139,
      "learning_rate": 2.1847955201086033e-05,
      "loss": 0.5837,
      "step": 17090
    },
    {
      "epoch": 2.8533288836976474,
      "grad_norm": 4.208588600158691,
      "learning_rate": 2.1830985915492956e-05,
      "loss": 0.3466,
      "step": 17100
    },
    {
      "epoch": 2.8549974970799266,
      "grad_norm": 8.802206993103027,
      "learning_rate": 2.1814016629899882e-05,
      "loss": 0.5686,
      "step": 17110
    },
    {
      "epoch": 2.8566661104622058,
      "grad_norm": 3.4683384895324707,
      "learning_rate": 2.1797047344306808e-05,
      "loss": 0.4156,
      "step": 17120
    },
    {
      "epoch": 2.8583347238444854,
      "grad_norm": 18.68597984313965,
      "learning_rate": 2.178007805871373e-05,
      "loss": 0.5734,
      "step": 17130
    },
    {
      "epoch": 2.8600033372267646,
      "grad_norm": 4.130858421325684,
      "learning_rate": 2.1763108773120653e-05,
      "loss": 0.5274,
      "step": 17140
    },
    {
      "epoch": 2.861671950609044,
      "grad_norm": 5.696335792541504,
      "learning_rate": 2.1746139487527575e-05,
      "loss": 0.7243,
      "step": 17150
    },
    {
      "epoch": 2.863340563991323,
      "grad_norm": 2.2427937984466553,
      "learning_rate": 2.1729170201934498e-05,
      "loss": 0.4634,
      "step": 17160
    },
    {
      "epoch": 2.8650091773736026,
      "grad_norm": 5.70860481262207,
      "learning_rate": 2.1712200916341423e-05,
      "loss": 0.4525,
      "step": 17170
    },
    {
      "epoch": 2.866677790755882,
      "grad_norm": 13.792566299438477,
      "learning_rate": 2.1695231630748346e-05,
      "loss": 0.6499,
      "step": 17180
    },
    {
      "epoch": 2.868346404138161,
      "grad_norm": 8.22870922088623,
      "learning_rate": 2.1678262345155272e-05,
      "loss": 0.6208,
      "step": 17190
    },
    {
      "epoch": 2.8700150175204406,
      "grad_norm": 7.367848873138428,
      "learning_rate": 2.1661293059562194e-05,
      "loss": 0.4109,
      "step": 17200
    },
    {
      "epoch": 2.87168363090272,
      "grad_norm": 5.800049781799316,
      "learning_rate": 2.1644323773969117e-05,
      "loss": 0.3948,
      "step": 17210
    },
    {
      "epoch": 2.873352244284999,
      "grad_norm": 5.330380439758301,
      "learning_rate": 2.162735448837604e-05,
      "loss": 0.6165,
      "step": 17220
    },
    {
      "epoch": 2.8750208576672787,
      "grad_norm": 9.945732116699219,
      "learning_rate": 2.1610385202782965e-05,
      "loss": 0.707,
      "step": 17230
    },
    {
      "epoch": 2.876689471049558,
      "grad_norm": 17.880565643310547,
      "learning_rate": 2.1593415917189888e-05,
      "loss": 0.5502,
      "step": 17240
    },
    {
      "epoch": 2.878358084431837,
      "grad_norm": 6.296970367431641,
      "learning_rate": 2.157644663159681e-05,
      "loss": 0.4299,
      "step": 17250
    },
    {
      "epoch": 2.8800266978141167,
      "grad_norm": 10.469583511352539,
      "learning_rate": 2.1559477346003733e-05,
      "loss": 0.4824,
      "step": 17260
    },
    {
      "epoch": 2.881695311196396,
      "grad_norm": 1.8300907611846924,
      "learning_rate": 2.154250806041066e-05,
      "loss": 0.6455,
      "step": 17270
    },
    {
      "epoch": 2.883363924578675,
      "grad_norm": 4.270146369934082,
      "learning_rate": 2.152553877481758e-05,
      "loss": 0.3109,
      "step": 17280
    },
    {
      "epoch": 2.8850325379609547,
      "grad_norm": 2.257969856262207,
      "learning_rate": 2.1508569489224507e-05,
      "loss": 0.439,
      "step": 17290
    },
    {
      "epoch": 2.886701151343234,
      "grad_norm": 12.189146041870117,
      "learning_rate": 2.149160020363143e-05,
      "loss": 0.5371,
      "step": 17300
    },
    {
      "epoch": 2.888369764725513,
      "grad_norm": 4.462913513183594,
      "learning_rate": 2.1474630918038352e-05,
      "loss": 0.5519,
      "step": 17310
    },
    {
      "epoch": 2.8900383781077923,
      "grad_norm": 1.4712284803390503,
      "learning_rate": 2.1457661632445274e-05,
      "loss": 0.3394,
      "step": 17320
    },
    {
      "epoch": 2.8917069914900715,
      "grad_norm": 4.218459129333496,
      "learning_rate": 2.1440692346852197e-05,
      "loss": 0.4447,
      "step": 17330
    },
    {
      "epoch": 2.893375604872351,
      "grad_norm": 6.369862079620361,
      "learning_rate": 2.1423723061259123e-05,
      "loss": 0.2181,
      "step": 17340
    },
    {
      "epoch": 2.8950442182546303,
      "grad_norm": 12.416379928588867,
      "learning_rate": 2.1406753775666045e-05,
      "loss": 0.4434,
      "step": 17350
    },
    {
      "epoch": 2.8967128316369095,
      "grad_norm": 3.007018566131592,
      "learning_rate": 2.138978449007297e-05,
      "loss": 0.5155,
      "step": 17360
    },
    {
      "epoch": 2.898381445019189,
      "grad_norm": 13.871879577636719,
      "learning_rate": 2.1372815204479894e-05,
      "loss": 0.5192,
      "step": 17370
    },
    {
      "epoch": 2.9000500584014683,
      "grad_norm": 8.452198028564453,
      "learning_rate": 2.1355845918886816e-05,
      "loss": 0.5486,
      "step": 17380
    },
    {
      "epoch": 2.9017186717837475,
      "grad_norm": 9.5822172164917,
      "learning_rate": 2.133887663329374e-05,
      "loss": 0.449,
      "step": 17390
    },
    {
      "epoch": 2.903387285166027,
      "grad_norm": 15.52386474609375,
      "learning_rate": 2.132190734770066e-05,
      "loss": 0.3832,
      "step": 17400
    },
    {
      "epoch": 2.9050558985483064,
      "grad_norm": 3.3331589698791504,
      "learning_rate": 2.1304938062107583e-05,
      "loss": 0.6706,
      "step": 17410
    },
    {
      "epoch": 2.9067245119305856,
      "grad_norm": 17.832489013671875,
      "learning_rate": 2.128796877651451e-05,
      "loss": 0.4492,
      "step": 17420
    },
    {
      "epoch": 2.908393125312865,
      "grad_norm": 11.724018096923828,
      "learning_rate": 2.1270999490921435e-05,
      "loss": 0.6219,
      "step": 17430
    },
    {
      "epoch": 2.9100617386951444,
      "grad_norm": 10.741238594055176,
      "learning_rate": 2.1254030205328358e-05,
      "loss": 0.5447,
      "step": 17440
    },
    {
      "epoch": 2.9117303520774236,
      "grad_norm": 3.305934429168701,
      "learning_rate": 2.123706091973528e-05,
      "loss": 0.4874,
      "step": 17450
    },
    {
      "epoch": 2.9133989654597032,
      "grad_norm": 1.7584526538848877,
      "learning_rate": 2.1220091634142203e-05,
      "loss": 0.4313,
      "step": 17460
    },
    {
      "epoch": 2.9150675788419824,
      "grad_norm": 7.844520568847656,
      "learning_rate": 2.1203122348549125e-05,
      "loss": 0.4333,
      "step": 17470
    },
    {
      "epoch": 2.9167361922242616,
      "grad_norm": 0.8601887822151184,
      "learning_rate": 2.118615306295605e-05,
      "loss": 0.1586,
      "step": 17480
    },
    {
      "epoch": 2.918404805606541,
      "grad_norm": 7.30903959274292,
      "learning_rate": 2.1169183777362974e-05,
      "loss": 0.4562,
      "step": 17490
    },
    {
      "epoch": 2.9200734189888204,
      "grad_norm": 4.8853254318237305,
      "learning_rate": 2.11522144917699e-05,
      "loss": 0.4103,
      "step": 17500
    },
    {
      "epoch": 2.9217420323710996,
      "grad_norm": 10.304550170898438,
      "learning_rate": 2.1135245206176822e-05,
      "loss": 0.4237,
      "step": 17510
    },
    {
      "epoch": 2.923410645753379,
      "grad_norm": 8.400004386901855,
      "learning_rate": 2.1118275920583744e-05,
      "loss": 0.3162,
      "step": 17520
    },
    {
      "epoch": 2.925079259135658,
      "grad_norm": 5.405850887298584,
      "learning_rate": 2.1101306634990667e-05,
      "loss": 0.3685,
      "step": 17530
    },
    {
      "epoch": 2.9267478725179377,
      "grad_norm": 3.0845768451690674,
      "learning_rate": 2.1084337349397593e-05,
      "loss": 0.6707,
      "step": 17540
    },
    {
      "epoch": 2.928416485900217,
      "grad_norm": 9.984786987304688,
      "learning_rate": 2.1067368063804515e-05,
      "loss": 0.5577,
      "step": 17550
    },
    {
      "epoch": 2.930085099282496,
      "grad_norm": 3.329159736633301,
      "learning_rate": 2.1050398778211438e-05,
      "loss": 0.6503,
      "step": 17560
    },
    {
      "epoch": 2.9317537126647757,
      "grad_norm": 10.159876823425293,
      "learning_rate": 2.103342949261836e-05,
      "loss": 0.536,
      "step": 17570
    },
    {
      "epoch": 2.933422326047055,
      "grad_norm": 3.478323459625244,
      "learning_rate": 2.1016460207025286e-05,
      "loss": 0.4632,
      "step": 17580
    },
    {
      "epoch": 2.935090939429334,
      "grad_norm": 1.8854446411132812,
      "learning_rate": 2.099949092143221e-05,
      "loss": 0.4403,
      "step": 17590
    },
    {
      "epoch": 2.9367595528116137,
      "grad_norm": 1.3358442783355713,
      "learning_rate": 2.0982521635839134e-05,
      "loss": 0.3955,
      "step": 17600
    },
    {
      "epoch": 2.938428166193893,
      "grad_norm": 1.286176085472107,
      "learning_rate": 2.0965552350246057e-05,
      "loss": 0.46,
      "step": 17610
    },
    {
      "epoch": 2.940096779576172,
      "grad_norm": 7.964682102203369,
      "learning_rate": 2.094858306465298e-05,
      "loss": 0.7251,
      "step": 17620
    },
    {
      "epoch": 2.9417653929584517,
      "grad_norm": 11.08484935760498,
      "learning_rate": 2.0931613779059902e-05,
      "loss": 0.756,
      "step": 17630
    },
    {
      "epoch": 2.943434006340731,
      "grad_norm": 6.740691184997559,
      "learning_rate": 2.0914644493466824e-05,
      "loss": 0.4745,
      "step": 17640
    },
    {
      "epoch": 2.94510261972301,
      "grad_norm": 3.9388961791992188,
      "learning_rate": 2.089767520787375e-05,
      "loss": 0.6978,
      "step": 17650
    },
    {
      "epoch": 2.9467712331052898,
      "grad_norm": 9.126646995544434,
      "learning_rate": 2.0880705922280673e-05,
      "loss": 0.7562,
      "step": 17660
    },
    {
      "epoch": 2.948439846487569,
      "grad_norm": 9.555181503295898,
      "learning_rate": 2.08637366366876e-05,
      "loss": 0.6544,
      "step": 17670
    },
    {
      "epoch": 2.950108459869848,
      "grad_norm": 3.402557134628296,
      "learning_rate": 2.084676735109452e-05,
      "loss": 0.3142,
      "step": 17680
    },
    {
      "epoch": 2.9517770732521273,
      "grad_norm": 6.964321136474609,
      "learning_rate": 2.0829798065501444e-05,
      "loss": 0.5716,
      "step": 17690
    },
    {
      "epoch": 2.9534456866344065,
      "grad_norm": 7.0600128173828125,
      "learning_rate": 2.0812828779908366e-05,
      "loss": 0.4843,
      "step": 17700
    },
    {
      "epoch": 2.955114300016686,
      "grad_norm": 7.617175102233887,
      "learning_rate": 2.079585949431529e-05,
      "loss": 0.4579,
      "step": 17710
    },
    {
      "epoch": 2.9567829133989654,
      "grad_norm": 4.549773693084717,
      "learning_rate": 2.077889020872221e-05,
      "loss": 0.5247,
      "step": 17720
    },
    {
      "epoch": 2.9584515267812446,
      "grad_norm": 10.29808235168457,
      "learning_rate": 2.0761920923129137e-05,
      "loss": 0.4792,
      "step": 17730
    },
    {
      "epoch": 2.960120140163524,
      "grad_norm": 7.079352378845215,
      "learning_rate": 2.0744951637536063e-05,
      "loss": 0.5819,
      "step": 17740
    },
    {
      "epoch": 2.9617887535458034,
      "grad_norm": 1.371954321861267,
      "learning_rate": 2.0727982351942985e-05,
      "loss": 0.2345,
      "step": 17750
    },
    {
      "epoch": 2.9634573669280826,
      "grad_norm": 2.1552340984344482,
      "learning_rate": 2.0711013066349908e-05,
      "loss": 0.7166,
      "step": 17760
    },
    {
      "epoch": 2.9651259803103622,
      "grad_norm": 6.504733085632324,
      "learning_rate": 2.069404378075683e-05,
      "loss": 0.5396,
      "step": 17770
    },
    {
      "epoch": 2.9667945936926414,
      "grad_norm": 4.5460734367370605,
      "learning_rate": 2.0677074495163753e-05,
      "loss": 0.4469,
      "step": 17780
    },
    {
      "epoch": 2.9684632070749206,
      "grad_norm": 10.634248733520508,
      "learning_rate": 2.066010520957068e-05,
      "loss": 0.6572,
      "step": 17790
    },
    {
      "epoch": 2.9701318204572003,
      "grad_norm": 5.672333717346191,
      "learning_rate": 2.06431359239776e-05,
      "loss": 0.4128,
      "step": 17800
    },
    {
      "epoch": 2.9718004338394794,
      "grad_norm": 1.5724940299987793,
      "learning_rate": 2.0626166638384527e-05,
      "loss": 0.614,
      "step": 17810
    },
    {
      "epoch": 2.9734690472217586,
      "grad_norm": 6.149106979370117,
      "learning_rate": 2.060919735279145e-05,
      "loss": 0.4539,
      "step": 17820
    },
    {
      "epoch": 2.9751376606040383,
      "grad_norm": 1.8185632228851318,
      "learning_rate": 2.0592228067198372e-05,
      "loss": 0.4588,
      "step": 17830
    },
    {
      "epoch": 2.9768062739863175,
      "grad_norm": 11.179733276367188,
      "learning_rate": 2.0575258781605294e-05,
      "loss": 0.4972,
      "step": 17840
    },
    {
      "epoch": 2.9784748873685967,
      "grad_norm": 11.998446464538574,
      "learning_rate": 2.055828949601222e-05,
      "loss": 0.5431,
      "step": 17850
    },
    {
      "epoch": 2.980143500750876,
      "grad_norm": 2.280848741531372,
      "learning_rate": 2.0541320210419143e-05,
      "loss": 0.3502,
      "step": 17860
    },
    {
      "epoch": 2.9818121141331555,
      "grad_norm": 5.316470146179199,
      "learning_rate": 2.0524350924826065e-05,
      "loss": 0.3646,
      "step": 17870
    },
    {
      "epoch": 2.9834807275154347,
      "grad_norm": 2.8794198036193848,
      "learning_rate": 2.0507381639232988e-05,
      "loss": 0.5506,
      "step": 17880
    },
    {
      "epoch": 2.985149340897714,
      "grad_norm": 9.134892463684082,
      "learning_rate": 2.0490412353639914e-05,
      "loss": 0.5774,
      "step": 17890
    },
    {
      "epoch": 2.986817954279993,
      "grad_norm": 0.2837400436401367,
      "learning_rate": 2.0473443068046836e-05,
      "loss": 0.5012,
      "step": 17900
    },
    {
      "epoch": 2.9884865676622727,
      "grad_norm": 8.466784477233887,
      "learning_rate": 2.045647378245376e-05,
      "loss": 0.6334,
      "step": 17910
    },
    {
      "epoch": 2.990155181044552,
      "grad_norm": 7.303955078125,
      "learning_rate": 2.0439504496860685e-05,
      "loss": 0.7257,
      "step": 17920
    },
    {
      "epoch": 2.991823794426831,
      "grad_norm": 3.326324701309204,
      "learning_rate": 2.0422535211267607e-05,
      "loss": 0.2683,
      "step": 17930
    },
    {
      "epoch": 2.9934924078091107,
      "grad_norm": 10.043778419494629,
      "learning_rate": 2.040556592567453e-05,
      "loss": 0.5829,
      "step": 17940
    },
    {
      "epoch": 2.99516102119139,
      "grad_norm": 8.210652351379395,
      "learning_rate": 2.0388596640081452e-05,
      "loss": 0.7426,
      "step": 17950
    },
    {
      "epoch": 2.996829634573669,
      "grad_norm": 8.663739204406738,
      "learning_rate": 2.0371627354488378e-05,
      "loss": 0.2212,
      "step": 17960
    },
    {
      "epoch": 2.9984982479559488,
      "grad_norm": 4.313270092010498,
      "learning_rate": 2.03546580688953e-05,
      "loss": 0.5528,
      "step": 17970
    },
    {
      "epoch": 3.000166861338228,
      "grad_norm": 3.0632169246673584,
      "learning_rate": 2.0337688783302226e-05,
      "loss": 0.4682,
      "step": 17980
    },
    {
      "epoch": 3.001835474720507,
      "grad_norm": 16.47184181213379,
      "learning_rate": 2.032071949770915e-05,
      "loss": 0.4849,
      "step": 17990
    },
    {
      "epoch": 3.003504088102787,
      "grad_norm": 5.85561466217041,
      "learning_rate": 2.030375021211607e-05,
      "loss": 0.2671,
      "step": 18000
    },
    {
      "epoch": 3.005172701485066,
      "grad_norm": 1.3818849325180054,
      "learning_rate": 2.0286780926522994e-05,
      "loss": 0.3961,
      "step": 18010
    },
    {
      "epoch": 3.006841314867345,
      "grad_norm": 2.526871681213379,
      "learning_rate": 2.0269811640929916e-05,
      "loss": 0.6055,
      "step": 18020
    },
    {
      "epoch": 3.0085099282496244,
      "grad_norm": 8.77935791015625,
      "learning_rate": 2.025284235533684e-05,
      "loss": 0.4685,
      "step": 18030
    },
    {
      "epoch": 3.010178541631904,
      "grad_norm": 2.973597288131714,
      "learning_rate": 2.0235873069743765e-05,
      "loss": 0.334,
      "step": 18040
    },
    {
      "epoch": 3.011847155014183,
      "grad_norm": 3.670395851135254,
      "learning_rate": 2.021890378415069e-05,
      "loss": 0.4996,
      "step": 18050
    },
    {
      "epoch": 3.0135157683964624,
      "grad_norm": 4.978419780731201,
      "learning_rate": 2.0201934498557613e-05,
      "loss": 0.1972,
      "step": 18060
    },
    {
      "epoch": 3.015184381778742,
      "grad_norm": 2.582318067550659,
      "learning_rate": 2.0184965212964535e-05,
      "loss": 0.3621,
      "step": 18070
    },
    {
      "epoch": 3.0168529951610212,
      "grad_norm": 2.472511053085327,
      "learning_rate": 2.0167995927371458e-05,
      "loss": 0.6897,
      "step": 18080
    },
    {
      "epoch": 3.0185216085433004,
      "grad_norm": 14.612839698791504,
      "learning_rate": 2.015102664177838e-05,
      "loss": 0.5126,
      "step": 18090
    },
    {
      "epoch": 3.0201902219255796,
      "grad_norm": 9.808667182922363,
      "learning_rate": 2.0134057356185306e-05,
      "loss": 0.6237,
      "step": 18100
    },
    {
      "epoch": 3.0218588353078593,
      "grad_norm": 2.717597007751465,
      "learning_rate": 2.011708807059223e-05,
      "loss": 0.4877,
      "step": 18110
    },
    {
      "epoch": 3.0235274486901385,
      "grad_norm": 4.095378398895264,
      "learning_rate": 2.0100118784999155e-05,
      "loss": 0.2861,
      "step": 18120
    },
    {
      "epoch": 3.0251960620724176,
      "grad_norm": 4.498961925506592,
      "learning_rate": 2.0083149499406077e-05,
      "loss": 0.3788,
      "step": 18130
    },
    {
      "epoch": 3.0268646754546973,
      "grad_norm": 3.7964577674865723,
      "learning_rate": 2.0066180213813e-05,
      "loss": 0.2803,
      "step": 18140
    },
    {
      "epoch": 3.0285332888369765,
      "grad_norm": 5.31336784362793,
      "learning_rate": 2.0049210928219922e-05,
      "loss": 0.4273,
      "step": 18150
    },
    {
      "epoch": 3.0302019022192557,
      "grad_norm": 2.8195078372955322,
      "learning_rate": 2.0032241642626845e-05,
      "loss": 0.5665,
      "step": 18160
    },
    {
      "epoch": 3.0318705156015353,
      "grad_norm": 7.803941249847412,
      "learning_rate": 2.001527235703377e-05,
      "loss": 0.2369,
      "step": 18170
    },
    {
      "epoch": 3.0335391289838145,
      "grad_norm": 8.40911865234375,
      "learning_rate": 1.9998303071440693e-05,
      "loss": 0.606,
      "step": 18180
    },
    {
      "epoch": 3.0352077423660937,
      "grad_norm": 2.077413320541382,
      "learning_rate": 1.9981333785847615e-05,
      "loss": 0.2943,
      "step": 18190
    },
    {
      "epoch": 3.036876355748373,
      "grad_norm": 0.4256957471370697,
      "learning_rate": 1.996436450025454e-05,
      "loss": 0.4019,
      "step": 18200
    },
    {
      "epoch": 3.0385449691306525,
      "grad_norm": 9.140447616577148,
      "learning_rate": 1.9947395214661464e-05,
      "loss": 0.5373,
      "step": 18210
    },
    {
      "epoch": 3.0402135825129317,
      "grad_norm": 11.225790023803711,
      "learning_rate": 1.9930425929068386e-05,
      "loss": 0.3601,
      "step": 18220
    },
    {
      "epoch": 3.041882195895211,
      "grad_norm": 2.594604253768921,
      "learning_rate": 1.9913456643475312e-05,
      "loss": 0.356,
      "step": 18230
    },
    {
      "epoch": 3.0435508092774906,
      "grad_norm": 14.118802070617676,
      "learning_rate": 1.9896487357882235e-05,
      "loss": 0.5146,
      "step": 18240
    },
    {
      "epoch": 3.0452194226597697,
      "grad_norm": 16.69139862060547,
      "learning_rate": 1.9879518072289157e-05,
      "loss": 0.581,
      "step": 18250
    },
    {
      "epoch": 3.046888036042049,
      "grad_norm": 5.909225940704346,
      "learning_rate": 1.986254878669608e-05,
      "loss": 0.2874,
      "step": 18260
    },
    {
      "epoch": 3.0485566494243286,
      "grad_norm": 11.887970924377441,
      "learning_rate": 1.9845579501103005e-05,
      "loss": 0.5364,
      "step": 18270
    },
    {
      "epoch": 3.0502252628066078,
      "grad_norm": 8.649931907653809,
      "learning_rate": 1.9828610215509928e-05,
      "loss": 0.6899,
      "step": 18280
    },
    {
      "epoch": 3.051893876188887,
      "grad_norm": 3.0211880207061768,
      "learning_rate": 1.9811640929916854e-05,
      "loss": 0.6528,
      "step": 18290
    },
    {
      "epoch": 3.053562489571166,
      "grad_norm": 2.1754448413848877,
      "learning_rate": 1.9794671644323776e-05,
      "loss": 0.6841,
      "step": 18300
    },
    {
      "epoch": 3.055231102953446,
      "grad_norm": 1.563303828239441,
      "learning_rate": 1.97777023587307e-05,
      "loss": 0.3653,
      "step": 18310
    },
    {
      "epoch": 3.056899716335725,
      "grad_norm": 10.059717178344727,
      "learning_rate": 1.976073307313762e-05,
      "loss": 0.3402,
      "step": 18320
    },
    {
      "epoch": 3.058568329718004,
      "grad_norm": 4.2365498542785645,
      "learning_rate": 1.9743763787544544e-05,
      "loss": 0.443,
      "step": 18330
    },
    {
      "epoch": 3.060236943100284,
      "grad_norm": 8.022868156433105,
      "learning_rate": 1.9726794501951466e-05,
      "loss": 0.4502,
      "step": 18340
    },
    {
      "epoch": 3.061905556482563,
      "grad_norm": 9.407331466674805,
      "learning_rate": 1.9709825216358392e-05,
      "loss": 0.6217,
      "step": 18350
    },
    {
      "epoch": 3.063574169864842,
      "grad_norm": 11.974284172058105,
      "learning_rate": 1.9692855930765318e-05,
      "loss": 0.5168,
      "step": 18360
    },
    {
      "epoch": 3.065242783247122,
      "grad_norm": 0.5607734322547913,
      "learning_rate": 1.967588664517224e-05,
      "loss": 0.4057,
      "step": 18370
    },
    {
      "epoch": 3.066911396629401,
      "grad_norm": 9.147968292236328,
      "learning_rate": 1.9658917359579163e-05,
      "loss": 0.4476,
      "step": 18380
    },
    {
      "epoch": 3.0685800100116802,
      "grad_norm": 10.8197021484375,
      "learning_rate": 1.9641948073986085e-05,
      "loss": 0.5851,
      "step": 18390
    },
    {
      "epoch": 3.0702486233939594,
      "grad_norm": 8.036608695983887,
      "learning_rate": 1.9624978788393008e-05,
      "loss": 0.4557,
      "step": 18400
    },
    {
      "epoch": 3.071917236776239,
      "grad_norm": 3.735584259033203,
      "learning_rate": 1.9608009502799934e-05,
      "loss": 0.4022,
      "step": 18410
    },
    {
      "epoch": 3.0735858501585183,
      "grad_norm": 1.641745924949646,
      "learning_rate": 1.9591040217206856e-05,
      "loss": 0.4421,
      "step": 18420
    },
    {
      "epoch": 3.0752544635407975,
      "grad_norm": 2.023252010345459,
      "learning_rate": 1.9574070931613782e-05,
      "loss": 0.4036,
      "step": 18430
    },
    {
      "epoch": 3.076923076923077,
      "grad_norm": 10.456960678100586,
      "learning_rate": 1.9557101646020705e-05,
      "loss": 0.606,
      "step": 18440
    },
    {
      "epoch": 3.0785916903053563,
      "grad_norm": 11.618942260742188,
      "learning_rate": 1.9540132360427627e-05,
      "loss": 0.7056,
      "step": 18450
    },
    {
      "epoch": 3.0802603036876355,
      "grad_norm": 3.038102865219116,
      "learning_rate": 1.952316307483455e-05,
      "loss": 0.4271,
      "step": 18460
    },
    {
      "epoch": 3.0819289170699147,
      "grad_norm": 5.173333168029785,
      "learning_rate": 1.9506193789241472e-05,
      "loss": 0.5406,
      "step": 18470
    },
    {
      "epoch": 3.0835975304521943,
      "grad_norm": 9.060860633850098,
      "learning_rate": 1.9489224503648398e-05,
      "loss": 0.8802,
      "step": 18480
    },
    {
      "epoch": 3.0852661438344735,
      "grad_norm": 1.4993141889572144,
      "learning_rate": 1.947225521805532e-05,
      "loss": 0.5292,
      "step": 18490
    },
    {
      "epoch": 3.0869347572167527,
      "grad_norm": 8.88851261138916,
      "learning_rate": 1.9455285932462243e-05,
      "loss": 0.2897,
      "step": 18500
    },
    {
      "epoch": 3.0886033705990323,
      "grad_norm": 9.793776512145996,
      "learning_rate": 1.943831664686917e-05,
      "loss": 0.4537,
      "step": 18510
    },
    {
      "epoch": 3.0902719839813115,
      "grad_norm": 0.7542399764060974,
      "learning_rate": 1.942134736127609e-05,
      "loss": 0.4148,
      "step": 18520
    },
    {
      "epoch": 3.0919405973635907,
      "grad_norm": 6.903501510620117,
      "learning_rate": 1.9404378075683014e-05,
      "loss": 0.5171,
      "step": 18530
    },
    {
      "epoch": 3.0936092107458704,
      "grad_norm": 6.1753339767456055,
      "learning_rate": 1.938740879008994e-05,
      "loss": 0.2861,
      "step": 18540
    },
    {
      "epoch": 3.0952778241281496,
      "grad_norm": 2.7629570960998535,
      "learning_rate": 1.9370439504496862e-05,
      "loss": 0.7721,
      "step": 18550
    },
    {
      "epoch": 3.0969464375104288,
      "grad_norm": 1.550408124923706,
      "learning_rate": 1.9353470218903785e-05,
      "loss": 0.2337,
      "step": 18560
    },
    {
      "epoch": 3.098615050892708,
      "grad_norm": 13.486093521118164,
      "learning_rate": 1.9336500933310707e-05,
      "loss": 0.4799,
      "step": 18570
    },
    {
      "epoch": 3.1002836642749876,
      "grad_norm": 1.7841991186141968,
      "learning_rate": 1.9319531647717633e-05,
      "loss": 0.3027,
      "step": 18580
    },
    {
      "epoch": 3.1019522776572668,
      "grad_norm": 0.7619149088859558,
      "learning_rate": 1.9302562362124555e-05,
      "loss": 0.3046,
      "step": 18590
    },
    {
      "epoch": 3.103620891039546,
      "grad_norm": 9.188558578491211,
      "learning_rate": 1.928559307653148e-05,
      "loss": 0.5243,
      "step": 18600
    },
    {
      "epoch": 3.1052895044218256,
      "grad_norm": 10.011667251586914,
      "learning_rate": 1.9268623790938404e-05,
      "loss": 0.5685,
      "step": 18610
    },
    {
      "epoch": 3.106958117804105,
      "grad_norm": 5.608222961425781,
      "learning_rate": 1.9251654505345326e-05,
      "loss": 0.2809,
      "step": 18620
    },
    {
      "epoch": 3.108626731186384,
      "grad_norm": 8.01570987701416,
      "learning_rate": 1.923468521975225e-05,
      "loss": 0.3814,
      "step": 18630
    },
    {
      "epoch": 3.1102953445686636,
      "grad_norm": 1.118247151374817,
      "learning_rate": 1.921771593415917e-05,
      "loss": 0.4473,
      "step": 18640
    },
    {
      "epoch": 3.111963957950943,
      "grad_norm": 5.681447982788086,
      "learning_rate": 1.9200746648566094e-05,
      "loss": 0.4051,
      "step": 18650
    },
    {
      "epoch": 3.113632571333222,
      "grad_norm": 0.5683812499046326,
      "learning_rate": 1.918377736297302e-05,
      "loss": 0.5351,
      "step": 18660
    },
    {
      "epoch": 3.115301184715501,
      "grad_norm": 2.0027616024017334,
      "learning_rate": 1.9166808077379946e-05,
      "loss": 0.1983,
      "step": 18670
    },
    {
      "epoch": 3.116969798097781,
      "grad_norm": 11.703315734863281,
      "learning_rate": 1.9149838791786868e-05,
      "loss": 0.4246,
      "step": 18680
    },
    {
      "epoch": 3.11863841148006,
      "grad_norm": 8.692164421081543,
      "learning_rate": 1.913286950619379e-05,
      "loss": 0.5243,
      "step": 18690
    },
    {
      "epoch": 3.1203070248623392,
      "grad_norm": 14.210343360900879,
      "learning_rate": 1.9115900220600713e-05,
      "loss": 0.6962,
      "step": 18700
    },
    {
      "epoch": 3.121975638244619,
      "grad_norm": 10.928671836853027,
      "learning_rate": 1.9098930935007635e-05,
      "loss": 0.7899,
      "step": 18710
    },
    {
      "epoch": 3.123644251626898,
      "grad_norm": 0.5615798234939575,
      "learning_rate": 1.9081961649414558e-05,
      "loss": 0.4412,
      "step": 18720
    },
    {
      "epoch": 3.1253128650091773,
      "grad_norm": 5.946246147155762,
      "learning_rate": 1.9064992363821484e-05,
      "loss": 0.2558,
      "step": 18730
    },
    {
      "epoch": 3.126981478391457,
      "grad_norm": 3.3355982303619385,
      "learning_rate": 1.904802307822841e-05,
      "loss": 0.3198,
      "step": 18740
    },
    {
      "epoch": 3.128650091773736,
      "grad_norm": 4.843700885772705,
      "learning_rate": 1.9031053792635332e-05,
      "loss": 0.4002,
      "step": 18750
    },
    {
      "epoch": 3.1303187051560153,
      "grad_norm": 1.2963961362838745,
      "learning_rate": 1.9014084507042255e-05,
      "loss": 0.3051,
      "step": 18760
    },
    {
      "epoch": 3.1319873185382945,
      "grad_norm": 1.6449015140533447,
      "learning_rate": 1.8997115221449177e-05,
      "loss": 0.3187,
      "step": 18770
    },
    {
      "epoch": 3.133655931920574,
      "grad_norm": 12.335567474365234,
      "learning_rate": 1.89801459358561e-05,
      "loss": 0.6798,
      "step": 18780
    },
    {
      "epoch": 3.1353245453028533,
      "grad_norm": 1.0408610105514526,
      "learning_rate": 1.8963176650263026e-05,
      "loss": 0.4126,
      "step": 18790
    },
    {
      "epoch": 3.1369931586851325,
      "grad_norm": 5.1124467849731445,
      "learning_rate": 1.8946207364669948e-05,
      "loss": 0.4178,
      "step": 18800
    },
    {
      "epoch": 3.138661772067412,
      "grad_norm": 2.1241538524627686,
      "learning_rate": 1.892923807907687e-05,
      "loss": 0.3759,
      "step": 18810
    },
    {
      "epoch": 3.1403303854496913,
      "grad_norm": 9.394920349121094,
      "learning_rate": 1.8912268793483796e-05,
      "loss": 0.4795,
      "step": 18820
    },
    {
      "epoch": 3.1419989988319705,
      "grad_norm": 1.6091643571853638,
      "learning_rate": 1.889529950789072e-05,
      "loss": 0.4414,
      "step": 18830
    },
    {
      "epoch": 3.1436676122142497,
      "grad_norm": 0.4959321916103363,
      "learning_rate": 1.887833022229764e-05,
      "loss": 0.4645,
      "step": 18840
    },
    {
      "epoch": 3.1453362255965294,
      "grad_norm": 0.2362225353717804,
      "learning_rate": 1.8861360936704567e-05,
      "loss": 0.3005,
      "step": 18850
    },
    {
      "epoch": 3.1470048389788086,
      "grad_norm": 1.6806480884552002,
      "learning_rate": 1.884439165111149e-05,
      "loss": 0.3393,
      "step": 18860
    },
    {
      "epoch": 3.1486734523610878,
      "grad_norm": 6.080609321594238,
      "learning_rate": 1.8827422365518412e-05,
      "loss": 0.411,
      "step": 18870
    },
    {
      "epoch": 3.1503420657433674,
      "grad_norm": 10.66662883758545,
      "learning_rate": 1.8810453079925335e-05,
      "loss": 0.3498,
      "step": 18880
    },
    {
      "epoch": 3.1520106791256466,
      "grad_norm": 3.0557916164398193,
      "learning_rate": 1.879348379433226e-05,
      "loss": 0.6155,
      "step": 18890
    },
    {
      "epoch": 3.153679292507926,
      "grad_norm": 6.7260332107543945,
      "learning_rate": 1.8776514508739183e-05,
      "loss": 0.3425,
      "step": 18900
    },
    {
      "epoch": 3.1553479058902054,
      "grad_norm": 4.619699954986572,
      "learning_rate": 1.875954522314611e-05,
      "loss": 0.7344,
      "step": 18910
    },
    {
      "epoch": 3.1570165192724846,
      "grad_norm": 4.434485912322998,
      "learning_rate": 1.874257593755303e-05,
      "loss": 0.7296,
      "step": 18920
    },
    {
      "epoch": 3.158685132654764,
      "grad_norm": 4.481062889099121,
      "learning_rate": 1.8725606651959954e-05,
      "loss": 0.3506,
      "step": 18930
    },
    {
      "epoch": 3.1603537460370434,
      "grad_norm": 10.751627922058105,
      "learning_rate": 1.8708637366366876e-05,
      "loss": 0.4555,
      "step": 18940
    },
    {
      "epoch": 3.1620223594193226,
      "grad_norm": 1.7044068574905396,
      "learning_rate": 1.86916680807738e-05,
      "loss": 0.5697,
      "step": 18950
    },
    {
      "epoch": 3.163690972801602,
      "grad_norm": 0.7122927904129028,
      "learning_rate": 1.867469879518072e-05,
      "loss": 0.3874,
      "step": 18960
    },
    {
      "epoch": 3.165359586183881,
      "grad_norm": 8.41396713256836,
      "learning_rate": 1.8657729509587647e-05,
      "loss": 0.4022,
      "step": 18970
    },
    {
      "epoch": 3.1670281995661607,
      "grad_norm": 7.759967803955078,
      "learning_rate": 1.8640760223994573e-05,
      "loss": 0.349,
      "step": 18980
    },
    {
      "epoch": 3.16869681294844,
      "grad_norm": 8.928180694580078,
      "learning_rate": 1.8623790938401496e-05,
      "loss": 0.2924,
      "step": 18990
    },
    {
      "epoch": 3.170365426330719,
      "grad_norm": 12.127985000610352,
      "learning_rate": 1.8606821652808418e-05,
      "loss": 0.4791,
      "step": 19000
    },
    {
      "epoch": 3.1720340397129987,
      "grad_norm": 6.516123294830322,
      "learning_rate": 1.858985236721534e-05,
      "loss": 0.591,
      "step": 19010
    },
    {
      "epoch": 3.173702653095278,
      "grad_norm": 21.940778732299805,
      "learning_rate": 1.8572883081622263e-05,
      "loss": 0.4206,
      "step": 19020
    },
    {
      "epoch": 3.175371266477557,
      "grad_norm": 5.700685501098633,
      "learning_rate": 1.8555913796029186e-05,
      "loss": 0.5442,
      "step": 19030
    },
    {
      "epoch": 3.1770398798598363,
      "grad_norm": 10.3059720993042,
      "learning_rate": 1.853894451043611e-05,
      "loss": 0.3846,
      "step": 19040
    },
    {
      "epoch": 3.178708493242116,
      "grad_norm": 8.83779239654541,
      "learning_rate": 1.8521975224843037e-05,
      "loss": 0.6653,
      "step": 19050
    },
    {
      "epoch": 3.180377106624395,
      "grad_norm": 11.0747652053833,
      "learning_rate": 1.850500593924996e-05,
      "loss": 0.5132,
      "step": 19060
    },
    {
      "epoch": 3.1820457200066743,
      "grad_norm": 5.411035060882568,
      "learning_rate": 1.8488036653656882e-05,
      "loss": 0.1916,
      "step": 19070
    },
    {
      "epoch": 3.183714333388954,
      "grad_norm": 2.0001723766326904,
      "learning_rate": 1.8471067368063805e-05,
      "loss": 0.4648,
      "step": 19080
    },
    {
      "epoch": 3.185382946771233,
      "grad_norm": 5.602260589599609,
      "learning_rate": 1.8454098082470727e-05,
      "loss": 0.8354,
      "step": 19090
    },
    {
      "epoch": 3.1870515601535123,
      "grad_norm": 14.108396530151367,
      "learning_rate": 1.8437128796877653e-05,
      "loss": 0.569,
      "step": 19100
    },
    {
      "epoch": 3.188720173535792,
      "grad_norm": 1.489606261253357,
      "learning_rate": 1.8420159511284576e-05,
      "loss": 0.4287,
      "step": 19110
    },
    {
      "epoch": 3.190388786918071,
      "grad_norm": 6.048004150390625,
      "learning_rate": 1.8403190225691498e-05,
      "loss": 0.6056,
      "step": 19120
    },
    {
      "epoch": 3.1920574003003503,
      "grad_norm": 8.227691650390625,
      "learning_rate": 1.8386220940098424e-05,
      "loss": 0.4662,
      "step": 19130
    },
    {
      "epoch": 3.1937260136826295,
      "grad_norm": 7.969057559967041,
      "learning_rate": 1.8369251654505346e-05,
      "loss": 0.3671,
      "step": 19140
    },
    {
      "epoch": 3.195394627064909,
      "grad_norm": 4.539898872375488,
      "learning_rate": 1.835228236891227e-05,
      "loss": 0.3564,
      "step": 19150
    },
    {
      "epoch": 3.1970632404471884,
      "grad_norm": 3.458418130874634,
      "learning_rate": 1.8335313083319195e-05,
      "loss": 0.5356,
      "step": 19160
    },
    {
      "epoch": 3.1987318538294676,
      "grad_norm": 7.55101203918457,
      "learning_rate": 1.8318343797726117e-05,
      "loss": 0.4413,
      "step": 19170
    },
    {
      "epoch": 3.200400467211747,
      "grad_norm": 9.135916709899902,
      "learning_rate": 1.830137451213304e-05,
      "loss": 0.5884,
      "step": 19180
    },
    {
      "epoch": 3.2020690805940264,
      "grad_norm": 4.006484508514404,
      "learning_rate": 1.8284405226539962e-05,
      "loss": 0.2016,
      "step": 19190
    },
    {
      "epoch": 3.2037376939763056,
      "grad_norm": 2.761937379837036,
      "learning_rate": 1.8267435940946888e-05,
      "loss": 0.5005,
      "step": 19200
    },
    {
      "epoch": 3.205406307358585,
      "grad_norm": 2.677119255065918,
      "learning_rate": 1.825046665535381e-05,
      "loss": 0.5053,
      "step": 19210
    },
    {
      "epoch": 3.2070749207408644,
      "grad_norm": 1.3201857805252075,
      "learning_rate": 1.8233497369760736e-05,
      "loss": 0.6321,
      "step": 19220
    },
    {
      "epoch": 3.2087435341231436,
      "grad_norm": 5.876120090484619,
      "learning_rate": 1.821652808416766e-05,
      "loss": 0.4295,
      "step": 19230
    },
    {
      "epoch": 3.210412147505423,
      "grad_norm": 5.294760227203369,
      "learning_rate": 1.819955879857458e-05,
      "loss": 0.2828,
      "step": 19240
    },
    {
      "epoch": 3.2120807608877024,
      "grad_norm": 2.406464099884033,
      "learning_rate": 1.8182589512981504e-05,
      "loss": 0.4795,
      "step": 19250
    },
    {
      "epoch": 3.2137493742699816,
      "grad_norm": 9.340993881225586,
      "learning_rate": 1.8165620227388426e-05,
      "loss": 0.5749,
      "step": 19260
    },
    {
      "epoch": 3.215417987652261,
      "grad_norm": 14.808595657348633,
      "learning_rate": 1.814865094179535e-05,
      "loss": 0.5013,
      "step": 19270
    },
    {
      "epoch": 3.2170866010345405,
      "grad_norm": 12.060214042663574,
      "learning_rate": 1.8131681656202275e-05,
      "loss": 0.5902,
      "step": 19280
    },
    {
      "epoch": 3.2187552144168197,
      "grad_norm": 2.924593925476074,
      "learning_rate": 1.81147123706092e-05,
      "loss": 0.4887,
      "step": 19290
    },
    {
      "epoch": 3.220423827799099,
      "grad_norm": 4.643330097198486,
      "learning_rate": 1.8097743085016123e-05,
      "loss": 0.3319,
      "step": 19300
    },
    {
      "epoch": 3.2220924411813785,
      "grad_norm": 3.7403969764709473,
      "learning_rate": 1.8080773799423046e-05,
      "loss": 0.5313,
      "step": 19310
    },
    {
      "epoch": 3.2237610545636577,
      "grad_norm": 2.9777235984802246,
      "learning_rate": 1.8063804513829968e-05,
      "loss": 0.6582,
      "step": 19320
    },
    {
      "epoch": 3.225429667945937,
      "grad_norm": 15.683996200561523,
      "learning_rate": 1.804683522823689e-05,
      "loss": 0.3505,
      "step": 19330
    },
    {
      "epoch": 3.227098281328216,
      "grad_norm": 6.15167760848999,
      "learning_rate": 1.8029865942643813e-05,
      "loss": 0.1772,
      "step": 19340
    },
    {
      "epoch": 3.2287668947104957,
      "grad_norm": 0.37560155987739563,
      "learning_rate": 1.801289665705074e-05,
      "loss": 0.4515,
      "step": 19350
    },
    {
      "epoch": 3.230435508092775,
      "grad_norm": 8.428376197814941,
      "learning_rate": 1.7995927371457665e-05,
      "loss": 0.3781,
      "step": 19360
    },
    {
      "epoch": 3.232104121475054,
      "grad_norm": 0.30803200602531433,
      "learning_rate": 1.7978958085864587e-05,
      "loss": 0.36,
      "step": 19370
    },
    {
      "epoch": 3.2337727348573337,
      "grad_norm": 6.039505958557129,
      "learning_rate": 1.796198880027151e-05,
      "loss": 0.6069,
      "step": 19380
    },
    {
      "epoch": 3.235441348239613,
      "grad_norm": 11.536115646362305,
      "learning_rate": 1.7945019514678432e-05,
      "loss": 0.5147,
      "step": 19390
    },
    {
      "epoch": 3.237109961621892,
      "grad_norm": 8.622820854187012,
      "learning_rate": 1.7928050229085355e-05,
      "loss": 0.4415,
      "step": 19400
    },
    {
      "epoch": 3.2387785750041713,
      "grad_norm": 4.888994216918945,
      "learning_rate": 1.791108094349228e-05,
      "loss": 0.6729,
      "step": 19410
    },
    {
      "epoch": 3.240447188386451,
      "grad_norm": 3.025773525238037,
      "learning_rate": 1.7894111657899203e-05,
      "loss": 0.305,
      "step": 19420
    },
    {
      "epoch": 3.24211580176873,
      "grad_norm": 3.0511136054992676,
      "learning_rate": 1.7877142372306126e-05,
      "loss": 0.5486,
      "step": 19430
    },
    {
      "epoch": 3.2437844151510093,
      "grad_norm": 0.5930811762809753,
      "learning_rate": 1.786017308671305e-05,
      "loss": 0.395,
      "step": 19440
    },
    {
      "epoch": 3.245453028533289,
      "grad_norm": 11.383970260620117,
      "learning_rate": 1.7843203801119974e-05,
      "loss": 0.7865,
      "step": 19450
    },
    {
      "epoch": 3.247121641915568,
      "grad_norm": 2.830819845199585,
      "learning_rate": 1.7826234515526896e-05,
      "loss": 0.4132,
      "step": 19460
    },
    {
      "epoch": 3.2487902552978474,
      "grad_norm": 8.802958488464355,
      "learning_rate": 1.7809265229933822e-05,
      "loss": 0.1987,
      "step": 19470
    },
    {
      "epoch": 3.250458868680127,
      "grad_norm": 7.2240400314331055,
      "learning_rate": 1.7792295944340745e-05,
      "loss": 0.5249,
      "step": 19480
    },
    {
      "epoch": 3.252127482062406,
      "grad_norm": 13.495311737060547,
      "learning_rate": 1.7775326658747667e-05,
      "loss": 0.3834,
      "step": 19490
    },
    {
      "epoch": 3.2537960954446854,
      "grad_norm": 5.472856044769287,
      "learning_rate": 1.775835737315459e-05,
      "loss": 0.63,
      "step": 19500
    },
    {
      "epoch": 3.255464708826965,
      "grad_norm": 11.988555908203125,
      "learning_rate": 1.7741388087561516e-05,
      "loss": 0.4975,
      "step": 19510
    },
    {
      "epoch": 3.2571333222092442,
      "grad_norm": 17.946435928344727,
      "learning_rate": 1.7724418801968438e-05,
      "loss": 0.6156,
      "step": 19520
    },
    {
      "epoch": 3.2588019355915234,
      "grad_norm": 5.996913433074951,
      "learning_rate": 1.770744951637536e-05,
      "loss": 0.3986,
      "step": 19530
    },
    {
      "epoch": 3.2604705489738026,
      "grad_norm": 1.4444514513015747,
      "learning_rate": 1.7690480230782287e-05,
      "loss": 0.2994,
      "step": 19540
    },
    {
      "epoch": 3.2621391623560823,
      "grad_norm": 7.042764663696289,
      "learning_rate": 1.767351094518921e-05,
      "loss": 0.481,
      "step": 19550
    },
    {
      "epoch": 3.2638077757383614,
      "grad_norm": 12.174149513244629,
      "learning_rate": 1.765654165959613e-05,
      "loss": 0.4692,
      "step": 19560
    },
    {
      "epoch": 3.2654763891206406,
      "grad_norm": 7.34243106842041,
      "learning_rate": 1.7639572374003054e-05,
      "loss": 0.5244,
      "step": 19570
    },
    {
      "epoch": 3.26714500250292,
      "grad_norm": 6.067040920257568,
      "learning_rate": 1.7622603088409976e-05,
      "loss": 0.4194,
      "step": 19580
    },
    {
      "epoch": 3.2688136158851995,
      "grad_norm": 9.05126953125,
      "learning_rate": 1.7605633802816902e-05,
      "loss": 0.7486,
      "step": 19590
    },
    {
      "epoch": 3.2704822292674787,
      "grad_norm": 3.5029754638671875,
      "learning_rate": 1.7588664517223828e-05,
      "loss": 0.468,
      "step": 19600
    },
    {
      "epoch": 3.272150842649758,
      "grad_norm": 2.209622859954834,
      "learning_rate": 1.757169523163075e-05,
      "loss": 0.3591,
      "step": 19610
    },
    {
      "epoch": 3.2738194560320375,
      "grad_norm": 0.5763607025146484,
      "learning_rate": 1.7554725946037673e-05,
      "loss": 0.2558,
      "step": 19620
    },
    {
      "epoch": 3.2754880694143167,
      "grad_norm": 12.581425666809082,
      "learning_rate": 1.7537756660444596e-05,
      "loss": 0.4362,
      "step": 19630
    },
    {
      "epoch": 3.277156682796596,
      "grad_norm": 6.425782203674316,
      "learning_rate": 1.7520787374851518e-05,
      "loss": 0.1883,
      "step": 19640
    },
    {
      "epoch": 3.2788252961788755,
      "grad_norm": 2.4167397022247314,
      "learning_rate": 1.750381808925844e-05,
      "loss": 0.5481,
      "step": 19650
    },
    {
      "epoch": 3.2804939095611547,
      "grad_norm": 3.05770206451416,
      "learning_rate": 1.7486848803665367e-05,
      "loss": 0.5124,
      "step": 19660
    },
    {
      "epoch": 3.282162522943434,
      "grad_norm": 6.717428684234619,
      "learning_rate": 1.7469879518072292e-05,
      "loss": 0.3753,
      "step": 19670
    },
    {
      "epoch": 3.2838311363257136,
      "grad_norm": 5.846678256988525,
      "learning_rate": 1.7452910232479215e-05,
      "loss": 0.5738,
      "step": 19680
    },
    {
      "epoch": 3.2854997497079927,
      "grad_norm": 0.7155637741088867,
      "learning_rate": 1.7435940946886137e-05,
      "loss": 0.3541,
      "step": 19690
    },
    {
      "epoch": 3.287168363090272,
      "grad_norm": 1.772234320640564,
      "learning_rate": 1.741897166129306e-05,
      "loss": 0.5137,
      "step": 19700
    },
    {
      "epoch": 3.288836976472551,
      "grad_norm": 12.123926162719727,
      "learning_rate": 1.7402002375699982e-05,
      "loss": 0.5822,
      "step": 19710
    },
    {
      "epoch": 3.2905055898548308,
      "grad_norm": 0.33537471294403076,
      "learning_rate": 1.7385033090106908e-05,
      "loss": 0.2791,
      "step": 19720
    },
    {
      "epoch": 3.29217420323711,
      "grad_norm": 19.825952529907227,
      "learning_rate": 1.736806380451383e-05,
      "loss": 0.5816,
      "step": 19730
    },
    {
      "epoch": 3.293842816619389,
      "grad_norm": 15.278820037841797,
      "learning_rate": 1.7351094518920753e-05,
      "loss": 0.4768,
      "step": 19740
    },
    {
      "epoch": 3.295511430001669,
      "grad_norm": 9.237190246582031,
      "learning_rate": 1.733412523332768e-05,
      "loss": 0.2931,
      "step": 19750
    },
    {
      "epoch": 3.297180043383948,
      "grad_norm": 2.789036273956299,
      "learning_rate": 1.73171559477346e-05,
      "loss": 0.3609,
      "step": 19760
    },
    {
      "epoch": 3.298848656766227,
      "grad_norm": 2.97432279586792,
      "learning_rate": 1.7300186662141524e-05,
      "loss": 0.3916,
      "step": 19770
    },
    {
      "epoch": 3.3005172701485064,
      "grad_norm": 10.134295463562012,
      "learning_rate": 1.728321737654845e-05,
      "loss": 0.4285,
      "step": 19780
    },
    {
      "epoch": 3.302185883530786,
      "grad_norm": 3.9239697456359863,
      "learning_rate": 1.7266248090955372e-05,
      "loss": 0.3489,
      "step": 19790
    },
    {
      "epoch": 3.303854496913065,
      "grad_norm": 5.503310203552246,
      "learning_rate": 1.7249278805362295e-05,
      "loss": 0.4814,
      "step": 19800
    },
    {
      "epoch": 3.3055231102953444,
      "grad_norm": 11.794273376464844,
      "learning_rate": 1.7232309519769217e-05,
      "loss": 0.7134,
      "step": 19810
    },
    {
      "epoch": 3.307191723677624,
      "grad_norm": 7.065369606018066,
      "learning_rate": 1.7215340234176143e-05,
      "loss": 0.4808,
      "step": 19820
    },
    {
      "epoch": 3.3088603370599032,
      "grad_norm": 5.223686218261719,
      "learning_rate": 1.7198370948583066e-05,
      "loss": 0.5266,
      "step": 19830
    },
    {
      "epoch": 3.3105289504421824,
      "grad_norm": 10.146659851074219,
      "learning_rate": 1.7181401662989988e-05,
      "loss": 0.4546,
      "step": 19840
    },
    {
      "epoch": 3.312197563824462,
      "grad_norm": 7.652139663696289,
      "learning_rate": 1.7164432377396914e-05,
      "loss": 0.3469,
      "step": 19850
    },
    {
      "epoch": 3.3138661772067413,
      "grad_norm": 12.240181922912598,
      "learning_rate": 1.7147463091803837e-05,
      "loss": 0.3747,
      "step": 19860
    },
    {
      "epoch": 3.3155347905890205,
      "grad_norm": 6.583772659301758,
      "learning_rate": 1.713049380621076e-05,
      "loss": 0.7126,
      "step": 19870
    },
    {
      "epoch": 3.3172034039713,
      "grad_norm": 4.9106597900390625,
      "learning_rate": 1.711352452061768e-05,
      "loss": 0.5784,
      "step": 19880
    },
    {
      "epoch": 3.3188720173535793,
      "grad_norm": 0.3188406825065613,
      "learning_rate": 1.7096555235024604e-05,
      "loss": 0.451,
      "step": 19890
    },
    {
      "epoch": 3.3205406307358585,
      "grad_norm": 3.547741174697876,
      "learning_rate": 1.707958594943153e-05,
      "loss": 0.3576,
      "step": 19900
    },
    {
      "epoch": 3.3222092441181377,
      "grad_norm": 9.296951293945312,
      "learning_rate": 1.7062616663838456e-05,
      "loss": 0.6379,
      "step": 19910
    },
    {
      "epoch": 3.3238778575004173,
      "grad_norm": 7.204918384552002,
      "learning_rate": 1.7045647378245378e-05,
      "loss": 0.4704,
      "step": 19920
    },
    {
      "epoch": 3.3255464708826965,
      "grad_norm": 3.3994009494781494,
      "learning_rate": 1.70286780926523e-05,
      "loss": 0.4176,
      "step": 19930
    },
    {
      "epoch": 3.3272150842649757,
      "grad_norm": 5.600364685058594,
      "learning_rate": 1.7011708807059223e-05,
      "loss": 0.3195,
      "step": 19940
    },
    {
      "epoch": 3.328883697647255,
      "grad_norm": 11.55344009399414,
      "learning_rate": 1.6994739521466146e-05,
      "loss": 0.561,
      "step": 19950
    },
    {
      "epoch": 3.3305523110295345,
      "grad_norm": 11.506808280944824,
      "learning_rate": 1.6977770235873068e-05,
      "loss": 0.3007,
      "step": 19960
    },
    {
      "epoch": 3.3322209244118137,
      "grad_norm": 6.991949558258057,
      "learning_rate": 1.6960800950279994e-05,
      "loss": 0.8246,
      "step": 19970
    },
    {
      "epoch": 3.333889537794093,
      "grad_norm": 8.681049346923828,
      "learning_rate": 1.694383166468692e-05,
      "loss": 0.555,
      "step": 19980
    },
    {
      "epoch": 3.3355581511763726,
      "grad_norm": 11.291352272033691,
      "learning_rate": 1.6926862379093842e-05,
      "loss": 0.3674,
      "step": 19990
    },
    {
      "epoch": 3.3372267645586517,
      "grad_norm": 13.64377212524414,
      "learning_rate": 1.6909893093500765e-05,
      "loss": 0.3885,
      "step": 20000
    },
    {
      "epoch": 3.338895377940931,
      "grad_norm": 7.508914947509766,
      "learning_rate": 1.6892923807907687e-05,
      "loss": 0.535,
      "step": 20010
    },
    {
      "epoch": 3.3405639913232106,
      "grad_norm": 9.524528503417969,
      "learning_rate": 1.687595452231461e-05,
      "loss": 0.3861,
      "step": 20020
    },
    {
      "epoch": 3.3422326047054898,
      "grad_norm": 9.90135669708252,
      "learning_rate": 1.6858985236721536e-05,
      "loss": 0.6757,
      "step": 20030
    },
    {
      "epoch": 3.343901218087769,
      "grad_norm": 10.78982162475586,
      "learning_rate": 1.6842015951128458e-05,
      "loss": 0.3972,
      "step": 20040
    },
    {
      "epoch": 3.3455698314700486,
      "grad_norm": 0.4485514760017395,
      "learning_rate": 1.682504666553538e-05,
      "loss": 0.3003,
      "step": 20050
    },
    {
      "epoch": 3.347238444852328,
      "grad_norm": 6.521347999572754,
      "learning_rate": 1.6808077379942307e-05,
      "loss": 0.4468,
      "step": 20060
    },
    {
      "epoch": 3.348907058234607,
      "grad_norm": 2.4084646701812744,
      "learning_rate": 1.679110809434923e-05,
      "loss": 0.4976,
      "step": 20070
    },
    {
      "epoch": 3.350575671616886,
      "grad_norm": 7.462820529937744,
      "learning_rate": 1.677413880875615e-05,
      "loss": 0.423,
      "step": 20080
    },
    {
      "epoch": 3.352244284999166,
      "grad_norm": 3.0264017581939697,
      "learning_rate": 1.6757169523163074e-05,
      "loss": 0.4413,
      "step": 20090
    },
    {
      "epoch": 3.353912898381445,
      "grad_norm": 10.238470077514648,
      "learning_rate": 1.674020023757e-05,
      "loss": 0.658,
      "step": 20100
    },
    {
      "epoch": 3.355581511763724,
      "grad_norm": 12.372074127197266,
      "learning_rate": 1.6723230951976922e-05,
      "loss": 0.5689,
      "step": 20110
    },
    {
      "epoch": 3.357250125146004,
      "grad_norm": 10.47078800201416,
      "learning_rate": 1.6706261666383845e-05,
      "loss": 0.5151,
      "step": 20120
    },
    {
      "epoch": 3.358918738528283,
      "grad_norm": 18.527141571044922,
      "learning_rate": 1.668929238079077e-05,
      "loss": 0.5534,
      "step": 20130
    },
    {
      "epoch": 3.3605873519105622,
      "grad_norm": 7.341375827789307,
      "learning_rate": 1.6672323095197693e-05,
      "loss": 0.2782,
      "step": 20140
    },
    {
      "epoch": 3.3622559652928414,
      "grad_norm": 14.6495361328125,
      "learning_rate": 1.6655353809604616e-05,
      "loss": 0.4162,
      "step": 20150
    },
    {
      "epoch": 3.363924578675121,
      "grad_norm": 8.098702430725098,
      "learning_rate": 1.663838452401154e-05,
      "loss": 0.6801,
      "step": 20160
    },
    {
      "epoch": 3.3655931920574003,
      "grad_norm": 10.777639389038086,
      "learning_rate": 1.6621415238418464e-05,
      "loss": 0.4965,
      "step": 20170
    },
    {
      "epoch": 3.3672618054396795,
      "grad_norm": 8.5531587600708,
      "learning_rate": 1.6604445952825387e-05,
      "loss": 0.4271,
      "step": 20180
    },
    {
      "epoch": 3.368930418821959,
      "grad_norm": 11.739169120788574,
      "learning_rate": 1.658747666723231e-05,
      "loss": 0.7558,
      "step": 20190
    },
    {
      "epoch": 3.3705990322042383,
      "grad_norm": 5.697546482086182,
      "learning_rate": 1.657050738163923e-05,
      "loss": 0.3336,
      "step": 20200
    },
    {
      "epoch": 3.3722676455865175,
      "grad_norm": 10.129261016845703,
      "learning_rate": 1.6553538096046157e-05,
      "loss": 0.5848,
      "step": 20210
    },
    {
      "epoch": 3.373936258968797,
      "grad_norm": 8.80758285522461,
      "learning_rate": 1.6536568810453083e-05,
      "loss": 0.7287,
      "step": 20220
    },
    {
      "epoch": 3.3756048723510763,
      "grad_norm": 9.02096176147461,
      "learning_rate": 1.6519599524860006e-05,
      "loss": 0.5692,
      "step": 20230
    },
    {
      "epoch": 3.3772734857333555,
      "grad_norm": 6.337288856506348,
      "learning_rate": 1.650263023926693e-05,
      "loss": 0.5999,
      "step": 20240
    },
    {
      "epoch": 3.378942099115635,
      "grad_norm": 3.4328291416168213,
      "learning_rate": 1.648566095367385e-05,
      "loss": 0.4877,
      "step": 20250
    },
    {
      "epoch": 3.3806107124979143,
      "grad_norm": 13.210273742675781,
      "learning_rate": 1.6468691668080773e-05,
      "loss": 0.5678,
      "step": 20260
    },
    {
      "epoch": 3.3822793258801935,
      "grad_norm": 2.832502603530884,
      "learning_rate": 1.6451722382487696e-05,
      "loss": 0.477,
      "step": 20270
    },
    {
      "epoch": 3.3839479392624727,
      "grad_norm": 13.62204360961914,
      "learning_rate": 1.643475309689462e-05,
      "loss": 0.6899,
      "step": 20280
    },
    {
      "epoch": 3.3856165526447524,
      "grad_norm": 5.98393440246582,
      "learning_rate": 1.6417783811301548e-05,
      "loss": 0.2589,
      "step": 20290
    },
    {
      "epoch": 3.3872851660270316,
      "grad_norm": 3.901543378829956,
      "learning_rate": 1.640081452570847e-05,
      "loss": 0.4512,
      "step": 20300
    },
    {
      "epoch": 3.3889537794093108,
      "grad_norm": 6.839956760406494,
      "learning_rate": 1.6383845240115393e-05,
      "loss": 0.2615,
      "step": 20310
    },
    {
      "epoch": 3.39062239279159,
      "grad_norm": 16.691200256347656,
      "learning_rate": 1.6366875954522315e-05,
      "loss": 0.3537,
      "step": 20320
    },
    {
      "epoch": 3.3922910061738696,
      "grad_norm": 1.830713152885437,
      "learning_rate": 1.6349906668929237e-05,
      "loss": 0.1649,
      "step": 20330
    },
    {
      "epoch": 3.3939596195561488,
      "grad_norm": 11.199603080749512,
      "learning_rate": 1.633293738333616e-05,
      "loss": 0.6807,
      "step": 20340
    },
    {
      "epoch": 3.395628232938428,
      "grad_norm": 6.997709274291992,
      "learning_rate": 1.6315968097743086e-05,
      "loss": 0.5913,
      "step": 20350
    },
    {
      "epoch": 3.3972968463207076,
      "grad_norm": 7.984649658203125,
      "learning_rate": 1.629899881215001e-05,
      "loss": 0.5137,
      "step": 20360
    },
    {
      "epoch": 3.398965459702987,
      "grad_norm": 4.023537635803223,
      "learning_rate": 1.6282029526556934e-05,
      "loss": 0.4179,
      "step": 20370
    },
    {
      "epoch": 3.400634073085266,
      "grad_norm": 10.410218238830566,
      "learning_rate": 1.6265060240963857e-05,
      "loss": 0.326,
      "step": 20380
    },
    {
      "epoch": 3.4023026864675456,
      "grad_norm": 2.110537528991699,
      "learning_rate": 1.624809095537078e-05,
      "loss": 0.5344,
      "step": 20390
    },
    {
      "epoch": 3.403971299849825,
      "grad_norm": 10.393730163574219,
      "learning_rate": 1.62311216697777e-05,
      "loss": 0.4396,
      "step": 20400
    },
    {
      "epoch": 3.405639913232104,
      "grad_norm": 17.665077209472656,
      "learning_rate": 1.6214152384184628e-05,
      "loss": 0.3047,
      "step": 20410
    },
    {
      "epoch": 3.4073085266143837,
      "grad_norm": 3.7330429553985596,
      "learning_rate": 1.619718309859155e-05,
      "loss": 0.4333,
      "step": 20420
    },
    {
      "epoch": 3.408977139996663,
      "grad_norm": 0.7851229310035706,
      "learning_rate": 1.6180213812998473e-05,
      "loss": 0.4949,
      "step": 20430
    },
    {
      "epoch": 3.410645753378942,
      "grad_norm": 0.3753877282142639,
      "learning_rate": 1.61632445274054e-05,
      "loss": 0.511,
      "step": 20440
    },
    {
      "epoch": 3.4123143667612212,
      "grad_norm": 10.15046501159668,
      "learning_rate": 1.614627524181232e-05,
      "loss": 0.3933,
      "step": 20450
    },
    {
      "epoch": 3.413982980143501,
      "grad_norm": 9.268479347229004,
      "learning_rate": 1.6129305956219243e-05,
      "loss": 0.6047,
      "step": 20460
    },
    {
      "epoch": 3.41565159352578,
      "grad_norm": 3.59303617477417,
      "learning_rate": 1.611233667062617e-05,
      "loss": 0.477,
      "step": 20470
    },
    {
      "epoch": 3.4173202069080593,
      "grad_norm": 10.973104476928711,
      "learning_rate": 1.6095367385033092e-05,
      "loss": 0.7166,
      "step": 20480
    },
    {
      "epoch": 3.418988820290339,
      "grad_norm": 8.447543144226074,
      "learning_rate": 1.6078398099440014e-05,
      "loss": 0.2969,
      "step": 20490
    },
    {
      "epoch": 3.420657433672618,
      "grad_norm": 4.825030326843262,
      "learning_rate": 1.6061428813846937e-05,
      "loss": 0.6205,
      "step": 20500
    },
    {
      "epoch": 3.4223260470548973,
      "grad_norm": 10.593999862670898,
      "learning_rate": 1.604445952825386e-05,
      "loss": 0.6443,
      "step": 20510
    },
    {
      "epoch": 3.4239946604371765,
      "grad_norm": 7.862653732299805,
      "learning_rate": 1.6027490242660785e-05,
      "loss": 0.6641,
      "step": 20520
    },
    {
      "epoch": 3.425663273819456,
      "grad_norm": 5.483391761779785,
      "learning_rate": 1.601052095706771e-05,
      "loss": 0.4178,
      "step": 20530
    },
    {
      "epoch": 3.4273318872017353,
      "grad_norm": 8.239118576049805,
      "learning_rate": 1.5993551671474633e-05,
      "loss": 0.4755,
      "step": 20540
    },
    {
      "epoch": 3.4290005005840145,
      "grad_norm": 6.912777900695801,
      "learning_rate": 1.5976582385881556e-05,
      "loss": 0.6,
      "step": 20550
    },
    {
      "epoch": 3.430669113966294,
      "grad_norm": 15.363707542419434,
      "learning_rate": 1.595961310028848e-05,
      "loss": 0.569,
      "step": 20560
    },
    {
      "epoch": 3.4323377273485733,
      "grad_norm": 7.1598801612854,
      "learning_rate": 1.59426438146954e-05,
      "loss": 0.4386,
      "step": 20570
    },
    {
      "epoch": 3.4340063407308525,
      "grad_norm": 3.2205750942230225,
      "learning_rate": 1.5925674529102323e-05,
      "loss": 0.5074,
      "step": 20580
    },
    {
      "epoch": 3.435674954113132,
      "grad_norm": 6.94536828994751,
      "learning_rate": 1.590870524350925e-05,
      "loss": 0.5602,
      "step": 20590
    },
    {
      "epoch": 3.4373435674954114,
      "grad_norm": 7.437716484069824,
      "learning_rate": 1.5891735957916175e-05,
      "loss": 0.3228,
      "step": 20600
    },
    {
      "epoch": 3.4390121808776906,
      "grad_norm": 12.47095012664795,
      "learning_rate": 1.5874766672323098e-05,
      "loss": 0.4734,
      "step": 20610
    },
    {
      "epoch": 3.44068079425997,
      "grad_norm": 5.14382791519165,
      "learning_rate": 1.585779738673002e-05,
      "loss": 0.4171,
      "step": 20620
    },
    {
      "epoch": 3.4423494076422494,
      "grad_norm": 1.0111398696899414,
      "learning_rate": 1.5840828101136943e-05,
      "loss": 0.2619,
      "step": 20630
    },
    {
      "epoch": 3.4440180210245286,
      "grad_norm": 4.178004741668701,
      "learning_rate": 1.5823858815543865e-05,
      "loss": 0.6479,
      "step": 20640
    },
    {
      "epoch": 3.445686634406808,
      "grad_norm": 15.08165454864502,
      "learning_rate": 1.5806889529950788e-05,
      "loss": 0.3745,
      "step": 20650
    },
    {
      "epoch": 3.4473552477890874,
      "grad_norm": 11.384830474853516,
      "learning_rate": 1.5789920244357713e-05,
      "loss": 0.55,
      "step": 20660
    },
    {
      "epoch": 3.4490238611713666,
      "grad_norm": 8.256916046142578,
      "learning_rate": 1.5772950958764636e-05,
      "loss": 0.4699,
      "step": 20670
    },
    {
      "epoch": 3.450692474553646,
      "grad_norm": 14.916784286499023,
      "learning_rate": 1.5755981673171562e-05,
      "loss": 0.452,
      "step": 20680
    },
    {
      "epoch": 3.452361087935925,
      "grad_norm": 10.515832901000977,
      "learning_rate": 1.5739012387578484e-05,
      "loss": 0.3406,
      "step": 20690
    },
    {
      "epoch": 3.4540297013182046,
      "grad_norm": 10.278810501098633,
      "learning_rate": 1.5722043101985407e-05,
      "loss": 0.517,
      "step": 20700
    },
    {
      "epoch": 3.455698314700484,
      "grad_norm": 9.37950325012207,
      "learning_rate": 1.570507381639233e-05,
      "loss": 0.5566,
      "step": 20710
    },
    {
      "epoch": 3.457366928082763,
      "grad_norm": 3.4114303588867188,
      "learning_rate": 1.5688104530799255e-05,
      "loss": 0.615,
      "step": 20720
    },
    {
      "epoch": 3.4590355414650427,
      "grad_norm": 5.32064962387085,
      "learning_rate": 1.5671135245206178e-05,
      "loss": 0.2718,
      "step": 20730
    },
    {
      "epoch": 3.460704154847322,
      "grad_norm": 7.964839458465576,
      "learning_rate": 1.56541659596131e-05,
      "loss": 0.7074,
      "step": 20740
    },
    {
      "epoch": 3.462372768229601,
      "grad_norm": 8.693984031677246,
      "learning_rate": 1.5637196674020026e-05,
      "loss": 0.3858,
      "step": 20750
    },
    {
      "epoch": 3.4640413816118807,
      "grad_norm": 4.706393718719482,
      "learning_rate": 1.562022738842695e-05,
      "loss": 0.3458,
      "step": 20760
    },
    {
      "epoch": 3.46570999499416,
      "grad_norm": 4.906608581542969,
      "learning_rate": 1.560325810283387e-05,
      "loss": 0.3012,
      "step": 20770
    },
    {
      "epoch": 3.467378608376439,
      "grad_norm": 3.8400254249572754,
      "learning_rate": 1.5586288817240797e-05,
      "loss": 0.286,
      "step": 20780
    },
    {
      "epoch": 3.4690472217587187,
      "grad_norm": 5.025485515594482,
      "learning_rate": 1.556931953164772e-05,
      "loss": 0.7002,
      "step": 20790
    },
    {
      "epoch": 3.470715835140998,
      "grad_norm": 0.9924274682998657,
      "learning_rate": 1.5552350246054642e-05,
      "loss": 0.3328,
      "step": 20800
    },
    {
      "epoch": 3.472384448523277,
      "grad_norm": 9.180130958557129,
      "learning_rate": 1.5535380960461564e-05,
      "loss": 0.4468,
      "step": 20810
    },
    {
      "epoch": 3.4740530619055563,
      "grad_norm": 1.6697840690612793,
      "learning_rate": 1.5518411674868487e-05,
      "loss": 0.459,
      "step": 20820
    },
    {
      "epoch": 3.475721675287836,
      "grad_norm": 10.24489974975586,
      "learning_rate": 1.5501442389275413e-05,
      "loss": 0.6226,
      "step": 20830
    },
    {
      "epoch": 3.477390288670115,
      "grad_norm": 15.845505714416504,
      "learning_rate": 1.548447310368234e-05,
      "loss": 0.6232,
      "step": 20840
    },
    {
      "epoch": 3.4790589020523943,
      "grad_norm": 18.438344955444336,
      "learning_rate": 1.546750381808926e-05,
      "loss": 0.4607,
      "step": 20850
    },
    {
      "epoch": 3.480727515434674,
      "grad_norm": 5.212401866912842,
      "learning_rate": 1.5450534532496183e-05,
      "loss": 0.2405,
      "step": 20860
    },
    {
      "epoch": 3.482396128816953,
      "grad_norm": 0.3412419557571411,
      "learning_rate": 1.5433565246903106e-05,
      "loss": 0.5733,
      "step": 20870
    },
    {
      "epoch": 3.4840647421992323,
      "grad_norm": 13.532567024230957,
      "learning_rate": 1.541659596131003e-05,
      "loss": 0.4767,
      "step": 20880
    },
    {
      "epoch": 3.4857333555815115,
      "grad_norm": 9.521524429321289,
      "learning_rate": 1.539962667571695e-05,
      "loss": 0.4778,
      "step": 20890
    },
    {
      "epoch": 3.487401968963791,
      "grad_norm": 14.475929260253906,
      "learning_rate": 1.5382657390123877e-05,
      "loss": 0.5425,
      "step": 20900
    },
    {
      "epoch": 3.4890705823460704,
      "grad_norm": 19.849130630493164,
      "learning_rate": 1.5365688104530803e-05,
      "loss": 0.8958,
      "step": 20910
    },
    {
      "epoch": 3.4907391957283496,
      "grad_norm": 6.9020161628723145,
      "learning_rate": 1.5348718818937725e-05,
      "loss": 0.434,
      "step": 20920
    },
    {
      "epoch": 3.492407809110629,
      "grad_norm": 8.842329025268555,
      "learning_rate": 1.5331749533344648e-05,
      "loss": 0.5402,
      "step": 20930
    },
    {
      "epoch": 3.4940764224929084,
      "grad_norm": 1.5121288299560547,
      "learning_rate": 1.531478024775157e-05,
      "loss": 0.5017,
      "step": 20940
    },
    {
      "epoch": 3.4957450358751876,
      "grad_norm": 5.423649787902832,
      "learning_rate": 1.5297810962158493e-05,
      "loss": 0.3937,
      "step": 20950
    },
    {
      "epoch": 3.4974136492574672,
      "grad_norm": 3.5963244438171387,
      "learning_rate": 1.5280841676565415e-05,
      "loss": 0.5008,
      "step": 20960
    },
    {
      "epoch": 3.4990822626397464,
      "grad_norm": 4.132384777069092,
      "learning_rate": 1.526387239097234e-05,
      "loss": 0.7067,
      "step": 20970
    },
    {
      "epoch": 3.5007508760220256,
      "grad_norm": 3.2073447704315186,
      "learning_rate": 1.5246903105379265e-05,
      "loss": 0.5329,
      "step": 20980
    },
    {
      "epoch": 3.5024194894043053,
      "grad_norm": 8.167985916137695,
      "learning_rate": 1.5229933819786188e-05,
      "loss": 0.337,
      "step": 20990
    },
    {
      "epoch": 3.5040881027865844,
      "grad_norm": 4.239880084991455,
      "learning_rate": 1.521296453419311e-05,
      "loss": 0.3825,
      "step": 21000
    },
    {
      "epoch": 3.5057567161688636,
      "grad_norm": 10.806497573852539,
      "learning_rate": 1.5195995248600034e-05,
      "loss": 0.3544,
      "step": 21010
    },
    {
      "epoch": 3.507425329551143,
      "grad_norm": 6.168087005615234,
      "learning_rate": 1.5179025963006957e-05,
      "loss": 0.5339,
      "step": 21020
    },
    {
      "epoch": 3.5090939429334225,
      "grad_norm": 7.979010105133057,
      "learning_rate": 1.5162056677413883e-05,
      "loss": 0.4925,
      "step": 21030
    },
    {
      "epoch": 3.5107625563157017,
      "grad_norm": 4.4941816329956055,
      "learning_rate": 1.5145087391820805e-05,
      "loss": 0.519,
      "step": 21040
    },
    {
      "epoch": 3.512431169697981,
      "grad_norm": 1.1671048402786255,
      "learning_rate": 1.512811810622773e-05,
      "loss": 0.1819,
      "step": 21050
    },
    {
      "epoch": 3.51409978308026,
      "grad_norm": 6.213686466217041,
      "learning_rate": 1.5111148820634652e-05,
      "loss": 0.3193,
      "step": 21060
    },
    {
      "epoch": 3.5157683964625397,
      "grad_norm": 6.2947773933410645,
      "learning_rate": 1.5094179535041574e-05,
      "loss": 0.347,
      "step": 21070
    },
    {
      "epoch": 3.517437009844819,
      "grad_norm": 1.5039218664169312,
      "learning_rate": 1.5077210249448498e-05,
      "loss": 0.3383,
      "step": 21080
    },
    {
      "epoch": 3.519105623227098,
      "grad_norm": 9.987080574035645,
      "learning_rate": 1.5060240963855424e-05,
      "loss": 0.4925,
      "step": 21090
    },
    {
      "epoch": 3.5207742366093777,
      "grad_norm": 10.056838989257812,
      "learning_rate": 1.5043271678262347e-05,
      "loss": 0.5284,
      "step": 21100
    },
    {
      "epoch": 3.522442849991657,
      "grad_norm": 7.4521050453186035,
      "learning_rate": 1.502630239266927e-05,
      "loss": 0.4648,
      "step": 21110
    },
    {
      "epoch": 3.524111463373936,
      "grad_norm": 3.6064648628234863,
      "learning_rate": 1.5009333107076194e-05,
      "loss": 0.3473,
      "step": 21120
    },
    {
      "epoch": 3.5257800767562157,
      "grad_norm": 5.653326988220215,
      "learning_rate": 1.4992363821483116e-05,
      "loss": 0.4328,
      "step": 21130
    },
    {
      "epoch": 3.527448690138495,
      "grad_norm": 1.6471236944198608,
      "learning_rate": 1.4975394535890038e-05,
      "loss": 0.4703,
      "step": 21140
    },
    {
      "epoch": 3.529117303520774,
      "grad_norm": 5.682011127471924,
      "learning_rate": 1.4958425250296963e-05,
      "loss": 0.7566,
      "step": 21150
    },
    {
      "epoch": 3.5307859169030538,
      "grad_norm": 11.125340461730957,
      "learning_rate": 1.4941455964703887e-05,
      "loss": 0.486,
      "step": 21160
    },
    {
      "epoch": 3.532454530285333,
      "grad_norm": 1.5724265575408936,
      "learning_rate": 1.4924486679110811e-05,
      "loss": 0.4007,
      "step": 21170
    },
    {
      "epoch": 3.534123143667612,
      "grad_norm": 10.306073188781738,
      "learning_rate": 1.4907517393517734e-05,
      "loss": 0.3058,
      "step": 21180
    },
    {
      "epoch": 3.535791757049892,
      "grad_norm": 3.4616317749023438,
      "learning_rate": 1.4890548107924656e-05,
      "loss": 0.4263,
      "step": 21190
    },
    {
      "epoch": 3.537460370432171,
      "grad_norm": 7.756343841552734,
      "learning_rate": 1.487357882233158e-05,
      "loss": 0.4318,
      "step": 21200
    },
    {
      "epoch": 3.53912898381445,
      "grad_norm": 2.6570348739624023,
      "learning_rate": 1.4856609536738503e-05,
      "loss": 0.3337,
      "step": 21210
    },
    {
      "epoch": 3.5407975971967294,
      "grad_norm": 9.045649528503418,
      "learning_rate": 1.4839640251145429e-05,
      "loss": 0.5107,
      "step": 21220
    },
    {
      "epoch": 3.5424662105790086,
      "grad_norm": 5.212908744812012,
      "learning_rate": 1.4822670965552351e-05,
      "loss": 0.4709,
      "step": 21230
    },
    {
      "epoch": 3.544134823961288,
      "grad_norm": 3.553218126296997,
      "learning_rate": 1.4805701679959275e-05,
      "loss": 0.527,
      "step": 21240
    },
    {
      "epoch": 3.5458034373435674,
      "grad_norm": 6.549098491668701,
      "learning_rate": 1.4788732394366198e-05,
      "loss": 0.6011,
      "step": 21250
    },
    {
      "epoch": 3.5474720507258466,
      "grad_norm": 8.731361389160156,
      "learning_rate": 1.477176310877312e-05,
      "loss": 0.4718,
      "step": 21260
    },
    {
      "epoch": 3.5491406641081262,
      "grad_norm": 1.553290605545044,
      "learning_rate": 1.4754793823180044e-05,
      "loss": 0.5089,
      "step": 21270
    },
    {
      "epoch": 3.5508092774904054,
      "grad_norm": 11.226030349731445,
      "learning_rate": 1.473782453758697e-05,
      "loss": 0.8424,
      "step": 21280
    },
    {
      "epoch": 3.5524778908726846,
      "grad_norm": 8.137680053710938,
      "learning_rate": 1.4720855251993893e-05,
      "loss": 0.3836,
      "step": 21290
    },
    {
      "epoch": 3.5541465042549643,
      "grad_norm": 11.12643814086914,
      "learning_rate": 1.4703885966400815e-05,
      "loss": 0.6456,
      "step": 21300
    },
    {
      "epoch": 3.5558151176372434,
      "grad_norm": 5.442044734954834,
      "learning_rate": 1.4686916680807738e-05,
      "loss": 0.4748,
      "step": 21310
    },
    {
      "epoch": 3.5574837310195226,
      "grad_norm": 6.1447577476501465,
      "learning_rate": 1.4669947395214662e-05,
      "loss": 0.3953,
      "step": 21320
    },
    {
      "epoch": 3.5591523444018023,
      "grad_norm": 6.6674394607543945,
      "learning_rate": 1.4652978109621584e-05,
      "loss": 0.4732,
      "step": 21330
    },
    {
      "epoch": 3.5608209577840815,
      "grad_norm": 6.3155436515808105,
      "learning_rate": 1.463600882402851e-05,
      "loss": 0.6333,
      "step": 21340
    },
    {
      "epoch": 3.5624895711663607,
      "grad_norm": 9.798623085021973,
      "learning_rate": 1.4619039538435433e-05,
      "loss": 0.6668,
      "step": 21350
    },
    {
      "epoch": 3.5641581845486403,
      "grad_norm": 2.5988616943359375,
      "learning_rate": 1.4602070252842357e-05,
      "loss": 0.4451,
      "step": 21360
    },
    {
      "epoch": 3.5658267979309195,
      "grad_norm": 5.440179347991943,
      "learning_rate": 1.458510096724928e-05,
      "loss": 0.4895,
      "step": 21370
    },
    {
      "epoch": 3.5674954113131987,
      "grad_norm": 30.792081832885742,
      "learning_rate": 1.4568131681656202e-05,
      "loss": 0.6458,
      "step": 21380
    },
    {
      "epoch": 3.5691640246954783,
      "grad_norm": 1.3749133348464966,
      "learning_rate": 1.4551162396063126e-05,
      "loss": 0.27,
      "step": 21390
    },
    {
      "epoch": 3.5708326380777575,
      "grad_norm": 1.0775665044784546,
      "learning_rate": 1.4534193110470052e-05,
      "loss": 0.2634,
      "step": 21400
    },
    {
      "epoch": 3.5725012514600367,
      "grad_norm": 3.1524503231048584,
      "learning_rate": 1.4517223824876974e-05,
      "loss": 0.3852,
      "step": 21410
    },
    {
      "epoch": 3.574169864842316,
      "grad_norm": 18.372272491455078,
      "learning_rate": 1.4500254539283897e-05,
      "loss": 0.6167,
      "step": 21420
    },
    {
      "epoch": 3.575838478224595,
      "grad_norm": 7.769835948944092,
      "learning_rate": 1.4483285253690821e-05,
      "loss": 0.3894,
      "step": 21430
    },
    {
      "epoch": 3.5775070916068747,
      "grad_norm": 5.4874982833862305,
      "learning_rate": 1.4466315968097744e-05,
      "loss": 0.5323,
      "step": 21440
    },
    {
      "epoch": 3.579175704989154,
      "grad_norm": 6.033565521240234,
      "learning_rate": 1.4449346682504666e-05,
      "loss": 0.4781,
      "step": 21450
    },
    {
      "epoch": 3.580844318371433,
      "grad_norm": 3.322904109954834,
      "learning_rate": 1.443237739691159e-05,
      "loss": 0.5039,
      "step": 21460
    },
    {
      "epoch": 3.5825129317537128,
      "grad_norm": 5.682880401611328,
      "learning_rate": 1.4415408111318514e-05,
      "loss": 0.4262,
      "step": 21470
    },
    {
      "epoch": 3.584181545135992,
      "grad_norm": 0.7657763957977295,
      "learning_rate": 1.4398438825725439e-05,
      "loss": 0.5507,
      "step": 21480
    },
    {
      "epoch": 3.585850158518271,
      "grad_norm": 6.098840236663818,
      "learning_rate": 1.4381469540132361e-05,
      "loss": 0.4874,
      "step": 21490
    },
    {
      "epoch": 3.587518771900551,
      "grad_norm": 6.807427406311035,
      "learning_rate": 1.4364500254539284e-05,
      "loss": 0.3568,
      "step": 21500
    },
    {
      "epoch": 3.58918738528283,
      "grad_norm": 10.13443374633789,
      "learning_rate": 1.4347530968946208e-05,
      "loss": 0.3748,
      "step": 21510
    },
    {
      "epoch": 3.590855998665109,
      "grad_norm": 3.2965798377990723,
      "learning_rate": 1.433056168335313e-05,
      "loss": 0.5615,
      "step": 21520
    },
    {
      "epoch": 3.592524612047389,
      "grad_norm": 9.109135627746582,
      "learning_rate": 1.4313592397760056e-05,
      "loss": 0.3911,
      "step": 21530
    },
    {
      "epoch": 3.594193225429668,
      "grad_norm": 1.231452465057373,
      "learning_rate": 1.4296623112166979e-05,
      "loss": 0.5144,
      "step": 21540
    },
    {
      "epoch": 3.595861838811947,
      "grad_norm": 14.46680736541748,
      "learning_rate": 1.4279653826573903e-05,
      "loss": 0.4186,
      "step": 21550
    },
    {
      "epoch": 3.597530452194227,
      "grad_norm": 9.391562461853027,
      "learning_rate": 1.4262684540980825e-05,
      "loss": 0.6589,
      "step": 21560
    },
    {
      "epoch": 3.599199065576506,
      "grad_norm": 9.873108863830566,
      "learning_rate": 1.4245715255387748e-05,
      "loss": 0.4377,
      "step": 21570
    },
    {
      "epoch": 3.6008676789587852,
      "grad_norm": 12.834451675415039,
      "learning_rate": 1.4228745969794672e-05,
      "loss": 0.5719,
      "step": 21580
    },
    {
      "epoch": 3.6025362923410644,
      "grad_norm": 17.406064987182617,
      "learning_rate": 1.4211776684201598e-05,
      "loss": 0.375,
      "step": 21590
    },
    {
      "epoch": 3.604204905723344,
      "grad_norm": 4.220244884490967,
      "learning_rate": 1.419480739860852e-05,
      "loss": 0.3809,
      "step": 21600
    },
    {
      "epoch": 3.6058735191056233,
      "grad_norm": 1.7478857040405273,
      "learning_rate": 1.4177838113015443e-05,
      "loss": 0.4832,
      "step": 21610
    },
    {
      "epoch": 3.6075421324879025,
      "grad_norm": 4.272480010986328,
      "learning_rate": 1.4160868827422365e-05,
      "loss": 0.5729,
      "step": 21620
    },
    {
      "epoch": 3.6092107458701816,
      "grad_norm": 8.768730163574219,
      "learning_rate": 1.414389954182929e-05,
      "loss": 0.4715,
      "step": 21630
    },
    {
      "epoch": 3.6108793592524613,
      "grad_norm": 8.029540061950684,
      "learning_rate": 1.4126930256236212e-05,
      "loss": 0.5089,
      "step": 21640
    },
    {
      "epoch": 3.6125479726347405,
      "grad_norm": 3.495932102203369,
      "learning_rate": 1.4109960970643138e-05,
      "loss": 0.5563,
      "step": 21650
    },
    {
      "epoch": 3.6142165860170197,
      "grad_norm": 3.760481119155884,
      "learning_rate": 1.409299168505006e-05,
      "loss": 0.4057,
      "step": 21660
    },
    {
      "epoch": 3.6158851993992993,
      "grad_norm": 7.2994537353515625,
      "learning_rate": 1.4076022399456984e-05,
      "loss": 0.4247,
      "step": 21670
    },
    {
      "epoch": 3.6175538127815785,
      "grad_norm": 5.294699668884277,
      "learning_rate": 1.4059053113863907e-05,
      "loss": 0.4814,
      "step": 21680
    },
    {
      "epoch": 3.6192224261638577,
      "grad_norm": 4.832099914550781,
      "learning_rate": 1.404208382827083e-05,
      "loss": 0.5528,
      "step": 21690
    },
    {
      "epoch": 3.6208910395461373,
      "grad_norm": 4.595348834991455,
      "learning_rate": 1.4025114542677754e-05,
      "loss": 0.3979,
      "step": 21700
    },
    {
      "epoch": 3.6225596529284165,
      "grad_norm": 3.2316689491271973,
      "learning_rate": 1.4008145257084676e-05,
      "loss": 0.2678,
      "step": 21710
    },
    {
      "epoch": 3.6242282663106957,
      "grad_norm": 9.131950378417969,
      "learning_rate": 1.3991175971491602e-05,
      "loss": 0.338,
      "step": 21720
    },
    {
      "epoch": 3.6258968796929754,
      "grad_norm": 3.4378325939178467,
      "learning_rate": 1.3974206685898524e-05,
      "loss": 0.1986,
      "step": 21730
    },
    {
      "epoch": 3.6275654930752546,
      "grad_norm": 19.0360164642334,
      "learning_rate": 1.3957237400305449e-05,
      "loss": 0.4256,
      "step": 21740
    },
    {
      "epoch": 3.6292341064575337,
      "grad_norm": 16.876523971557617,
      "learning_rate": 1.3940268114712371e-05,
      "loss": 0.7727,
      "step": 21750
    },
    {
      "epoch": 3.6309027198398134,
      "grad_norm": 2.0648579597473145,
      "learning_rate": 1.3923298829119294e-05,
      "loss": 0.2855,
      "step": 21760
    },
    {
      "epoch": 3.6325713332220926,
      "grad_norm": 2.9249467849731445,
      "learning_rate": 1.3906329543526218e-05,
      "loss": 0.452,
      "step": 21770
    },
    {
      "epoch": 3.6342399466043718,
      "grad_norm": 2.0757222175598145,
      "learning_rate": 1.3889360257933142e-05,
      "loss": 0.4609,
      "step": 21780
    },
    {
      "epoch": 3.635908559986651,
      "grad_norm": 1.7999060153961182,
      "learning_rate": 1.3872390972340066e-05,
      "loss": 0.4033,
      "step": 21790
    },
    {
      "epoch": 3.63757717336893,
      "grad_norm": 4.462344646453857,
      "learning_rate": 1.3855421686746989e-05,
      "loss": 0.5892,
      "step": 21800
    },
    {
      "epoch": 3.63924578675121,
      "grad_norm": 4.358793258666992,
      "learning_rate": 1.3838452401153911e-05,
      "loss": 0.3923,
      "step": 21810
    },
    {
      "epoch": 3.640914400133489,
      "grad_norm": 8.445449829101562,
      "learning_rate": 1.3821483115560835e-05,
      "loss": 0.5035,
      "step": 21820
    },
    {
      "epoch": 3.642583013515768,
      "grad_norm": 7.710372447967529,
      "learning_rate": 1.3804513829967758e-05,
      "loss": 0.5076,
      "step": 21830
    },
    {
      "epoch": 3.644251626898048,
      "grad_norm": 6.3618597984313965,
      "learning_rate": 1.3787544544374684e-05,
      "loss": 0.2213,
      "step": 21840
    },
    {
      "epoch": 3.645920240280327,
      "grad_norm": 4.935911178588867,
      "learning_rate": 1.3770575258781606e-05,
      "loss": 0.2792,
      "step": 21850
    },
    {
      "epoch": 3.647588853662606,
      "grad_norm": 5.07505989074707,
      "learning_rate": 1.375360597318853e-05,
      "loss": 0.8631,
      "step": 21860
    },
    {
      "epoch": 3.649257467044886,
      "grad_norm": 9.152627944946289,
      "learning_rate": 1.3736636687595453e-05,
      "loss": 0.4863,
      "step": 21870
    },
    {
      "epoch": 3.650926080427165,
      "grad_norm": 8.237475395202637,
      "learning_rate": 1.3719667402002375e-05,
      "loss": 0.5312,
      "step": 21880
    },
    {
      "epoch": 3.6525946938094442,
      "grad_norm": 5.459736347198486,
      "learning_rate": 1.37026981164093e-05,
      "loss": 0.4958,
      "step": 21890
    },
    {
      "epoch": 3.654263307191724,
      "grad_norm": 0.5567907094955444,
      "learning_rate": 1.3685728830816225e-05,
      "loss": 0.2963,
      "step": 21900
    },
    {
      "epoch": 3.655931920574003,
      "grad_norm": 19.75520896911621,
      "learning_rate": 1.3668759545223148e-05,
      "loss": 0.7117,
      "step": 21910
    },
    {
      "epoch": 3.6576005339562823,
      "grad_norm": 4.634608268737793,
      "learning_rate": 1.365179025963007e-05,
      "loss": 0.4483,
      "step": 21920
    },
    {
      "epoch": 3.659269147338562,
      "grad_norm": 5.802261829376221,
      "learning_rate": 1.3634820974036993e-05,
      "loss": 0.3793,
      "step": 21930
    },
    {
      "epoch": 3.660937760720841,
      "grad_norm": 2.7761924266815186,
      "learning_rate": 1.3617851688443917e-05,
      "loss": 0.3712,
      "step": 21940
    },
    {
      "epoch": 3.6626063741031203,
      "grad_norm": 5.641130447387695,
      "learning_rate": 1.360088240285084e-05,
      "loss": 0.3509,
      "step": 21950
    },
    {
      "epoch": 3.6642749874853995,
      "grad_norm": 5.99363899230957,
      "learning_rate": 1.3583913117257762e-05,
      "loss": 0.2446,
      "step": 21960
    },
    {
      "epoch": 3.665943600867679,
      "grad_norm": 5.5264363288879395,
      "learning_rate": 1.3566943831664688e-05,
      "loss": 0.3899,
      "step": 21970
    },
    {
      "epoch": 3.6676122142499583,
      "grad_norm": 6.533481597900391,
      "learning_rate": 1.3549974546071612e-05,
      "loss": 0.294,
      "step": 21980
    },
    {
      "epoch": 3.6692808276322375,
      "grad_norm": 13.030243873596191,
      "learning_rate": 1.3533005260478535e-05,
      "loss": 0.3852,
      "step": 21990
    },
    {
      "epoch": 3.6709494410145167,
      "grad_norm": 10.184091567993164,
      "learning_rate": 1.3516035974885457e-05,
      "loss": 0.5646,
      "step": 22000
    },
    {
      "epoch": 3.6726180543967963,
      "grad_norm": 10.84040641784668,
      "learning_rate": 1.3499066689292381e-05,
      "loss": 0.6006,
      "step": 22010
    },
    {
      "epoch": 3.6742866677790755,
      "grad_norm": 1.3421316146850586,
      "learning_rate": 1.3482097403699304e-05,
      "loss": 0.4442,
      "step": 22020
    },
    {
      "epoch": 3.6759552811613547,
      "grad_norm": 8.875486373901367,
      "learning_rate": 1.346512811810623e-05,
      "loss": 0.4925,
      "step": 22030
    },
    {
      "epoch": 3.6776238945436344,
      "grad_norm": 0.11620049923658371,
      "learning_rate": 1.3448158832513152e-05,
      "loss": 0.4797,
      "step": 22040
    },
    {
      "epoch": 3.6792925079259136,
      "grad_norm": 0.2515764534473419,
      "learning_rate": 1.3431189546920076e-05,
      "loss": 0.2892,
      "step": 22050
    },
    {
      "epoch": 3.6809611213081928,
      "grad_norm": 7.556341648101807,
      "learning_rate": 1.3414220261326999e-05,
      "loss": 0.3499,
      "step": 22060
    },
    {
      "epoch": 3.6826297346904724,
      "grad_norm": 2.3173062801361084,
      "learning_rate": 1.3397250975733921e-05,
      "loss": 0.3528,
      "step": 22070
    },
    {
      "epoch": 3.6842983480727516,
      "grad_norm": 10.107316017150879,
      "learning_rate": 1.3380281690140845e-05,
      "loss": 0.3865,
      "step": 22080
    },
    {
      "epoch": 3.6859669614550308,
      "grad_norm": 1.182074785232544,
      "learning_rate": 1.336331240454777e-05,
      "loss": 0.33,
      "step": 22090
    },
    {
      "epoch": 3.6876355748373104,
      "grad_norm": 0.9183834791183472,
      "learning_rate": 1.3346343118954694e-05,
      "loss": 0.3955,
      "step": 22100
    },
    {
      "epoch": 3.6893041882195896,
      "grad_norm": 6.857943058013916,
      "learning_rate": 1.3329373833361616e-05,
      "loss": 0.3544,
      "step": 22110
    },
    {
      "epoch": 3.690972801601869,
      "grad_norm": 1.3659051656723022,
      "learning_rate": 1.3312404547768539e-05,
      "loss": 0.3002,
      "step": 22120
    },
    {
      "epoch": 3.6926414149841484,
      "grad_norm": 7.838952541351318,
      "learning_rate": 1.3295435262175463e-05,
      "loss": 0.4728,
      "step": 22130
    },
    {
      "epoch": 3.6943100283664276,
      "grad_norm": 0.3784197270870209,
      "learning_rate": 1.3278465976582385e-05,
      "loss": 0.5366,
      "step": 22140
    },
    {
      "epoch": 3.695978641748707,
      "grad_norm": 13.672210693359375,
      "learning_rate": 1.3261496690989311e-05,
      "loss": 0.5856,
      "step": 22150
    },
    {
      "epoch": 3.697647255130986,
      "grad_norm": 3.806122064590454,
      "learning_rate": 1.3244527405396234e-05,
      "loss": 0.5844,
      "step": 22160
    },
    {
      "epoch": 3.699315868513265,
      "grad_norm": 8.69237232208252,
      "learning_rate": 1.3227558119803158e-05,
      "loss": 0.3326,
      "step": 22170
    },
    {
      "epoch": 3.700984481895545,
      "grad_norm": 4.910954475402832,
      "learning_rate": 1.321058883421008e-05,
      "loss": 0.5441,
      "step": 22180
    },
    {
      "epoch": 3.702653095277824,
      "grad_norm": 3.714055061340332,
      "learning_rate": 1.3193619548617003e-05,
      "loss": 0.335,
      "step": 22190
    },
    {
      "epoch": 3.7043217086601032,
      "grad_norm": 5.907240867614746,
      "learning_rate": 1.3176650263023927e-05,
      "loss": 0.618,
      "step": 22200
    },
    {
      "epoch": 3.705990322042383,
      "grad_norm": 5.104451656341553,
      "learning_rate": 1.3159680977430853e-05,
      "loss": 0.3333,
      "step": 22210
    },
    {
      "epoch": 3.707658935424662,
      "grad_norm": 4.963960647583008,
      "learning_rate": 1.3142711691837775e-05,
      "loss": 0.3393,
      "step": 22220
    },
    {
      "epoch": 3.7093275488069413,
      "grad_norm": 2.143972873687744,
      "learning_rate": 1.3125742406244698e-05,
      "loss": 0.5579,
      "step": 22230
    },
    {
      "epoch": 3.710996162189221,
      "grad_norm": 1.074933648109436,
      "learning_rate": 1.310877312065162e-05,
      "loss": 0.2737,
      "step": 22240
    },
    {
      "epoch": 3.7126647755715,
      "grad_norm": 10.746609687805176,
      "learning_rate": 1.3091803835058545e-05,
      "loss": 0.4517,
      "step": 22250
    },
    {
      "epoch": 3.7143333889537793,
      "grad_norm": 6.83725118637085,
      "learning_rate": 1.3074834549465467e-05,
      "loss": 0.6036,
      "step": 22260
    },
    {
      "epoch": 3.716002002336059,
      "grad_norm": 8.587327003479004,
      "learning_rate": 1.305786526387239e-05,
      "loss": 0.4119,
      "step": 22270
    },
    {
      "epoch": 3.717670615718338,
      "grad_norm": 7.689730167388916,
      "learning_rate": 1.3040895978279315e-05,
      "loss": 0.3891,
      "step": 22280
    },
    {
      "epoch": 3.7193392291006173,
      "grad_norm": 13.0352144241333,
      "learning_rate": 1.302392669268624e-05,
      "loss": 0.625,
      "step": 22290
    },
    {
      "epoch": 3.721007842482897,
      "grad_norm": 1.2515990734100342,
      "learning_rate": 1.3006957407093162e-05,
      "loss": 0.441,
      "step": 22300
    },
    {
      "epoch": 3.722676455865176,
      "grad_norm": 4.965420722961426,
      "learning_rate": 1.2989988121500085e-05,
      "loss": 0.4256,
      "step": 22310
    },
    {
      "epoch": 3.7243450692474553,
      "grad_norm": 9.49255084991455,
      "learning_rate": 1.2973018835907009e-05,
      "loss": 0.4797,
      "step": 22320
    },
    {
      "epoch": 3.7260136826297345,
      "grad_norm": 6.120324611663818,
      "learning_rate": 1.2956049550313931e-05,
      "loss": 0.3303,
      "step": 22330
    },
    {
      "epoch": 3.727682296012014,
      "grad_norm": 1.12483549118042,
      "learning_rate": 1.2939080264720857e-05,
      "loss": 0.1918,
      "step": 22340
    },
    {
      "epoch": 3.7293509093942934,
      "grad_norm": 9.004396438598633,
      "learning_rate": 1.292211097912778e-05,
      "loss": 0.3779,
      "step": 22350
    },
    {
      "epoch": 3.7310195227765726,
      "grad_norm": 14.226170539855957,
      "learning_rate": 1.2905141693534704e-05,
      "loss": 0.5393,
      "step": 22360
    },
    {
      "epoch": 3.7326881361588518,
      "grad_norm": 3.181990623474121,
      "learning_rate": 1.2888172407941626e-05,
      "loss": 0.4414,
      "step": 22370
    },
    {
      "epoch": 3.7343567495411314,
      "grad_norm": 9.336409568786621,
      "learning_rate": 1.2871203122348549e-05,
      "loss": 0.3932,
      "step": 22380
    },
    {
      "epoch": 3.7360253629234106,
      "grad_norm": 19.909305572509766,
      "learning_rate": 1.2854233836755473e-05,
      "loss": 0.4264,
      "step": 22390
    },
    {
      "epoch": 3.73769397630569,
      "grad_norm": 10.67166805267334,
      "learning_rate": 1.2837264551162397e-05,
      "loss": 0.4695,
      "step": 22400
    },
    {
      "epoch": 3.7393625896879694,
      "grad_norm": 2.5419094562530518,
      "learning_rate": 1.2820295265569321e-05,
      "loss": 0.5507,
      "step": 22410
    },
    {
      "epoch": 3.7410312030702486,
      "grad_norm": 4.168397426605225,
      "learning_rate": 1.2803325979976244e-05,
      "loss": 0.348,
      "step": 22420
    },
    {
      "epoch": 3.742699816452528,
      "grad_norm": 17.291685104370117,
      "learning_rate": 1.2786356694383166e-05,
      "loss": 0.5071,
      "step": 22430
    },
    {
      "epoch": 3.7443684298348074,
      "grad_norm": 2.001579761505127,
      "learning_rate": 1.276938740879009e-05,
      "loss": 0.3473,
      "step": 22440
    },
    {
      "epoch": 3.7460370432170866,
      "grad_norm": 10.772066116333008,
      "learning_rate": 1.2752418123197013e-05,
      "loss": 0.527,
      "step": 22450
    },
    {
      "epoch": 3.747705656599366,
      "grad_norm": 4.201481342315674,
      "learning_rate": 1.2735448837603939e-05,
      "loss": 0.5631,
      "step": 22460
    },
    {
      "epoch": 3.7493742699816455,
      "grad_norm": 6.13948917388916,
      "learning_rate": 1.2718479552010861e-05,
      "loss": 0.7448,
      "step": 22470
    },
    {
      "epoch": 3.7510428833639247,
      "grad_norm": 2.2301666736602783,
      "learning_rate": 1.2701510266417785e-05,
      "loss": 0.3415,
      "step": 22480
    },
    {
      "epoch": 3.752711496746204,
      "grad_norm": 4.875730514526367,
      "learning_rate": 1.2684540980824708e-05,
      "loss": 0.5302,
      "step": 22490
    },
    {
      "epoch": 3.7543801101284835,
      "grad_norm": 7.197399139404297,
      "learning_rate": 1.266757169523163e-05,
      "loss": 0.6732,
      "step": 22500
    },
    {
      "epoch": 3.7560487235107627,
      "grad_norm": 0.7166779637336731,
      "learning_rate": 1.2650602409638555e-05,
      "loss": 0.5142,
      "step": 22510
    },
    {
      "epoch": 3.757717336893042,
      "grad_norm": 1.5157535076141357,
      "learning_rate": 1.2633633124045477e-05,
      "loss": 0.2449,
      "step": 22520
    },
    {
      "epoch": 3.759385950275321,
      "grad_norm": 1.6144962310791016,
      "learning_rate": 1.2616663838452403e-05,
      "loss": 0.4244,
      "step": 22530
    },
    {
      "epoch": 3.7610545636576003,
      "grad_norm": 5.0288238525390625,
      "learning_rate": 1.2599694552859325e-05,
      "loss": 0.371,
      "step": 22540
    },
    {
      "epoch": 3.76272317703988,
      "grad_norm": 1.6927143335342407,
      "learning_rate": 1.2582725267266248e-05,
      "loss": 0.1968,
      "step": 22550
    },
    {
      "epoch": 3.764391790422159,
      "grad_norm": 14.436712265014648,
      "learning_rate": 1.2565755981673172e-05,
      "loss": 0.537,
      "step": 22560
    },
    {
      "epoch": 3.7660604038044383,
      "grad_norm": 9.645889282226562,
      "learning_rate": 1.2548786696080095e-05,
      "loss": 0.5279,
      "step": 22570
    },
    {
      "epoch": 3.767729017186718,
      "grad_norm": 2.5775699615478516,
      "learning_rate": 1.2531817410487017e-05,
      "loss": 0.5305,
      "step": 22580
    },
    {
      "epoch": 3.769397630568997,
      "grad_norm": 6.913022041320801,
      "learning_rate": 1.2514848124893943e-05,
      "loss": 0.429,
      "step": 22590
    },
    {
      "epoch": 3.7710662439512763,
      "grad_norm": 2.928638458251953,
      "learning_rate": 1.2497878839300867e-05,
      "loss": 0.3228,
      "step": 22600
    },
    {
      "epoch": 3.772734857333556,
      "grad_norm": 9.093987464904785,
      "learning_rate": 1.248090955370779e-05,
      "loss": 0.5989,
      "step": 22610
    },
    {
      "epoch": 3.774403470715835,
      "grad_norm": 1.794108271598816,
      "learning_rate": 1.2463940268114712e-05,
      "loss": 0.638,
      "step": 22620
    },
    {
      "epoch": 3.7760720840981143,
      "grad_norm": 7.232219696044922,
      "learning_rate": 1.2446970982521636e-05,
      "loss": 0.4424,
      "step": 22630
    },
    {
      "epoch": 3.777740697480394,
      "grad_norm": 19.29568862915039,
      "learning_rate": 1.243000169692856e-05,
      "loss": 0.3205,
      "step": 22640
    },
    {
      "epoch": 3.779409310862673,
      "grad_norm": 18.22261619567871,
      "learning_rate": 1.2413032411335483e-05,
      "loss": 0.5042,
      "step": 22650
    },
    {
      "epoch": 3.7810779242449524,
      "grad_norm": 16.054241180419922,
      "learning_rate": 1.2396063125742405e-05,
      "loss": 0.55,
      "step": 22660
    },
    {
      "epoch": 3.782746537627232,
      "grad_norm": 7.34987735748291,
      "learning_rate": 1.2379093840149331e-05,
      "loss": 0.5115,
      "step": 22670
    },
    {
      "epoch": 3.784415151009511,
      "grad_norm": 14.806339263916016,
      "learning_rate": 1.2362124554556254e-05,
      "loss": 0.6789,
      "step": 22680
    },
    {
      "epoch": 3.7860837643917904,
      "grad_norm": 4.172762870788574,
      "learning_rate": 1.2345155268963176e-05,
      "loss": 0.1998,
      "step": 22690
    },
    {
      "epoch": 3.7877523777740696,
      "grad_norm": 0.3008766174316406,
      "learning_rate": 1.23281859833701e-05,
      "loss": 0.3421,
      "step": 22700
    },
    {
      "epoch": 3.7894209911563492,
      "grad_norm": 6.761843681335449,
      "learning_rate": 1.2311216697777025e-05,
      "loss": 0.5665,
      "step": 22710
    },
    {
      "epoch": 3.7910896045386284,
      "grad_norm": 5.777462482452393,
      "learning_rate": 1.2294247412183947e-05,
      "loss": 0.3064,
      "step": 22720
    },
    {
      "epoch": 3.7927582179209076,
      "grad_norm": 8.343890190124512,
      "learning_rate": 1.2277278126590871e-05,
      "loss": 0.4817,
      "step": 22730
    },
    {
      "epoch": 3.794426831303187,
      "grad_norm": 6.656505584716797,
      "learning_rate": 1.2260308840997794e-05,
      "loss": 0.3351,
      "step": 22740
    },
    {
      "epoch": 3.7960954446854664,
      "grad_norm": 5.248838424682617,
      "learning_rate": 1.2243339555404718e-05,
      "loss": 0.3716,
      "step": 22750
    },
    {
      "epoch": 3.7977640580677456,
      "grad_norm": 3.4728188514709473,
      "learning_rate": 1.2226370269811642e-05,
      "loss": 0.5514,
      "step": 22760
    },
    {
      "epoch": 3.799432671450025,
      "grad_norm": 15.480159759521484,
      "learning_rate": 1.2209400984218565e-05,
      "loss": 0.601,
      "step": 22770
    },
    {
      "epoch": 3.8011012848323045,
      "grad_norm": 5.473849773406982,
      "learning_rate": 1.2192431698625487e-05,
      "loss": 0.5426,
      "step": 22780
    },
    {
      "epoch": 3.8027698982145837,
      "grad_norm": 0.8147560954093933,
      "learning_rate": 1.2175462413032413e-05,
      "loss": 0.3474,
      "step": 22790
    },
    {
      "epoch": 3.804438511596863,
      "grad_norm": 17.836660385131836,
      "learning_rate": 1.2158493127439336e-05,
      "loss": 0.6503,
      "step": 22800
    },
    {
      "epoch": 3.8061071249791425,
      "grad_norm": 3.4363481998443604,
      "learning_rate": 1.2141523841846258e-05,
      "loss": 0.352,
      "step": 22810
    },
    {
      "epoch": 3.8077757383614217,
      "grad_norm": 7.775789260864258,
      "learning_rate": 1.2124554556253182e-05,
      "loss": 0.4232,
      "step": 22820
    },
    {
      "epoch": 3.809444351743701,
      "grad_norm": 15.684122085571289,
      "learning_rate": 1.2107585270660106e-05,
      "loss": 0.369,
      "step": 22830
    },
    {
      "epoch": 3.8111129651259805,
      "grad_norm": 14.313753128051758,
      "learning_rate": 1.2090615985067029e-05,
      "loss": 0.5898,
      "step": 22840
    },
    {
      "epoch": 3.8127815785082597,
      "grad_norm": 6.977942943572998,
      "learning_rate": 1.2073646699473953e-05,
      "loss": 0.6672,
      "step": 22850
    },
    {
      "epoch": 3.814450191890539,
      "grad_norm": 23.15298080444336,
      "learning_rate": 1.2056677413880876e-05,
      "loss": 0.5045,
      "step": 22860
    },
    {
      "epoch": 3.8161188052728185,
      "grad_norm": 3.841752052307129,
      "learning_rate": 1.20397081282878e-05,
      "loss": 0.7764,
      "step": 22870
    },
    {
      "epoch": 3.8177874186550977,
      "grad_norm": 14.404985427856445,
      "learning_rate": 1.2022738842694724e-05,
      "loss": 0.4681,
      "step": 22880
    },
    {
      "epoch": 3.819456032037377,
      "grad_norm": 14.88090991973877,
      "learning_rate": 1.2005769557101646e-05,
      "loss": 0.5061,
      "step": 22890
    },
    {
      "epoch": 3.821124645419656,
      "grad_norm": 1.3003525733947754,
      "learning_rate": 1.198880027150857e-05,
      "loss": 0.4358,
      "step": 22900
    },
    {
      "epoch": 3.8227932588019353,
      "grad_norm": 5.265035629272461,
      "learning_rate": 1.1971830985915493e-05,
      "loss": 0.3753,
      "step": 22910
    },
    {
      "epoch": 3.824461872184215,
      "grad_norm": 0.6767675280570984,
      "learning_rate": 1.1954861700322417e-05,
      "loss": 0.4088,
      "step": 22920
    },
    {
      "epoch": 3.826130485566494,
      "grad_norm": 0.9086028337478638,
      "learning_rate": 1.193789241472934e-05,
      "loss": 0.4958,
      "step": 22930
    },
    {
      "epoch": 3.8277990989487733,
      "grad_norm": 5.026811599731445,
      "learning_rate": 1.1920923129136264e-05,
      "loss": 0.2925,
      "step": 22940
    },
    {
      "epoch": 3.829467712331053,
      "grad_norm": 6.7776288986206055,
      "learning_rate": 1.1903953843543188e-05,
      "loss": 0.3393,
      "step": 22950
    },
    {
      "epoch": 3.831136325713332,
      "grad_norm": 1.4965686798095703,
      "learning_rate": 1.188698455795011e-05,
      "loss": 0.3992,
      "step": 22960
    },
    {
      "epoch": 3.8328049390956114,
      "grad_norm": 2.7529971599578857,
      "learning_rate": 1.1870015272357033e-05,
      "loss": 0.3485,
      "step": 22970
    },
    {
      "epoch": 3.834473552477891,
      "grad_norm": 5.945746898651123,
      "learning_rate": 1.1853045986763959e-05,
      "loss": 0.4693,
      "step": 22980
    },
    {
      "epoch": 3.83614216586017,
      "grad_norm": 10.691657066345215,
      "learning_rate": 1.1836076701170881e-05,
      "loss": 1.0181,
      "step": 22990
    },
    {
      "epoch": 3.8378107792424494,
      "grad_norm": 1.0451804399490356,
      "learning_rate": 1.1819107415577804e-05,
      "loss": 0.29,
      "step": 23000
    },
    {
      "epoch": 3.839479392624729,
      "grad_norm": 12.716261863708496,
      "learning_rate": 1.1802138129984728e-05,
      "loss": 0.3293,
      "step": 23010
    },
    {
      "epoch": 3.8411480060070082,
      "grad_norm": 9.95383358001709,
      "learning_rate": 1.1785168844391652e-05,
      "loss": 0.4423,
      "step": 23020
    },
    {
      "epoch": 3.8428166193892874,
      "grad_norm": 5.233340263366699,
      "learning_rate": 1.1768199558798575e-05,
      "loss": 0.4775,
      "step": 23030
    },
    {
      "epoch": 3.844485232771567,
      "grad_norm": 2.768878221511841,
      "learning_rate": 1.1751230273205499e-05,
      "loss": 0.3576,
      "step": 23040
    },
    {
      "epoch": 3.8461538461538463,
      "grad_norm": 4.963455677032471,
      "learning_rate": 1.1734260987612421e-05,
      "loss": 0.2556,
      "step": 23050
    },
    {
      "epoch": 3.8478224595361255,
      "grad_norm": 3.051177978515625,
      "learning_rate": 1.1717291702019346e-05,
      "loss": 0.4757,
      "step": 23060
    },
    {
      "epoch": 3.8494910729184046,
      "grad_norm": 7.640807628631592,
      "learning_rate": 1.170032241642627e-05,
      "loss": 0.2564,
      "step": 23070
    },
    {
      "epoch": 3.8511596863006843,
      "grad_norm": 1.7837296724319458,
      "learning_rate": 1.1683353130833192e-05,
      "loss": 0.4851,
      "step": 23080
    },
    {
      "epoch": 3.8528282996829635,
      "grad_norm": 1.2779232263565063,
      "learning_rate": 1.1666383845240115e-05,
      "loss": 0.3361,
      "step": 23090
    },
    {
      "epoch": 3.8544969130652427,
      "grad_norm": 6.94432258605957,
      "learning_rate": 1.164941455964704e-05,
      "loss": 0.4076,
      "step": 23100
    },
    {
      "epoch": 3.856165526447522,
      "grad_norm": 10.191204071044922,
      "learning_rate": 1.1632445274053963e-05,
      "loss": 0.4487,
      "step": 23110
    },
    {
      "epoch": 3.8578341398298015,
      "grad_norm": 14.291394233703613,
      "learning_rate": 1.1615475988460886e-05,
      "loss": 0.5853,
      "step": 23120
    },
    {
      "epoch": 3.8595027532120807,
      "grad_norm": 15.273712158203125,
      "learning_rate": 1.159850670286781e-05,
      "loss": 0.5536,
      "step": 23130
    },
    {
      "epoch": 3.86117136659436,
      "grad_norm": 7.348406791687012,
      "learning_rate": 1.1581537417274734e-05,
      "loss": 0.3464,
      "step": 23140
    },
    {
      "epoch": 3.8628399799766395,
      "grad_norm": 8.649250984191895,
      "learning_rate": 1.1564568131681656e-05,
      "loss": 0.4264,
      "step": 23150
    },
    {
      "epoch": 3.8645085933589187,
      "grad_norm": 10.883270263671875,
      "learning_rate": 1.154759884608858e-05,
      "loss": 0.3238,
      "step": 23160
    },
    {
      "epoch": 3.866177206741198,
      "grad_norm": 7.8910627365112305,
      "learning_rate": 1.1530629560495503e-05,
      "loss": 0.6444,
      "step": 23170
    },
    {
      "epoch": 3.8678458201234776,
      "grad_norm": 8.573360443115234,
      "learning_rate": 1.1513660274902427e-05,
      "loss": 0.5579,
      "step": 23180
    },
    {
      "epoch": 3.8695144335057567,
      "grad_norm": 3.133486270904541,
      "learning_rate": 1.149669098930935e-05,
      "loss": 0.318,
      "step": 23190
    },
    {
      "epoch": 3.871183046888036,
      "grad_norm": 7.401906967163086,
      "learning_rate": 1.1479721703716274e-05,
      "loss": 0.5163,
      "step": 23200
    },
    {
      "epoch": 3.8728516602703156,
      "grad_norm": 8.116914749145508,
      "learning_rate": 1.1462752418123198e-05,
      "loss": 0.3907,
      "step": 23210
    },
    {
      "epoch": 3.8745202736525948,
      "grad_norm": 11.154315948486328,
      "learning_rate": 1.144578313253012e-05,
      "loss": 0.3139,
      "step": 23220
    },
    {
      "epoch": 3.876188887034874,
      "grad_norm": 3.251164436340332,
      "learning_rate": 1.1428813846937045e-05,
      "loss": 0.5673,
      "step": 23230
    },
    {
      "epoch": 3.8778575004171536,
      "grad_norm": 4.8913044929504395,
      "learning_rate": 1.1411844561343967e-05,
      "loss": 0.5683,
      "step": 23240
    },
    {
      "epoch": 3.879526113799433,
      "grad_norm": 8.639629364013672,
      "learning_rate": 1.1394875275750891e-05,
      "loss": 0.376,
      "step": 23250
    },
    {
      "epoch": 3.881194727181712,
      "grad_norm": 4.408048629760742,
      "learning_rate": 1.1377905990157816e-05,
      "loss": 0.3413,
      "step": 23260
    },
    {
      "epoch": 3.882863340563991,
      "grad_norm": 12.695176124572754,
      "learning_rate": 1.1360936704564738e-05,
      "loss": 0.4623,
      "step": 23270
    },
    {
      "epoch": 3.8845319539462704,
      "grad_norm": 4.39380407333374,
      "learning_rate": 1.134396741897166e-05,
      "loss": 0.6574,
      "step": 23280
    },
    {
      "epoch": 3.88620056732855,
      "grad_norm": 8.268701553344727,
      "learning_rate": 1.1326998133378586e-05,
      "loss": 0.4927,
      "step": 23290
    },
    {
      "epoch": 3.887869180710829,
      "grad_norm": 2.1692776679992676,
      "learning_rate": 1.1310028847785509e-05,
      "loss": 0.3272,
      "step": 23300
    },
    {
      "epoch": 3.8895377940931084,
      "grad_norm": 2.8952364921569824,
      "learning_rate": 1.1293059562192431e-05,
      "loss": 0.3172,
      "step": 23310
    },
    {
      "epoch": 3.891206407475388,
      "grad_norm": 5.252682685852051,
      "learning_rate": 1.1276090276599356e-05,
      "loss": 0.5356,
      "step": 23320
    },
    {
      "epoch": 3.8928750208576672,
      "grad_norm": 2.0576982498168945,
      "learning_rate": 1.125912099100628e-05,
      "loss": 0.532,
      "step": 23330
    },
    {
      "epoch": 3.8945436342399464,
      "grad_norm": 4.286144733428955,
      "learning_rate": 1.1242151705413202e-05,
      "loss": 0.68,
      "step": 23340
    },
    {
      "epoch": 3.896212247622226,
      "grad_norm": 2.793342113494873,
      "learning_rate": 1.1225182419820126e-05,
      "loss": 0.4239,
      "step": 23350
    },
    {
      "epoch": 3.8978808610045053,
      "grad_norm": 4.157629013061523,
      "learning_rate": 1.1208213134227049e-05,
      "loss": 0.5176,
      "step": 23360
    },
    {
      "epoch": 3.8995494743867845,
      "grad_norm": 12.83005142211914,
      "learning_rate": 1.1191243848633973e-05,
      "loss": 0.469,
      "step": 23370
    },
    {
      "epoch": 3.901218087769064,
      "grad_norm": 11.03664493560791,
      "learning_rate": 1.1174274563040897e-05,
      "loss": 0.5074,
      "step": 23380
    },
    {
      "epoch": 3.9028867011513433,
      "grad_norm": 6.320962905883789,
      "learning_rate": 1.115730527744782e-05,
      "loss": 0.511,
      "step": 23390
    },
    {
      "epoch": 3.9045553145336225,
      "grad_norm": 2.959578275680542,
      "learning_rate": 1.1140335991854742e-05,
      "loss": 0.264,
      "step": 23400
    },
    {
      "epoch": 3.906223927915902,
      "grad_norm": 14.342965126037598,
      "learning_rate": 1.1123366706261668e-05,
      "loss": 0.5115,
      "step": 23410
    },
    {
      "epoch": 3.9078925412981813,
      "grad_norm": 2.7341160774230957,
      "learning_rate": 1.110639742066859e-05,
      "loss": 0.4525,
      "step": 23420
    },
    {
      "epoch": 3.9095611546804605,
      "grad_norm": 10.480937004089355,
      "learning_rate": 1.1089428135075513e-05,
      "loss": 0.5904,
      "step": 23430
    },
    {
      "epoch": 3.9112297680627397,
      "grad_norm": 3.770751476287842,
      "learning_rate": 1.1072458849482437e-05,
      "loss": 0.215,
      "step": 23440
    },
    {
      "epoch": 3.9128983814450193,
      "grad_norm": 9.243997573852539,
      "learning_rate": 1.1055489563889362e-05,
      "loss": 0.2842,
      "step": 23450
    },
    {
      "epoch": 3.9145669948272985,
      "grad_norm": 6.864137649536133,
      "learning_rate": 1.1038520278296284e-05,
      "loss": 0.4537,
      "step": 23460
    },
    {
      "epoch": 3.9162356082095777,
      "grad_norm": 8.145936965942383,
      "learning_rate": 1.1021550992703206e-05,
      "loss": 0.4073,
      "step": 23470
    },
    {
      "epoch": 3.917904221591857,
      "grad_norm": 9.21652603149414,
      "learning_rate": 1.100458170711013e-05,
      "loss": 0.2798,
      "step": 23480
    },
    {
      "epoch": 3.9195728349741366,
      "grad_norm": 3.2707571983337402,
      "learning_rate": 1.0987612421517055e-05,
      "loss": 0.4799,
      "step": 23490
    },
    {
      "epoch": 3.9212414483564157,
      "grad_norm": 13.364790916442871,
      "learning_rate": 1.0970643135923977e-05,
      "loss": 0.5334,
      "step": 23500
    },
    {
      "epoch": 3.922910061738695,
      "grad_norm": 3.2400269508361816,
      "learning_rate": 1.0953673850330902e-05,
      "loss": 0.2407,
      "step": 23510
    },
    {
      "epoch": 3.9245786751209746,
      "grad_norm": 3.494126796722412,
      "learning_rate": 1.0936704564737826e-05,
      "loss": 0.5259,
      "step": 23520
    },
    {
      "epoch": 3.9262472885032538,
      "grad_norm": 9.238883018493652,
      "learning_rate": 1.0919735279144748e-05,
      "loss": 0.4789,
      "step": 23530
    },
    {
      "epoch": 3.927915901885533,
      "grad_norm": 5.298973560333252,
      "learning_rate": 1.0902765993551672e-05,
      "loss": 0.4785,
      "step": 23540
    },
    {
      "epoch": 3.9295845152678126,
      "grad_norm": 9.745321273803711,
      "learning_rate": 1.0885796707958595e-05,
      "loss": 0.5401,
      "step": 23550
    },
    {
      "epoch": 3.931253128650092,
      "grad_norm": 0.14902257919311523,
      "learning_rate": 1.0868827422365519e-05,
      "loss": 0.5181,
      "step": 23560
    },
    {
      "epoch": 3.932921742032371,
      "grad_norm": 3.154196262359619,
      "learning_rate": 1.0851858136772443e-05,
      "loss": 0.4385,
      "step": 23570
    },
    {
      "epoch": 3.9345903554146506,
      "grad_norm": 2.8140926361083984,
      "learning_rate": 1.0834888851179366e-05,
      "loss": 0.4978,
      "step": 23580
    },
    {
      "epoch": 3.93625896879693,
      "grad_norm": 4.885828971862793,
      "learning_rate": 1.0817919565586288e-05,
      "loss": 0.4128,
      "step": 23590
    },
    {
      "epoch": 3.937927582179209,
      "grad_norm": 10.464475631713867,
      "learning_rate": 1.0800950279993214e-05,
      "loss": 0.4527,
      "step": 23600
    },
    {
      "epoch": 3.9395961955614887,
      "grad_norm": 11.22702693939209,
      "learning_rate": 1.0783980994400137e-05,
      "loss": 0.6943,
      "step": 23610
    },
    {
      "epoch": 3.941264808943768,
      "grad_norm": 12.527548789978027,
      "learning_rate": 1.0767011708807059e-05,
      "loss": 0.2687,
      "step": 23620
    },
    {
      "epoch": 3.942933422326047,
      "grad_norm": 7.777620792388916,
      "learning_rate": 1.0750042423213983e-05,
      "loss": 0.4676,
      "step": 23630
    },
    {
      "epoch": 3.9446020357083262,
      "grad_norm": 17.82241439819336,
      "learning_rate": 1.0733073137620907e-05,
      "loss": 0.4605,
      "step": 23640
    },
    {
      "epoch": 3.9462706490906054,
      "grad_norm": 13.220590591430664,
      "learning_rate": 1.071610385202783e-05,
      "loss": 0.5001,
      "step": 23650
    },
    {
      "epoch": 3.947939262472885,
      "grad_norm": 8.157442092895508,
      "learning_rate": 1.0699134566434754e-05,
      "loss": 0.5079,
      "step": 23660
    },
    {
      "epoch": 3.9496078758551643,
      "grad_norm": 12.2260160446167,
      "learning_rate": 1.0682165280841677e-05,
      "loss": 0.4969,
      "step": 23670
    },
    {
      "epoch": 3.9512764892374435,
      "grad_norm": 22.398881912231445,
      "learning_rate": 1.06651959952486e-05,
      "loss": 0.511,
      "step": 23680
    },
    {
      "epoch": 3.952945102619723,
      "grad_norm": 5.750604152679443,
      "learning_rate": 1.0648226709655525e-05,
      "loss": 0.4531,
      "step": 23690
    },
    {
      "epoch": 3.9546137160020023,
      "grad_norm": 5.386317729949951,
      "learning_rate": 1.0631257424062447e-05,
      "loss": 0.6181,
      "step": 23700
    },
    {
      "epoch": 3.9562823293842815,
      "grad_norm": 8.973119735717773,
      "learning_rate": 1.061428813846937e-05,
      "loss": 0.6044,
      "step": 23710
    },
    {
      "epoch": 3.957950942766561,
      "grad_norm": 1.450184941291809,
      "learning_rate": 1.0597318852876294e-05,
      "loss": 0.4742,
      "step": 23720
    },
    {
      "epoch": 3.9596195561488403,
      "grad_norm": 8.861308097839355,
      "learning_rate": 1.0580349567283218e-05,
      "loss": 0.5488,
      "step": 23730
    },
    {
      "epoch": 3.9612881695311195,
      "grad_norm": 5.057744979858398,
      "learning_rate": 1.056338028169014e-05,
      "loss": 0.7319,
      "step": 23740
    },
    {
      "epoch": 3.962956782913399,
      "grad_norm": 8.175012588500977,
      "learning_rate": 1.0546410996097065e-05,
      "loss": 0.503,
      "step": 23750
    },
    {
      "epoch": 3.9646253962956783,
      "grad_norm": 4.176311016082764,
      "learning_rate": 1.0529441710503989e-05,
      "loss": 0.4812,
      "step": 23760
    },
    {
      "epoch": 3.9662940096779575,
      "grad_norm": 0.8737664222717285,
      "learning_rate": 1.0512472424910912e-05,
      "loss": 0.5026,
      "step": 23770
    },
    {
      "epoch": 3.967962623060237,
      "grad_norm": 2.739583969116211,
      "learning_rate": 1.0495503139317834e-05,
      "loss": 0.4689,
      "step": 23780
    },
    {
      "epoch": 3.9696312364425164,
      "grad_norm": 11.20552921295166,
      "learning_rate": 1.0478533853724758e-05,
      "loss": 0.5435,
      "step": 23790
    },
    {
      "epoch": 3.9712998498247956,
      "grad_norm": 4.126083850860596,
      "learning_rate": 1.0461564568131682e-05,
      "loss": 0.5421,
      "step": 23800
    },
    {
      "epoch": 3.972968463207075,
      "grad_norm": 8.588315963745117,
      "learning_rate": 1.0444595282538605e-05,
      "loss": 0.5463,
      "step": 23810
    },
    {
      "epoch": 3.9746370765893544,
      "grad_norm": 3.6451416015625,
      "learning_rate": 1.0427625996945529e-05,
      "loss": 0.2699,
      "step": 23820
    },
    {
      "epoch": 3.9763056899716336,
      "grad_norm": 1.9210795164108276,
      "learning_rate": 1.0410656711352453e-05,
      "loss": 0.4011,
      "step": 23830
    },
    {
      "epoch": 3.9779743033539128,
      "grad_norm": 14.574687004089355,
      "learning_rate": 1.0393687425759376e-05,
      "loss": 0.6952,
      "step": 23840
    },
    {
      "epoch": 3.979642916736192,
      "grad_norm": 8.918639183044434,
      "learning_rate": 1.03767181401663e-05,
      "loss": 0.3754,
      "step": 23850
    },
    {
      "epoch": 3.9813115301184716,
      "grad_norm": 11.240829467773438,
      "learning_rate": 1.0359748854573222e-05,
      "loss": 0.4511,
      "step": 23860
    },
    {
      "epoch": 3.982980143500751,
      "grad_norm": 12.993289947509766,
      "learning_rate": 1.0342779568980147e-05,
      "loss": 0.4633,
      "step": 23870
    },
    {
      "epoch": 3.98464875688303,
      "grad_norm": 7.518040180206299,
      "learning_rate": 1.032581028338707e-05,
      "loss": 0.5224,
      "step": 23880
    },
    {
      "epoch": 3.9863173702653096,
      "grad_norm": 7.982352256774902,
      "learning_rate": 1.0308840997793993e-05,
      "loss": 0.3314,
      "step": 23890
    },
    {
      "epoch": 3.987985983647589,
      "grad_norm": 8.064695358276367,
      "learning_rate": 1.0291871712200916e-05,
      "loss": 0.2921,
      "step": 23900
    },
    {
      "epoch": 3.989654597029868,
      "grad_norm": 6.069455623626709,
      "learning_rate": 1.0274902426607842e-05,
      "loss": 0.2601,
      "step": 23910
    },
    {
      "epoch": 3.9913232104121477,
      "grad_norm": 8.69825267791748,
      "learning_rate": 1.0257933141014764e-05,
      "loss": 0.6168,
      "step": 23920
    },
    {
      "epoch": 3.992991823794427,
      "grad_norm": 4.994811534881592,
      "learning_rate": 1.0240963855421687e-05,
      "loss": 0.4725,
      "step": 23930
    },
    {
      "epoch": 3.994660437176706,
      "grad_norm": 5.457808971405029,
      "learning_rate": 1.022399456982861e-05,
      "loss": 0.1506,
      "step": 23940
    },
    {
      "epoch": 3.9963290505589857,
      "grad_norm": 8.06889533996582,
      "learning_rate": 1.0207025284235535e-05,
      "loss": 0.5476,
      "step": 23950
    },
    {
      "epoch": 3.997997663941265,
      "grad_norm": 7.066934108734131,
      "learning_rate": 1.0190055998642457e-05,
      "loss": 0.3489,
      "step": 23960
    },
    {
      "epoch": 3.999666277323544,
      "grad_norm": 9.940686225891113,
      "learning_rate": 1.0173086713049382e-05,
      "loss": 0.5675,
      "step": 23970
    },
    {
      "epoch": 4.001334890705824,
      "grad_norm": 1.839131236076355,
      "learning_rate": 1.0156117427456304e-05,
      "loss": 0.5363,
      "step": 23980
    },
    {
      "epoch": 4.0030035040881025,
      "grad_norm": 5.7445197105407715,
      "learning_rate": 1.0139148141863228e-05,
      "loss": 0.2502,
      "step": 23990
    },
    {
      "epoch": 4.004672117470382,
      "grad_norm": 13.343177795410156,
      "learning_rate": 1.012217885627015e-05,
      "loss": 0.3794,
      "step": 24000
    },
    {
      "epoch": 4.006340730852662,
      "grad_norm": 0.8656461834907532,
      "learning_rate": 1.0105209570677075e-05,
      "loss": 0.3767,
      "step": 24010
    },
    {
      "epoch": 4.0080093442349405,
      "grad_norm": 7.313538551330566,
      "learning_rate": 1.0088240285083997e-05,
      "loss": 0.4735,
      "step": 24020
    },
    {
      "epoch": 4.00967795761722,
      "grad_norm": 0.49466729164123535,
      "learning_rate": 1.0071270999490922e-05,
      "loss": 0.3387,
      "step": 24030
    },
    {
      "epoch": 4.0113465709995,
      "grad_norm": 0.6180391907691956,
      "learning_rate": 1.0054301713897846e-05,
      "loss": 0.3591,
      "step": 24040
    },
    {
      "epoch": 4.0130151843817785,
      "grad_norm": 4.656497001647949,
      "learning_rate": 1.0037332428304768e-05,
      "loss": 0.2558,
      "step": 24050
    },
    {
      "epoch": 4.014683797764058,
      "grad_norm": 8.444700241088867,
      "learning_rate": 1.0020363142711692e-05,
      "loss": 0.3798,
      "step": 24060
    },
    {
      "epoch": 4.016352411146338,
      "grad_norm": 6.259903430938721,
      "learning_rate": 1.0003393857118617e-05,
      "loss": 0.5538,
      "step": 24070
    },
    {
      "epoch": 4.0180210245286165,
      "grad_norm": 5.346273422241211,
      "learning_rate": 9.986424571525539e-06,
      "loss": 0.3899,
      "step": 24080
    },
    {
      "epoch": 4.019689637910896,
      "grad_norm": 6.052495002746582,
      "learning_rate": 9.969455285932462e-06,
      "loss": 0.3945,
      "step": 24090
    },
    {
      "epoch": 4.021358251293176,
      "grad_norm": 16.684904098510742,
      "learning_rate": 9.952486000339386e-06,
      "loss": 0.5759,
      "step": 24100
    },
    {
      "epoch": 4.023026864675455,
      "grad_norm": 6.653503894805908,
      "learning_rate": 9.93551671474631e-06,
      "loss": 0.3154,
      "step": 24110
    },
    {
      "epoch": 4.024695478057734,
      "grad_norm": 1.482018232345581,
      "learning_rate": 9.918547429153232e-06,
      "loss": 0.1824,
      "step": 24120
    },
    {
      "epoch": 4.026364091440013,
      "grad_norm": 8.48513126373291,
      "learning_rate": 9.901578143560157e-06,
      "loss": 0.3437,
      "step": 24130
    },
    {
      "epoch": 4.028032704822293,
      "grad_norm": 2.25888729095459,
      "learning_rate": 9.88460885796708e-06,
      "loss": 0.4751,
      "step": 24140
    },
    {
      "epoch": 4.029701318204572,
      "grad_norm": 4.035806655883789,
      "learning_rate": 9.867639572374003e-06,
      "loss": 0.2779,
      "step": 24150
    },
    {
      "epoch": 4.031369931586851,
      "grad_norm": 10.477704048156738,
      "learning_rate": 9.850670286780928e-06,
      "loss": 0.5022,
      "step": 24160
    },
    {
      "epoch": 4.033038544969131,
      "grad_norm": 5.648310661315918,
      "learning_rate": 9.83370100118785e-06,
      "loss": 0.4874,
      "step": 24170
    },
    {
      "epoch": 4.03470715835141,
      "grad_norm": 2.9133405685424805,
      "learning_rate": 9.816731715594774e-06,
      "loss": 0.3094,
      "step": 24180
    },
    {
      "epoch": 4.036375771733689,
      "grad_norm": 0.5232240557670593,
      "learning_rate": 9.799762430001698e-06,
      "loss": 0.5121,
      "step": 24190
    },
    {
      "epoch": 4.038044385115969,
      "grad_norm": 17.582279205322266,
      "learning_rate": 9.78279314440862e-06,
      "loss": 0.4154,
      "step": 24200
    },
    {
      "epoch": 4.039712998498248,
      "grad_norm": 16.10737419128418,
      "learning_rate": 9.765823858815543e-06,
      "loss": 0.5134,
      "step": 24210
    },
    {
      "epoch": 4.041381611880527,
      "grad_norm": 1.696399450302124,
      "learning_rate": 9.74885457322247e-06,
      "loss": 0.3453,
      "step": 24220
    },
    {
      "epoch": 4.043050225262807,
      "grad_norm": 0.7025350332260132,
      "learning_rate": 9.731885287629392e-06,
      "loss": 0.4683,
      "step": 24230
    },
    {
      "epoch": 4.044718838645086,
      "grad_norm": 4.682060718536377,
      "learning_rate": 9.714916002036314e-06,
      "loss": 0.2408,
      "step": 24240
    },
    {
      "epoch": 4.046387452027365,
      "grad_norm": 3.2513630390167236,
      "learning_rate": 9.697946716443238e-06,
      "loss": 0.4843,
      "step": 24250
    },
    {
      "epoch": 4.048056065409645,
      "grad_norm": 14.392263412475586,
      "learning_rate": 9.680977430850163e-06,
      "loss": 0.4029,
      "step": 24260
    },
    {
      "epoch": 4.049724678791924,
      "grad_norm": 4.635587215423584,
      "learning_rate": 9.664008145257085e-06,
      "loss": 0.3318,
      "step": 24270
    },
    {
      "epoch": 4.051393292174203,
      "grad_norm": 3.3690853118896484,
      "learning_rate": 9.647038859664008e-06,
      "loss": 0.4352,
      "step": 24280
    },
    {
      "epoch": 4.053061905556483,
      "grad_norm": 10.548876762390137,
      "learning_rate": 9.630069574070932e-06,
      "loss": 0.3506,
      "step": 24290
    },
    {
      "epoch": 4.0547305189387615,
      "grad_norm": 13.170390129089355,
      "learning_rate": 9.613100288477856e-06,
      "loss": 0.3832,
      "step": 24300
    },
    {
      "epoch": 4.056399132321041,
      "grad_norm": 5.4386820793151855,
      "learning_rate": 9.596131002884778e-06,
      "loss": 0.3656,
      "step": 24310
    },
    {
      "epoch": 4.058067745703321,
      "grad_norm": 7.497888565063477,
      "learning_rate": 9.579161717291703e-06,
      "loss": 0.4106,
      "step": 24320
    },
    {
      "epoch": 4.0597363590855995,
      "grad_norm": 1.0412421226501465,
      "learning_rate": 9.562192431698625e-06,
      "loss": 0.4456,
      "step": 24330
    },
    {
      "epoch": 4.061404972467879,
      "grad_norm": 0.2972257137298584,
      "learning_rate": 9.54522314610555e-06,
      "loss": 0.4335,
      "step": 24340
    },
    {
      "epoch": 4.063073585850159,
      "grad_norm": 5.082659721374512,
      "learning_rate": 9.528253860512473e-06,
      "loss": 0.4031,
      "step": 24350
    },
    {
      "epoch": 4.0647421992324375,
      "grad_norm": 2.5445046424865723,
      "learning_rate": 9.511284574919396e-06,
      "loss": 0.5044,
      "step": 24360
    },
    {
      "epoch": 4.066410812614717,
      "grad_norm": 13.458425521850586,
      "learning_rate": 9.49431528932632e-06,
      "loss": 0.6025,
      "step": 24370
    },
    {
      "epoch": 4.068079425996997,
      "grad_norm": 7.880295276641846,
      "learning_rate": 9.477346003733244e-06,
      "loss": 0.6081,
      "step": 24380
    },
    {
      "epoch": 4.0697480393792755,
      "grad_norm": 11.369725227355957,
      "learning_rate": 9.460376718140167e-06,
      "loss": 0.5168,
      "step": 24390
    },
    {
      "epoch": 4.071416652761555,
      "grad_norm": 7.433011054992676,
      "learning_rate": 9.44340743254709e-06,
      "loss": 0.5151,
      "step": 24400
    },
    {
      "epoch": 4.073085266143835,
      "grad_norm": 12.905034065246582,
      "learning_rate": 9.426438146954013e-06,
      "loss": 0.6456,
      "step": 24410
    },
    {
      "epoch": 4.074753879526114,
      "grad_norm": 1.1140358448028564,
      "learning_rate": 9.409468861360938e-06,
      "loss": 0.3215,
      "step": 24420
    },
    {
      "epoch": 4.076422492908393,
      "grad_norm": 9.20281982421875,
      "learning_rate": 9.39249957576786e-06,
      "loss": 0.6556,
      "step": 24430
    },
    {
      "epoch": 4.078091106290673,
      "grad_norm": 9.613609313964844,
      "learning_rate": 9.375530290174784e-06,
      "loss": 0.3167,
      "step": 24440
    },
    {
      "epoch": 4.079759719672952,
      "grad_norm": 12.178567886352539,
      "learning_rate": 9.358561004581708e-06,
      "loss": 0.564,
      "step": 24450
    },
    {
      "epoch": 4.081428333055231,
      "grad_norm": 6.596679210662842,
      "learning_rate": 9.341591718988631e-06,
      "loss": 0.3137,
      "step": 24460
    },
    {
      "epoch": 4.083096946437511,
      "grad_norm": 4.8152079582214355,
      "learning_rate": 9.324622433395555e-06,
      "loss": 0.4737,
      "step": 24470
    },
    {
      "epoch": 4.08476555981979,
      "grad_norm": 12.153231620788574,
      "learning_rate": 9.307653147802478e-06,
      "loss": 0.4516,
      "step": 24480
    },
    {
      "epoch": 4.086434173202069,
      "grad_norm": 12.03254222869873,
      "learning_rate": 9.290683862209402e-06,
      "loss": 0.5441,
      "step": 24490
    },
    {
      "epoch": 4.088102786584348,
      "grad_norm": 6.724767208099365,
      "learning_rate": 9.273714576616326e-06,
      "loss": 0.3834,
      "step": 24500
    },
    {
      "epoch": 4.089771399966628,
      "grad_norm": 6.731882095336914,
      "learning_rate": 9.256745291023248e-06,
      "loss": 0.4623,
      "step": 24510
    },
    {
      "epoch": 4.091440013348907,
      "grad_norm": 3.36336088180542,
      "learning_rate": 9.239776005430171e-06,
      "loss": 0.2145,
      "step": 24520
    },
    {
      "epoch": 4.093108626731186,
      "grad_norm": 5.008838176727295,
      "learning_rate": 9.222806719837097e-06,
      "loss": 0.5409,
      "step": 24530
    },
    {
      "epoch": 4.094777240113466,
      "grad_norm": 5.6311421394348145,
      "learning_rate": 9.20583743424402e-06,
      "loss": 0.4747,
      "step": 24540
    },
    {
      "epoch": 4.096445853495745,
      "grad_norm": 1.1115946769714355,
      "learning_rate": 9.188868148650942e-06,
      "loss": 0.3487,
      "step": 24550
    },
    {
      "epoch": 4.098114466878024,
      "grad_norm": 15.069982528686523,
      "learning_rate": 9.171898863057864e-06,
      "loss": 0.5041,
      "step": 24560
    },
    {
      "epoch": 4.099783080260304,
      "grad_norm": 10.68872356414795,
      "learning_rate": 9.15492957746479e-06,
      "loss": 0.5569,
      "step": 24570
    },
    {
      "epoch": 4.101451693642583,
      "grad_norm": 6.190112113952637,
      "learning_rate": 9.137960291871713e-06,
      "loss": 0.3461,
      "step": 24580
    },
    {
      "epoch": 4.103120307024862,
      "grad_norm": 9.7101411819458,
      "learning_rate": 9.120991006278635e-06,
      "loss": 0.6597,
      "step": 24590
    },
    {
      "epoch": 4.104788920407142,
      "grad_norm": 5.251349449157715,
      "learning_rate": 9.10402172068556e-06,
      "loss": 0.3095,
      "step": 24600
    },
    {
      "epoch": 4.106457533789421,
      "grad_norm": 0.9650323987007141,
      "learning_rate": 9.087052435092483e-06,
      "loss": 0.623,
      "step": 24610
    },
    {
      "epoch": 4.1081261471717,
      "grad_norm": 9.539114952087402,
      "learning_rate": 9.070083149499406e-06,
      "loss": 0.5689,
      "step": 24620
    },
    {
      "epoch": 4.10979476055398,
      "grad_norm": 1.0902482271194458,
      "learning_rate": 9.05311386390633e-06,
      "loss": 0.3572,
      "step": 24630
    },
    {
      "epoch": 4.111463373936259,
      "grad_norm": 1.8915785551071167,
      "learning_rate": 9.036144578313253e-06,
      "loss": 0.4687,
      "step": 24640
    },
    {
      "epoch": 4.113131987318538,
      "grad_norm": 8.48188304901123,
      "learning_rate": 9.019175292720177e-06,
      "loss": 0.3614,
      "step": 24650
    },
    {
      "epoch": 4.114800600700818,
      "grad_norm": 3.1420786380767822,
      "learning_rate": 9.002206007127101e-06,
      "loss": 0.5928,
      "step": 24660
    },
    {
      "epoch": 4.1164692140830965,
      "grad_norm": 3.4698305130004883,
      "learning_rate": 8.985236721534023e-06,
      "loss": 0.3,
      "step": 24670
    },
    {
      "epoch": 4.118137827465376,
      "grad_norm": 1.742368221282959,
      "learning_rate": 8.968267435940948e-06,
      "loss": 0.3799,
      "step": 24680
    },
    {
      "epoch": 4.119806440847656,
      "grad_norm": 2.0194897651672363,
      "learning_rate": 8.951298150347872e-06,
      "loss": 0.2185,
      "step": 24690
    },
    {
      "epoch": 4.1214750542299345,
      "grad_norm": 11.93918514251709,
      "learning_rate": 8.934328864754794e-06,
      "loss": 0.4739,
      "step": 24700
    },
    {
      "epoch": 4.123143667612214,
      "grad_norm": 6.7659196853637695,
      "learning_rate": 8.917359579161717e-06,
      "loss": 0.2765,
      "step": 24710
    },
    {
      "epoch": 4.124812280994494,
      "grad_norm": 1.0670267343521118,
      "learning_rate": 8.900390293568641e-06,
      "loss": 0.5449,
      "step": 24720
    },
    {
      "epoch": 4.126480894376773,
      "grad_norm": 14.297426223754883,
      "learning_rate": 8.883421007975565e-06,
      "loss": 0.595,
      "step": 24730
    },
    {
      "epoch": 4.128149507759052,
      "grad_norm": 0.40233251452445984,
      "learning_rate": 8.866451722382488e-06,
      "loss": 0.3045,
      "step": 24740
    },
    {
      "epoch": 4.129818121141332,
      "grad_norm": 5.9116363525390625,
      "learning_rate": 8.849482436789412e-06,
      "loss": 0.3901,
      "step": 24750
    },
    {
      "epoch": 4.131486734523611,
      "grad_norm": 11.56021785736084,
      "learning_rate": 8.832513151196336e-06,
      "loss": 0.3757,
      "step": 24760
    },
    {
      "epoch": 4.13315534790589,
      "grad_norm": 2.25779390335083,
      "learning_rate": 8.815543865603258e-06,
      "loss": 0.6447,
      "step": 24770
    },
    {
      "epoch": 4.13482396128817,
      "grad_norm": 9.264330863952637,
      "learning_rate": 8.798574580010183e-06,
      "loss": 0.3752,
      "step": 24780
    },
    {
      "epoch": 4.136492574670449,
      "grad_norm": 3.953810691833496,
      "learning_rate": 8.781605294417105e-06,
      "loss": 0.3068,
      "step": 24790
    },
    {
      "epoch": 4.138161188052728,
      "grad_norm": 14.156438827514648,
      "learning_rate": 8.76463600882403e-06,
      "loss": 0.5863,
      "step": 24800
    },
    {
      "epoch": 4.139829801435008,
      "grad_norm": 6.352057933807373,
      "learning_rate": 8.747666723230952e-06,
      "loss": 0.3029,
      "step": 24810
    },
    {
      "epoch": 4.141498414817287,
      "grad_norm": 10.068145751953125,
      "learning_rate": 8.730697437637876e-06,
      "loss": 0.5161,
      "step": 24820
    },
    {
      "epoch": 4.143167028199566,
      "grad_norm": 2.7527801990509033,
      "learning_rate": 8.713728152044798e-06,
      "loss": 0.4006,
      "step": 24830
    },
    {
      "epoch": 4.144835641581846,
      "grad_norm": 1.235034465789795,
      "learning_rate": 8.696758866451723e-06,
      "loss": 0.6119,
      "step": 24840
    },
    {
      "epoch": 4.146504254964125,
      "grad_norm": 12.500327110290527,
      "learning_rate": 8.679789580858647e-06,
      "loss": 0.4036,
      "step": 24850
    },
    {
      "epoch": 4.148172868346404,
      "grad_norm": 10.280517578125,
      "learning_rate": 8.66282029526557e-06,
      "loss": 0.4796,
      "step": 24860
    },
    {
      "epoch": 4.149841481728683,
      "grad_norm": 13.309111595153809,
      "learning_rate": 8.645851009672492e-06,
      "loss": 0.6147,
      "step": 24870
    },
    {
      "epoch": 4.151510095110963,
      "grad_norm": 17.03380584716797,
      "learning_rate": 8.628881724079418e-06,
      "loss": 0.3919,
      "step": 24880
    },
    {
      "epoch": 4.153178708493242,
      "grad_norm": 4.610077381134033,
      "learning_rate": 8.61191243848634e-06,
      "loss": 0.4007,
      "step": 24890
    },
    {
      "epoch": 4.154847321875521,
      "grad_norm": 0.09573010355234146,
      "learning_rate": 8.594943152893263e-06,
      "loss": 0.3212,
      "step": 24900
    },
    {
      "epoch": 4.156515935257801,
      "grad_norm": 11.844785690307617,
      "learning_rate": 8.577973867300187e-06,
      "loss": 0.625,
      "step": 24910
    },
    {
      "epoch": 4.15818454864008,
      "grad_norm": 5.781532287597656,
      "learning_rate": 8.561004581707111e-06,
      "loss": 0.5378,
      "step": 24920
    },
    {
      "epoch": 4.159853162022359,
      "grad_norm": 14.303425788879395,
      "learning_rate": 8.544035296114033e-06,
      "loss": 0.4338,
      "step": 24930
    },
    {
      "epoch": 4.161521775404639,
      "grad_norm": 1.1369637250900269,
      "learning_rate": 8.527066010520958e-06,
      "loss": 0.3132,
      "step": 24940
    },
    {
      "epoch": 4.163190388786918,
      "grad_norm": 2.744520425796509,
      "learning_rate": 8.51009672492788e-06,
      "loss": 0.4821,
      "step": 24950
    },
    {
      "epoch": 4.164859002169197,
      "grad_norm": 8.181071281433105,
      "learning_rate": 8.493127439334804e-06,
      "loss": 0.5084,
      "step": 24960
    },
    {
      "epoch": 4.166527615551477,
      "grad_norm": 2.6509921550750732,
      "learning_rate": 8.476158153741729e-06,
      "loss": 0.6368,
      "step": 24970
    },
    {
      "epoch": 4.168196228933756,
      "grad_norm": 11.901603698730469,
      "learning_rate": 8.459188868148651e-06,
      "loss": 0.5417,
      "step": 24980
    },
    {
      "epoch": 4.169864842316035,
      "grad_norm": 12.105852127075195,
      "learning_rate": 8.442219582555575e-06,
      "loss": 0.531,
      "step": 24990
    },
    {
      "epoch": 4.171533455698315,
      "grad_norm": 14.031044006347656,
      "learning_rate": 8.4252502969625e-06,
      "loss": 0.1569,
      "step": 25000
    },
    {
      "epoch": 4.173202069080594,
      "grad_norm": 4.311269760131836,
      "learning_rate": 8.408281011369422e-06,
      "loss": 0.517,
      "step": 25010
    },
    {
      "epoch": 4.174870682462873,
      "grad_norm": 9.163872718811035,
      "learning_rate": 8.391311725776344e-06,
      "loss": 0.495,
      "step": 25020
    },
    {
      "epoch": 4.176539295845153,
      "grad_norm": 6.765301704406738,
      "learning_rate": 8.374342440183269e-06,
      "loss": 0.4679,
      "step": 25030
    },
    {
      "epoch": 4.178207909227432,
      "grad_norm": 6.052847862243652,
      "learning_rate": 8.357373154590193e-06,
      "loss": 0.5075,
      "step": 25040
    },
    {
      "epoch": 4.179876522609711,
      "grad_norm": 14.155113220214844,
      "learning_rate": 8.340403868997115e-06,
      "loss": 0.5391,
      "step": 25050
    },
    {
      "epoch": 4.181545135991991,
      "grad_norm": 10.505096435546875,
      "learning_rate": 8.32343458340404e-06,
      "loss": 0.3531,
      "step": 25060
    },
    {
      "epoch": 4.18321374937427,
      "grad_norm": 5.507193088531494,
      "learning_rate": 8.306465297810964e-06,
      "loss": 0.4229,
      "step": 25070
    },
    {
      "epoch": 4.184882362756549,
      "grad_norm": 14.891902923583984,
      "learning_rate": 8.289496012217886e-06,
      "loss": 0.2424,
      "step": 25080
    },
    {
      "epoch": 4.186550976138829,
      "grad_norm": 6.0166015625,
      "learning_rate": 8.272526726624809e-06,
      "loss": 0.3992,
      "step": 25090
    },
    {
      "epoch": 4.188219589521108,
      "grad_norm": 15.963926315307617,
      "learning_rate": 8.255557441031733e-06,
      "loss": 0.3755,
      "step": 25100
    },
    {
      "epoch": 4.189888202903387,
      "grad_norm": 7.159134387969971,
      "learning_rate": 8.238588155438657e-06,
      "loss": 0.4573,
      "step": 25110
    },
    {
      "epoch": 4.191556816285667,
      "grad_norm": 2.530512809753418,
      "learning_rate": 8.22161886984558e-06,
      "loss": 0.3889,
      "step": 25120
    },
    {
      "epoch": 4.193225429667946,
      "grad_norm": 7.671024799346924,
      "learning_rate": 8.204649584252504e-06,
      "loss": 0.4487,
      "step": 25130
    },
    {
      "epoch": 4.194894043050225,
      "grad_norm": 10.161107063293457,
      "learning_rate": 8.187680298659426e-06,
      "loss": 0.3895,
      "step": 25140
    },
    {
      "epoch": 4.196562656432505,
      "grad_norm": 6.8119096755981445,
      "learning_rate": 8.17071101306635e-06,
      "loss": 0.4063,
      "step": 25150
    },
    {
      "epoch": 4.198231269814784,
      "grad_norm": 6.628838539123535,
      "learning_rate": 8.153741727473274e-06,
      "loss": 0.4747,
      "step": 25160
    },
    {
      "epoch": 4.199899883197063,
      "grad_norm": 10.491214752197266,
      "learning_rate": 8.136772441880197e-06,
      "loss": 0.3847,
      "step": 25170
    },
    {
      "epoch": 4.201568496579343,
      "grad_norm": 8.288521766662598,
      "learning_rate": 8.11980315628712e-06,
      "loss": 0.3685,
      "step": 25180
    },
    {
      "epoch": 4.203237109961622,
      "grad_norm": 1.242976427078247,
      "learning_rate": 8.102833870694045e-06,
      "loss": 0.3381,
      "step": 25190
    },
    {
      "epoch": 4.204905723343901,
      "grad_norm": 0.8349002599716187,
      "learning_rate": 8.085864585100968e-06,
      "loss": 0.5109,
      "step": 25200
    },
    {
      "epoch": 4.206574336726181,
      "grad_norm": 8.620119094848633,
      "learning_rate": 8.06889529950789e-06,
      "loss": 0.3728,
      "step": 25210
    },
    {
      "epoch": 4.20824295010846,
      "grad_norm": 2.310767889022827,
      "learning_rate": 8.051926013914814e-06,
      "loss": 0.2488,
      "step": 25220
    },
    {
      "epoch": 4.209911563490739,
      "grad_norm": 6.25481653213501,
      "learning_rate": 8.034956728321739e-06,
      "loss": 0.3486,
      "step": 25230
    },
    {
      "epoch": 4.211580176873018,
      "grad_norm": 0.6802835464477539,
      "learning_rate": 8.017987442728661e-06,
      "loss": 0.3516,
      "step": 25240
    },
    {
      "epoch": 4.213248790255298,
      "grad_norm": 2.106910467147827,
      "learning_rate": 8.001018157135585e-06,
      "loss": 0.49,
      "step": 25250
    },
    {
      "epoch": 4.214917403637577,
      "grad_norm": 1.0317845344543457,
      "learning_rate": 7.984048871542508e-06,
      "loss": 0.2915,
      "step": 25260
    },
    {
      "epoch": 4.216586017019856,
      "grad_norm": 8.912444114685059,
      "learning_rate": 7.967079585949432e-06,
      "loss": 0.3993,
      "step": 25270
    },
    {
      "epoch": 4.218254630402136,
      "grad_norm": 0.40351423621177673,
      "learning_rate": 7.950110300356356e-06,
      "loss": 0.4168,
      "step": 25280
    },
    {
      "epoch": 4.219923243784415,
      "grad_norm": 14.059187889099121,
      "learning_rate": 7.933141014763279e-06,
      "loss": 0.2532,
      "step": 25290
    },
    {
      "epoch": 4.221591857166694,
      "grad_norm": 10.203642845153809,
      "learning_rate": 7.916171729170203e-06,
      "loss": 0.2378,
      "step": 25300
    },
    {
      "epoch": 4.223260470548974,
      "grad_norm": 6.661139488220215,
      "learning_rate": 7.899202443577127e-06,
      "loss": 0.3488,
      "step": 25310
    },
    {
      "epoch": 4.224929083931253,
      "grad_norm": 2.0712552070617676,
      "learning_rate": 7.88223315798405e-06,
      "loss": 0.2341,
      "step": 25320
    },
    {
      "epoch": 4.226597697313532,
      "grad_norm": 3.400623321533203,
      "learning_rate": 7.865263872390972e-06,
      "loss": 0.5137,
      "step": 25330
    },
    {
      "epoch": 4.228266310695812,
      "grad_norm": 8.897428512573242,
      "learning_rate": 7.848294586797896e-06,
      "loss": 0.4318,
      "step": 25340
    },
    {
      "epoch": 4.2299349240780915,
      "grad_norm": 9.494372367858887,
      "learning_rate": 7.83132530120482e-06,
      "loss": 0.5537,
      "step": 25350
    },
    {
      "epoch": 4.23160353746037,
      "grad_norm": 7.766665935516357,
      "learning_rate": 7.814356015611743e-06,
      "loss": 0.3526,
      "step": 25360
    },
    {
      "epoch": 4.23327215084265,
      "grad_norm": 8.822992324829102,
      "learning_rate": 7.797386730018665e-06,
      "loss": 0.4916,
      "step": 25370
    },
    {
      "epoch": 4.2349407642249295,
      "grad_norm": 0.37540993094444275,
      "learning_rate": 7.780417444425591e-06,
      "loss": 0.3491,
      "step": 25380
    },
    {
      "epoch": 4.236609377607208,
      "grad_norm": 10.53169059753418,
      "learning_rate": 7.763448158832514e-06,
      "loss": 0.3138,
      "step": 25390
    },
    {
      "epoch": 4.238277990989488,
      "grad_norm": 4.584096908569336,
      "learning_rate": 7.746478873239436e-06,
      "loss": 0.4346,
      "step": 25400
    },
    {
      "epoch": 4.239946604371767,
      "grad_norm": 3.4740893840789795,
      "learning_rate": 7.72950958764636e-06,
      "loss": 0.3664,
      "step": 25410
    },
    {
      "epoch": 4.241615217754046,
      "grad_norm": 11.940385818481445,
      "learning_rate": 7.712540302053284e-06,
      "loss": 0.7478,
      "step": 25420
    },
    {
      "epoch": 4.243283831136326,
      "grad_norm": 8.764742851257324,
      "learning_rate": 7.695571016460207e-06,
      "loss": 0.4726,
      "step": 25430
    },
    {
      "epoch": 4.244952444518605,
      "grad_norm": 8.625372886657715,
      "learning_rate": 7.678601730867131e-06,
      "loss": 0.4521,
      "step": 25440
    },
    {
      "epoch": 4.246621057900884,
      "grad_norm": 5.738000392913818,
      "learning_rate": 7.661632445274054e-06,
      "loss": 0.4285,
      "step": 25450
    },
    {
      "epoch": 4.248289671283164,
      "grad_norm": 3.2037532329559326,
      "learning_rate": 7.644663159680978e-06,
      "loss": 0.4031,
      "step": 25460
    },
    {
      "epoch": 4.249958284665443,
      "grad_norm": 2.4392995834350586,
      "learning_rate": 7.627693874087902e-06,
      "loss": 0.2829,
      "step": 25470
    },
    {
      "epoch": 4.251626898047722,
      "grad_norm": 10.409759521484375,
      "learning_rate": 7.6107245884948244e-06,
      "loss": 0.3581,
      "step": 25480
    },
    {
      "epoch": 4.253295511430002,
      "grad_norm": 1.601253628730774,
      "learning_rate": 7.593755302901748e-06,
      "loss": 0.3845,
      "step": 25490
    },
    {
      "epoch": 4.254964124812281,
      "grad_norm": 13.610912322998047,
      "learning_rate": 7.576786017308672e-06,
      "loss": 0.3366,
      "step": 25500
    },
    {
      "epoch": 4.25663273819456,
      "grad_norm": 15.078635215759277,
      "learning_rate": 7.559816731715595e-06,
      "loss": 0.4714,
      "step": 25510
    },
    {
      "epoch": 4.25830135157684,
      "grad_norm": 0.62328040599823,
      "learning_rate": 7.542847446122519e-06,
      "loss": 0.33,
      "step": 25520
    },
    {
      "epoch": 4.259969964959119,
      "grad_norm": 8.957526206970215,
      "learning_rate": 7.525878160529443e-06,
      "loss": 0.4491,
      "step": 25530
    },
    {
      "epoch": 4.261638578341398,
      "grad_norm": 3.325549602508545,
      "learning_rate": 7.508908874936365e-06,
      "loss": 0.2434,
      "step": 25540
    },
    {
      "epoch": 4.263307191723678,
      "grad_norm": 1.7426129579544067,
      "learning_rate": 7.491939589343289e-06,
      "loss": 0.4097,
      "step": 25550
    },
    {
      "epoch": 4.264975805105957,
      "grad_norm": 13.610692024230957,
      "learning_rate": 7.474970303750213e-06,
      "loss": 0.4128,
      "step": 25560
    },
    {
      "epoch": 4.266644418488236,
      "grad_norm": 10.377896308898926,
      "learning_rate": 7.458001018157136e-06,
      "loss": 0.3491,
      "step": 25570
    },
    {
      "epoch": 4.268313031870516,
      "grad_norm": 7.193716049194336,
      "learning_rate": 7.4410317325640595e-06,
      "loss": 0.3173,
      "step": 25580
    },
    {
      "epoch": 4.269981645252795,
      "grad_norm": 8.454743385314941,
      "learning_rate": 7.424062446970984e-06,
      "loss": 0.3501,
      "step": 25590
    },
    {
      "epoch": 4.271650258635074,
      "grad_norm": 2.988154649734497,
      "learning_rate": 7.407093161377906e-06,
      "loss": 0.218,
      "step": 25600
    },
    {
      "epoch": 4.273318872017353,
      "grad_norm": 2.003404378890991,
      "learning_rate": 7.3901238757848295e-06,
      "loss": 0.3258,
      "step": 25610
    },
    {
      "epoch": 4.274987485399633,
      "grad_norm": 0.6586875319480896,
      "learning_rate": 7.373154590191753e-06,
      "loss": 0.542,
      "step": 25620
    },
    {
      "epoch": 4.276656098781912,
      "grad_norm": 3.654438018798828,
      "learning_rate": 7.356185304598677e-06,
      "loss": 0.4258,
      "step": 25630
    },
    {
      "epoch": 4.278324712164191,
      "grad_norm": 7.9785356521606445,
      "learning_rate": 7.3392160190056e-06,
      "loss": 0.4327,
      "step": 25640
    },
    {
      "epoch": 4.279993325546471,
      "grad_norm": 12.085888862609863,
      "learning_rate": 7.322246733412523e-06,
      "loss": 0.3784,
      "step": 25650
    },
    {
      "epoch": 4.2816619389287505,
      "grad_norm": 9.660905838012695,
      "learning_rate": 7.305277447819448e-06,
      "loss": 0.5067,
      "step": 25660
    },
    {
      "epoch": 4.283330552311029,
      "grad_norm": 4.4431562423706055,
      "learning_rate": 7.28830816222637e-06,
      "loss": 0.425,
      "step": 25670
    },
    {
      "epoch": 4.284999165693309,
      "grad_norm": 12.944758415222168,
      "learning_rate": 7.271338876633294e-06,
      "loss": 0.6511,
      "step": 25680
    },
    {
      "epoch": 4.2866677790755885,
      "grad_norm": 10.088142395019531,
      "learning_rate": 7.254369591040218e-06,
      "loss": 0.4374,
      "step": 25690
    },
    {
      "epoch": 4.288336392457867,
      "grad_norm": 6.58002233505249,
      "learning_rate": 7.237400305447141e-06,
      "loss": 0.7276,
      "step": 25700
    },
    {
      "epoch": 4.290005005840147,
      "grad_norm": 6.657474040985107,
      "learning_rate": 7.220431019854064e-06,
      "loss": 0.3306,
      "step": 25710
    },
    {
      "epoch": 4.2916736192224265,
      "grad_norm": 1.7544831037521362,
      "learning_rate": 7.203461734260989e-06,
      "loss": 0.3323,
      "step": 25720
    },
    {
      "epoch": 4.293342232604705,
      "grad_norm": 4.725585460662842,
      "learning_rate": 7.186492448667911e-06,
      "loss": 0.3163,
      "step": 25730
    },
    {
      "epoch": 4.295010845986985,
      "grad_norm": 4.5039381980896,
      "learning_rate": 7.1695231630748345e-06,
      "loss": 0.2674,
      "step": 25740
    },
    {
      "epoch": 4.2966794593692645,
      "grad_norm": 8.166399955749512,
      "learning_rate": 7.152553877481759e-06,
      "loss": 0.2405,
      "step": 25750
    },
    {
      "epoch": 4.298348072751543,
      "grad_norm": 10.004121780395508,
      "learning_rate": 7.135584591888682e-06,
      "loss": 0.4273,
      "step": 25760
    },
    {
      "epoch": 4.300016686133823,
      "grad_norm": 5.523026466369629,
      "learning_rate": 7.1186153062956045e-06,
      "loss": 0.5136,
      "step": 25770
    },
    {
      "epoch": 4.301685299516102,
      "grad_norm": 3.0542283058166504,
      "learning_rate": 7.1016460207025295e-06,
      "loss": 0.4691,
      "step": 25780
    },
    {
      "epoch": 4.303353912898381,
      "grad_norm": 12.683801651000977,
      "learning_rate": 7.084676735109452e-06,
      "loss": 0.6013,
      "step": 25790
    },
    {
      "epoch": 4.305022526280661,
      "grad_norm": 10.85103702545166,
      "learning_rate": 7.067707449516375e-06,
      "loss": 0.6488,
      "step": 25800
    },
    {
      "epoch": 4.30669113966294,
      "grad_norm": 11.88544750213623,
      "learning_rate": 7.0507381639232995e-06,
      "loss": 0.5456,
      "step": 25810
    },
    {
      "epoch": 4.308359753045219,
      "grad_norm": 3.988389730453491,
      "learning_rate": 7.033768878330223e-06,
      "loss": 0.3616,
      "step": 25820
    },
    {
      "epoch": 4.310028366427499,
      "grad_norm": 4.431445598602295,
      "learning_rate": 7.016799592737145e-06,
      "loss": 0.5737,
      "step": 25830
    },
    {
      "epoch": 4.311696979809778,
      "grad_norm": 16.25014877319336,
      "learning_rate": 6.99983030714407e-06,
      "loss": 0.6323,
      "step": 25840
    },
    {
      "epoch": 4.313365593192057,
      "grad_norm": 6.3366169929504395,
      "learning_rate": 6.982861021550993e-06,
      "loss": 0.5263,
      "step": 25850
    },
    {
      "epoch": 4.315034206574337,
      "grad_norm": 10.234457969665527,
      "learning_rate": 6.965891735957916e-06,
      "loss": 0.3605,
      "step": 25860
    },
    {
      "epoch": 4.316702819956616,
      "grad_norm": 1.7127031087875366,
      "learning_rate": 6.94892245036484e-06,
      "loss": 0.4382,
      "step": 25870
    },
    {
      "epoch": 4.318371433338895,
      "grad_norm": 3.2268943786621094,
      "learning_rate": 6.931953164771764e-06,
      "loss": 0.2803,
      "step": 25880
    },
    {
      "epoch": 4.320040046721175,
      "grad_norm": 17.47093963623047,
      "learning_rate": 6.914983879178687e-06,
      "loss": 0.5541,
      "step": 25890
    },
    {
      "epoch": 4.321708660103454,
      "grad_norm": 2.8333146572113037,
      "learning_rate": 6.8980145935856095e-06,
      "loss": 0.357,
      "step": 25900
    },
    {
      "epoch": 4.323377273485733,
      "grad_norm": 4.195406436920166,
      "learning_rate": 6.881045307992534e-06,
      "loss": 0.3985,
      "step": 25910
    },
    {
      "epoch": 4.325045886868013,
      "grad_norm": 21.635051727294922,
      "learning_rate": 6.864076022399457e-06,
      "loss": 0.365,
      "step": 25920
    },
    {
      "epoch": 4.326714500250292,
      "grad_norm": 14.583856582641602,
      "learning_rate": 6.84710673680638e-06,
      "loss": 0.4293,
      "step": 25930
    },
    {
      "epoch": 4.328383113632571,
      "grad_norm": 10.0009765625,
      "learning_rate": 6.8301374512133046e-06,
      "loss": 0.3687,
      "step": 25940
    },
    {
      "epoch": 4.330051727014851,
      "grad_norm": 2.4391584396362305,
      "learning_rate": 6.813168165620228e-06,
      "loss": 0.4398,
      "step": 25950
    },
    {
      "epoch": 4.33172034039713,
      "grad_norm": 7.68290901184082,
      "learning_rate": 6.79619888002715e-06,
      "loss": 0.3406,
      "step": 25960
    },
    {
      "epoch": 4.3333889537794095,
      "grad_norm": 5.250075817108154,
      "learning_rate": 6.779229594434075e-06,
      "loss": 0.4636,
      "step": 25970
    },
    {
      "epoch": 4.335057567161689,
      "grad_norm": 5.880498886108398,
      "learning_rate": 6.762260308840998e-06,
      "loss": 0.5525,
      "step": 25980
    },
    {
      "epoch": 4.336726180543968,
      "grad_norm": 5.355949401855469,
      "learning_rate": 6.745291023247921e-06,
      "loss": 0.3259,
      "step": 25990
    },
    {
      "epoch": 4.3383947939262475,
      "grad_norm": 7.149447917938232,
      "learning_rate": 6.728321737654845e-06,
      "loss": 0.3513,
      "step": 26000
    },
    {
      "epoch": 4.340063407308526,
      "grad_norm": 9.259510040283203,
      "learning_rate": 6.711352452061769e-06,
      "loss": 0.5101,
      "step": 26010
    },
    {
      "epoch": 4.341732020690806,
      "grad_norm": 2.24530291557312,
      "learning_rate": 6.694383166468691e-06,
      "loss": 0.4556,
      "step": 26020
    },
    {
      "epoch": 4.3434006340730855,
      "grad_norm": 0.9138747453689575,
      "learning_rate": 6.677413880875616e-06,
      "loss": 0.3248,
      "step": 26030
    },
    {
      "epoch": 4.345069247455364,
      "grad_norm": 0.18457068502902985,
      "learning_rate": 6.660444595282539e-06,
      "loss": 0.2688,
      "step": 26040
    },
    {
      "epoch": 4.346737860837644,
      "grad_norm": 1.0885181427001953,
      "learning_rate": 6.643475309689462e-06,
      "loss": 0.3454,
      "step": 26050
    },
    {
      "epoch": 4.3484064742199235,
      "grad_norm": 3.6070470809936523,
      "learning_rate": 6.626506024096386e-06,
      "loss": 0.2822,
      "step": 26060
    },
    {
      "epoch": 4.350075087602202,
      "grad_norm": 8.72636604309082,
      "learning_rate": 6.60953673850331e-06,
      "loss": 0.4667,
      "step": 26070
    },
    {
      "epoch": 4.351743700984482,
      "grad_norm": 3.1474709510803223,
      "learning_rate": 6.592567452910232e-06,
      "loss": 0.4539,
      "step": 26080
    },
    {
      "epoch": 4.353412314366762,
      "grad_norm": 4.2105255126953125,
      "learning_rate": 6.575598167317157e-06,
      "loss": 0.2933,
      "step": 26090
    },
    {
      "epoch": 4.35508092774904,
      "grad_norm": 13.31529426574707,
      "learning_rate": 6.55862888172408e-06,
      "loss": 0.5062,
      "step": 26100
    },
    {
      "epoch": 4.35674954113132,
      "grad_norm": 4.929466247558594,
      "learning_rate": 6.541659596131003e-06,
      "loss": 0.4405,
      "step": 26110
    },
    {
      "epoch": 4.3584181545136,
      "grad_norm": 14.165456771850586,
      "learning_rate": 6.524690310537927e-06,
      "loss": 0.6512,
      "step": 26120
    },
    {
      "epoch": 4.360086767895878,
      "grad_norm": 13.692586898803711,
      "learning_rate": 6.5077210249448504e-06,
      "loss": 0.3316,
      "step": 26130
    },
    {
      "epoch": 4.361755381278158,
      "grad_norm": 2.5381412506103516,
      "learning_rate": 6.490751739351773e-06,
      "loss": 0.4383,
      "step": 26140
    },
    {
      "epoch": 4.363423994660437,
      "grad_norm": 4.643336772918701,
      "learning_rate": 6.473782453758698e-06,
      "loss": 0.4083,
      "step": 26150
    },
    {
      "epoch": 4.365092608042716,
      "grad_norm": 7.2826714515686035,
      "learning_rate": 6.4568131681656204e-06,
      "loss": 0.4564,
      "step": 26160
    },
    {
      "epoch": 4.366761221424996,
      "grad_norm": 10.264361381530762,
      "learning_rate": 6.439843882572544e-06,
      "loss": 0.6393,
      "step": 26170
    },
    {
      "epoch": 4.368429834807275,
      "grad_norm": 6.265644073486328,
      "learning_rate": 6.422874596979467e-06,
      "loss": 0.3182,
      "step": 26180
    },
    {
      "epoch": 4.370098448189554,
      "grad_norm": 3.720048427581787,
      "learning_rate": 6.405905311386391e-06,
      "loss": 0.5384,
      "step": 26190
    },
    {
      "epoch": 4.371767061571834,
      "grad_norm": 3.4506564140319824,
      "learning_rate": 6.388936025793315e-06,
      "loss": 0.3672,
      "step": 26200
    },
    {
      "epoch": 4.373435674954113,
      "grad_norm": 9.429092407226562,
      "learning_rate": 6.371966740200237e-06,
      "loss": 0.6087,
      "step": 26210
    },
    {
      "epoch": 4.375104288336392,
      "grad_norm": 10.90219497680664,
      "learning_rate": 6.354997454607161e-06,
      "loss": 0.4043,
      "step": 26220
    },
    {
      "epoch": 4.376772901718672,
      "grad_norm": 6.138680458068848,
      "learning_rate": 6.338028169014085e-06,
      "loss": 0.7941,
      "step": 26230
    },
    {
      "epoch": 4.378441515100951,
      "grad_norm": 3.074021339416504,
      "learning_rate": 6.321058883421008e-06,
      "loss": 0.4322,
      "step": 26240
    },
    {
      "epoch": 4.3801101284832304,
      "grad_norm": 11.001702308654785,
      "learning_rate": 6.304089597827932e-06,
      "loss": 0.4286,
      "step": 26250
    },
    {
      "epoch": 4.38177874186551,
      "grad_norm": 12.614580154418945,
      "learning_rate": 6.2871203122348555e-06,
      "loss": 0.4507,
      "step": 26260
    },
    {
      "epoch": 4.383447355247789,
      "grad_norm": 0.6022441983222961,
      "learning_rate": 6.270151026641778e-06,
      "loss": 0.4354,
      "step": 26270
    },
    {
      "epoch": 4.3851159686300685,
      "grad_norm": 15.436945915222168,
      "learning_rate": 6.253181741048703e-06,
      "loss": 0.3442,
      "step": 26280
    },
    {
      "epoch": 4.386784582012348,
      "grad_norm": 9.616364479064941,
      "learning_rate": 6.2362124554556255e-06,
      "loss": 0.6747,
      "step": 26290
    },
    {
      "epoch": 4.388453195394627,
      "grad_norm": 2.9133670330047607,
      "learning_rate": 6.21924316986255e-06,
      "loss": 0.2808,
      "step": 26300
    },
    {
      "epoch": 4.3901218087769065,
      "grad_norm": 9.255575180053711,
      "learning_rate": 6.202273884269472e-06,
      "loss": 0.468,
      "step": 26310
    },
    {
      "epoch": 4.391790422159186,
      "grad_norm": 15.170294761657715,
      "learning_rate": 6.185304598676396e-06,
      "loss": 0.5353,
      "step": 26320
    },
    {
      "epoch": 4.393459035541465,
      "grad_norm": 8.080717086791992,
      "learning_rate": 6.16833531308332e-06,
      "loss": 0.5311,
      "step": 26330
    },
    {
      "epoch": 4.3951276489237445,
      "grad_norm": 5.242458820343018,
      "learning_rate": 6.151366027490243e-06,
      "loss": 0.3213,
      "step": 26340
    },
    {
      "epoch": 4.396796262306024,
      "grad_norm": 1.8530884981155396,
      "learning_rate": 6.134396741897166e-06,
      "loss": 0.2425,
      "step": 26350
    },
    {
      "epoch": 4.398464875688303,
      "grad_norm": 5.29703950881958,
      "learning_rate": 6.1174274563040905e-06,
      "loss": 0.4195,
      "step": 26360
    },
    {
      "epoch": 4.4001334890705825,
      "grad_norm": 5.109864711761475,
      "learning_rate": 6.100458170711013e-06,
      "loss": 0.4303,
      "step": 26370
    },
    {
      "epoch": 4.401802102452861,
      "grad_norm": 4.419559001922607,
      "learning_rate": 6.083488885117937e-06,
      "loss": 0.4141,
      "step": 26380
    },
    {
      "epoch": 4.403470715835141,
      "grad_norm": 3.7554819583892822,
      "learning_rate": 6.06651959952486e-06,
      "loss": 0.3485,
      "step": 26390
    },
    {
      "epoch": 4.405139329217421,
      "grad_norm": 4.876214981079102,
      "learning_rate": 6.049550313931784e-06,
      "loss": 0.2407,
      "step": 26400
    },
    {
      "epoch": 4.406807942599699,
      "grad_norm": 8.695779800415039,
      "learning_rate": 6.032581028338707e-06,
      "loss": 0.4156,
      "step": 26410
    },
    {
      "epoch": 4.408476555981979,
      "grad_norm": 4.470856189727783,
      "learning_rate": 6.0156117427456305e-06,
      "loss": 0.6013,
      "step": 26420
    },
    {
      "epoch": 4.410145169364259,
      "grad_norm": 5.963262557983398,
      "learning_rate": 5.998642457152554e-06,
      "loss": 0.4895,
      "step": 26430
    },
    {
      "epoch": 4.411813782746537,
      "grad_norm": 1.4263262748718262,
      "learning_rate": 5.981673171559478e-06,
      "loss": 0.2873,
      "step": 26440
    },
    {
      "epoch": 4.413482396128817,
      "grad_norm": 7.322517395019531,
      "learning_rate": 5.9647038859664005e-06,
      "loss": 0.3958,
      "step": 26450
    },
    {
      "epoch": 4.415151009511097,
      "grad_norm": 11.219895362854004,
      "learning_rate": 5.947734600373325e-06,
      "loss": 0.3057,
      "step": 26460
    },
    {
      "epoch": 4.416819622893375,
      "grad_norm": 5.231565952301025,
      "learning_rate": 5.930765314780248e-06,
      "loss": 0.5315,
      "step": 26470
    },
    {
      "epoch": 4.418488236275655,
      "grad_norm": 1.042220115661621,
      "learning_rate": 5.913796029187171e-06,
      "loss": 0.2541,
      "step": 26480
    },
    {
      "epoch": 4.420156849657935,
      "grad_norm": 4.033961296081543,
      "learning_rate": 5.896826743594095e-06,
      "loss": 0.5221,
      "step": 26490
    },
    {
      "epoch": 4.421825463040213,
      "grad_norm": 15.538555145263672,
      "learning_rate": 5.879857458001019e-06,
      "loss": 0.5404,
      "step": 26500
    },
    {
      "epoch": 4.423494076422493,
      "grad_norm": 12.183585166931152,
      "learning_rate": 5.862888172407942e-06,
      "loss": 0.4476,
      "step": 26510
    },
    {
      "epoch": 4.425162689804772,
      "grad_norm": 3.228740930557251,
      "learning_rate": 5.8459188868148655e-06,
      "loss": 0.2285,
      "step": 26520
    },
    {
      "epoch": 4.426831303187051,
      "grad_norm": 11.685312271118164,
      "learning_rate": 5.828949601221789e-06,
      "loss": 0.3682,
      "step": 26530
    },
    {
      "epoch": 4.428499916569331,
      "grad_norm": 12.038434982299805,
      "learning_rate": 5.811980315628712e-06,
      "loss": 0.5687,
      "step": 26540
    },
    {
      "epoch": 4.43016852995161,
      "grad_norm": 8.78089714050293,
      "learning_rate": 5.795011030035636e-06,
      "loss": 0.3574,
      "step": 26550
    },
    {
      "epoch": 4.4318371433338895,
      "grad_norm": 7.290548324584961,
      "learning_rate": 5.778041744442559e-06,
      "loss": 0.5236,
      "step": 26560
    },
    {
      "epoch": 4.433505756716169,
      "grad_norm": 7.627509593963623,
      "learning_rate": 5.761072458849483e-06,
      "loss": 0.6865,
      "step": 26570
    },
    {
      "epoch": 4.435174370098448,
      "grad_norm": 5.827643871307373,
      "learning_rate": 5.744103173256406e-06,
      "loss": 0.3114,
      "step": 26580
    },
    {
      "epoch": 4.4368429834807275,
      "grad_norm": 11.684551239013672,
      "learning_rate": 5.72713388766333e-06,
      "loss": 0.5877,
      "step": 26590
    },
    {
      "epoch": 4.438511596863007,
      "grad_norm": 5.6003007888793945,
      "learning_rate": 5.710164602070253e-06,
      "loss": 0.5246,
      "step": 26600
    },
    {
      "epoch": 4.440180210245286,
      "grad_norm": 5.0462517738342285,
      "learning_rate": 5.693195316477177e-06,
      "loss": 0.3204,
      "step": 26610
    },
    {
      "epoch": 4.4418488236275655,
      "grad_norm": 5.942358493804932,
      "learning_rate": 5.6762260308841e-06,
      "loss": 0.2829,
      "step": 26620
    },
    {
      "epoch": 4.443517437009845,
      "grad_norm": 4.690689563751221,
      "learning_rate": 5.659256745291024e-06,
      "loss": 0.4193,
      "step": 26630
    },
    {
      "epoch": 4.445186050392124,
      "grad_norm": 7.569028377532959,
      "learning_rate": 5.642287459697946e-06,
      "loss": 0.4104,
      "step": 26640
    },
    {
      "epoch": 4.4468546637744035,
      "grad_norm": 2.678823232650757,
      "learning_rate": 5.6253181741048705e-06,
      "loss": 0.441,
      "step": 26650
    },
    {
      "epoch": 4.448523277156683,
      "grad_norm": 9.640120506286621,
      "learning_rate": 5.608348888511794e-06,
      "loss": 0.2336,
      "step": 26660
    },
    {
      "epoch": 4.450191890538962,
      "grad_norm": 4.151401996612549,
      "learning_rate": 5.591379602918717e-06,
      "loss": 0.5297,
      "step": 26670
    },
    {
      "epoch": 4.4518605039212416,
      "grad_norm": 9.345378875732422,
      "learning_rate": 5.5744103173256405e-06,
      "loss": 0.4206,
      "step": 26680
    },
    {
      "epoch": 4.453529117303521,
      "grad_norm": 10.171698570251465,
      "learning_rate": 5.557441031732565e-06,
      "loss": 0.5829,
      "step": 26690
    },
    {
      "epoch": 4.4551977306858,
      "grad_norm": 5.8328423500061035,
      "learning_rate": 5.540471746139487e-06,
      "loss": 0.6109,
      "step": 26700
    },
    {
      "epoch": 4.45686634406808,
      "grad_norm": 4.036594390869141,
      "learning_rate": 5.523502460546411e-06,
      "loss": 0.3982,
      "step": 26710
    },
    {
      "epoch": 4.458534957450359,
      "grad_norm": 3.6138980388641357,
      "learning_rate": 5.506533174953335e-06,
      "loss": 0.3626,
      "step": 26720
    },
    {
      "epoch": 4.460203570832638,
      "grad_norm": 10.242173194885254,
      "learning_rate": 5.489563889360258e-06,
      "loss": 0.5464,
      "step": 26730
    },
    {
      "epoch": 4.461872184214918,
      "grad_norm": 5.2942962646484375,
      "learning_rate": 5.472594603767181e-06,
      "loss": 0.2204,
      "step": 26740
    },
    {
      "epoch": 4.463540797597196,
      "grad_norm": 10.36677360534668,
      "learning_rate": 5.4556253181741056e-06,
      "loss": 0.8659,
      "step": 26750
    },
    {
      "epoch": 4.465209410979476,
      "grad_norm": 1.2463016510009766,
      "learning_rate": 5.438656032581028e-06,
      "loss": 0.5454,
      "step": 26760
    },
    {
      "epoch": 4.466878024361756,
      "grad_norm": 0.9812593460083008,
      "learning_rate": 5.421686746987952e-06,
      "loss": 0.4329,
      "step": 26770
    },
    {
      "epoch": 4.468546637744034,
      "grad_norm": 2.3662357330322266,
      "learning_rate": 5.4047174613948756e-06,
      "loss": 0.2962,
      "step": 26780
    },
    {
      "epoch": 4.470215251126314,
      "grad_norm": 11.643699645996094,
      "learning_rate": 5.387748175801799e-06,
      "loss": 0.5584,
      "step": 26790
    },
    {
      "epoch": 4.471883864508594,
      "grad_norm": 1.540608525276184,
      "learning_rate": 5.370778890208722e-06,
      "loss": 0.3014,
      "step": 26800
    },
    {
      "epoch": 4.473552477890872,
      "grad_norm": 3.2751429080963135,
      "learning_rate": 5.3538096046156456e-06,
      "loss": 0.3643,
      "step": 26810
    },
    {
      "epoch": 4.475221091273152,
      "grad_norm": 6.8767008781433105,
      "learning_rate": 5.33684031902257e-06,
      "loss": 0.2838,
      "step": 26820
    },
    {
      "epoch": 4.476889704655432,
      "grad_norm": 5.191498279571533,
      "learning_rate": 5.319871033429493e-06,
      "loss": 0.3096,
      "step": 26830
    },
    {
      "epoch": 4.47855831803771,
      "grad_norm": 7.29722261428833,
      "learning_rate": 5.302901747836416e-06,
      "loss": 0.4685,
      "step": 26840
    },
    {
      "epoch": 4.48022693141999,
      "grad_norm": 7.411319732666016,
      "learning_rate": 5.28593246224334e-06,
      "loss": 0.4265,
      "step": 26850
    },
    {
      "epoch": 4.48189554480227,
      "grad_norm": 2.6621062755584717,
      "learning_rate": 5.268963176650264e-06,
      "loss": 0.4431,
      "step": 26860
    },
    {
      "epoch": 4.4835641581845485,
      "grad_norm": 9.973551750183105,
      "learning_rate": 5.251993891057186e-06,
      "loss": 0.3398,
      "step": 26870
    },
    {
      "epoch": 4.485232771566828,
      "grad_norm": 7.130317211151123,
      "learning_rate": 5.235024605464111e-06,
      "loss": 0.5053,
      "step": 26880
    },
    {
      "epoch": 4.486901384949107,
      "grad_norm": 2.263413667678833,
      "learning_rate": 5.218055319871034e-06,
      "loss": 0.1732,
      "step": 26890
    },
    {
      "epoch": 4.4885699983313865,
      "grad_norm": 7.7837958335876465,
      "learning_rate": 5.201086034277957e-06,
      "loss": 0.2741,
      "step": 26900
    },
    {
      "epoch": 4.490238611713666,
      "grad_norm": 9.334626197814941,
      "learning_rate": 5.184116748684881e-06,
      "loss": 0.4832,
      "step": 26910
    },
    {
      "epoch": 4.491907225095945,
      "grad_norm": 8.79013442993164,
      "learning_rate": 5.167147463091804e-06,
      "loss": 0.2164,
      "step": 26920
    },
    {
      "epoch": 4.4935758384782245,
      "grad_norm": 9.46176528930664,
      "learning_rate": 5.150178177498727e-06,
      "loss": 0.2844,
      "step": 26930
    },
    {
      "epoch": 4.495244451860504,
      "grad_norm": 0.06908433884382248,
      "learning_rate": 5.1332088919056514e-06,
      "loss": 0.3298,
      "step": 26940
    },
    {
      "epoch": 4.496913065242783,
      "grad_norm": 2.7595086097717285,
      "learning_rate": 5.116239606312574e-06,
      "loss": 0.2391,
      "step": 26950
    },
    {
      "epoch": 4.4985816786250625,
      "grad_norm": 4.770755290985107,
      "learning_rate": 5.099270320719498e-06,
      "loss": 0.5915,
      "step": 26960
    },
    {
      "epoch": 4.500250292007342,
      "grad_norm": 12.305266380310059,
      "learning_rate": 5.0823010351264214e-06,
      "loss": 0.4354,
      "step": 26970
    },
    {
      "epoch": 4.501918905389621,
      "grad_norm": 15.193796157836914,
      "learning_rate": 5.065331749533345e-06,
      "loss": 0.4292,
      "step": 26980
    },
    {
      "epoch": 4.5035875187719006,
      "grad_norm": 7.242340087890625,
      "learning_rate": 5.048362463940268e-06,
      "loss": 0.544,
      "step": 26990
    },
    {
      "epoch": 4.50525613215418,
      "grad_norm": 7.549778461456299,
      "learning_rate": 5.031393178347192e-06,
      "loss": 0.5036,
      "step": 27000
    },
    {
      "epoch": 4.506924745536459,
      "grad_norm": 9.471549987792969,
      "learning_rate": 5.014423892754115e-06,
      "loss": 0.5194,
      "step": 27010
    },
    {
      "epoch": 4.508593358918739,
      "grad_norm": 8.024505615234375,
      "learning_rate": 4.997454607161039e-06,
      "loss": 0.2478,
      "step": 27020
    },
    {
      "epoch": 4.510261972301018,
      "grad_norm": 5.451275825500488,
      "learning_rate": 4.980485321567962e-06,
      "loss": 0.4178,
      "step": 27030
    },
    {
      "epoch": 4.511930585683297,
      "grad_norm": 19.92279624938965,
      "learning_rate": 4.963516035974886e-06,
      "loss": 0.3605,
      "step": 27040
    },
    {
      "epoch": 4.513599199065577,
      "grad_norm": 4.958181381225586,
      "learning_rate": 4.946546750381809e-06,
      "loss": 0.6789,
      "step": 27050
    },
    {
      "epoch": 4.515267812447856,
      "grad_norm": 22.56696128845215,
      "learning_rate": 4.929577464788732e-06,
      "loss": 0.7036,
      "step": 27060
    },
    {
      "epoch": 4.516936425830135,
      "grad_norm": 10.440452575683594,
      "learning_rate": 4.912608179195656e-06,
      "loss": 0.4781,
      "step": 27070
    },
    {
      "epoch": 4.518605039212415,
      "grad_norm": 4.128088474273682,
      "learning_rate": 4.89563889360258e-06,
      "loss": 0.3327,
      "step": 27080
    },
    {
      "epoch": 4.520273652594694,
      "grad_norm": 19.47671127319336,
      "learning_rate": 4.878669608009503e-06,
      "loss": 0.253,
      "step": 27090
    },
    {
      "epoch": 4.521942265976973,
      "grad_norm": 0.7932700514793396,
      "learning_rate": 4.8617003224164265e-06,
      "loss": 0.3008,
      "step": 27100
    },
    {
      "epoch": 4.523610879359253,
      "grad_norm": 6.80713415145874,
      "learning_rate": 4.84473103682335e-06,
      "loss": 0.3678,
      "step": 27110
    },
    {
      "epoch": 4.525279492741531,
      "grad_norm": 4.143061637878418,
      "learning_rate": 4.827761751230273e-06,
      "loss": 0.4817,
      "step": 27120
    },
    {
      "epoch": 4.526948106123811,
      "grad_norm": 14.79421329498291,
      "learning_rate": 4.810792465637197e-06,
      "loss": 0.4149,
      "step": 27130
    },
    {
      "epoch": 4.528616719506091,
      "grad_norm": 5.463493347167969,
      "learning_rate": 4.793823180044121e-06,
      "loss": 0.4996,
      "step": 27140
    },
    {
      "epoch": 4.530285332888369,
      "grad_norm": 2.9220998287200928,
      "learning_rate": 4.776853894451044e-06,
      "loss": 0.4182,
      "step": 27150
    },
    {
      "epoch": 4.531953946270649,
      "grad_norm": 4.1392292976379395,
      "learning_rate": 4.759884608857967e-06,
      "loss": 0.2024,
      "step": 27160
    },
    {
      "epoch": 4.533622559652929,
      "grad_norm": 0.7524837255477905,
      "learning_rate": 4.7429153232648915e-06,
      "loss": 0.1256,
      "step": 27170
    },
    {
      "epoch": 4.5352911730352075,
      "grad_norm": 5.072991847991943,
      "learning_rate": 4.725946037671814e-06,
      "loss": 0.3487,
      "step": 27180
    },
    {
      "epoch": 4.536959786417487,
      "grad_norm": 2.4376580715179443,
      "learning_rate": 4.708976752078738e-06,
      "loss": 0.6209,
      "step": 27190
    },
    {
      "epoch": 4.538628399799767,
      "grad_norm": 8.414578437805176,
      "learning_rate": 4.692007466485661e-06,
      "loss": 0.9755,
      "step": 27200
    },
    {
      "epoch": 4.5402970131820455,
      "grad_norm": 12.051188468933105,
      "learning_rate": 4.675038180892585e-06,
      "loss": 0.4745,
      "step": 27210
    },
    {
      "epoch": 4.541965626564325,
      "grad_norm": 4.4601922035217285,
      "learning_rate": 4.658068895299508e-06,
      "loss": 0.3193,
      "step": 27220
    },
    {
      "epoch": 4.543634239946605,
      "grad_norm": 8.713332176208496,
      "learning_rate": 4.6410996097064315e-06,
      "loss": 0.5008,
      "step": 27230
    },
    {
      "epoch": 4.5453028533288835,
      "grad_norm": 0.27414774894714355,
      "learning_rate": 4.624130324113355e-06,
      "loss": 0.2811,
      "step": 27240
    },
    {
      "epoch": 4.546971466711163,
      "grad_norm": 4.663654327392578,
      "learning_rate": 4.607161038520279e-06,
      "loss": 0.3647,
      "step": 27250
    },
    {
      "epoch": 4.548640080093442,
      "grad_norm": 11.777816772460938,
      "learning_rate": 4.5901917529272015e-06,
      "loss": 0.204,
      "step": 27260
    },
    {
      "epoch": 4.5503086934757215,
      "grad_norm": 5.760405540466309,
      "learning_rate": 4.573222467334126e-06,
      "loss": 0.3248,
      "step": 27270
    },
    {
      "epoch": 4.551977306858001,
      "grad_norm": 4.224729061126709,
      "learning_rate": 4.556253181741049e-06,
      "loss": 0.3903,
      "step": 27280
    },
    {
      "epoch": 4.55364592024028,
      "grad_norm": 15.853448867797852,
      "learning_rate": 4.539283896147972e-06,
      "loss": 0.7033,
      "step": 27290
    },
    {
      "epoch": 4.55531453362256,
      "grad_norm": 6.15757942199707,
      "learning_rate": 4.522314610554896e-06,
      "loss": 0.5609,
      "step": 27300
    },
    {
      "epoch": 4.556983147004839,
      "grad_norm": 0.8649584054946899,
      "learning_rate": 4.50534532496182e-06,
      "loss": 0.6565,
      "step": 27310
    },
    {
      "epoch": 4.558651760387118,
      "grad_norm": 4.651737689971924,
      "learning_rate": 4.488376039368742e-06,
      "loss": 0.7981,
      "step": 27320
    },
    {
      "epoch": 4.560320373769398,
      "grad_norm": 4.6519365310668945,
      "learning_rate": 4.4714067537756665e-06,
      "loss": 0.3909,
      "step": 27330
    },
    {
      "epoch": 4.561988987151677,
      "grad_norm": 14.5289945602417,
      "learning_rate": 4.454437468182589e-06,
      "loss": 0.5279,
      "step": 27340
    },
    {
      "epoch": 4.563657600533956,
      "grad_norm": 1.6498231887817383,
      "learning_rate": 4.437468182589513e-06,
      "loss": 0.2929,
      "step": 27350
    },
    {
      "epoch": 4.565326213916236,
      "grad_norm": 10.615535736083984,
      "learning_rate": 4.4204988969964365e-06,
      "loss": 0.4498,
      "step": 27360
    },
    {
      "epoch": 4.566994827298515,
      "grad_norm": 3.8148064613342285,
      "learning_rate": 4.40352961140336e-06,
      "loss": 0.5558,
      "step": 27370
    },
    {
      "epoch": 4.568663440680794,
      "grad_norm": 14.215672492980957,
      "learning_rate": 4.386560325810283e-06,
      "loss": 0.554,
      "step": 27380
    },
    {
      "epoch": 4.570332054063074,
      "grad_norm": 1.0331956148147583,
      "learning_rate": 4.369591040217207e-06,
      "loss": 0.2869,
      "step": 27390
    },
    {
      "epoch": 4.572000667445353,
      "grad_norm": 10.18546199798584,
      "learning_rate": 4.352621754624131e-06,
      "loss": 0.519,
      "step": 27400
    },
    {
      "epoch": 4.573669280827632,
      "grad_norm": 5.959697246551514,
      "learning_rate": 4.335652469031054e-06,
      "loss": 0.385,
      "step": 27410
    },
    {
      "epoch": 4.575337894209912,
      "grad_norm": 3.1103525161743164,
      "learning_rate": 4.318683183437977e-06,
      "loss": 0.2379,
      "step": 27420
    },
    {
      "epoch": 4.577006507592191,
      "grad_norm": 24.735319137573242,
      "learning_rate": 4.301713897844901e-06,
      "loss": 0.4331,
      "step": 27430
    },
    {
      "epoch": 4.57867512097447,
      "grad_norm": 1.3552563190460205,
      "learning_rate": 4.284744612251825e-06,
      "loss": 0.4704,
      "step": 27440
    },
    {
      "epoch": 4.58034373435675,
      "grad_norm": 8.087861061096191,
      "learning_rate": 4.267775326658748e-06,
      "loss": 0.4357,
      "step": 27450
    },
    {
      "epoch": 4.582012347739029,
      "grad_norm": 3.6647017002105713,
      "learning_rate": 4.2508060410656715e-06,
      "loss": 0.2089,
      "step": 27460
    },
    {
      "epoch": 4.583680961121308,
      "grad_norm": 11.987323760986328,
      "learning_rate": 4.233836755472595e-06,
      "loss": 0.5732,
      "step": 27470
    },
    {
      "epoch": 4.585349574503588,
      "grad_norm": 14.741922378540039,
      "learning_rate": 4.216867469879518e-06,
      "loss": 0.5541,
      "step": 27480
    },
    {
      "epoch": 4.5870181878858665,
      "grad_norm": 11.704388618469238,
      "learning_rate": 4.1998981842864415e-06,
      "loss": 0.4597,
      "step": 27490
    },
    {
      "epoch": 4.588686801268146,
      "grad_norm": 3.283461570739746,
      "learning_rate": 4.182928898693366e-06,
      "loss": 0.5104,
      "step": 27500
    },
    {
      "epoch": 4.590355414650426,
      "grad_norm": 18.203319549560547,
      "learning_rate": 4.165959613100288e-06,
      "loss": 0.5473,
      "step": 27510
    },
    {
      "epoch": 4.5920240280327045,
      "grad_norm": 5.96457052230835,
      "learning_rate": 4.148990327507212e-06,
      "loss": 0.2904,
      "step": 27520
    },
    {
      "epoch": 4.593692641414984,
      "grad_norm": 4.927668571472168,
      "learning_rate": 4.132021041914136e-06,
      "loss": 0.3165,
      "step": 27530
    },
    {
      "epoch": 4.595361254797264,
      "grad_norm": 9.186430931091309,
      "learning_rate": 4.115051756321059e-06,
      "loss": 0.4848,
      "step": 27540
    },
    {
      "epoch": 4.5970298681795425,
      "grad_norm": 9.023248672485352,
      "learning_rate": 4.098082470727982e-06,
      "loss": 0.4004,
      "step": 27550
    },
    {
      "epoch": 4.598698481561822,
      "grad_norm": 8.87434196472168,
      "learning_rate": 4.0811131851349066e-06,
      "loss": 0.2716,
      "step": 27560
    },
    {
      "epoch": 4.600367094944102,
      "grad_norm": 5.566499710083008,
      "learning_rate": 4.064143899541829e-06,
      "loss": 0.3226,
      "step": 27570
    },
    {
      "epoch": 4.6020357083263805,
      "grad_norm": 10.615403175354004,
      "learning_rate": 4.047174613948753e-06,
      "loss": 0.4692,
      "step": 27580
    },
    {
      "epoch": 4.60370432170866,
      "grad_norm": 0.06860506534576416,
      "learning_rate": 4.030205328355676e-06,
      "loss": 0.3663,
      "step": 27590
    },
    {
      "epoch": 4.60537293509094,
      "grad_norm": 10.249652862548828,
      "learning_rate": 4.0132360427626e-06,
      "loss": 0.3122,
      "step": 27600
    },
    {
      "epoch": 4.607041548473219,
      "grad_norm": 12.233277320861816,
      "learning_rate": 3.996266757169523e-06,
      "loss": 0.4789,
      "step": 27610
    },
    {
      "epoch": 4.608710161855498,
      "grad_norm": 11.594950675964355,
      "learning_rate": 3.9792974715764466e-06,
      "loss": 0.3546,
      "step": 27620
    },
    {
      "epoch": 4.610378775237777,
      "grad_norm": 4.510375022888184,
      "learning_rate": 3.96232818598337e-06,
      "loss": 0.7866,
      "step": 27630
    },
    {
      "epoch": 4.612047388620057,
      "grad_norm": 4.0992302894592285,
      "learning_rate": 3.945358900390294e-06,
      "loss": 0.378,
      "step": 27640
    },
    {
      "epoch": 4.613716002002336,
      "grad_norm": 11.032925605773926,
      "learning_rate": 3.9283896147972166e-06,
      "loss": 0.4322,
      "step": 27650
    },
    {
      "epoch": 4.615384615384615,
      "grad_norm": 9.459973335266113,
      "learning_rate": 3.911420329204141e-06,
      "loss": 0.2463,
      "step": 27660
    },
    {
      "epoch": 4.617053228766895,
      "grad_norm": 4.164830684661865,
      "learning_rate": 3.894451043611064e-06,
      "loss": 0.3473,
      "step": 27670
    },
    {
      "epoch": 4.618721842149174,
      "grad_norm": 8.597286224365234,
      "learning_rate": 3.877481758017987e-06,
      "loss": 0.5834,
      "step": 27680
    },
    {
      "epoch": 4.620390455531453,
      "grad_norm": 8.460758209228516,
      "learning_rate": 3.860512472424911e-06,
      "loss": 0.4915,
      "step": 27690
    },
    {
      "epoch": 4.622059068913733,
      "grad_norm": 1.569588541984558,
      "learning_rate": 3.843543186831835e-06,
      "loss": 0.334,
      "step": 27700
    },
    {
      "epoch": 4.623727682296012,
      "grad_norm": 3.900373935699463,
      "learning_rate": 3.826573901238758e-06,
      "loss": 0.6751,
      "step": 27710
    },
    {
      "epoch": 4.625396295678291,
      "grad_norm": 2.529665231704712,
      "learning_rate": 3.8096046156456816e-06,
      "loss": 0.4982,
      "step": 27720
    },
    {
      "epoch": 4.627064909060571,
      "grad_norm": 16.11164665222168,
      "learning_rate": 3.7926353300526045e-06,
      "loss": 0.5308,
      "step": 27730
    },
    {
      "epoch": 4.62873352244285,
      "grad_norm": 9.070671081542969,
      "learning_rate": 3.7756660444595283e-06,
      "loss": 0.4998,
      "step": 27740
    },
    {
      "epoch": 4.630402135825129,
      "grad_norm": 3.2038838863372803,
      "learning_rate": 3.758696758866452e-06,
      "loss": 0.3204,
      "step": 27750
    },
    {
      "epoch": 4.632070749207409,
      "grad_norm": 4.049376010894775,
      "learning_rate": 3.741727473273375e-06,
      "loss": 0.6367,
      "step": 27760
    },
    {
      "epoch": 4.633739362589688,
      "grad_norm": 4.735989093780518,
      "learning_rate": 3.7247581876802987e-06,
      "loss": 0.3956,
      "step": 27770
    },
    {
      "epoch": 4.635407975971967,
      "grad_norm": 4.39578914642334,
      "learning_rate": 3.7077889020872224e-06,
      "loss": 0.4652,
      "step": 27780
    },
    {
      "epoch": 4.637076589354247,
      "grad_norm": 4.359110355377197,
      "learning_rate": 3.6908196164941454e-06,
      "loss": 0.3083,
      "step": 27790
    },
    {
      "epoch": 4.638745202736526,
      "grad_norm": 3.187370777130127,
      "learning_rate": 3.673850330901069e-06,
      "loss": 0.572,
      "step": 27800
    },
    {
      "epoch": 4.640413816118805,
      "grad_norm": 1.8795850276947021,
      "learning_rate": 3.656881045307993e-06,
      "loss": 0.3763,
      "step": 27810
    },
    {
      "epoch": 4.642082429501085,
      "grad_norm": 2.4118146896362305,
      "learning_rate": 3.6399117597149158e-06,
      "loss": 0.6235,
      "step": 27820
    },
    {
      "epoch": 4.643751042883364,
      "grad_norm": 16.848817825317383,
      "learning_rate": 3.6229424741218395e-06,
      "loss": 0.6978,
      "step": 27830
    },
    {
      "epoch": 4.645419656265643,
      "grad_norm": 1.5558254718780518,
      "learning_rate": 3.6059731885287633e-06,
      "loss": 0.3371,
      "step": 27840
    },
    {
      "epoch": 4.647088269647923,
      "grad_norm": 14.767452239990234,
      "learning_rate": 3.5890039029356866e-06,
      "loss": 0.4335,
      "step": 27850
    },
    {
      "epoch": 4.6487568830302015,
      "grad_norm": 14.585131645202637,
      "learning_rate": 3.57203461734261e-06,
      "loss": 0.4176,
      "step": 27860
    },
    {
      "epoch": 4.650425496412481,
      "grad_norm": 10.734338760375977,
      "learning_rate": 3.5550653317495333e-06,
      "loss": 0.3975,
      "step": 27870
    },
    {
      "epoch": 4.652094109794761,
      "grad_norm": 3.6971023082733154,
      "learning_rate": 3.538096046156457e-06,
      "loss": 0.3748,
      "step": 27880
    },
    {
      "epoch": 4.6537627231770395,
      "grad_norm": 10.768775939941406,
      "learning_rate": 3.521126760563381e-06,
      "loss": 0.4821,
      "step": 27890
    },
    {
      "epoch": 4.655431336559319,
      "grad_norm": 0.564854621887207,
      "learning_rate": 3.5041574749703037e-06,
      "loss": 0.4299,
      "step": 27900
    },
    {
      "epoch": 4.657099949941599,
      "grad_norm": 2.9039509296417236,
      "learning_rate": 3.4871881893772275e-06,
      "loss": 0.3623,
      "step": 27910
    },
    {
      "epoch": 4.658768563323878,
      "grad_norm": 20.294485092163086,
      "learning_rate": 3.4702189037841512e-06,
      "loss": 0.8676,
      "step": 27920
    },
    {
      "epoch": 4.660437176706157,
      "grad_norm": 6.666386604309082,
      "learning_rate": 3.453249618191074e-06,
      "loss": 0.4168,
      "step": 27930
    },
    {
      "epoch": 4.662105790088437,
      "grad_norm": 7.138855457305908,
      "learning_rate": 3.436280332597998e-06,
      "loss": 0.4133,
      "step": 27940
    },
    {
      "epoch": 4.663774403470716,
      "grad_norm": 5.611097812652588,
      "learning_rate": 3.4193110470049217e-06,
      "loss": 0.2356,
      "step": 27950
    },
    {
      "epoch": 4.665443016852995,
      "grad_norm": 7.258909225463867,
      "learning_rate": 3.4023417614118446e-06,
      "loss": 0.27,
      "step": 27960
    },
    {
      "epoch": 4.667111630235275,
      "grad_norm": 3.6037776470184326,
      "learning_rate": 3.3853724758187683e-06,
      "loss": 0.4256,
      "step": 27970
    },
    {
      "epoch": 4.668780243617554,
      "grad_norm": 9.430267333984375,
      "learning_rate": 3.368403190225692e-06,
      "loss": 0.6217,
      "step": 27980
    },
    {
      "epoch": 4.670448856999833,
      "grad_norm": 7.0974531173706055,
      "learning_rate": 3.351433904632615e-06,
      "loss": 0.5914,
      "step": 27990
    },
    {
      "epoch": 4.672117470382112,
      "grad_norm": 3.8440449237823486,
      "learning_rate": 3.3344646190395387e-06,
      "loss": 0.4141,
      "step": 28000
    },
    {
      "epoch": 4.673786083764392,
      "grad_norm": 1.8768490552902222,
      "learning_rate": 3.3174953334464617e-06,
      "loss": 0.2111,
      "step": 28010
    },
    {
      "epoch": 4.675454697146671,
      "grad_norm": 6.417397975921631,
      "learning_rate": 3.3005260478533854e-06,
      "loss": 0.3834,
      "step": 28020
    },
    {
      "epoch": 4.67712331052895,
      "grad_norm": 2.2927799224853516,
      "learning_rate": 3.283556762260309e-06,
      "loss": 0.2599,
      "step": 28030
    },
    {
      "epoch": 4.67879192391123,
      "grad_norm": 15.622052192687988,
      "learning_rate": 3.266587476667232e-06,
      "loss": 0.4936,
      "step": 28040
    },
    {
      "epoch": 4.680460537293509,
      "grad_norm": 5.727102756500244,
      "learning_rate": 3.249618191074156e-06,
      "loss": 0.3692,
      "step": 28050
    },
    {
      "epoch": 4.682129150675788,
      "grad_norm": 5.126687049865723,
      "learning_rate": 3.2326489054810796e-06,
      "loss": 0.643,
      "step": 28060
    },
    {
      "epoch": 4.683797764058068,
      "grad_norm": 11.165399551391602,
      "learning_rate": 3.2156796198880025e-06,
      "loss": 0.4762,
      "step": 28070
    },
    {
      "epoch": 4.685466377440347,
      "grad_norm": 18.002153396606445,
      "learning_rate": 3.1987103342949263e-06,
      "loss": 0.5099,
      "step": 28080
    },
    {
      "epoch": 4.687134990822626,
      "grad_norm": 8.026331901550293,
      "learning_rate": 3.18174104870185e-06,
      "loss": 0.3245,
      "step": 28090
    },
    {
      "epoch": 4.688803604204906,
      "grad_norm": 2.102890968322754,
      "learning_rate": 3.164771763108773e-06,
      "loss": 0.4947,
      "step": 28100
    },
    {
      "epoch": 4.690472217587185,
      "grad_norm": 16.326236724853516,
      "learning_rate": 3.1478024775156967e-06,
      "loss": 0.3276,
      "step": 28110
    },
    {
      "epoch": 4.692140830969464,
      "grad_norm": 12.265741348266602,
      "learning_rate": 3.1308331919226204e-06,
      "loss": 0.3738,
      "step": 28120
    },
    {
      "epoch": 4.693809444351744,
      "grad_norm": 15.606084823608398,
      "learning_rate": 3.1138639063295433e-06,
      "loss": 0.4416,
      "step": 28130
    },
    {
      "epoch": 4.695478057734023,
      "grad_norm": 1.3592042922973633,
      "learning_rate": 3.096894620736467e-06,
      "loss": 0.4028,
      "step": 28140
    },
    {
      "epoch": 4.697146671116302,
      "grad_norm": 6.657772541046143,
      "learning_rate": 3.0799253351433904e-06,
      "loss": 0.5559,
      "step": 28150
    },
    {
      "epoch": 4.698815284498582,
      "grad_norm": 5.034238338470459,
      "learning_rate": 3.062956049550314e-06,
      "loss": 0.4753,
      "step": 28160
    },
    {
      "epoch": 4.700483897880861,
      "grad_norm": 12.878704071044922,
      "learning_rate": 3.0459867639572375e-06,
      "loss": 0.4965,
      "step": 28170
    },
    {
      "epoch": 4.70215251126314,
      "grad_norm": 3.564218521118164,
      "learning_rate": 3.0290174783641613e-06,
      "loss": 0.5986,
      "step": 28180
    },
    {
      "epoch": 4.70382112464542,
      "grad_norm": 2.237445116043091,
      "learning_rate": 3.0120481927710846e-06,
      "loss": 0.2591,
      "step": 28190
    },
    {
      "epoch": 4.705489738027699,
      "grad_norm": 13.984978675842285,
      "learning_rate": 2.995078907178008e-06,
      "loss": 0.3677,
      "step": 28200
    },
    {
      "epoch": 4.707158351409978,
      "grad_norm": 8.621935844421387,
      "learning_rate": 2.9781096215849317e-06,
      "loss": 0.3648,
      "step": 28210
    },
    {
      "epoch": 4.708826964792258,
      "grad_norm": 22.02422332763672,
      "learning_rate": 2.961140335991855e-06,
      "loss": 0.7055,
      "step": 28220
    },
    {
      "epoch": 4.7104955781745375,
      "grad_norm": 12.978578567504883,
      "learning_rate": 2.9441710503987784e-06,
      "loss": 0.4361,
      "step": 28230
    },
    {
      "epoch": 4.712164191556816,
      "grad_norm": 2.4640932083129883,
      "learning_rate": 2.9272017648057017e-06,
      "loss": 0.5781,
      "step": 28240
    },
    {
      "epoch": 4.713832804939096,
      "grad_norm": 13.984292984008789,
      "learning_rate": 2.9102324792126255e-06,
      "loss": 0.3003,
      "step": 28250
    },
    {
      "epoch": 4.715501418321375,
      "grad_norm": 9.828998565673828,
      "learning_rate": 2.893263193619549e-06,
      "loss": 0.6195,
      "step": 28260
    },
    {
      "epoch": 4.717170031703654,
      "grad_norm": 1.7950493097305298,
      "learning_rate": 2.876293908026472e-06,
      "loss": 0.4383,
      "step": 28270
    },
    {
      "epoch": 4.718838645085934,
      "grad_norm": 8.998209953308105,
      "learning_rate": 2.859324622433396e-06,
      "loss": 0.3389,
      "step": 28280
    },
    {
      "epoch": 4.720507258468213,
      "grad_norm": 9.311209678649902,
      "learning_rate": 2.8423553368403192e-06,
      "loss": 0.5577,
      "step": 28290
    },
    {
      "epoch": 4.722175871850492,
      "grad_norm": 7.146440029144287,
      "learning_rate": 2.8253860512472426e-06,
      "loss": 0.4651,
      "step": 28300
    },
    {
      "epoch": 4.723844485232772,
      "grad_norm": 9.610803604125977,
      "learning_rate": 2.808416765654166e-06,
      "loss": 0.6014,
      "step": 28310
    },
    {
      "epoch": 4.725513098615051,
      "grad_norm": 8.23552131652832,
      "learning_rate": 2.7914474800610896e-06,
      "loss": 0.5331,
      "step": 28320
    },
    {
      "epoch": 4.72718171199733,
      "grad_norm": 1.4768612384796143,
      "learning_rate": 2.774478194468013e-06,
      "loss": 0.3304,
      "step": 28330
    },
    {
      "epoch": 4.72885032537961,
      "grad_norm": 6.288115978240967,
      "learning_rate": 2.7575089088749363e-06,
      "loss": 0.392,
      "step": 28340
    },
    {
      "epoch": 4.730518938761889,
      "grad_norm": 12.496910095214844,
      "learning_rate": 2.74053962328186e-06,
      "loss": 0.6041,
      "step": 28350
    },
    {
      "epoch": 4.732187552144168,
      "grad_norm": 14.756828308105469,
      "learning_rate": 2.7235703376887834e-06,
      "loss": 0.179,
      "step": 28360
    },
    {
      "epoch": 4.733856165526447,
      "grad_norm": 2.697047233581543,
      "learning_rate": 2.7066010520957067e-06,
      "loss": 0.3634,
      "step": 28370
    },
    {
      "epoch": 4.735524778908727,
      "grad_norm": 12.529047966003418,
      "learning_rate": 2.68963176650263e-06,
      "loss": 0.6833,
      "step": 28380
    },
    {
      "epoch": 4.737193392291006,
      "grad_norm": 12.166882514953613,
      "learning_rate": 2.672662480909554e-06,
      "loss": 0.7208,
      "step": 28390
    },
    {
      "epoch": 4.738862005673285,
      "grad_norm": 6.248283386230469,
      "learning_rate": 2.655693195316477e-06,
      "loss": 0.3921,
      "step": 28400
    },
    {
      "epoch": 4.740530619055565,
      "grad_norm": 5.542830944061279,
      "learning_rate": 2.6387239097234005e-06,
      "loss": 0.4793,
      "step": 28410
    },
    {
      "epoch": 4.742199232437844,
      "grad_norm": 7.850545883178711,
      "learning_rate": 2.6217546241303243e-06,
      "loss": 0.6127,
      "step": 28420
    },
    {
      "epoch": 4.743867845820123,
      "grad_norm": 3.1456878185272217,
      "learning_rate": 2.6047853385372476e-06,
      "loss": 0.3346,
      "step": 28430
    },
    {
      "epoch": 4.745536459202403,
      "grad_norm": 9.721813201904297,
      "learning_rate": 2.587816052944171e-06,
      "loss": 0.2606,
      "step": 28440
    },
    {
      "epoch": 4.747205072584682,
      "grad_norm": 8.573012351989746,
      "learning_rate": 2.5708467673510947e-06,
      "loss": 0.3702,
      "step": 28450
    },
    {
      "epoch": 4.748873685966961,
      "grad_norm": 0.8746256828308105,
      "learning_rate": 2.553877481758018e-06,
      "loss": 0.2093,
      "step": 28460
    },
    {
      "epoch": 4.750542299349241,
      "grad_norm": 2.286346197128296,
      "learning_rate": 2.5369081961649418e-06,
      "loss": 0.471,
      "step": 28470
    },
    {
      "epoch": 4.75221091273152,
      "grad_norm": 10.379775047302246,
      "learning_rate": 2.519938910571865e-06,
      "loss": 0.4028,
      "step": 28480
    },
    {
      "epoch": 4.753879526113799,
      "grad_norm": 2.616427183151245,
      "learning_rate": 2.502969624978789e-06,
      "loss": 0.3121,
      "step": 28490
    },
    {
      "epoch": 4.755548139496079,
      "grad_norm": 15.26917839050293,
      "learning_rate": 2.486000339385712e-06,
      "loss": 0.495,
      "step": 28500
    },
    {
      "epoch": 4.757216752878358,
      "grad_norm": 15.636959075927734,
      "learning_rate": 2.4690310537926355e-06,
      "loss": 0.4055,
      "step": 28510
    },
    {
      "epoch": 4.758885366260637,
      "grad_norm": 9.864370346069336,
      "learning_rate": 2.452061768199559e-06,
      "loss": 0.5079,
      "step": 28520
    },
    {
      "epoch": 4.760553979642917,
      "grad_norm": 16.409761428833008,
      "learning_rate": 2.4350924826064826e-06,
      "loss": 0.4323,
      "step": 28530
    },
    {
      "epoch": 4.7622225930251965,
      "grad_norm": 4.644152641296387,
      "learning_rate": 2.418123197013406e-06,
      "loss": 0.6066,
      "step": 28540
    },
    {
      "epoch": 4.763891206407475,
      "grad_norm": 7.220130920410156,
      "learning_rate": 2.4011539114203293e-06,
      "loss": 0.1661,
      "step": 28550
    },
    {
      "epoch": 4.765559819789755,
      "grad_norm": 4.330395698547363,
      "learning_rate": 2.384184625827253e-06,
      "loss": 0.6967,
      "step": 28560
    },
    {
      "epoch": 4.7672284331720345,
      "grad_norm": 5.429389476776123,
      "learning_rate": 2.3672153402341764e-06,
      "loss": 0.5075,
      "step": 28570
    },
    {
      "epoch": 4.768897046554313,
      "grad_norm": 2.3940255641937256,
      "learning_rate": 2.3502460546410997e-06,
      "loss": 0.3506,
      "step": 28580
    },
    {
      "epoch": 4.770565659936593,
      "grad_norm": 15.339335441589355,
      "learning_rate": 2.333276769048023e-06,
      "loss": 0.4359,
      "step": 28590
    },
    {
      "epoch": 4.7722342733188725,
      "grad_norm": 11.55215835571289,
      "learning_rate": 2.316307483454947e-06,
      "loss": 0.241,
      "step": 28600
    },
    {
      "epoch": 4.773902886701151,
      "grad_norm": 8.8091459274292,
      "learning_rate": 2.29933819786187e-06,
      "loss": 0.472,
      "step": 28610
    },
    {
      "epoch": 4.775571500083431,
      "grad_norm": 1.2794125080108643,
      "learning_rate": 2.2823689122687935e-06,
      "loss": 0.385,
      "step": 28620
    },
    {
      "epoch": 4.77724011346571,
      "grad_norm": 8.694263458251953,
      "learning_rate": 2.2653996266757172e-06,
      "loss": 0.4554,
      "step": 28630
    },
    {
      "epoch": 4.778908726847989,
      "grad_norm": 12.986907005310059,
      "learning_rate": 2.2484303410826405e-06,
      "loss": 0.378,
      "step": 28640
    },
    {
      "epoch": 4.780577340230269,
      "grad_norm": 13.763426780700684,
      "learning_rate": 2.231461055489564e-06,
      "loss": 0.4895,
      "step": 28650
    },
    {
      "epoch": 4.782245953612548,
      "grad_norm": 3.8095273971557617,
      "learning_rate": 2.2144917698964872e-06,
      "loss": 0.2714,
      "step": 28660
    },
    {
      "epoch": 4.783914566994827,
      "grad_norm": 8.124462127685547,
      "learning_rate": 2.197522484303411e-06,
      "loss": 0.3941,
      "step": 28670
    },
    {
      "epoch": 4.785583180377107,
      "grad_norm": 7.372978687286377,
      "learning_rate": 2.1805531987103343e-06,
      "loss": 0.4209,
      "step": 28680
    },
    {
      "epoch": 4.787251793759386,
      "grad_norm": 6.682231426239014,
      "learning_rate": 2.1635839131172576e-06,
      "loss": 0.4939,
      "step": 28690
    },
    {
      "epoch": 4.788920407141665,
      "grad_norm": 9.25926685333252,
      "learning_rate": 2.1466146275241814e-06,
      "loss": 0.4405,
      "step": 28700
    },
    {
      "epoch": 4.790589020523945,
      "grad_norm": 6.965194225311279,
      "learning_rate": 2.1296453419311047e-06,
      "loss": 0.6287,
      "step": 28710
    },
    {
      "epoch": 4.792257633906224,
      "grad_norm": 3.498305559158325,
      "learning_rate": 2.112676056338028e-06,
      "loss": 0.1964,
      "step": 28720
    },
    {
      "epoch": 4.793926247288503,
      "grad_norm": 1.569023609161377,
      "learning_rate": 2.0957067707449514e-06,
      "loss": 0.4306,
      "step": 28730
    },
    {
      "epoch": 4.795594860670782,
      "grad_norm": 0.25684916973114014,
      "learning_rate": 2.078737485151875e-06,
      "loss": 0.3194,
      "step": 28740
    },
    {
      "epoch": 4.797263474053062,
      "grad_norm": 11.01168441772461,
      "learning_rate": 2.0617681995587985e-06,
      "loss": 0.4153,
      "step": 28750
    },
    {
      "epoch": 4.798932087435341,
      "grad_norm": 8.971485137939453,
      "learning_rate": 2.0447989139657222e-06,
      "loss": 0.3284,
      "step": 28760
    },
    {
      "epoch": 4.80060070081762,
      "grad_norm": 13.312873840332031,
      "learning_rate": 2.0278296283726456e-06,
      "loss": 0.6006,
      "step": 28770
    },
    {
      "epoch": 4.8022693141999,
      "grad_norm": 8.383880615234375,
      "learning_rate": 2.0108603427795693e-06,
      "loss": 0.4766,
      "step": 28780
    },
    {
      "epoch": 4.803937927582179,
      "grad_norm": 17.107446670532227,
      "learning_rate": 1.9938910571864927e-06,
      "loss": 0.5616,
      "step": 28790
    },
    {
      "epoch": 4.805606540964458,
      "grad_norm": 8.457159996032715,
      "learning_rate": 1.976921771593416e-06,
      "loss": 0.5544,
      "step": 28800
    },
    {
      "epoch": 4.807275154346738,
      "grad_norm": 11.412432670593262,
      "learning_rate": 1.9599524860003398e-06,
      "loss": 0.8865,
      "step": 28810
    },
    {
      "epoch": 4.808943767729017,
      "grad_norm": 9.681863784790039,
      "learning_rate": 1.942983200407263e-06,
      "loss": 0.253,
      "step": 28820
    },
    {
      "epoch": 4.810612381111296,
      "grad_norm": 4.6387038230896,
      "learning_rate": 1.9260139148141864e-06,
      "loss": 0.5462,
      "step": 28830
    },
    {
      "epoch": 4.812280994493576,
      "grad_norm": 10.601468086242676,
      "learning_rate": 1.90904462922111e-06,
      "loss": 0.4644,
      "step": 28840
    },
    {
      "epoch": 4.8139496078758555,
      "grad_norm": 8.78188705444336,
      "learning_rate": 1.8920753436280335e-06,
      "loss": 0.5911,
      "step": 28850
    },
    {
      "epoch": 4.815618221258134,
      "grad_norm": 2.6684484481811523,
      "learning_rate": 1.8751060580349568e-06,
      "loss": 0.5131,
      "step": 28860
    },
    {
      "epoch": 4.817286834640414,
      "grad_norm": 9.800418853759766,
      "learning_rate": 1.8581367724418802e-06,
      "loss": 0.5279,
      "step": 28870
    },
    {
      "epoch": 4.8189554480226935,
      "grad_norm": 8.505687713623047,
      "learning_rate": 1.841167486848804e-06,
      "loss": 0.4308,
      "step": 28880
    },
    {
      "epoch": 4.820624061404972,
      "grad_norm": 2.5807487964630127,
      "learning_rate": 1.8241982012557273e-06,
      "loss": 0.2162,
      "step": 28890
    },
    {
      "epoch": 4.822292674787252,
      "grad_norm": 3.6487011909484863,
      "learning_rate": 1.8072289156626506e-06,
      "loss": 0.3637,
      "step": 28900
    },
    {
      "epoch": 4.8239612881695315,
      "grad_norm": 10.952140808105469,
      "learning_rate": 1.7902596300695744e-06,
      "loss": 0.2917,
      "step": 28910
    },
    {
      "epoch": 4.82562990155181,
      "grad_norm": 3.102332353591919,
      "learning_rate": 1.7732903444764977e-06,
      "loss": 0.553,
      "step": 28920
    },
    {
      "epoch": 4.82729851493409,
      "grad_norm": 2.191225290298462,
      "learning_rate": 1.756321058883421e-06,
      "loss": 0.4199,
      "step": 28930
    },
    {
      "epoch": 4.8289671283163695,
      "grad_norm": 16.808727264404297,
      "learning_rate": 1.7393517732903444e-06,
      "loss": 0.4124,
      "step": 28940
    },
    {
      "epoch": 4.830635741698648,
      "grad_norm": 2.638378858566284,
      "learning_rate": 1.7223824876972681e-06,
      "loss": 0.6037,
      "step": 28950
    },
    {
      "epoch": 4.832304355080928,
      "grad_norm": 6.884650230407715,
      "learning_rate": 1.7054132021041914e-06,
      "loss": 0.5959,
      "step": 28960
    },
    {
      "epoch": 4.833972968463208,
      "grad_norm": 8.152355194091797,
      "learning_rate": 1.6884439165111148e-06,
      "loss": 0.3906,
      "step": 28970
    },
    {
      "epoch": 4.835641581845486,
      "grad_norm": 5.9509196281433105,
      "learning_rate": 1.6714746309180385e-06,
      "loss": 0.375,
      "step": 28980
    },
    {
      "epoch": 4.837310195227766,
      "grad_norm": 2.977505683898926,
      "learning_rate": 1.6545053453249619e-06,
      "loss": 0.3999,
      "step": 28990
    },
    {
      "epoch": 4.838978808610045,
      "grad_norm": 10.458916664123535,
      "learning_rate": 1.6375360597318854e-06,
      "loss": 0.4845,
      "step": 29000
    },
    {
      "epoch": 4.840647421992324,
      "grad_norm": 2.7771871089935303,
      "learning_rate": 1.6205667741388088e-06,
      "loss": 0.6227,
      "step": 29010
    },
    {
      "epoch": 4.842316035374604,
      "grad_norm": 2.446383237838745,
      "learning_rate": 1.6035974885457325e-06,
      "loss": 0.6095,
      "step": 29020
    },
    {
      "epoch": 4.843984648756883,
      "grad_norm": 9.23054027557373,
      "learning_rate": 1.5866282029526558e-06,
      "loss": 0.5388,
      "step": 29030
    },
    {
      "epoch": 4.845653262139162,
      "grad_norm": 10.535865783691406,
      "learning_rate": 1.5696589173595792e-06,
      "loss": 0.4855,
      "step": 29040
    },
    {
      "epoch": 4.847321875521442,
      "grad_norm": 20.01048469543457,
      "learning_rate": 1.5526896317665027e-06,
      "loss": 0.3481,
      "step": 29050
    },
    {
      "epoch": 4.848990488903721,
      "grad_norm": 6.078312873840332,
      "learning_rate": 1.5357203461734263e-06,
      "loss": 0.4682,
      "step": 29060
    },
    {
      "epoch": 4.850659102286,
      "grad_norm": 5.670669078826904,
      "learning_rate": 1.5187510605803496e-06,
      "loss": 0.3421,
      "step": 29070
    },
    {
      "epoch": 4.85232771566828,
      "grad_norm": 11.866235733032227,
      "learning_rate": 1.5017817749872731e-06,
      "loss": 0.3925,
      "step": 29080
    },
    {
      "epoch": 4.853996329050559,
      "grad_norm": 2.1818110942840576,
      "learning_rate": 1.4848124893941965e-06,
      "loss": 0.2576,
      "step": 29090
    },
    {
      "epoch": 4.855664942432838,
      "grad_norm": 1.3511018753051758,
      "learning_rate": 1.46784320380112e-06,
      "loss": 0.5472,
      "step": 29100
    },
    {
      "epoch": 4.857333555815117,
      "grad_norm": 14.207592964172363,
      "learning_rate": 1.4508739182080434e-06,
      "loss": 0.4422,
      "step": 29110
    },
    {
      "epoch": 4.859002169197397,
      "grad_norm": 11.18665885925293,
      "learning_rate": 1.433904632614967e-06,
      "loss": 0.374,
      "step": 29120
    },
    {
      "epoch": 4.860670782579676,
      "grad_norm": 2.0537474155426025,
      "learning_rate": 1.4169353470218904e-06,
      "loss": 0.4186,
      "step": 29130
    },
    {
      "epoch": 4.862339395961955,
      "grad_norm": 5.960107326507568,
      "learning_rate": 1.399966061428814e-06,
      "loss": 0.6509,
      "step": 29140
    },
    {
      "epoch": 4.864008009344235,
      "grad_norm": 6.075536251068115,
      "learning_rate": 1.3829967758357375e-06,
      "loss": 0.3618,
      "step": 29150
    },
    {
      "epoch": 4.8656766227265145,
      "grad_norm": 9.226007461547852,
      "learning_rate": 1.3660274902426609e-06,
      "loss": 0.4176,
      "step": 29160
    },
    {
      "epoch": 4.867345236108793,
      "grad_norm": 9.881996154785156,
      "learning_rate": 1.3490582046495844e-06,
      "loss": 0.461,
      "step": 29170
    },
    {
      "epoch": 4.869013849491073,
      "grad_norm": 1.558213710784912,
      "learning_rate": 1.3320889190565077e-06,
      "loss": 0.176,
      "step": 29180
    },
    {
      "epoch": 4.8706824628733525,
      "grad_norm": 3.190882921218872,
      "learning_rate": 1.3151196334634313e-06,
      "loss": 0.3345,
      "step": 29190
    },
    {
      "epoch": 4.872351076255631,
      "grad_norm": 3.4024600982666016,
      "learning_rate": 1.2981503478703548e-06,
      "loss": 0.3976,
      "step": 29200
    },
    {
      "epoch": 4.874019689637911,
      "grad_norm": 1.973730444908142,
      "learning_rate": 1.2811810622772782e-06,
      "loss": 0.2546,
      "step": 29210
    },
    {
      "epoch": 4.8756883030201905,
      "grad_norm": 1.9566717147827148,
      "learning_rate": 1.2642117766842017e-06,
      "loss": 0.2864,
      "step": 29220
    },
    {
      "epoch": 4.877356916402469,
      "grad_norm": 14.249190330505371,
      "learning_rate": 1.247242491091125e-06,
      "loss": 0.3008,
      "step": 29230
    },
    {
      "epoch": 4.879025529784749,
      "grad_norm": 4.913933753967285,
      "learning_rate": 1.2302732054980486e-06,
      "loss": 0.4172,
      "step": 29240
    },
    {
      "epoch": 4.8806941431670285,
      "grad_norm": 13.461971282958984,
      "learning_rate": 1.213303919904972e-06,
      "loss": 0.5986,
      "step": 29250
    },
    {
      "epoch": 4.882362756549307,
      "grad_norm": 8.66723346710205,
      "learning_rate": 1.1963346343118955e-06,
      "loss": 0.5538,
      "step": 29260
    },
    {
      "epoch": 4.884031369931587,
      "grad_norm": 5.2298359870910645,
      "learning_rate": 1.1793653487188188e-06,
      "loss": 0.4677,
      "step": 29270
    },
    {
      "epoch": 4.885699983313867,
      "grad_norm": 17.596574783325195,
      "learning_rate": 1.1623960631257424e-06,
      "loss": 0.2632,
      "step": 29280
    },
    {
      "epoch": 4.887368596696145,
      "grad_norm": 14.05492115020752,
      "learning_rate": 1.145426777532666e-06,
      "loss": 0.4898,
      "step": 29290
    },
    {
      "epoch": 4.889037210078425,
      "grad_norm": 8.423757553100586,
      "learning_rate": 1.1284574919395894e-06,
      "loss": 0.5808,
      "step": 29300
    },
    {
      "epoch": 4.890705823460705,
      "grad_norm": 11.995893478393555,
      "learning_rate": 1.111488206346513e-06,
      "loss": 0.4977,
      "step": 29310
    },
    {
      "epoch": 4.892374436842983,
      "grad_norm": 10.026716232299805,
      "learning_rate": 1.0945189207534363e-06,
      "loss": 0.3187,
      "step": 29320
    },
    {
      "epoch": 4.894043050225263,
      "grad_norm": 7.230522155761719,
      "learning_rate": 1.0775496351603599e-06,
      "loss": 0.5184,
      "step": 29330
    },
    {
      "epoch": 4.895711663607543,
      "grad_norm": 11.187030792236328,
      "learning_rate": 1.0605803495672832e-06,
      "loss": 0.2515,
      "step": 29340
    },
    {
      "epoch": 4.897380276989821,
      "grad_norm": 10.093032836914062,
      "learning_rate": 1.0436110639742067e-06,
      "loss": 0.391,
      "step": 29350
    },
    {
      "epoch": 4.899048890372101,
      "grad_norm": 12.893834114074707,
      "learning_rate": 1.0266417783811303e-06,
      "loss": 0.4214,
      "step": 29360
    },
    {
      "epoch": 4.90071750375438,
      "grad_norm": 10.29601764678955,
      "learning_rate": 1.0096724927880536e-06,
      "loss": 0.5152,
      "step": 29370
    },
    {
      "epoch": 4.902386117136659,
      "grad_norm": 13.501660346984863,
      "learning_rate": 9.927032071949772e-07,
      "loss": 0.4681,
      "step": 29380
    },
    {
      "epoch": 4.904054730518939,
      "grad_norm": 10.217233657836914,
      "learning_rate": 9.757339216019005e-07,
      "loss": 0.3101,
      "step": 29390
    },
    {
      "epoch": 4.905723343901218,
      "grad_norm": 19.440080642700195,
      "learning_rate": 9.58764636008824e-07,
      "loss": 0.6083,
      "step": 29400
    },
    {
      "epoch": 4.907391957283497,
      "grad_norm": 3.6875200271606445,
      "learning_rate": 9.417953504157475e-07,
      "loss": 0.5218,
      "step": 29410
    },
    {
      "epoch": 4.909060570665777,
      "grad_norm": 4.773377418518066,
      "learning_rate": 9.24826064822671e-07,
      "loss": 0.3905,
      "step": 29420
    },
    {
      "epoch": 4.910729184048056,
      "grad_norm": 12.000645637512207,
      "learning_rate": 9.078567792295946e-07,
      "loss": 0.4605,
      "step": 29430
    },
    {
      "epoch": 4.9123977974303354,
      "grad_norm": 2.138317346572876,
      "learning_rate": 8.908874936365179e-07,
      "loss": 0.4508,
      "step": 29440
    },
    {
      "epoch": 4.914066410812615,
      "grad_norm": 20.819942474365234,
      "learning_rate": 8.739182080434415e-07,
      "loss": 0.5302,
      "step": 29450
    },
    {
      "epoch": 4.915735024194894,
      "grad_norm": 10.091134071350098,
      "learning_rate": 8.569489224503648e-07,
      "loss": 0.3152,
      "step": 29460
    },
    {
      "epoch": 4.9174036375771735,
      "grad_norm": 0.13324546813964844,
      "learning_rate": 8.399796368572883e-07,
      "loss": 0.2397,
      "step": 29470
    },
    {
      "epoch": 4.919072250959452,
      "grad_norm": 10.25597858428955,
      "learning_rate": 8.230103512642118e-07,
      "loss": 0.4735,
      "step": 29480
    },
    {
      "epoch": 4.920740864341732,
      "grad_norm": 1.5952730178833008,
      "learning_rate": 8.060410656711353e-07,
      "loss": 0.3633,
      "step": 29490
    },
    {
      "epoch": 4.9224094777240115,
      "grad_norm": 3.938390016555786,
      "learning_rate": 7.890717800780589e-07,
      "loss": 0.4902,
      "step": 29500
    },
    {
      "epoch": 4.92407809110629,
      "grad_norm": 9.680706024169922,
      "learning_rate": 7.721024944849822e-07,
      "loss": 0.2974,
      "step": 29510
    },
    {
      "epoch": 4.92574670448857,
      "grad_norm": 3.3565804958343506,
      "learning_rate": 7.551332088919056e-07,
      "loss": 0.5726,
      "step": 29520
    },
    {
      "epoch": 4.9274153178708495,
      "grad_norm": 6.478567600250244,
      "learning_rate": 7.381639232988292e-07,
      "loss": 0.4325,
      "step": 29530
    },
    {
      "epoch": 4.929083931253128,
      "grad_norm": 3.857396364212036,
      "learning_rate": 7.211946377057526e-07,
      "loss": 0.4295,
      "step": 29540
    },
    {
      "epoch": 4.930752544635408,
      "grad_norm": 7.74690580368042,
      "learning_rate": 7.042253521126761e-07,
      "loss": 0.366,
      "step": 29550
    },
    {
      "epoch": 4.9324211580176875,
      "grad_norm": 7.612547397613525,
      "learning_rate": 6.872560665195996e-07,
      "loss": 0.4081,
      "step": 29560
    },
    {
      "epoch": 4.934089771399966,
      "grad_norm": 23.390798568725586,
      "learning_rate": 6.70286780926523e-07,
      "loss": 0.5935,
      "step": 29570
    },
    {
      "epoch": 4.935758384782246,
      "grad_norm": 2.3085474967956543,
      "learning_rate": 6.533174953334465e-07,
      "loss": 0.347,
      "step": 29580
    },
    {
      "epoch": 4.937426998164526,
      "grad_norm": 10.377259254455566,
      "learning_rate": 6.363482097403699e-07,
      "loss": 0.4644,
      "step": 29590
    },
    {
      "epoch": 4.939095611546804,
      "grad_norm": 2.325793504714966,
      "learning_rate": 6.193789241472935e-07,
      "loss": 0.3889,
      "step": 29600
    },
    {
      "epoch": 4.940764224929084,
      "grad_norm": 4.890490531921387,
      "learning_rate": 6.024096385542169e-07,
      "loss": 0.3312,
      "step": 29610
    },
    {
      "epoch": 4.942432838311364,
      "grad_norm": 11.268077850341797,
      "learning_rate": 5.854403529611403e-07,
      "loss": 0.5858,
      "step": 29620
    },
    {
      "epoch": 4.944101451693642,
      "grad_norm": 11.097050666809082,
      "learning_rate": 5.684710673680639e-07,
      "loss": 0.5397,
      "step": 29630
    },
    {
      "epoch": 4.945770065075922,
      "grad_norm": 16.340675354003906,
      "learning_rate": 5.515017817749873e-07,
      "loss": 0.4397,
      "step": 29640
    },
    {
      "epoch": 4.947438678458202,
      "grad_norm": 3.3939311504364014,
      "learning_rate": 5.345324961819108e-07,
      "loss": 0.4023,
      "step": 29650
    },
    {
      "epoch": 4.94910729184048,
      "grad_norm": 13.835369110107422,
      "learning_rate": 5.175632105888342e-07,
      "loss": 0.6004,
      "step": 29660
    },
    {
      "epoch": 4.95077590522276,
      "grad_norm": 6.979340076446533,
      "learning_rate": 5.005939249957576e-07,
      "loss": 0.2849,
      "step": 29670
    },
    {
      "epoch": 4.95244451860504,
      "grad_norm": 7.592298984527588,
      "learning_rate": 4.836246394026812e-07,
      "loss": 0.2387,
      "step": 29680
    },
    {
      "epoch": 4.954113131987318,
      "grad_norm": 8.9656343460083,
      "learning_rate": 4.666553538096047e-07,
      "loss": 0.3182,
      "step": 29690
    },
    {
      "epoch": 4.955781745369598,
      "grad_norm": 11.849417686462402,
      "learning_rate": 4.496860682165281e-07,
      "loss": 0.5165,
      "step": 29700
    },
    {
      "epoch": 4.957450358751878,
      "grad_norm": 7.4514875411987305,
      "learning_rate": 4.3271678262345156e-07,
      "loss": 0.3751,
      "step": 29710
    },
    {
      "epoch": 4.959118972134156,
      "grad_norm": 6.248235702514648,
      "learning_rate": 4.1574749703037505e-07,
      "loss": 0.4343,
      "step": 29720
    },
    {
      "epoch": 4.960787585516436,
      "grad_norm": 10.318286895751953,
      "learning_rate": 3.987782114372985e-07,
      "loss": 0.4187,
      "step": 29730
    },
    {
      "epoch": 4.962456198898715,
      "grad_norm": 21.16115951538086,
      "learning_rate": 3.81808925844222e-07,
      "loss": 0.4489,
      "step": 29740
    },
    {
      "epoch": 4.9641248122809944,
      "grad_norm": 5.499946594238281,
      "learning_rate": 3.648396402511454e-07,
      "loss": 0.4436,
      "step": 29750
    },
    {
      "epoch": 4.965793425663274,
      "grad_norm": 6.943513870239258,
      "learning_rate": 3.478703546580689e-07,
      "loss": 0.3483,
      "step": 29760
    },
    {
      "epoch": 4.967462039045553,
      "grad_norm": 0.5207656621932983,
      "learning_rate": 3.309010690649924e-07,
      "loss": 0.4182,
      "step": 29770
    },
    {
      "epoch": 4.9691306524278325,
      "grad_norm": 5.9725213050842285,
      "learning_rate": 3.1393178347191585e-07,
      "loss": 0.2968,
      "step": 29780
    },
    {
      "epoch": 4.970799265810112,
      "grad_norm": 5.626160621643066,
      "learning_rate": 2.969624978788393e-07,
      "loss": 0.4774,
      "step": 29790
    },
    {
      "epoch": 4.972467879192391,
      "grad_norm": 9.327621459960938,
      "learning_rate": 2.799932122857628e-07,
      "loss": 0.274,
      "step": 29800
    },
    {
      "epoch": 4.9741364925746705,
      "grad_norm": 0.2975025773048401,
      "learning_rate": 2.6302392669268627e-07,
      "loss": 0.3312,
      "step": 29810
    },
    {
      "epoch": 4.97580510595695,
      "grad_norm": 2.0348033905029297,
      "learning_rate": 2.460546410996097e-07,
      "loss": 0.2921,
      "step": 29820
    },
    {
      "epoch": 4.977473719339229,
      "grad_norm": 7.336203575134277,
      "learning_rate": 2.2908535550653317e-07,
      "loss": 0.4042,
      "step": 29830
    },
    {
      "epoch": 4.9791423327215085,
      "grad_norm": 12.381935119628906,
      "learning_rate": 2.1211606991345667e-07,
      "loss": 0.4103,
      "step": 29840
    },
    {
      "epoch": 4.980810946103787,
      "grad_norm": 13.624434471130371,
      "learning_rate": 1.9514678432038013e-07,
      "loss": 0.3215,
      "step": 29850
    },
    {
      "epoch": 4.982479559486067,
      "grad_norm": 0.5981125235557556,
      "learning_rate": 1.7817749872730357e-07,
      "loss": 0.3419,
      "step": 29860
    },
    {
      "epoch": 4.9841481728683465,
      "grad_norm": 11.183202743530273,
      "learning_rate": 1.6120821313422706e-07,
      "loss": 0.4116,
      "step": 29870
    },
    {
      "epoch": 4.985816786250625,
      "grad_norm": 1.0197649002075195,
      "learning_rate": 1.4423892754115053e-07,
      "loss": 0.2693,
      "step": 29880
    },
    {
      "epoch": 4.987485399632905,
      "grad_norm": 3.450221538543701,
      "learning_rate": 1.27269641948074e-07,
      "loss": 0.4063,
      "step": 29890
    },
    {
      "epoch": 4.989154013015185,
      "grad_norm": 1.8300983905792236,
      "learning_rate": 1.1030035635499745e-07,
      "loss": 0.3945,
      "step": 29900
    },
    {
      "epoch": 4.990822626397463,
      "grad_norm": 16.273290634155273,
      "learning_rate": 9.333107076192093e-08,
      "loss": 0.4544,
      "step": 29910
    },
    {
      "epoch": 4.992491239779743,
      "grad_norm": 6.776735305786133,
      "learning_rate": 7.636178516884439e-08,
      "loss": 0.4067,
      "step": 29920
    },
    {
      "epoch": 4.994159853162023,
      "grad_norm": 7.3608222007751465,
      "learning_rate": 5.9392499575767864e-08,
      "loss": 0.5394,
      "step": 29930
    },
    {
      "epoch": 4.995828466544301,
      "grad_norm": 5.08292818069458,
      "learning_rate": 4.2423213982691336e-08,
      "loss": 0.525,
      "step": 29940
    },
    {
      "epoch": 4.997497079926581,
      "grad_norm": 5.4243574142456055,
      "learning_rate": 2.54539283896148e-08,
      "loss": 0.4068,
      "step": 29950
    },
    {
      "epoch": 4.999165693308861,
      "grad_norm": 11.837408065795898,
      "learning_rate": 8.484642796538267e-09,
      "loss": 0.4105,
      "step": 29960
    }
  ],
  "logging_steps": 10,
  "max_steps": 29965,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 2997,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.243623927620096e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
