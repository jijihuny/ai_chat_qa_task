{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.000166861338228,
  "eval_steps": 500,
  "global_step": 5994,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016686133822793258,
      "grad_norm": 3.79904842376709,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 2.2711,
      "step": 10
    },
    {
      "epoch": 0.0033372267645586516,
      "grad_norm": 7.182623386383057,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 2.1339,
      "step": 20
    },
    {
      "epoch": 0.005005840146837978,
      "grad_norm": 3.6051793098449707,
      "learning_rate": 3e-06,
      "loss": 1.34,
      "step": 30
    },
    {
      "epoch": 0.006674453529117303,
      "grad_norm": 10.527331352233887,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.6011,
      "step": 40
    },
    {
      "epoch": 0.00834306691139663,
      "grad_norm": 9.845771789550781,
      "learning_rate": 5e-06,
      "loss": 1.7923,
      "step": 50
    },
    {
      "epoch": 0.010011680293675955,
      "grad_norm": 5.402078151702881,
      "learning_rate": 6e-06,
      "loss": 1.4371,
      "step": 60
    },
    {
      "epoch": 0.011680293675955281,
      "grad_norm": 6.919439315795898,
      "learning_rate": 7.000000000000001e-06,
      "loss": 1.2766,
      "step": 70
    },
    {
      "epoch": 0.013348907058234607,
      "grad_norm": 3.590498447418213,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.7382,
      "step": 80
    },
    {
      "epoch": 0.015017520440513932,
      "grad_norm": 4.093526840209961,
      "learning_rate": 9e-06,
      "loss": 1.9864,
      "step": 90
    },
    {
      "epoch": 0.01668613382279326,
      "grad_norm": 3.692473888397217,
      "learning_rate": 1e-05,
      "loss": 2.0114,
      "step": 100
    },
    {
      "epoch": 0.018354747205072585,
      "grad_norm": 4.630216121673584,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.1169,
      "step": 110
    },
    {
      "epoch": 0.02002336058735191,
      "grad_norm": 5.28784704208374,
      "learning_rate": 1.2e-05,
      "loss": 2.0046,
      "step": 120
    },
    {
      "epoch": 0.021691973969631236,
      "grad_norm": 5.671827793121338,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 2.0312,
      "step": 130
    },
    {
      "epoch": 0.023360587351910562,
      "grad_norm": 8.2243013381958,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.5622,
      "step": 140
    },
    {
      "epoch": 0.025029200734189887,
      "grad_norm": 2.4041430950164795,
      "learning_rate": 1.5e-05,
      "loss": 1.461,
      "step": 150
    },
    {
      "epoch": 0.026697814116469213,
      "grad_norm": 4.042627334594727,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.7078,
      "step": 160
    },
    {
      "epoch": 0.02836642749874854,
      "grad_norm": 5.533896446228027,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 1.6739,
      "step": 170
    },
    {
      "epoch": 0.030035040881027864,
      "grad_norm": 6.103790760040283,
      "learning_rate": 1.8e-05,
      "loss": 1.2197,
      "step": 180
    },
    {
      "epoch": 0.03170365426330719,
      "grad_norm": 3.236377000808716,
      "learning_rate": 1.9e-05,
      "loss": 1.1634,
      "step": 190
    },
    {
      "epoch": 0.03337226764558652,
      "grad_norm": 3.6233606338500977,
      "learning_rate": 2e-05,
      "loss": 1.4596,
      "step": 200
    },
    {
      "epoch": 0.035040881027865844,
      "grad_norm": 2.819995403289795,
      "learning_rate": 2.1e-05,
      "loss": 0.883,
      "step": 210
    },
    {
      "epoch": 0.03670949441014517,
      "grad_norm": 3.301626443862915,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.9302,
      "step": 220
    },
    {
      "epoch": 0.038378107792424496,
      "grad_norm": 5.836217403411865,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 1.3534,
      "step": 230
    },
    {
      "epoch": 0.04004672117470382,
      "grad_norm": 2.4070804119110107,
      "learning_rate": 2.4e-05,
      "loss": 1.6678,
      "step": 240
    },
    {
      "epoch": 0.04171533455698315,
      "grad_norm": 5.188549041748047,
      "learning_rate": 2.5e-05,
      "loss": 1.5615,
      "step": 250
    },
    {
      "epoch": 0.04338394793926247,
      "grad_norm": 3.4946234226226807,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.1306,
      "step": 260
    },
    {
      "epoch": 0.0450525613215418,
      "grad_norm": 3.4348223209381104,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.9766,
      "step": 270
    },
    {
      "epoch": 0.046721174703821124,
      "grad_norm": 6.687024116516113,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.2911,
      "step": 280
    },
    {
      "epoch": 0.04838978808610045,
      "grad_norm": 8.801347732543945,
      "learning_rate": 2.9e-05,
      "loss": 1.1132,
      "step": 290
    },
    {
      "epoch": 0.050058401468379775,
      "grad_norm": 4.196038722991943,
      "learning_rate": 3e-05,
      "loss": 1.1248,
      "step": 300
    },
    {
      "epoch": 0.0517270148506591,
      "grad_norm": 2.8185057640075684,
      "learning_rate": 3.1e-05,
      "loss": 1.5625,
      "step": 310
    },
    {
      "epoch": 0.053395628232938426,
      "grad_norm": 7.821621417999268,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.1737,
      "step": 320
    },
    {
      "epoch": 0.05506424161521775,
      "grad_norm": 3.4825820922851562,
      "learning_rate": 3.3e-05,
      "loss": 1.2027,
      "step": 330
    },
    {
      "epoch": 0.05673285499749708,
      "grad_norm": 3.456397294998169,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.8523,
      "step": 340
    },
    {
      "epoch": 0.0584014683797764,
      "grad_norm": 6.144533634185791,
      "learning_rate": 3.5e-05,
      "loss": 1.1343,
      "step": 350
    },
    {
      "epoch": 0.06007008176205573,
      "grad_norm": 3.0288727283477783,
      "learning_rate": 3.6e-05,
      "loss": 1.0722,
      "step": 360
    },
    {
      "epoch": 0.06173869514433506,
      "grad_norm": 2.38199520111084,
      "learning_rate": 3.7e-05,
      "loss": 1.2691,
      "step": 370
    },
    {
      "epoch": 0.06340730852661439,
      "grad_norm": 9.471896171569824,
      "learning_rate": 3.8e-05,
      "loss": 1.1728,
      "step": 380
    },
    {
      "epoch": 0.0650759219088937,
      "grad_norm": 5.951301574707031,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.0393,
      "step": 390
    },
    {
      "epoch": 0.06674453529117304,
      "grad_norm": 4.838334560394287,
      "learning_rate": 4e-05,
      "loss": 0.6978,
      "step": 400
    },
    {
      "epoch": 0.06841314867345236,
      "grad_norm": 5.813498497009277,
      "learning_rate": 4.1e-05,
      "loss": 0.782,
      "step": 410
    },
    {
      "epoch": 0.07008176205573169,
      "grad_norm": 6.75109338760376,
      "learning_rate": 4.2e-05,
      "loss": 1.1449,
      "step": 420
    },
    {
      "epoch": 0.07175037543801101,
      "grad_norm": 1.5278546810150146,
      "learning_rate": 4.3e-05,
      "loss": 0.8661,
      "step": 430
    },
    {
      "epoch": 0.07341898882029034,
      "grad_norm": 4.583148002624512,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.058,
      "step": 440
    },
    {
      "epoch": 0.07508760220256966,
      "grad_norm": 7.415335178375244,
      "learning_rate": 4.5e-05,
      "loss": 1.0257,
      "step": 450
    },
    {
      "epoch": 0.07675621558484899,
      "grad_norm": 7.909468173980713,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.1085,
      "step": 460
    },
    {
      "epoch": 0.07842482896712831,
      "grad_norm": 3.7046473026275635,
      "learning_rate": 4.7e-05,
      "loss": 0.9718,
      "step": 470
    },
    {
      "epoch": 0.08009344234940764,
      "grad_norm": 5.0828118324279785,
      "learning_rate": 4.8e-05,
      "loss": 0.7885,
      "step": 480
    },
    {
      "epoch": 0.08176205573168698,
      "grad_norm": 1.0030765533447266,
      "learning_rate": 4.9e-05,
      "loss": 0.8161,
      "step": 490
    },
    {
      "epoch": 0.0834306691139663,
      "grad_norm": 14.221755027770996,
      "learning_rate": 5e-05,
      "loss": 0.8607,
      "step": 500
    },
    {
      "epoch": 0.08509928249624563,
      "grad_norm": 3.024747133255005,
      "learning_rate": 4.998303071440693e-05,
      "loss": 0.8952,
      "step": 510
    },
    {
      "epoch": 0.08676789587852494,
      "grad_norm": 8.669234275817871,
      "learning_rate": 4.996606142881385e-05,
      "loss": 0.782,
      "step": 520
    },
    {
      "epoch": 0.08843650926080428,
      "grad_norm": 5.924846649169922,
      "learning_rate": 4.994909214322077e-05,
      "loss": 1.2535,
      "step": 530
    },
    {
      "epoch": 0.0901051226430836,
      "grad_norm": 3.6464028358459473,
      "learning_rate": 4.993212285762769e-05,
      "loss": 0.8856,
      "step": 540
    },
    {
      "epoch": 0.09177373602536293,
      "grad_norm": 6.805990219116211,
      "learning_rate": 4.991515357203462e-05,
      "loss": 1.1457,
      "step": 550
    },
    {
      "epoch": 0.09344234940764225,
      "grad_norm": 5.023440837860107,
      "learning_rate": 4.989818428644154e-05,
      "loss": 0.6686,
      "step": 560
    },
    {
      "epoch": 0.09511096278992158,
      "grad_norm": 7.8002753257751465,
      "learning_rate": 4.988121500084847e-05,
      "loss": 1.3947,
      "step": 570
    },
    {
      "epoch": 0.0967795761722009,
      "grad_norm": 6.081882953643799,
      "learning_rate": 4.986424571525539e-05,
      "loss": 0.7415,
      "step": 580
    },
    {
      "epoch": 0.09844818955448023,
      "grad_norm": 3.987962007522583,
      "learning_rate": 4.9847276429662315e-05,
      "loss": 0.8556,
      "step": 590
    },
    {
      "epoch": 0.10011680293675955,
      "grad_norm": 8.489911079406738,
      "learning_rate": 4.983030714406924e-05,
      "loss": 0.7891,
      "step": 600
    },
    {
      "epoch": 0.10178541631903888,
      "grad_norm": 7.404081344604492,
      "learning_rate": 4.981333785847616e-05,
      "loss": 1.1191,
      "step": 610
    },
    {
      "epoch": 0.1034540297013182,
      "grad_norm": 4.346790313720703,
      "learning_rate": 4.9796368572883086e-05,
      "loss": 0.8707,
      "step": 620
    },
    {
      "epoch": 0.10512264308359753,
      "grad_norm": 7.410800933837891,
      "learning_rate": 4.9779399287290005e-05,
      "loss": 0.9092,
      "step": 630
    },
    {
      "epoch": 0.10679125646587685,
      "grad_norm": 2.9268417358398438,
      "learning_rate": 4.976243000169693e-05,
      "loss": 1.0369,
      "step": 640
    },
    {
      "epoch": 0.10845986984815618,
      "grad_norm": 4.379398822784424,
      "learning_rate": 4.974546071610386e-05,
      "loss": 0.8231,
      "step": 650
    },
    {
      "epoch": 0.1101284832304355,
      "grad_norm": 5.61354923248291,
      "learning_rate": 4.9728491430510776e-05,
      "loss": 0.9985,
      "step": 660
    },
    {
      "epoch": 0.11179709661271484,
      "grad_norm": 6.251413822174072,
      "learning_rate": 4.97115221449177e-05,
      "loss": 1.087,
      "step": 670
    },
    {
      "epoch": 0.11346570999499415,
      "grad_norm": 4.300239086151123,
      "learning_rate": 4.969455285932462e-05,
      "loss": 0.9326,
      "step": 680
    },
    {
      "epoch": 0.11513432337727349,
      "grad_norm": 1.7682503461837769,
      "learning_rate": 4.967758357373155e-05,
      "loss": 0.9457,
      "step": 690
    },
    {
      "epoch": 0.1168029367595528,
      "grad_norm": 6.0161309242248535,
      "learning_rate": 4.966061428813847e-05,
      "loss": 0.7711,
      "step": 700
    },
    {
      "epoch": 0.11847155014183214,
      "grad_norm": 7.376018047332764,
      "learning_rate": 4.96436450025454e-05,
      "loss": 0.7476,
      "step": 710
    },
    {
      "epoch": 0.12014016352411146,
      "grad_norm": 5.102439880371094,
      "learning_rate": 4.962667571695232e-05,
      "loss": 0.7983,
      "step": 720
    },
    {
      "epoch": 0.12180877690639079,
      "grad_norm": 5.592205047607422,
      "learning_rate": 4.960970643135924e-05,
      "loss": 0.7777,
      "step": 730
    },
    {
      "epoch": 0.12347739028867012,
      "grad_norm": 10.7653169631958,
      "learning_rate": 4.959273714576617e-05,
      "loss": 1.2737,
      "step": 740
    },
    {
      "epoch": 0.12514600367094944,
      "grad_norm": 4.526876926422119,
      "learning_rate": 4.957576786017309e-05,
      "loss": 1.1343,
      "step": 750
    },
    {
      "epoch": 0.12681461705322877,
      "grad_norm": 4.812863826751709,
      "learning_rate": 4.9558798574580014e-05,
      "loss": 0.9902,
      "step": 760
    },
    {
      "epoch": 0.1284832304355081,
      "grad_norm": 7.757281303405762,
      "learning_rate": 4.954182928898693e-05,
      "loss": 0.9167,
      "step": 770
    },
    {
      "epoch": 0.1301518438177874,
      "grad_norm": 6.603519916534424,
      "learning_rate": 4.952486000339386e-05,
      "loss": 0.975,
      "step": 780
    },
    {
      "epoch": 0.13182045720006674,
      "grad_norm": 7.927140712738037,
      "learning_rate": 4.950789071780078e-05,
      "loss": 1.0208,
      "step": 790
    },
    {
      "epoch": 0.13348907058234608,
      "grad_norm": 3.15924072265625,
      "learning_rate": 4.9490921432207704e-05,
      "loss": 0.8477,
      "step": 800
    },
    {
      "epoch": 0.1351576839646254,
      "grad_norm": 4.649066925048828,
      "learning_rate": 4.947395214661463e-05,
      "loss": 1.0416,
      "step": 810
    },
    {
      "epoch": 0.1368262973469047,
      "grad_norm": 13.645825386047363,
      "learning_rate": 4.9456982861021556e-05,
      "loss": 0.8851,
      "step": 820
    },
    {
      "epoch": 0.13849491072918405,
      "grad_norm": 1.7864187955856323,
      "learning_rate": 4.944001357542848e-05,
      "loss": 0.6225,
      "step": 830
    },
    {
      "epoch": 0.14016352411146338,
      "grad_norm": 6.079190731048584,
      "learning_rate": 4.94230442898354e-05,
      "loss": 0.8376,
      "step": 840
    },
    {
      "epoch": 0.1418321374937427,
      "grad_norm": 7.027696132659912,
      "learning_rate": 4.940607500424233e-05,
      "loss": 1.2121,
      "step": 850
    },
    {
      "epoch": 0.14350075087602202,
      "grad_norm": 6.695984840393066,
      "learning_rate": 4.9389105718649246e-05,
      "loss": 0.965,
      "step": 860
    },
    {
      "epoch": 0.14516936425830135,
      "grad_norm": 7.41980504989624,
      "learning_rate": 4.937213643305617e-05,
      "loss": 1.0054,
      "step": 870
    },
    {
      "epoch": 0.14683797764058068,
      "grad_norm": 6.862626075744629,
      "learning_rate": 4.935516714746309e-05,
      "loss": 0.9562,
      "step": 880
    },
    {
      "epoch": 0.14850659102286,
      "grad_norm": 6.646551609039307,
      "learning_rate": 4.933819786187002e-05,
      "loss": 0.868,
      "step": 890
    },
    {
      "epoch": 0.15017520440513932,
      "grad_norm": 5.551462650299072,
      "learning_rate": 4.932122857627694e-05,
      "loss": 0.7635,
      "step": 900
    },
    {
      "epoch": 0.15184381778741865,
      "grad_norm": 1.2757699489593506,
      "learning_rate": 4.930425929068386e-05,
      "loss": 0.8866,
      "step": 910
    },
    {
      "epoch": 0.15351243116969798,
      "grad_norm": 9.283583641052246,
      "learning_rate": 4.928729000509079e-05,
      "loss": 0.8781,
      "step": 920
    },
    {
      "epoch": 0.15518104455197732,
      "grad_norm": 8.223098754882812,
      "learning_rate": 4.9270320719497707e-05,
      "loss": 0.8231,
      "step": 930
    },
    {
      "epoch": 0.15684965793425662,
      "grad_norm": 6.533298015594482,
      "learning_rate": 4.925335143390464e-05,
      "loss": 0.885,
      "step": 940
    },
    {
      "epoch": 0.15851827131653595,
      "grad_norm": 10.064355850219727,
      "learning_rate": 4.923638214831156e-05,
      "loss": 1.0058,
      "step": 950
    },
    {
      "epoch": 0.16018688469881529,
      "grad_norm": 3.179572582244873,
      "learning_rate": 4.9219412862718484e-05,
      "loss": 1.0824,
      "step": 960
    },
    {
      "epoch": 0.16185549808109462,
      "grad_norm": 6.268158435821533,
      "learning_rate": 4.92024435771254e-05,
      "loss": 0.8083,
      "step": 970
    },
    {
      "epoch": 0.16352411146337395,
      "grad_norm": 7.978918552398682,
      "learning_rate": 4.918547429153233e-05,
      "loss": 0.8201,
      "step": 980
    },
    {
      "epoch": 0.16519272484565325,
      "grad_norm": 7.36130952835083,
      "learning_rate": 4.9168505005939255e-05,
      "loss": 0.8972,
      "step": 990
    },
    {
      "epoch": 0.1668613382279326,
      "grad_norm": 6.709752559661865,
      "learning_rate": 4.9151535720346174e-05,
      "loss": 1.0784,
      "step": 1000
    },
    {
      "epoch": 0.16852995161021192,
      "grad_norm": 3.5908749103546143,
      "learning_rate": 4.91345664347531e-05,
      "loss": 0.8012,
      "step": 1010
    },
    {
      "epoch": 0.17019856499249125,
      "grad_norm": 6.520646572113037,
      "learning_rate": 4.911759714916002e-05,
      "loss": 0.7217,
      "step": 1020
    },
    {
      "epoch": 0.17186717837477056,
      "grad_norm": 9.398969650268555,
      "learning_rate": 4.9100627863566945e-05,
      "loss": 0.6783,
      "step": 1030
    },
    {
      "epoch": 0.1735357917570499,
      "grad_norm": 4.233824253082275,
      "learning_rate": 4.908365857797387e-05,
      "loss": 0.7253,
      "step": 1040
    },
    {
      "epoch": 0.17520440513932922,
      "grad_norm": 7.398596286773682,
      "learning_rate": 4.906668929238079e-05,
      "loss": 0.7637,
      "step": 1050
    },
    {
      "epoch": 0.17687301852160855,
      "grad_norm": 6.739957809448242,
      "learning_rate": 4.9049720006787716e-05,
      "loss": 0.8031,
      "step": 1060
    },
    {
      "epoch": 0.17854163190388786,
      "grad_norm": 7.9486985206604,
      "learning_rate": 4.903275072119464e-05,
      "loss": 0.9621,
      "step": 1070
    },
    {
      "epoch": 0.1802102452861672,
      "grad_norm": 7.202178955078125,
      "learning_rate": 4.901578143560157e-05,
      "loss": 0.9566,
      "step": 1080
    },
    {
      "epoch": 0.18187885866844652,
      "grad_norm": 6.278532028198242,
      "learning_rate": 4.899881215000849e-05,
      "loss": 0.9441,
      "step": 1090
    },
    {
      "epoch": 0.18354747205072586,
      "grad_norm": 5.335439205169678,
      "learning_rate": 4.898184286441541e-05,
      "loss": 0.6441,
      "step": 1100
    },
    {
      "epoch": 0.18521608543300516,
      "grad_norm": 4.642460346221924,
      "learning_rate": 4.896487357882233e-05,
      "loss": 0.7104,
      "step": 1110
    },
    {
      "epoch": 0.1868846988152845,
      "grad_norm": 8.298638343811035,
      "learning_rate": 4.894790429322926e-05,
      "loss": 1.1014,
      "step": 1120
    },
    {
      "epoch": 0.18855331219756383,
      "grad_norm": 4.595211982727051,
      "learning_rate": 4.8930935007636183e-05,
      "loss": 0.7587,
      "step": 1130
    },
    {
      "epoch": 0.19022192557984316,
      "grad_norm": 3.740720748901367,
      "learning_rate": 4.89139657220431e-05,
      "loss": 0.9371,
      "step": 1140
    },
    {
      "epoch": 0.19189053896212246,
      "grad_norm": 9.767218589782715,
      "learning_rate": 4.889699643645003e-05,
      "loss": 0.6969,
      "step": 1150
    },
    {
      "epoch": 0.1935591523444018,
      "grad_norm": 7.882074356079102,
      "learning_rate": 4.888002715085695e-05,
      "loss": 0.4989,
      "step": 1160
    },
    {
      "epoch": 0.19522776572668113,
      "grad_norm": 8.713257789611816,
      "learning_rate": 4.886305786526387e-05,
      "loss": 0.6804,
      "step": 1170
    },
    {
      "epoch": 0.19689637910896046,
      "grad_norm": 6.494941711425781,
      "learning_rate": 4.884608857967079e-05,
      "loss": 0.6599,
      "step": 1180
    },
    {
      "epoch": 0.19856499249123977,
      "grad_norm": 5.320250511169434,
      "learning_rate": 4.8829119294077725e-05,
      "loss": 1.2213,
      "step": 1190
    },
    {
      "epoch": 0.2002336058735191,
      "grad_norm": 6.940951824188232,
      "learning_rate": 4.8812150008484644e-05,
      "loss": 1.151,
      "step": 1200
    },
    {
      "epoch": 0.20190221925579843,
      "grad_norm": 3.36543869972229,
      "learning_rate": 4.879518072289157e-05,
      "loss": 0.9777,
      "step": 1210
    },
    {
      "epoch": 0.20357083263807776,
      "grad_norm": 3.555760145187378,
      "learning_rate": 4.8778211437298496e-05,
      "loss": 0.9639,
      "step": 1220
    },
    {
      "epoch": 0.2052394460203571,
      "grad_norm": 7.245345592498779,
      "learning_rate": 4.8761242151705415e-05,
      "loss": 0.8724,
      "step": 1230
    },
    {
      "epoch": 0.2069080594026364,
      "grad_norm": 6.3500189781188965,
      "learning_rate": 4.874427286611234e-05,
      "loss": 0.7266,
      "step": 1240
    },
    {
      "epoch": 0.20857667278491573,
      "grad_norm": 7.472283363342285,
      "learning_rate": 4.872730358051926e-05,
      "loss": 0.7371,
      "step": 1250
    },
    {
      "epoch": 0.21024528616719507,
      "grad_norm": 7.047811985015869,
      "learning_rate": 4.8710334294926186e-05,
      "loss": 0.6164,
      "step": 1260
    },
    {
      "epoch": 0.2119138995494744,
      "grad_norm": 6.9337639808654785,
      "learning_rate": 4.869336500933311e-05,
      "loss": 0.7654,
      "step": 1270
    },
    {
      "epoch": 0.2135825129317537,
      "grad_norm": 15.973174095153809,
      "learning_rate": 4.867639572374003e-05,
      "loss": 1.0935,
      "step": 1280
    },
    {
      "epoch": 0.21525112631403304,
      "grad_norm": 8.758491516113281,
      "learning_rate": 4.865942643814696e-05,
      "loss": 0.7974,
      "step": 1290
    },
    {
      "epoch": 0.21691973969631237,
      "grad_norm": 1.8754631280899048,
      "learning_rate": 4.8642457152553876e-05,
      "loss": 0.5935,
      "step": 1300
    },
    {
      "epoch": 0.2185883530785917,
      "grad_norm": 7.009585857391357,
      "learning_rate": 4.862548786696081e-05,
      "loss": 1.0534,
      "step": 1310
    },
    {
      "epoch": 0.220256966460871,
      "grad_norm": 4.205259799957275,
      "learning_rate": 4.860851858136773e-05,
      "loss": 0.9154,
      "step": 1320
    },
    {
      "epoch": 0.22192557984315034,
      "grad_norm": 6.721474647521973,
      "learning_rate": 4.8591549295774653e-05,
      "loss": 0.6583,
      "step": 1330
    },
    {
      "epoch": 0.22359419322542967,
      "grad_norm": 3.5803515911102295,
      "learning_rate": 4.857458001018157e-05,
      "loss": 0.8278,
      "step": 1340
    },
    {
      "epoch": 0.225262806607709,
      "grad_norm": 4.124467849731445,
      "learning_rate": 4.85576107245885e-05,
      "loss": 0.5003,
      "step": 1350
    },
    {
      "epoch": 0.2269314199899883,
      "grad_norm": 10.031257629394531,
      "learning_rate": 4.8540641438995424e-05,
      "loss": 0.9419,
      "step": 1360
    },
    {
      "epoch": 0.22860003337226764,
      "grad_norm": 7.770595073699951,
      "learning_rate": 4.8523672153402343e-05,
      "loss": 0.9462,
      "step": 1370
    },
    {
      "epoch": 0.23026864675454697,
      "grad_norm": 6.804042339324951,
      "learning_rate": 4.850670286780927e-05,
      "loss": 0.862,
      "step": 1380
    },
    {
      "epoch": 0.2319372601368263,
      "grad_norm": 3.2381014823913574,
      "learning_rate": 4.848973358221619e-05,
      "loss": 0.8612,
      "step": 1390
    },
    {
      "epoch": 0.2336058735191056,
      "grad_norm": 5.524078845977783,
      "learning_rate": 4.8472764296623114e-05,
      "loss": 0.7656,
      "step": 1400
    },
    {
      "epoch": 0.23527448690138494,
      "grad_norm": 5.035533905029297,
      "learning_rate": 4.845579501103003e-05,
      "loss": 1.071,
      "step": 1410
    },
    {
      "epoch": 0.23694310028366428,
      "grad_norm": 9.536332130432129,
      "learning_rate": 4.843882572543696e-05,
      "loss": 0.6668,
      "step": 1420
    },
    {
      "epoch": 0.2386117136659436,
      "grad_norm": 5.131458282470703,
      "learning_rate": 4.8421856439843885e-05,
      "loss": 1.0141,
      "step": 1430
    },
    {
      "epoch": 0.2402803270482229,
      "grad_norm": 5.364581108093262,
      "learning_rate": 4.840488715425081e-05,
      "loss": 1.0744,
      "step": 1440
    },
    {
      "epoch": 0.24194894043050225,
      "grad_norm": 10.462005615234375,
      "learning_rate": 4.838791786865774e-05,
      "loss": 0.7962,
      "step": 1450
    },
    {
      "epoch": 0.24361755381278158,
      "grad_norm": 3.568871021270752,
      "learning_rate": 4.8370948583064656e-05,
      "loss": 0.8062,
      "step": 1460
    },
    {
      "epoch": 0.2452861671950609,
      "grad_norm": 5.848616600036621,
      "learning_rate": 4.835397929747158e-05,
      "loss": 0.8054,
      "step": 1470
    },
    {
      "epoch": 0.24695478057734024,
      "grad_norm": 2.287451982498169,
      "learning_rate": 4.83370100118785e-05,
      "loss": 0.7035,
      "step": 1480
    },
    {
      "epoch": 0.24862339395961955,
      "grad_norm": 7.200664043426514,
      "learning_rate": 4.832004072628543e-05,
      "loss": 0.8759,
      "step": 1490
    },
    {
      "epoch": 0.2502920073418989,
      "grad_norm": 4.867833614349365,
      "learning_rate": 4.8303071440692346e-05,
      "loss": 0.8476,
      "step": 1500
    },
    {
      "epoch": 0.2519606207241782,
      "grad_norm": 4.327005863189697,
      "learning_rate": 4.828610215509927e-05,
      "loss": 0.877,
      "step": 1510
    },
    {
      "epoch": 0.25362923410645755,
      "grad_norm": 7.988686561584473,
      "learning_rate": 4.82691328695062e-05,
      "loss": 0.7017,
      "step": 1520
    },
    {
      "epoch": 0.25529784748873685,
      "grad_norm": 3.5522189140319824,
      "learning_rate": 4.825216358391312e-05,
      "loss": 0.8189,
      "step": 1530
    },
    {
      "epoch": 0.2569664608710162,
      "grad_norm": 9.3996000289917,
      "learning_rate": 4.823519429832004e-05,
      "loss": 0.8931,
      "step": 1540
    },
    {
      "epoch": 0.2586350742532955,
      "grad_norm": 6.500752925872803,
      "learning_rate": 4.821822501272696e-05,
      "loss": 1.0194,
      "step": 1550
    },
    {
      "epoch": 0.2603036876355748,
      "grad_norm": 5.607722282409668,
      "learning_rate": 4.8201255727133894e-05,
      "loss": 0.841,
      "step": 1560
    },
    {
      "epoch": 0.2619723010178542,
      "grad_norm": 7.975852966308594,
      "learning_rate": 4.8184286441540813e-05,
      "loss": 0.9532,
      "step": 1570
    },
    {
      "epoch": 0.2636409144001335,
      "grad_norm": 4.294488906860352,
      "learning_rate": 4.816731715594774e-05,
      "loss": 0.5532,
      "step": 1580
    },
    {
      "epoch": 0.2653095277824128,
      "grad_norm": 5.220550537109375,
      "learning_rate": 4.815034787035466e-05,
      "loss": 1.0821,
      "step": 1590
    },
    {
      "epoch": 0.26697814116469215,
      "grad_norm": 10.952414512634277,
      "learning_rate": 4.8133378584761584e-05,
      "loss": 0.9575,
      "step": 1600
    },
    {
      "epoch": 0.26864675454697146,
      "grad_norm": 3.701032876968384,
      "learning_rate": 4.811640929916851e-05,
      "loss": 0.8681,
      "step": 1610
    },
    {
      "epoch": 0.2703153679292508,
      "grad_norm": 4.70545768737793,
      "learning_rate": 4.809944001357543e-05,
      "loss": 0.9569,
      "step": 1620
    },
    {
      "epoch": 0.2719839813115301,
      "grad_norm": 7.7636284828186035,
      "learning_rate": 4.8082470727982355e-05,
      "loss": 0.7812,
      "step": 1630
    },
    {
      "epoch": 0.2736525946938094,
      "grad_norm": 6.987236022949219,
      "learning_rate": 4.8065501442389274e-05,
      "loss": 0.7828,
      "step": 1640
    },
    {
      "epoch": 0.2753212080760888,
      "grad_norm": 4.573073863983154,
      "learning_rate": 4.80485321567962e-05,
      "loss": 0.9015,
      "step": 1650
    },
    {
      "epoch": 0.2769898214583681,
      "grad_norm": 3.060145854949951,
      "learning_rate": 4.8031562871203126e-05,
      "loss": 0.6581,
      "step": 1660
    },
    {
      "epoch": 0.2786584348406474,
      "grad_norm": 9.09227466583252,
      "learning_rate": 4.8014593585610045e-05,
      "loss": 0.9648,
      "step": 1670
    },
    {
      "epoch": 0.28032704822292676,
      "grad_norm": 7.815892219543457,
      "learning_rate": 4.799762430001697e-05,
      "loss": 0.8771,
      "step": 1680
    },
    {
      "epoch": 0.28199566160520606,
      "grad_norm": 8.1954984664917,
      "learning_rate": 4.79806550144239e-05,
      "loss": 0.4498,
      "step": 1690
    },
    {
      "epoch": 0.2836642749874854,
      "grad_norm": 4.973302364349365,
      "learning_rate": 4.796368572883082e-05,
      "loss": 0.9246,
      "step": 1700
    },
    {
      "epoch": 0.2853328883697647,
      "grad_norm": 8.766337394714355,
      "learning_rate": 4.794671644323774e-05,
      "loss": 0.5424,
      "step": 1710
    },
    {
      "epoch": 0.28700150175204403,
      "grad_norm": 5.705307960510254,
      "learning_rate": 4.792974715764467e-05,
      "loss": 1.1216,
      "step": 1720
    },
    {
      "epoch": 0.2886701151343234,
      "grad_norm": 5.839878082275391,
      "learning_rate": 4.791277787205159e-05,
      "loss": 0.5954,
      "step": 1730
    },
    {
      "epoch": 0.2903387285166027,
      "grad_norm": 6.5529465675354,
      "learning_rate": 4.789580858645851e-05,
      "loss": 0.8163,
      "step": 1740
    },
    {
      "epoch": 0.29200734189888206,
      "grad_norm": 8.933732986450195,
      "learning_rate": 4.787883930086544e-05,
      "loss": 0.8461,
      "step": 1750
    },
    {
      "epoch": 0.29367595528116136,
      "grad_norm": 4.6097187995910645,
      "learning_rate": 4.786187001527236e-05,
      "loss": 0.5946,
      "step": 1760
    },
    {
      "epoch": 0.29534456866344067,
      "grad_norm": 7.7182183265686035,
      "learning_rate": 4.7844900729679284e-05,
      "loss": 0.6555,
      "step": 1770
    },
    {
      "epoch": 0.29701318204572,
      "grad_norm": 2.280984878540039,
      "learning_rate": 4.78279314440862e-05,
      "loss": 0.7076,
      "step": 1780
    },
    {
      "epoch": 0.29868179542799933,
      "grad_norm": 8.5516357421875,
      "learning_rate": 4.781096215849313e-05,
      "loss": 0.9988,
      "step": 1790
    },
    {
      "epoch": 0.30035040881027864,
      "grad_norm": 4.809074878692627,
      "learning_rate": 4.779399287290005e-05,
      "loss": 0.8486,
      "step": 1800
    },
    {
      "epoch": 0.302019022192558,
      "grad_norm": 3.1991074085235596,
      "learning_rate": 4.777702358730698e-05,
      "loss": 0.6643,
      "step": 1810
    },
    {
      "epoch": 0.3036876355748373,
      "grad_norm": 7.109113693237305,
      "learning_rate": 4.77600543017139e-05,
      "loss": 0.7072,
      "step": 1820
    },
    {
      "epoch": 0.30535624895711666,
      "grad_norm": 5.751452922821045,
      "learning_rate": 4.7743085016120825e-05,
      "loss": 0.4635,
      "step": 1830
    },
    {
      "epoch": 0.30702486233939597,
      "grad_norm": 6.025784015655518,
      "learning_rate": 4.772611573052775e-05,
      "loss": 0.7978,
      "step": 1840
    },
    {
      "epoch": 0.30869347572167527,
      "grad_norm": 7.925596714019775,
      "learning_rate": 4.770914644493467e-05,
      "loss": 0.7224,
      "step": 1850
    },
    {
      "epoch": 0.31036208910395463,
      "grad_norm": 2.901942729949951,
      "learning_rate": 4.7692177159341596e-05,
      "loss": 0.8664,
      "step": 1860
    },
    {
      "epoch": 0.31203070248623394,
      "grad_norm": 4.568856239318848,
      "learning_rate": 4.7675207873748515e-05,
      "loss": 0.8431,
      "step": 1870
    },
    {
      "epoch": 0.31369931586851324,
      "grad_norm": 3.7095916271209717,
      "learning_rate": 4.765823858815544e-05,
      "loss": 0.6565,
      "step": 1880
    },
    {
      "epoch": 0.3153679292507926,
      "grad_norm": 6.966523170471191,
      "learning_rate": 4.764126930256237e-05,
      "loss": 0.6631,
      "step": 1890
    },
    {
      "epoch": 0.3170365426330719,
      "grad_norm": 3.690739631652832,
      "learning_rate": 4.7624300016969286e-05,
      "loss": 1.0129,
      "step": 1900
    },
    {
      "epoch": 0.31870515601535127,
      "grad_norm": 10.0293550491333,
      "learning_rate": 4.760733073137621e-05,
      "loss": 0.8941,
      "step": 1910
    },
    {
      "epoch": 0.32037376939763057,
      "grad_norm": 5.941582679748535,
      "learning_rate": 4.759036144578313e-05,
      "loss": 0.727,
      "step": 1920
    },
    {
      "epoch": 0.3220423827799099,
      "grad_norm": 7.233775615692139,
      "learning_rate": 4.757339216019006e-05,
      "loss": 0.8189,
      "step": 1930
    },
    {
      "epoch": 0.32371099616218924,
      "grad_norm": 6.904343605041504,
      "learning_rate": 4.755642287459698e-05,
      "loss": 0.9145,
      "step": 1940
    },
    {
      "epoch": 0.32537960954446854,
      "grad_norm": 7.455501079559326,
      "learning_rate": 4.753945358900391e-05,
      "loss": 0.639,
      "step": 1950
    },
    {
      "epoch": 0.3270482229267479,
      "grad_norm": 2.02082896232605,
      "learning_rate": 4.752248430341083e-05,
      "loss": 0.62,
      "step": 1960
    },
    {
      "epoch": 0.3287168363090272,
      "grad_norm": 0.8728573322296143,
      "learning_rate": 4.7505515017817754e-05,
      "loss": 0.4651,
      "step": 1970
    },
    {
      "epoch": 0.3303854496913065,
      "grad_norm": 6.1950836181640625,
      "learning_rate": 4.748854573222468e-05,
      "loss": 1.0393,
      "step": 1980
    },
    {
      "epoch": 0.33205406307358587,
      "grad_norm": 6.6434712409973145,
      "learning_rate": 4.74715764466316e-05,
      "loss": 0.5031,
      "step": 1990
    },
    {
      "epoch": 0.3337226764558652,
      "grad_norm": 5.445943355560303,
      "learning_rate": 4.7454607161038524e-05,
      "loss": 0.5834,
      "step": 2000
    },
    {
      "epoch": 0.3353912898381445,
      "grad_norm": 2.0814809799194336,
      "learning_rate": 4.7437637875445444e-05,
      "loss": 0.7365,
      "step": 2010
    },
    {
      "epoch": 0.33705990322042384,
      "grad_norm": 8.248190879821777,
      "learning_rate": 4.742066858985237e-05,
      "loss": 0.5451,
      "step": 2020
    },
    {
      "epoch": 0.33872851660270314,
      "grad_norm": 5.852273464202881,
      "learning_rate": 4.740369930425929e-05,
      "loss": 0.6407,
      "step": 2030
    },
    {
      "epoch": 0.3403971299849825,
      "grad_norm": 10.305346488952637,
      "learning_rate": 4.7386730018666214e-05,
      "loss": 0.7857,
      "step": 2040
    },
    {
      "epoch": 0.3420657433672618,
      "grad_norm": 3.9916467666625977,
      "learning_rate": 4.736976073307314e-05,
      "loss": 0.595,
      "step": 2050
    },
    {
      "epoch": 0.3437343567495411,
      "grad_norm": 7.858857154846191,
      "learning_rate": 4.7352791447480066e-05,
      "loss": 0.8684,
      "step": 2060
    },
    {
      "epoch": 0.3454029701318205,
      "grad_norm": 4.695647716522217,
      "learning_rate": 4.733582216188699e-05,
      "loss": 0.5155,
      "step": 2070
    },
    {
      "epoch": 0.3470715835140998,
      "grad_norm": 3.2922067642211914,
      "learning_rate": 4.731885287629391e-05,
      "loss": 0.9256,
      "step": 2080
    },
    {
      "epoch": 0.3487401968963791,
      "grad_norm": 8.713995933532715,
      "learning_rate": 4.730188359070084e-05,
      "loss": 0.5911,
      "step": 2090
    },
    {
      "epoch": 0.35040881027865844,
      "grad_norm": 9.679231643676758,
      "learning_rate": 4.7284914305107756e-05,
      "loss": 0.7517,
      "step": 2100
    },
    {
      "epoch": 0.35207742366093775,
      "grad_norm": 10.645646095275879,
      "learning_rate": 4.726794501951468e-05,
      "loss": 1.1755,
      "step": 2110
    },
    {
      "epoch": 0.3537460370432171,
      "grad_norm": 6.195444107055664,
      "learning_rate": 4.72509757339216e-05,
      "loss": 0.7128,
      "step": 2120
    },
    {
      "epoch": 0.3554146504254964,
      "grad_norm": 9.371164321899414,
      "learning_rate": 4.723400644832853e-05,
      "loss": 0.5542,
      "step": 2130
    },
    {
      "epoch": 0.3570832638077757,
      "grad_norm": 0.7653611302375793,
      "learning_rate": 4.721703716273545e-05,
      "loss": 0.6224,
      "step": 2140
    },
    {
      "epoch": 0.3587518771900551,
      "grad_norm": 7.059931755065918,
      "learning_rate": 4.720006787714237e-05,
      "loss": 0.7012,
      "step": 2150
    },
    {
      "epoch": 0.3604204905723344,
      "grad_norm": 7.114423751831055,
      "learning_rate": 4.71830985915493e-05,
      "loss": 0.8296,
      "step": 2160
    },
    {
      "epoch": 0.3620891039546137,
      "grad_norm": 8.467098236083984,
      "learning_rate": 4.716612930595622e-05,
      "loss": 1.0739,
      "step": 2170
    },
    {
      "epoch": 0.36375771733689305,
      "grad_norm": 8.871882438659668,
      "learning_rate": 4.714916002036314e-05,
      "loss": 0.6668,
      "step": 2180
    },
    {
      "epoch": 0.36542633071917235,
      "grad_norm": 5.060971260070801,
      "learning_rate": 4.713219073477007e-05,
      "loss": 0.8976,
      "step": 2190
    },
    {
      "epoch": 0.3670949441014517,
      "grad_norm": 4.836977005004883,
      "learning_rate": 4.7115221449176994e-05,
      "loss": 0.6857,
      "step": 2200
    },
    {
      "epoch": 0.368763557483731,
      "grad_norm": 1.0327324867248535,
      "learning_rate": 4.7098252163583914e-05,
      "loss": 0.5267,
      "step": 2210
    },
    {
      "epoch": 0.3704321708660103,
      "grad_norm": 11.744599342346191,
      "learning_rate": 4.708128287799084e-05,
      "loss": 0.8479,
      "step": 2220
    },
    {
      "epoch": 0.3721007842482897,
      "grad_norm": 2.2491724491119385,
      "learning_rate": 4.7064313592397765e-05,
      "loss": 0.5676,
      "step": 2230
    },
    {
      "epoch": 0.373769397630569,
      "grad_norm": 2.801563024520874,
      "learning_rate": 4.7047344306804684e-05,
      "loss": 0.7472,
      "step": 2240
    },
    {
      "epoch": 0.37543801101284835,
      "grad_norm": 4.369708061218262,
      "learning_rate": 4.703037502121161e-05,
      "loss": 0.6448,
      "step": 2250
    },
    {
      "epoch": 0.37710662439512765,
      "grad_norm": 7.913095474243164,
      "learning_rate": 4.701340573561853e-05,
      "loss": 0.7612,
      "step": 2260
    },
    {
      "epoch": 0.37877523777740696,
      "grad_norm": 3.4856364727020264,
      "learning_rate": 4.6996436450025455e-05,
      "loss": 0.5559,
      "step": 2270
    },
    {
      "epoch": 0.3804438511596863,
      "grad_norm": 3.502786636352539,
      "learning_rate": 4.697946716443238e-05,
      "loss": 1.1246,
      "step": 2280
    },
    {
      "epoch": 0.3821124645419656,
      "grad_norm": 4.559396743774414,
      "learning_rate": 4.69624978788393e-05,
      "loss": 0.514,
      "step": 2290
    },
    {
      "epoch": 0.38378107792424493,
      "grad_norm": 10.608229637145996,
      "learning_rate": 4.6945528593246226e-05,
      "loss": 0.4996,
      "step": 2300
    },
    {
      "epoch": 0.3854496913065243,
      "grad_norm": 2.7624449729919434,
      "learning_rate": 4.692855930765315e-05,
      "loss": 0.4467,
      "step": 2310
    },
    {
      "epoch": 0.3871183046888036,
      "grad_norm": 2.5176689624786377,
      "learning_rate": 4.691159002206008e-05,
      "loss": 0.514,
      "step": 2320
    },
    {
      "epoch": 0.38878691807108295,
      "grad_norm": 29.607952117919922,
      "learning_rate": 4.6894620736467e-05,
      "loss": 0.8919,
      "step": 2330
    },
    {
      "epoch": 0.39045553145336226,
      "grad_norm": 7.312753200531006,
      "learning_rate": 4.687765145087392e-05,
      "loss": 0.6584,
      "step": 2340
    },
    {
      "epoch": 0.39212414483564156,
      "grad_norm": 10.338000297546387,
      "learning_rate": 4.686068216528084e-05,
      "loss": 0.8212,
      "step": 2350
    },
    {
      "epoch": 0.3937927582179209,
      "grad_norm": 7.501348972320557,
      "learning_rate": 4.684371287968777e-05,
      "loss": 0.984,
      "step": 2360
    },
    {
      "epoch": 0.39546137160020023,
      "grad_norm": 7.75062894821167,
      "learning_rate": 4.6826743594094694e-05,
      "loss": 0.6952,
      "step": 2370
    },
    {
      "epoch": 0.39712998498247953,
      "grad_norm": 13.856389045715332,
      "learning_rate": 4.680977430850161e-05,
      "loss": 0.8905,
      "step": 2380
    },
    {
      "epoch": 0.3987985983647589,
      "grad_norm": 4.53643798828125,
      "learning_rate": 4.679280502290854e-05,
      "loss": 0.8118,
      "step": 2390
    },
    {
      "epoch": 0.4004672117470382,
      "grad_norm": 5.516713619232178,
      "learning_rate": 4.677583573731546e-05,
      "loss": 0.7209,
      "step": 2400
    },
    {
      "epoch": 0.40213582512931756,
      "grad_norm": 8.12226390838623,
      "learning_rate": 4.6758866451722384e-05,
      "loss": 0.5662,
      "step": 2410
    },
    {
      "epoch": 0.40380443851159686,
      "grad_norm": 5.107851505279541,
      "learning_rate": 4.67418971661293e-05,
      "loss": 0.6007,
      "step": 2420
    },
    {
      "epoch": 0.40547305189387617,
      "grad_norm": 4.949314594268799,
      "learning_rate": 4.672492788053623e-05,
      "loss": 0.7865,
      "step": 2430
    },
    {
      "epoch": 0.40714166527615553,
      "grad_norm": 13.078946113586426,
      "learning_rate": 4.6707958594943154e-05,
      "loss": 0.8915,
      "step": 2440
    },
    {
      "epoch": 0.40881027865843483,
      "grad_norm": 10.92952823638916,
      "learning_rate": 4.669098930935008e-05,
      "loss": 0.9305,
      "step": 2450
    },
    {
      "epoch": 0.4104788920407142,
      "grad_norm": 9.040349960327148,
      "learning_rate": 4.6674020023757006e-05,
      "loss": 0.7678,
      "step": 2460
    },
    {
      "epoch": 0.4121475054229935,
      "grad_norm": 4.435794353485107,
      "learning_rate": 4.6657050738163925e-05,
      "loss": 0.8393,
      "step": 2470
    },
    {
      "epoch": 0.4138161188052728,
      "grad_norm": 5.322741985321045,
      "learning_rate": 4.664008145257085e-05,
      "loss": 0.6357,
      "step": 2480
    },
    {
      "epoch": 0.41548473218755216,
      "grad_norm": 8.003535270690918,
      "learning_rate": 4.662311216697777e-05,
      "loss": 0.6056,
      "step": 2490
    },
    {
      "epoch": 0.41715334556983147,
      "grad_norm": 1.591697096824646,
      "learning_rate": 4.6606142881384696e-05,
      "loss": 0.8547,
      "step": 2500
    },
    {
      "epoch": 0.4188219589521108,
      "grad_norm": 1.3259997367858887,
      "learning_rate": 4.658917359579162e-05,
      "loss": 0.6272,
      "step": 2510
    },
    {
      "epoch": 0.42049057233439013,
      "grad_norm": 13.520787239074707,
      "learning_rate": 4.657220431019854e-05,
      "loss": 0.7843,
      "step": 2520
    },
    {
      "epoch": 0.42215918571666944,
      "grad_norm": 5.95968770980835,
      "learning_rate": 4.655523502460547e-05,
      "loss": 0.9054,
      "step": 2530
    },
    {
      "epoch": 0.4238277990989488,
      "grad_norm": 5.7057108879089355,
      "learning_rate": 4.6538265739012386e-05,
      "loss": 0.9363,
      "step": 2540
    },
    {
      "epoch": 0.4254964124812281,
      "grad_norm": 9.836874961853027,
      "learning_rate": 4.652129645341931e-05,
      "loss": 0.9169,
      "step": 2550
    },
    {
      "epoch": 0.4271650258635074,
      "grad_norm": 8.053886413574219,
      "learning_rate": 4.650432716782624e-05,
      "loss": 0.8474,
      "step": 2560
    },
    {
      "epoch": 0.42883363924578677,
      "grad_norm": 8.77614688873291,
      "learning_rate": 4.6487357882233164e-05,
      "loss": 0.8068,
      "step": 2570
    },
    {
      "epoch": 0.4305022526280661,
      "grad_norm": 7.959442138671875,
      "learning_rate": 4.647038859664008e-05,
      "loss": 0.8546,
      "step": 2580
    },
    {
      "epoch": 0.4321708660103454,
      "grad_norm": 3.8034751415252686,
      "learning_rate": 4.645341931104701e-05,
      "loss": 0.5859,
      "step": 2590
    },
    {
      "epoch": 0.43383947939262474,
      "grad_norm": 9.123007774353027,
      "learning_rate": 4.6436450025453935e-05,
      "loss": 0.7763,
      "step": 2600
    },
    {
      "epoch": 0.43550809277490404,
      "grad_norm": 7.685604572296143,
      "learning_rate": 4.6419480739860854e-05,
      "loss": 0.891,
      "step": 2610
    },
    {
      "epoch": 0.4371767061571834,
      "grad_norm": 6.3122758865356445,
      "learning_rate": 4.640251145426778e-05,
      "loss": 0.6462,
      "step": 2620
    },
    {
      "epoch": 0.4388453195394627,
      "grad_norm": 4.7552385330200195,
      "learning_rate": 4.63855421686747e-05,
      "loss": 0.788,
      "step": 2630
    },
    {
      "epoch": 0.440513932921742,
      "grad_norm": 4.40580940246582,
      "learning_rate": 4.6368572883081625e-05,
      "loss": 0.7864,
      "step": 2640
    },
    {
      "epoch": 0.4421825463040214,
      "grad_norm": 3.5186431407928467,
      "learning_rate": 4.6351603597488544e-05,
      "loss": 0.6606,
      "step": 2650
    },
    {
      "epoch": 0.4438511596863007,
      "grad_norm": 6.737544536590576,
      "learning_rate": 4.633463431189547e-05,
      "loss": 0.6714,
      "step": 2660
    },
    {
      "epoch": 0.44551977306858,
      "grad_norm": 2.1617512702941895,
      "learning_rate": 4.6317665026302395e-05,
      "loss": 0.7032,
      "step": 2670
    },
    {
      "epoch": 0.44718838645085934,
      "grad_norm": 7.921445369720459,
      "learning_rate": 4.6300695740709314e-05,
      "loss": 0.7676,
      "step": 2680
    },
    {
      "epoch": 0.44885699983313865,
      "grad_norm": 7.745121955871582,
      "learning_rate": 4.628372645511625e-05,
      "loss": 0.521,
      "step": 2690
    },
    {
      "epoch": 0.450525613215418,
      "grad_norm": 5.564319133758545,
      "learning_rate": 4.6266757169523166e-05,
      "loss": 0.8596,
      "step": 2700
    },
    {
      "epoch": 0.4521942265976973,
      "grad_norm": 8.842630386352539,
      "learning_rate": 4.624978788393009e-05,
      "loss": 1.1088,
      "step": 2710
    },
    {
      "epoch": 0.4538628399799766,
      "grad_norm": 6.874146461486816,
      "learning_rate": 4.623281859833701e-05,
      "loss": 0.7688,
      "step": 2720
    },
    {
      "epoch": 0.455531453362256,
      "grad_norm": 6.731356143951416,
      "learning_rate": 4.621584931274394e-05,
      "loss": 1.2956,
      "step": 2730
    },
    {
      "epoch": 0.4572000667445353,
      "grad_norm": 5.926513195037842,
      "learning_rate": 4.6198880027150856e-05,
      "loss": 0.6151,
      "step": 2740
    },
    {
      "epoch": 0.45886868012681464,
      "grad_norm": 7.988247871398926,
      "learning_rate": 4.618191074155778e-05,
      "loss": 0.9961,
      "step": 2750
    },
    {
      "epoch": 0.46053729350909395,
      "grad_norm": 11.19940185546875,
      "learning_rate": 4.616494145596471e-05,
      "loss": 0.6395,
      "step": 2760
    },
    {
      "epoch": 0.46220590689137325,
      "grad_norm": 4.873228549957275,
      "learning_rate": 4.614797217037163e-05,
      "loss": 0.6646,
      "step": 2770
    },
    {
      "epoch": 0.4638745202736526,
      "grad_norm": 9.068425178527832,
      "learning_rate": 4.613100288477855e-05,
      "loss": 0.6746,
      "step": 2780
    },
    {
      "epoch": 0.4655431336559319,
      "grad_norm": 8.824785232543945,
      "learning_rate": 4.611403359918547e-05,
      "loss": 1.117,
      "step": 2790
    },
    {
      "epoch": 0.4672117470382112,
      "grad_norm": 10.9446439743042,
      "learning_rate": 4.60970643135924e-05,
      "loss": 0.9726,
      "step": 2800
    },
    {
      "epoch": 0.4688803604204906,
      "grad_norm": 6.258224010467529,
      "learning_rate": 4.6080095027999324e-05,
      "loss": 0.6232,
      "step": 2810
    },
    {
      "epoch": 0.4705489738027699,
      "grad_norm": 7.989435195922852,
      "learning_rate": 4.606312574240625e-05,
      "loss": 0.92,
      "step": 2820
    },
    {
      "epoch": 0.47221758718504925,
      "grad_norm": 10.53073501586914,
      "learning_rate": 4.604615645681317e-05,
      "loss": 1.0279,
      "step": 2830
    },
    {
      "epoch": 0.47388620056732855,
      "grad_norm": 12.071959495544434,
      "learning_rate": 4.6029187171220095e-05,
      "loss": 0.6907,
      "step": 2840
    },
    {
      "epoch": 0.47555481394960786,
      "grad_norm": 7.947457313537598,
      "learning_rate": 4.601221788562702e-05,
      "loss": 0.6597,
      "step": 2850
    },
    {
      "epoch": 0.4772234273318872,
      "grad_norm": 3.26193904876709,
      "learning_rate": 4.599524860003394e-05,
      "loss": 0.5111,
      "step": 2860
    },
    {
      "epoch": 0.4788920407141665,
      "grad_norm": 6.503660202026367,
      "learning_rate": 4.5978279314440865e-05,
      "loss": 0.7881,
      "step": 2870
    },
    {
      "epoch": 0.4805606540964458,
      "grad_norm": 5.391315937042236,
      "learning_rate": 4.5961310028847785e-05,
      "loss": 0.5728,
      "step": 2880
    },
    {
      "epoch": 0.4822292674787252,
      "grad_norm": 3.878417491912842,
      "learning_rate": 4.594434074325471e-05,
      "loss": 0.666,
      "step": 2890
    },
    {
      "epoch": 0.4838978808610045,
      "grad_norm": 3.0510120391845703,
      "learning_rate": 4.5927371457661636e-05,
      "loss": 0.6521,
      "step": 2900
    },
    {
      "epoch": 0.48556649424328385,
      "grad_norm": 6.52381706237793,
      "learning_rate": 4.5910402172068555e-05,
      "loss": 0.5154,
      "step": 2910
    },
    {
      "epoch": 0.48723510762556316,
      "grad_norm": 11.997550964355469,
      "learning_rate": 4.589343288647548e-05,
      "loss": 0.8797,
      "step": 2920
    },
    {
      "epoch": 0.48890372100784246,
      "grad_norm": 2.5449585914611816,
      "learning_rate": 4.587646360088241e-05,
      "loss": 0.4899,
      "step": 2930
    },
    {
      "epoch": 0.4905723343901218,
      "grad_norm": 2.677755355834961,
      "learning_rate": 4.585949431528933e-05,
      "loss": 0.5137,
      "step": 2940
    },
    {
      "epoch": 0.4922409477724011,
      "grad_norm": 5.397197723388672,
      "learning_rate": 4.584252502969625e-05,
      "loss": 0.9506,
      "step": 2950
    },
    {
      "epoch": 0.4939095611546805,
      "grad_norm": 3.6196203231811523,
      "learning_rate": 4.582555574410318e-05,
      "loss": 0.7175,
      "step": 2960
    },
    {
      "epoch": 0.4955781745369598,
      "grad_norm": 4.9687700271606445,
      "learning_rate": 4.58085864585101e-05,
      "loss": 0.7461,
      "step": 2970
    },
    {
      "epoch": 0.4972467879192391,
      "grad_norm": 10.7244873046875,
      "learning_rate": 4.579161717291702e-05,
      "loss": 0.5131,
      "step": 2980
    },
    {
      "epoch": 0.49891540130151846,
      "grad_norm": 4.382063388824463,
      "learning_rate": 4.577464788732395e-05,
      "loss": 0.6938,
      "step": 2990
    },
    {
      "epoch": 0.5005840146837978,
      "grad_norm": 15.06298542022705,
      "learning_rate": 4.575767860173087e-05,
      "loss": 0.7685,
      "step": 3000
    },
    {
      "epoch": 0.5022526280660771,
      "grad_norm": 11.215258598327637,
      "learning_rate": 4.5740709316137794e-05,
      "loss": 0.6438,
      "step": 3010
    },
    {
      "epoch": 0.5039212414483564,
      "grad_norm": 4.09038782119751,
      "learning_rate": 4.572374003054471e-05,
      "loss": 0.7529,
      "step": 3020
    },
    {
      "epoch": 0.5055898548306358,
      "grad_norm": 5.056797027587891,
      "learning_rate": 4.570677074495164e-05,
      "loss": 0.8101,
      "step": 3030
    },
    {
      "epoch": 0.5072584682129151,
      "grad_norm": 1.333560585975647,
      "learning_rate": 4.568980145935856e-05,
      "loss": 0.7968,
      "step": 3040
    },
    {
      "epoch": 0.5089270815951944,
      "grad_norm": 8.449666023254395,
      "learning_rate": 4.5672832173765484e-05,
      "loss": 0.8319,
      "step": 3050
    },
    {
      "epoch": 0.5105956949774737,
      "grad_norm": 2.4076712131500244,
      "learning_rate": 4.565586288817241e-05,
      "loss": 0.5137,
      "step": 3060
    },
    {
      "epoch": 0.512264308359753,
      "grad_norm": 6.741187572479248,
      "learning_rate": 4.5638893602579335e-05,
      "loss": 0.5956,
      "step": 3070
    },
    {
      "epoch": 0.5139329217420324,
      "grad_norm": 12.210086822509766,
      "learning_rate": 4.562192431698626e-05,
      "loss": 0.8891,
      "step": 3080
    },
    {
      "epoch": 0.5156015351243117,
      "grad_norm": 2.90934681892395,
      "learning_rate": 4.560495503139318e-05,
      "loss": 0.4891,
      "step": 3090
    },
    {
      "epoch": 0.517270148506591,
      "grad_norm": 6.5355048179626465,
      "learning_rate": 4.5587985745800106e-05,
      "loss": 0.7645,
      "step": 3100
    },
    {
      "epoch": 0.5189387618888703,
      "grad_norm": 5.370847225189209,
      "learning_rate": 4.5571016460207025e-05,
      "loss": 0.7511,
      "step": 3110
    },
    {
      "epoch": 0.5206073752711496,
      "grad_norm": 7.274075984954834,
      "learning_rate": 4.555404717461395e-05,
      "loss": 0.6446,
      "step": 3120
    },
    {
      "epoch": 0.522275988653429,
      "grad_norm": 1.7459408044815063,
      "learning_rate": 4.553707788902088e-05,
      "loss": 0.8842,
      "step": 3130
    },
    {
      "epoch": 0.5239446020357084,
      "grad_norm": 7.273232936859131,
      "learning_rate": 4.5520108603427796e-05,
      "loss": 0.5839,
      "step": 3140
    },
    {
      "epoch": 0.5256132154179877,
      "grad_norm": 5.582218170166016,
      "learning_rate": 4.550313931783472e-05,
      "loss": 0.5211,
      "step": 3150
    },
    {
      "epoch": 0.527281828800267,
      "grad_norm": 6.476041316986084,
      "learning_rate": 4.548617003224164e-05,
      "loss": 0.5596,
      "step": 3160
    },
    {
      "epoch": 0.5289504421825463,
      "grad_norm": 10.81589412689209,
      "learning_rate": 4.546920074664857e-05,
      "loss": 0.7506,
      "step": 3170
    },
    {
      "epoch": 0.5306190555648256,
      "grad_norm": 8.805999755859375,
      "learning_rate": 4.545223146105549e-05,
      "loss": 0.8487,
      "step": 3180
    },
    {
      "epoch": 0.532287668947105,
      "grad_norm": 10.624868392944336,
      "learning_rate": 4.543526217546242e-05,
      "loss": 0.8112,
      "step": 3190
    },
    {
      "epoch": 0.5339562823293843,
      "grad_norm": 4.965054988861084,
      "learning_rate": 4.541829288986934e-05,
      "loss": 0.6627,
      "step": 3200
    },
    {
      "epoch": 0.5356248957116636,
      "grad_norm": 13.000005722045898,
      "learning_rate": 4.5401323604276264e-05,
      "loss": 0.8799,
      "step": 3210
    },
    {
      "epoch": 0.5372935090939429,
      "grad_norm": 12.053877830505371,
      "learning_rate": 4.538435431868319e-05,
      "loss": 0.6146,
      "step": 3220
    },
    {
      "epoch": 0.5389621224762222,
      "grad_norm": 6.057259559631348,
      "learning_rate": 4.536738503309011e-05,
      "loss": 0.698,
      "step": 3230
    },
    {
      "epoch": 0.5406307358585016,
      "grad_norm": 6.611588001251221,
      "learning_rate": 4.5350415747497035e-05,
      "loss": 0.7164,
      "step": 3240
    },
    {
      "epoch": 0.5422993492407809,
      "grad_norm": 6.577651023864746,
      "learning_rate": 4.5333446461903954e-05,
      "loss": 0.6433,
      "step": 3250
    },
    {
      "epoch": 0.5439679626230602,
      "grad_norm": 12.571124076843262,
      "learning_rate": 4.531647717631088e-05,
      "loss": 0.6316,
      "step": 3260
    },
    {
      "epoch": 0.5456365760053395,
      "grad_norm": 5.339183330535889,
      "learning_rate": 4.52995078907178e-05,
      "loss": 0.6235,
      "step": 3270
    },
    {
      "epoch": 0.5473051893876189,
      "grad_norm": 4.490739345550537,
      "learning_rate": 4.5282538605124725e-05,
      "loss": 0.6044,
      "step": 3280
    },
    {
      "epoch": 0.5489738027698983,
      "grad_norm": 6.58143949508667,
      "learning_rate": 4.526556931953165e-05,
      "loss": 0.9361,
      "step": 3290
    },
    {
      "epoch": 0.5506424161521776,
      "grad_norm": 2.867375373840332,
      "learning_rate": 4.524860003393857e-05,
      "loss": 0.5557,
      "step": 3300
    },
    {
      "epoch": 0.5523110295344569,
      "grad_norm": 4.522762775421143,
      "learning_rate": 4.52316307483455e-05,
      "loss": 0.7428,
      "step": 3310
    },
    {
      "epoch": 0.5539796429167362,
      "grad_norm": 7.414405345916748,
      "learning_rate": 4.521466146275242e-05,
      "loss": 0.5488,
      "step": 3320
    },
    {
      "epoch": 0.5556482562990155,
      "grad_norm": 0.2303873896598816,
      "learning_rate": 4.519769217715935e-05,
      "loss": 0.5146,
      "step": 3330
    },
    {
      "epoch": 0.5573168696812948,
      "grad_norm": 9.322689056396484,
      "learning_rate": 4.5180722891566266e-05,
      "loss": 0.5328,
      "step": 3340
    },
    {
      "epoch": 0.5589854830635742,
      "grad_norm": 6.545580863952637,
      "learning_rate": 4.516375360597319e-05,
      "loss": 0.5253,
      "step": 3350
    },
    {
      "epoch": 0.5606540964458535,
      "grad_norm": 5.050734996795654,
      "learning_rate": 4.514678432038011e-05,
      "loss": 0.8012,
      "step": 3360
    },
    {
      "epoch": 0.5623227098281328,
      "grad_norm": 2.5080630779266357,
      "learning_rate": 4.512981503478704e-05,
      "loss": 0.4401,
      "step": 3370
    },
    {
      "epoch": 0.5639913232104121,
      "grad_norm": 5.957943916320801,
      "learning_rate": 4.511284574919396e-05,
      "loss": 0.8909,
      "step": 3380
    },
    {
      "epoch": 0.5656599365926914,
      "grad_norm": 3.8780250549316406,
      "learning_rate": 4.509587646360088e-05,
      "loss": 0.85,
      "step": 3390
    },
    {
      "epoch": 0.5673285499749708,
      "grad_norm": 11.064157485961914,
      "learning_rate": 4.507890717800781e-05,
      "loss": 0.7281,
      "step": 3400
    },
    {
      "epoch": 0.5689971633572501,
      "grad_norm": 7.285651683807373,
      "learning_rate": 4.506193789241473e-05,
      "loss": 0.7347,
      "step": 3410
    },
    {
      "epoch": 0.5706657767395295,
      "grad_norm": 11.072522163391113,
      "learning_rate": 4.504496860682165e-05,
      "loss": 0.8291,
      "step": 3420
    },
    {
      "epoch": 0.5723343901218088,
      "grad_norm": 7.517305374145508,
      "learning_rate": 4.502799932122858e-05,
      "loss": 0.5967,
      "step": 3430
    },
    {
      "epoch": 0.5740030035040881,
      "grad_norm": 6.676129341125488,
      "learning_rate": 4.5011030035635505e-05,
      "loss": 0.6264,
      "step": 3440
    },
    {
      "epoch": 0.5756716168863675,
      "grad_norm": 2.1607940196990967,
      "learning_rate": 4.4994060750042424e-05,
      "loss": 0.8075,
      "step": 3450
    },
    {
      "epoch": 0.5773402302686468,
      "grad_norm": 8.994595527648926,
      "learning_rate": 4.497709146444935e-05,
      "loss": 0.8443,
      "step": 3460
    },
    {
      "epoch": 0.5790088436509261,
      "grad_norm": 4.227181434631348,
      "learning_rate": 4.4960122178856276e-05,
      "loss": 0.4253,
      "step": 3470
    },
    {
      "epoch": 0.5806774570332054,
      "grad_norm": 6.6641998291015625,
      "learning_rate": 4.4943152893263195e-05,
      "loss": 0.9796,
      "step": 3480
    },
    {
      "epoch": 0.5823460704154847,
      "grad_norm": 10.859420776367188,
      "learning_rate": 4.492618360767012e-05,
      "loss": 1.1375,
      "step": 3490
    },
    {
      "epoch": 0.5840146837977641,
      "grad_norm": 2.8551836013793945,
      "learning_rate": 4.490921432207704e-05,
      "loss": 0.7302,
      "step": 3500
    },
    {
      "epoch": 0.5856832971800434,
      "grad_norm": 6.613987445831299,
      "learning_rate": 4.4892245036483966e-05,
      "loss": 0.6564,
      "step": 3510
    },
    {
      "epoch": 0.5873519105623227,
      "grad_norm": 11.335299491882324,
      "learning_rate": 4.487527575089089e-05,
      "loss": 0.6665,
      "step": 3520
    },
    {
      "epoch": 0.589020523944602,
      "grad_norm": 8.634321212768555,
      "learning_rate": 4.485830646529781e-05,
      "loss": 0.4841,
      "step": 3530
    },
    {
      "epoch": 0.5906891373268813,
      "grad_norm": 5.651169300079346,
      "learning_rate": 4.4841337179704736e-05,
      "loss": 0.885,
      "step": 3540
    },
    {
      "epoch": 0.5923577507091606,
      "grad_norm": 8.284029006958008,
      "learning_rate": 4.4824367894111655e-05,
      "loss": 0.733,
      "step": 3550
    },
    {
      "epoch": 0.59402636409144,
      "grad_norm": 4.870278835296631,
      "learning_rate": 4.480739860851859e-05,
      "loss": 0.5902,
      "step": 3560
    },
    {
      "epoch": 0.5956949774737194,
      "grad_norm": 1.6221606731414795,
      "learning_rate": 4.479042932292551e-05,
      "loss": 0.7004,
      "step": 3570
    },
    {
      "epoch": 0.5973635908559987,
      "grad_norm": 5.988095283508301,
      "learning_rate": 4.477346003733243e-05,
      "loss": 0.5431,
      "step": 3580
    },
    {
      "epoch": 0.599032204238278,
      "grad_norm": 12.33525562286377,
      "learning_rate": 4.475649075173935e-05,
      "loss": 0.7791,
      "step": 3590
    },
    {
      "epoch": 0.6007008176205573,
      "grad_norm": 3.564730644226074,
      "learning_rate": 4.473952146614628e-05,
      "loss": 0.3783,
      "step": 3600
    },
    {
      "epoch": 0.6023694310028367,
      "grad_norm": 1.3369828462600708,
      "learning_rate": 4.4722552180553204e-05,
      "loss": 0.504,
      "step": 3610
    },
    {
      "epoch": 0.604038044385116,
      "grad_norm": 9.598024368286133,
      "learning_rate": 4.470558289496012e-05,
      "loss": 0.7133,
      "step": 3620
    },
    {
      "epoch": 0.6057066577673953,
      "grad_norm": 2.0081145763397217,
      "learning_rate": 4.468861360936705e-05,
      "loss": 0.6167,
      "step": 3630
    },
    {
      "epoch": 0.6073752711496746,
      "grad_norm": 11.848610877990723,
      "learning_rate": 4.467164432377397e-05,
      "loss": 0.9504,
      "step": 3640
    },
    {
      "epoch": 0.6090438845319539,
      "grad_norm": 7.685434818267822,
      "learning_rate": 4.4654675038180894e-05,
      "loss": 1.0392,
      "step": 3650
    },
    {
      "epoch": 0.6107124979142333,
      "grad_norm": 2.3584744930267334,
      "learning_rate": 4.463770575258781e-05,
      "loss": 0.7798,
      "step": 3660
    },
    {
      "epoch": 0.6123811112965126,
      "grad_norm": 2.1608529090881348,
      "learning_rate": 4.462073646699474e-05,
      "loss": 0.772,
      "step": 3670
    },
    {
      "epoch": 0.6140497246787919,
      "grad_norm": 6.198031425476074,
      "learning_rate": 4.4603767181401665e-05,
      "loss": 0.8374,
      "step": 3680
    },
    {
      "epoch": 0.6157183380610712,
      "grad_norm": 1.9841444492340088,
      "learning_rate": 4.458679789580859e-05,
      "loss": 0.5618,
      "step": 3690
    },
    {
      "epoch": 0.6173869514433505,
      "grad_norm": 4.772614479064941,
      "learning_rate": 4.4569828610215516e-05,
      "loss": 0.6631,
      "step": 3700
    },
    {
      "epoch": 0.61905556482563,
      "grad_norm": 7.690817832946777,
      "learning_rate": 4.4552859324622436e-05,
      "loss": 0.9196,
      "step": 3710
    },
    {
      "epoch": 0.6207241782079093,
      "grad_norm": 4.0594329833984375,
      "learning_rate": 4.453589003902936e-05,
      "loss": 0.8049,
      "step": 3720
    },
    {
      "epoch": 0.6223927915901886,
      "grad_norm": 11.17619514465332,
      "learning_rate": 4.451892075343628e-05,
      "loss": 0.7519,
      "step": 3730
    },
    {
      "epoch": 0.6240614049724679,
      "grad_norm": 3.2281079292297363,
      "learning_rate": 4.4501951467843206e-05,
      "loss": 0.4604,
      "step": 3740
    },
    {
      "epoch": 0.6257300183547472,
      "grad_norm": 11.126716613769531,
      "learning_rate": 4.448498218225013e-05,
      "loss": 0.7375,
      "step": 3750
    },
    {
      "epoch": 0.6273986317370265,
      "grad_norm": 10.761882781982422,
      "learning_rate": 4.446801289665705e-05,
      "loss": 0.5572,
      "step": 3760
    },
    {
      "epoch": 0.6290672451193059,
      "grad_norm": 6.086820125579834,
      "learning_rate": 4.445104361106398e-05,
      "loss": 0.6703,
      "step": 3770
    },
    {
      "epoch": 0.6307358585015852,
      "grad_norm": 3.4272842407226562,
      "learning_rate": 4.4434074325470896e-05,
      "loss": 0.7716,
      "step": 3780
    },
    {
      "epoch": 0.6324044718838645,
      "grad_norm": 6.36560583114624,
      "learning_rate": 4.441710503987782e-05,
      "loss": 0.6836,
      "step": 3790
    },
    {
      "epoch": 0.6340730852661438,
      "grad_norm": 5.820964813232422,
      "learning_rate": 4.440013575428474e-05,
      "loss": 0.9726,
      "step": 3800
    },
    {
      "epoch": 0.6357416986484231,
      "grad_norm": 4.31635856628418,
      "learning_rate": 4.4383166468691674e-05,
      "loss": 0.6364,
      "step": 3810
    },
    {
      "epoch": 0.6374103120307025,
      "grad_norm": 5.013918399810791,
      "learning_rate": 4.436619718309859e-05,
      "loss": 0.6099,
      "step": 3820
    },
    {
      "epoch": 0.6390789254129818,
      "grad_norm": 11.187727928161621,
      "learning_rate": 4.434922789750552e-05,
      "loss": 0.6747,
      "step": 3830
    },
    {
      "epoch": 0.6407475387952611,
      "grad_norm": 4.538517475128174,
      "learning_rate": 4.4332258611912445e-05,
      "loss": 0.8539,
      "step": 3840
    },
    {
      "epoch": 0.6424161521775404,
      "grad_norm": 11.29455280303955,
      "learning_rate": 4.4315289326319364e-05,
      "loss": 0.6492,
      "step": 3850
    },
    {
      "epoch": 0.6440847655598197,
      "grad_norm": 1.6599870920181274,
      "learning_rate": 4.429832004072629e-05,
      "loss": 0.4763,
      "step": 3860
    },
    {
      "epoch": 0.6457533789420992,
      "grad_norm": 6.313650131225586,
      "learning_rate": 4.428135075513321e-05,
      "loss": 0.5956,
      "step": 3870
    },
    {
      "epoch": 0.6474219923243785,
      "grad_norm": 5.534331798553467,
      "learning_rate": 4.4264381469540135e-05,
      "loss": 0.764,
      "step": 3880
    },
    {
      "epoch": 0.6490906057066578,
      "grad_norm": 9.088793754577637,
      "learning_rate": 4.4247412183947054e-05,
      "loss": 0.5761,
      "step": 3890
    },
    {
      "epoch": 0.6507592190889371,
      "grad_norm": 8.311659812927246,
      "learning_rate": 4.423044289835398e-05,
      "loss": 0.7424,
      "step": 3900
    },
    {
      "epoch": 0.6524278324712164,
      "grad_norm": 7.157122611999512,
      "learning_rate": 4.4213473612760906e-05,
      "loss": 0.6607,
      "step": 3910
    },
    {
      "epoch": 0.6540964458534958,
      "grad_norm": 0.7818027138710022,
      "learning_rate": 4.4196504327167825e-05,
      "loss": 0.6013,
      "step": 3920
    },
    {
      "epoch": 0.6557650592357751,
      "grad_norm": 7.288153171539307,
      "learning_rate": 4.417953504157476e-05,
      "loss": 0.6452,
      "step": 3930
    },
    {
      "epoch": 0.6574336726180544,
      "grad_norm": 6.079833030700684,
      "learning_rate": 4.4162565755981676e-05,
      "loss": 0.848,
      "step": 3940
    },
    {
      "epoch": 0.6591022860003337,
      "grad_norm": 4.627569675445557,
      "learning_rate": 4.41455964703886e-05,
      "loss": 0.8348,
      "step": 3950
    },
    {
      "epoch": 0.660770899382613,
      "grad_norm": 3.3344967365264893,
      "learning_rate": 4.412862718479552e-05,
      "loss": 0.5287,
      "step": 3960
    },
    {
      "epoch": 0.6624395127648923,
      "grad_norm": 3.169508218765259,
      "learning_rate": 4.411165789920245e-05,
      "loss": 0.6444,
      "step": 3970
    },
    {
      "epoch": 0.6641081261471717,
      "grad_norm": 11.020730972290039,
      "learning_rate": 4.4094688613609366e-05,
      "loss": 1.0034,
      "step": 3980
    },
    {
      "epoch": 0.665776739529451,
      "grad_norm": 3.730940341949463,
      "learning_rate": 4.407771932801629e-05,
      "loss": 0.6404,
      "step": 3990
    },
    {
      "epoch": 0.6674453529117303,
      "grad_norm": 6.057884216308594,
      "learning_rate": 4.406075004242322e-05,
      "loss": 0.9318,
      "step": 4000
    },
    {
      "epoch": 0.6691139662940097,
      "grad_norm": 5.110137462615967,
      "learning_rate": 4.404378075683014e-05,
      "loss": 0.5163,
      "step": 4010
    },
    {
      "epoch": 0.670782579676289,
      "grad_norm": 6.410297393798828,
      "learning_rate": 4.402681147123706e-05,
      "loss": 0.8918,
      "step": 4020
    },
    {
      "epoch": 0.6724511930585684,
      "grad_norm": 1.6978691816329956,
      "learning_rate": 4.400984218564398e-05,
      "loss": 0.6854,
      "step": 4030
    },
    {
      "epoch": 0.6741198064408477,
      "grad_norm": 5.275461673736572,
      "learning_rate": 4.399287290005091e-05,
      "loss": 0.6092,
      "step": 4040
    },
    {
      "epoch": 0.675788419823127,
      "grad_norm": 7.0557684898376465,
      "learning_rate": 4.3975903614457834e-05,
      "loss": 1.0875,
      "step": 4050
    },
    {
      "epoch": 0.6774570332054063,
      "grad_norm": 6.935187339782715,
      "learning_rate": 4.395893432886476e-05,
      "loss": 0.5459,
      "step": 4060
    },
    {
      "epoch": 0.6791256465876856,
      "grad_norm": 6.831294536590576,
      "learning_rate": 4.394196504327168e-05,
      "loss": 0.7378,
      "step": 4070
    },
    {
      "epoch": 0.680794259969965,
      "grad_norm": 9.966114044189453,
      "learning_rate": 4.3924995757678605e-05,
      "loss": 0.8799,
      "step": 4080
    },
    {
      "epoch": 0.6824628733522443,
      "grad_norm": 9.209259986877441,
      "learning_rate": 4.390802647208553e-05,
      "loss": 0.888,
      "step": 4090
    },
    {
      "epoch": 0.6841314867345236,
      "grad_norm": 5.225367069244385,
      "learning_rate": 4.389105718649245e-05,
      "loss": 0.396,
      "step": 4100
    },
    {
      "epoch": 0.6858001001168029,
      "grad_norm": 1.920923113822937,
      "learning_rate": 4.3874087900899376e-05,
      "loss": 0.6196,
      "step": 4110
    },
    {
      "epoch": 0.6874687134990822,
      "grad_norm": 7.026232719421387,
      "learning_rate": 4.3857118615306295e-05,
      "loss": 0.6714,
      "step": 4120
    },
    {
      "epoch": 0.6891373268813615,
      "grad_norm": 13.045815467834473,
      "learning_rate": 4.384014932971322e-05,
      "loss": 0.8097,
      "step": 4130
    },
    {
      "epoch": 0.690805940263641,
      "grad_norm": 2.4661405086517334,
      "learning_rate": 4.3823180044120147e-05,
      "loss": 0.7737,
      "step": 4140
    },
    {
      "epoch": 0.6924745536459203,
      "grad_norm": 5.793625354766846,
      "learning_rate": 4.3806210758527066e-05,
      "loss": 0.4437,
      "step": 4150
    },
    {
      "epoch": 0.6941431670281996,
      "grad_norm": 8.876808166503906,
      "learning_rate": 4.378924147293399e-05,
      "loss": 0.4919,
      "step": 4160
    },
    {
      "epoch": 0.6958117804104789,
      "grad_norm": 7.983793258666992,
      "learning_rate": 4.377227218734091e-05,
      "loss": 0.8774,
      "step": 4170
    },
    {
      "epoch": 0.6974803937927582,
      "grad_norm": 6.038628101348877,
      "learning_rate": 4.375530290174784e-05,
      "loss": 0.6683,
      "step": 4180
    },
    {
      "epoch": 0.6991490071750376,
      "grad_norm": 5.2195305824279785,
      "learning_rate": 4.373833361615476e-05,
      "loss": 0.629,
      "step": 4190
    },
    {
      "epoch": 0.7008176205573169,
      "grad_norm": 4.324680805206299,
      "learning_rate": 4.372136433056169e-05,
      "loss": 0.5957,
      "step": 4200
    },
    {
      "epoch": 0.7024862339395962,
      "grad_norm": 6.242452144622803,
      "learning_rate": 4.370439504496861e-05,
      "loss": 0.8208,
      "step": 4210
    },
    {
      "epoch": 0.7041548473218755,
      "grad_norm": 4.399996757507324,
      "learning_rate": 4.368742575937553e-05,
      "loss": 0.5191,
      "step": 4220
    },
    {
      "epoch": 0.7058234607041548,
      "grad_norm": 9.633098602294922,
      "learning_rate": 4.367045647378246e-05,
      "loss": 0.6857,
      "step": 4230
    },
    {
      "epoch": 0.7074920740864342,
      "grad_norm": 7.2886552810668945,
      "learning_rate": 4.365348718818938e-05,
      "loss": 0.5933,
      "step": 4240
    },
    {
      "epoch": 0.7091606874687135,
      "grad_norm": 11.15596866607666,
      "learning_rate": 4.3636517902596304e-05,
      "loss": 0.5833,
      "step": 4250
    },
    {
      "epoch": 0.7108293008509928,
      "grad_norm": 6.6126179695129395,
      "learning_rate": 4.361954861700322e-05,
      "loss": 0.7148,
      "step": 4260
    },
    {
      "epoch": 0.7124979142332721,
      "grad_norm": 7.988368988037109,
      "learning_rate": 4.360257933141015e-05,
      "loss": 0.6919,
      "step": 4270
    },
    {
      "epoch": 0.7141665276155514,
      "grad_norm": 4.636739730834961,
      "learning_rate": 4.358561004581707e-05,
      "loss": 0.5343,
      "step": 4280
    },
    {
      "epoch": 0.7158351409978309,
      "grad_norm": 2.458247661590576,
      "learning_rate": 4.3568640760223994e-05,
      "loss": 0.7939,
      "step": 4290
    },
    {
      "epoch": 0.7175037543801102,
      "grad_norm": 9.745562553405762,
      "learning_rate": 4.355167147463092e-05,
      "loss": 0.9542,
      "step": 4300
    },
    {
      "epoch": 0.7191723677623895,
      "grad_norm": 8.619933128356934,
      "learning_rate": 4.3534702189037846e-05,
      "loss": 0.5411,
      "step": 4310
    },
    {
      "epoch": 0.7208409811446688,
      "grad_norm": 4.163063049316406,
      "learning_rate": 4.351773290344477e-05,
      "loss": 0.4796,
      "step": 4320
    },
    {
      "epoch": 0.7225095945269481,
      "grad_norm": 4.938095569610596,
      "learning_rate": 4.350076361785169e-05,
      "loss": 0.5654,
      "step": 4330
    },
    {
      "epoch": 0.7241782079092274,
      "grad_norm": 1.0958913564682007,
      "learning_rate": 4.3483794332258617e-05,
      "loss": 0.7761,
      "step": 4340
    },
    {
      "epoch": 0.7258468212915068,
      "grad_norm": 4.2065253257751465,
      "learning_rate": 4.3466825046665536e-05,
      "loss": 0.6785,
      "step": 4350
    },
    {
      "epoch": 0.7275154346737861,
      "grad_norm": 8.917095184326172,
      "learning_rate": 4.344985576107246e-05,
      "loss": 0.5883,
      "step": 4360
    },
    {
      "epoch": 0.7291840480560654,
      "grad_norm": 2.5580339431762695,
      "learning_rate": 4.343288647547938e-05,
      "loss": 0.6331,
      "step": 4370
    },
    {
      "epoch": 0.7308526614383447,
      "grad_norm": 6.877384662628174,
      "learning_rate": 4.3415917189886307e-05,
      "loss": 0.5563,
      "step": 4380
    },
    {
      "epoch": 0.732521274820624,
      "grad_norm": 6.859411716461182,
      "learning_rate": 4.339894790429323e-05,
      "loss": 0.7242,
      "step": 4390
    },
    {
      "epoch": 0.7341898882029034,
      "grad_norm": 2.2898499965667725,
      "learning_rate": 4.338197861870015e-05,
      "loss": 0.5761,
      "step": 4400
    },
    {
      "epoch": 0.7358585015851827,
      "grad_norm": 2.8775618076324463,
      "learning_rate": 4.336500933310708e-05,
      "loss": 0.4545,
      "step": 4410
    },
    {
      "epoch": 0.737527114967462,
      "grad_norm": 5.726140975952148,
      "learning_rate": 4.3348040047513996e-05,
      "loss": 0.3783,
      "step": 4420
    },
    {
      "epoch": 0.7391957283497413,
      "grad_norm": 9.304436683654785,
      "learning_rate": 4.333107076192093e-05,
      "loss": 0.8976,
      "step": 4430
    },
    {
      "epoch": 0.7408643417320206,
      "grad_norm": 9.143195152282715,
      "learning_rate": 4.331410147632785e-05,
      "loss": 0.6382,
      "step": 4440
    },
    {
      "epoch": 0.7425329551143001,
      "grad_norm": 6.717785358428955,
      "learning_rate": 4.3297132190734774e-05,
      "loss": 0.8884,
      "step": 4450
    },
    {
      "epoch": 0.7442015684965794,
      "grad_norm": 7.7591633796691895,
      "learning_rate": 4.32801629051417e-05,
      "loss": 0.4417,
      "step": 4460
    },
    {
      "epoch": 0.7458701818788587,
      "grad_norm": 14.082252502441406,
      "learning_rate": 4.326319361954862e-05,
      "loss": 0.5161,
      "step": 4470
    },
    {
      "epoch": 0.747538795261138,
      "grad_norm": 5.869783878326416,
      "learning_rate": 4.3246224333955545e-05,
      "loss": 0.6385,
      "step": 4480
    },
    {
      "epoch": 0.7492074086434173,
      "grad_norm": 6.809282302856445,
      "learning_rate": 4.3229255048362464e-05,
      "loss": 0.8744,
      "step": 4490
    },
    {
      "epoch": 0.7508760220256967,
      "grad_norm": 7.007767677307129,
      "learning_rate": 4.321228576276939e-05,
      "loss": 0.7173,
      "step": 4500
    },
    {
      "epoch": 0.752544635407976,
      "grad_norm": 5.034497261047363,
      "learning_rate": 4.319531647717631e-05,
      "loss": 0.7064,
      "step": 4510
    },
    {
      "epoch": 0.7542132487902553,
      "grad_norm": 6.769698619842529,
      "learning_rate": 4.3178347191583235e-05,
      "loss": 0.5916,
      "step": 4520
    },
    {
      "epoch": 0.7558818621725346,
      "grad_norm": 7.849212169647217,
      "learning_rate": 4.316137790599016e-05,
      "loss": 0.9192,
      "step": 4530
    },
    {
      "epoch": 0.7575504755548139,
      "grad_norm": 5.8093976974487305,
      "learning_rate": 4.314440862039708e-05,
      "loss": 0.8365,
      "step": 4540
    },
    {
      "epoch": 0.7592190889370932,
      "grad_norm": 8.804215431213379,
      "learning_rate": 4.312743933480401e-05,
      "loss": 0.5565,
      "step": 4550
    },
    {
      "epoch": 0.7608877023193726,
      "grad_norm": 3.0501999855041504,
      "learning_rate": 4.311047004921093e-05,
      "loss": 0.4349,
      "step": 4560
    },
    {
      "epoch": 0.7625563157016519,
      "grad_norm": 6.776899814605713,
      "learning_rate": 4.309350076361786e-05,
      "loss": 0.8511,
      "step": 4570
    },
    {
      "epoch": 0.7642249290839312,
      "grad_norm": 2.5118634700775146,
      "learning_rate": 4.3076531478024777e-05,
      "loss": 0.7892,
      "step": 4580
    },
    {
      "epoch": 0.7658935424662106,
      "grad_norm": 11.172088623046875,
      "learning_rate": 4.30595621924317e-05,
      "loss": 0.7853,
      "step": 4590
    },
    {
      "epoch": 0.7675621558484899,
      "grad_norm": 2.560894012451172,
      "learning_rate": 4.304259290683862e-05,
      "loss": 0.5424,
      "step": 4600
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 4.666998863220215,
      "learning_rate": 4.302562362124555e-05,
      "loss": 0.5385,
      "step": 4610
    },
    {
      "epoch": 0.7708993826130486,
      "grad_norm": 5.732120513916016,
      "learning_rate": 4.300865433565247e-05,
      "loss": 0.5812,
      "step": 4620
    },
    {
      "epoch": 0.7725679959953279,
      "grad_norm": 5.363624572753906,
      "learning_rate": 4.299168505005939e-05,
      "loss": 0.8825,
      "step": 4630
    },
    {
      "epoch": 0.7742366093776072,
      "grad_norm": 1.8749109506607056,
      "learning_rate": 4.297471576446632e-05,
      "loss": 0.7005,
      "step": 4640
    },
    {
      "epoch": 0.7759052227598865,
      "grad_norm": 7.661915302276611,
      "learning_rate": 4.295774647887324e-05,
      "loss": 0.9213,
      "step": 4650
    },
    {
      "epoch": 0.7775738361421659,
      "grad_norm": 1.9717713594436646,
      "learning_rate": 4.294077719328016e-05,
      "loss": 0.7454,
      "step": 4660
    },
    {
      "epoch": 0.7792424495244452,
      "grad_norm": 5.182392597198486,
      "learning_rate": 4.292380790768709e-05,
      "loss": 0.6689,
      "step": 4670
    },
    {
      "epoch": 0.7809110629067245,
      "grad_norm": 5.68388032913208,
      "learning_rate": 4.2906838622094015e-05,
      "loss": 0.6146,
      "step": 4680
    },
    {
      "epoch": 0.7825796762890038,
      "grad_norm": 6.913420677185059,
      "learning_rate": 4.2889869336500934e-05,
      "loss": 0.4854,
      "step": 4690
    },
    {
      "epoch": 0.7842482896712831,
      "grad_norm": 10.628379821777344,
      "learning_rate": 4.287290005090786e-05,
      "loss": 0.4944,
      "step": 4700
    },
    {
      "epoch": 0.7859169030535625,
      "grad_norm": 16.107650756835938,
      "learning_rate": 4.2855930765314786e-05,
      "loss": 0.7472,
      "step": 4710
    },
    {
      "epoch": 0.7875855164358418,
      "grad_norm": 4.507907390594482,
      "learning_rate": 4.2838961479721705e-05,
      "loss": 0.6286,
      "step": 4720
    },
    {
      "epoch": 0.7892541298181212,
      "grad_norm": 6.041508674621582,
      "learning_rate": 4.282199219412863e-05,
      "loss": 0.6341,
      "step": 4730
    },
    {
      "epoch": 0.7909227432004005,
      "grad_norm": 6.264540672302246,
      "learning_rate": 4.280502290853555e-05,
      "loss": 0.5388,
      "step": 4740
    },
    {
      "epoch": 0.7925913565826798,
      "grad_norm": 3.2984392642974854,
      "learning_rate": 4.2788053622942476e-05,
      "loss": 0.3909,
      "step": 4750
    },
    {
      "epoch": 0.7942599699649591,
      "grad_norm": 11.084415435791016,
      "learning_rate": 4.27710843373494e-05,
      "loss": 0.618,
      "step": 4760
    },
    {
      "epoch": 0.7959285833472385,
      "grad_norm": 2.163936138153076,
      "learning_rate": 4.275411505175632e-05,
      "loss": 0.588,
      "step": 4770
    },
    {
      "epoch": 0.7975971967295178,
      "grad_norm": 8.102314949035645,
      "learning_rate": 4.273714576616325e-05,
      "loss": 0.5813,
      "step": 4780
    },
    {
      "epoch": 0.7992658101117971,
      "grad_norm": 5.020988464355469,
      "learning_rate": 4.2720176480570166e-05,
      "loss": 0.7614,
      "step": 4790
    },
    {
      "epoch": 0.8009344234940764,
      "grad_norm": 11.799640655517578,
      "learning_rate": 4.27032071949771e-05,
      "loss": 0.5358,
      "step": 4800
    },
    {
      "epoch": 0.8026030368763557,
      "grad_norm": 2.727512836456299,
      "learning_rate": 4.268623790938402e-05,
      "loss": 0.7992,
      "step": 4810
    },
    {
      "epoch": 0.8042716502586351,
      "grad_norm": 5.246251583099365,
      "learning_rate": 4.266926862379094e-05,
      "loss": 0.8267,
      "step": 4820
    },
    {
      "epoch": 0.8059402636409144,
      "grad_norm": 2.2390408515930176,
      "learning_rate": 4.265229933819786e-05,
      "loss": 0.6939,
      "step": 4830
    },
    {
      "epoch": 0.8076088770231937,
      "grad_norm": 12.144538879394531,
      "learning_rate": 4.263533005260479e-05,
      "loss": 1.2697,
      "step": 4840
    },
    {
      "epoch": 0.809277490405473,
      "grad_norm": 16.213382720947266,
      "learning_rate": 4.2618360767011714e-05,
      "loss": 0.2932,
      "step": 4850
    },
    {
      "epoch": 0.8109461037877523,
      "grad_norm": 1.259371280670166,
      "learning_rate": 4.260139148141863e-05,
      "loss": 0.577,
      "step": 4860
    },
    {
      "epoch": 0.8126147171700318,
      "grad_norm": 7.75519323348999,
      "learning_rate": 4.258442219582556e-05,
      "loss": 0.6545,
      "step": 4870
    },
    {
      "epoch": 0.8142833305523111,
      "grad_norm": 5.786462306976318,
      "learning_rate": 4.256745291023248e-05,
      "loss": 0.7855,
      "step": 4880
    },
    {
      "epoch": 0.8159519439345904,
      "grad_norm": 5.113633155822754,
      "learning_rate": 4.2550483624639404e-05,
      "loss": 0.706,
      "step": 4890
    },
    {
      "epoch": 0.8176205573168697,
      "grad_norm": 19.406774520874023,
      "learning_rate": 4.253351433904632e-05,
      "loss": 0.614,
      "step": 4900
    },
    {
      "epoch": 0.819289170699149,
      "grad_norm": 9.980238914489746,
      "learning_rate": 4.251654505345325e-05,
      "loss": 0.7607,
      "step": 4910
    },
    {
      "epoch": 0.8209577840814284,
      "grad_norm": 6.8825578689575195,
      "learning_rate": 4.2499575767860175e-05,
      "loss": 0.6379,
      "step": 4920
    },
    {
      "epoch": 0.8226263974637077,
      "grad_norm": 4.099124431610107,
      "learning_rate": 4.24826064822671e-05,
      "loss": 0.5603,
      "step": 4930
    },
    {
      "epoch": 0.824295010845987,
      "grad_norm": 8.896454811096191,
      "learning_rate": 4.246563719667403e-05,
      "loss": 0.6675,
      "step": 4940
    },
    {
      "epoch": 0.8259636242282663,
      "grad_norm": 3.420372486114502,
      "learning_rate": 4.2448667911080946e-05,
      "loss": 0.6642,
      "step": 4950
    },
    {
      "epoch": 0.8276322376105456,
      "grad_norm": 4.121123313903809,
      "learning_rate": 4.243169862548787e-05,
      "loss": 0.5786,
      "step": 4960
    },
    {
      "epoch": 0.8293008509928249,
      "grad_norm": 11.6819486618042,
      "learning_rate": 4.241472933989479e-05,
      "loss": 0.6982,
      "step": 4970
    },
    {
      "epoch": 0.8309694643751043,
      "grad_norm": 3.4534575939178467,
      "learning_rate": 4.239776005430172e-05,
      "loss": 0.7891,
      "step": 4980
    },
    {
      "epoch": 0.8326380777573836,
      "grad_norm": 8.859320640563965,
      "learning_rate": 4.2380790768708636e-05,
      "loss": 1.2304,
      "step": 4990
    },
    {
      "epoch": 0.8343066911396629,
      "grad_norm": 6.556605815887451,
      "learning_rate": 4.236382148311556e-05,
      "loss": 0.8358,
      "step": 5000
    },
    {
      "epoch": 0.8359753045219422,
      "grad_norm": 4.322836399078369,
      "learning_rate": 4.234685219752249e-05,
      "loss": 0.8217,
      "step": 5010
    },
    {
      "epoch": 0.8376439179042215,
      "grad_norm": 3.1503679752349854,
      "learning_rate": 4.232988291192941e-05,
      "loss": 0.6249,
      "step": 5020
    },
    {
      "epoch": 0.839312531286501,
      "grad_norm": 2.255263566970825,
      "learning_rate": 4.231291362633633e-05,
      "loss": 0.5876,
      "step": 5030
    },
    {
      "epoch": 0.8409811446687803,
      "grad_norm": 5.652220249176025,
      "learning_rate": 4.229594434074325e-05,
      "loss": 0.543,
      "step": 5040
    },
    {
      "epoch": 0.8426497580510596,
      "grad_norm": 11.563413619995117,
      "learning_rate": 4.2278975055150184e-05,
      "loss": 0.8175,
      "step": 5050
    },
    {
      "epoch": 0.8443183714333389,
      "grad_norm": 8.309548377990723,
      "learning_rate": 4.22620057695571e-05,
      "loss": 0.7823,
      "step": 5060
    },
    {
      "epoch": 0.8459869848156182,
      "grad_norm": 5.631826877593994,
      "learning_rate": 4.224503648396403e-05,
      "loss": 0.9053,
      "step": 5070
    },
    {
      "epoch": 0.8476555981978976,
      "grad_norm": 3.193244457244873,
      "learning_rate": 4.2228067198370955e-05,
      "loss": 0.6664,
      "step": 5080
    },
    {
      "epoch": 0.8493242115801769,
      "grad_norm": 5.531043529510498,
      "learning_rate": 4.2211097912777874e-05,
      "loss": 0.6202,
      "step": 5090
    },
    {
      "epoch": 0.8509928249624562,
      "grad_norm": 5.683650016784668,
      "learning_rate": 4.21941286271848e-05,
      "loss": 0.5661,
      "step": 5100
    },
    {
      "epoch": 0.8526614383447355,
      "grad_norm": 4.842739105224609,
      "learning_rate": 4.217715934159172e-05,
      "loss": 0.6468,
      "step": 5110
    },
    {
      "epoch": 0.8543300517270148,
      "grad_norm": 1.5164592266082764,
      "learning_rate": 4.2160190055998645e-05,
      "loss": 0.4893,
      "step": 5120
    },
    {
      "epoch": 0.8559986651092941,
      "grad_norm": 6.801939010620117,
      "learning_rate": 4.2143220770405564e-05,
      "loss": 0.7387,
      "step": 5130
    },
    {
      "epoch": 0.8576672784915735,
      "grad_norm": 3.8182663917541504,
      "learning_rate": 4.212625148481249e-05,
      "loss": 0.7449,
      "step": 5140
    },
    {
      "epoch": 0.8593358918738528,
      "grad_norm": 8.898698806762695,
      "learning_rate": 4.2109282199219416e-05,
      "loss": 0.5247,
      "step": 5150
    },
    {
      "epoch": 0.8610045052561321,
      "grad_norm": 1.3317350149154663,
      "learning_rate": 4.2092312913626335e-05,
      "loss": 0.5463,
      "step": 5160
    },
    {
      "epoch": 0.8626731186384115,
      "grad_norm": 6.228333950042725,
      "learning_rate": 4.207534362803326e-05,
      "loss": 0.8868,
      "step": 5170
    },
    {
      "epoch": 0.8643417320206908,
      "grad_norm": 9.815524101257324,
      "learning_rate": 4.205837434244019e-05,
      "loss": 0.7263,
      "step": 5180
    },
    {
      "epoch": 0.8660103454029702,
      "grad_norm": 7.110081672668457,
      "learning_rate": 4.204140505684711e-05,
      "loss": 0.8358,
      "step": 5190
    },
    {
      "epoch": 0.8676789587852495,
      "grad_norm": 4.962923049926758,
      "learning_rate": 4.202443577125403e-05,
      "loss": 0.7162,
      "step": 5200
    },
    {
      "epoch": 0.8693475721675288,
      "grad_norm": 2.563223361968994,
      "learning_rate": 4.200746648566096e-05,
      "loss": 0.6345,
      "step": 5210
    },
    {
      "epoch": 0.8710161855498081,
      "grad_norm": 3.371091365814209,
      "learning_rate": 4.199049720006788e-05,
      "loss": 0.744,
      "step": 5220
    },
    {
      "epoch": 0.8726847989320874,
      "grad_norm": 4.976936340332031,
      "learning_rate": 4.19735279144748e-05,
      "loss": 0.5955,
      "step": 5230
    },
    {
      "epoch": 0.8743534123143668,
      "grad_norm": 6.893229961395264,
      "learning_rate": 4.195655862888173e-05,
      "loss": 0.4491,
      "step": 5240
    },
    {
      "epoch": 0.8760220256966461,
      "grad_norm": 7.070240020751953,
      "learning_rate": 4.193958934328865e-05,
      "loss": 0.5659,
      "step": 5250
    },
    {
      "epoch": 0.8776906390789254,
      "grad_norm": 3.339160442352295,
      "learning_rate": 4.1922620057695573e-05,
      "loss": 0.6215,
      "step": 5260
    },
    {
      "epoch": 0.8793592524612047,
      "grad_norm": 2.8599579334259033,
      "learning_rate": 4.190565077210249e-05,
      "loss": 0.6877,
      "step": 5270
    },
    {
      "epoch": 0.881027865843484,
      "grad_norm": 5.412785530090332,
      "learning_rate": 4.188868148650942e-05,
      "loss": 0.7017,
      "step": 5280
    },
    {
      "epoch": 0.8826964792257634,
      "grad_norm": 8.722503662109375,
      "learning_rate": 4.1871712200916344e-05,
      "loss": 0.646,
      "step": 5290
    },
    {
      "epoch": 0.8843650926080427,
      "grad_norm": 4.836069107055664,
      "learning_rate": 4.185474291532327e-05,
      "loss": 0.5328,
      "step": 5300
    },
    {
      "epoch": 0.886033705990322,
      "grad_norm": 9.418333053588867,
      "learning_rate": 4.183777362973019e-05,
      "loss": 0.9033,
      "step": 5310
    },
    {
      "epoch": 0.8877023193726014,
      "grad_norm": 7.081607818603516,
      "learning_rate": 4.1820804344137115e-05,
      "loss": 0.7894,
      "step": 5320
    },
    {
      "epoch": 0.8893709327548807,
      "grad_norm": 4.9945526123046875,
      "learning_rate": 4.180383505854404e-05,
      "loss": 0.5195,
      "step": 5330
    },
    {
      "epoch": 0.89103954613716,
      "grad_norm": 6.5386176109313965,
      "learning_rate": 4.178686577295096e-05,
      "loss": 0.6565,
      "step": 5340
    },
    {
      "epoch": 0.8927081595194394,
      "grad_norm": 6.849684715270996,
      "learning_rate": 4.1769896487357886e-05,
      "loss": 0.5634,
      "step": 5350
    },
    {
      "epoch": 0.8943767729017187,
      "grad_norm": 13.75334358215332,
      "learning_rate": 4.1752927201764805e-05,
      "loss": 0.7763,
      "step": 5360
    },
    {
      "epoch": 0.896045386283998,
      "grad_norm": 6.718748092651367,
      "learning_rate": 4.173595791617173e-05,
      "loss": 0.9289,
      "step": 5370
    },
    {
      "epoch": 0.8977139996662773,
      "grad_norm": 12.123237609863281,
      "learning_rate": 4.171898863057866e-05,
      "loss": 0.5061,
      "step": 5380
    },
    {
      "epoch": 0.8993826130485566,
      "grad_norm": 10.28067684173584,
      "learning_rate": 4.1702019344985576e-05,
      "loss": 0.8026,
      "step": 5390
    },
    {
      "epoch": 0.901051226430836,
      "grad_norm": 8.775866508483887,
      "learning_rate": 4.16850500593925e-05,
      "loss": 0.7988,
      "step": 5400
    },
    {
      "epoch": 0.9027198398131153,
      "grad_norm": 7.310545444488525,
      "learning_rate": 4.166808077379942e-05,
      "loss": 0.5305,
      "step": 5410
    },
    {
      "epoch": 0.9043884531953946,
      "grad_norm": 2.226510763168335,
      "learning_rate": 4.165111148820635e-05,
      "loss": 0.7124,
      "step": 5420
    },
    {
      "epoch": 0.9060570665776739,
      "grad_norm": 7.282177448272705,
      "learning_rate": 4.163414220261327e-05,
      "loss": 0.4127,
      "step": 5430
    },
    {
      "epoch": 0.9077256799599532,
      "grad_norm": 10.734892845153809,
      "learning_rate": 4.16171729170202e-05,
      "loss": 0.6631,
      "step": 5440
    },
    {
      "epoch": 0.9093942933422327,
      "grad_norm": 11.756941795349121,
      "learning_rate": 4.160020363142712e-05,
      "loss": 0.5773,
      "step": 5450
    },
    {
      "epoch": 0.911062906724512,
      "grad_norm": 3.166810989379883,
      "learning_rate": 4.1583234345834043e-05,
      "loss": 0.6495,
      "step": 5460
    },
    {
      "epoch": 0.9127315201067913,
      "grad_norm": 4.299709320068359,
      "learning_rate": 4.156626506024097e-05,
      "loss": 0.6716,
      "step": 5470
    },
    {
      "epoch": 0.9144001334890706,
      "grad_norm": 4.141909599304199,
      "learning_rate": 4.154929577464789e-05,
      "loss": 0.7565,
      "step": 5480
    },
    {
      "epoch": 0.9160687468713499,
      "grad_norm": 9.989823341369629,
      "learning_rate": 4.1532326489054814e-05,
      "loss": 0.6365,
      "step": 5490
    },
    {
      "epoch": 0.9177373602536293,
      "grad_norm": 7.672375679016113,
      "learning_rate": 4.1515357203461733e-05,
      "loss": 0.8757,
      "step": 5500
    },
    {
      "epoch": 0.9194059736359086,
      "grad_norm": 6.755795478820801,
      "learning_rate": 4.149838791786866e-05,
      "loss": 0.673,
      "step": 5510
    },
    {
      "epoch": 0.9210745870181879,
      "grad_norm": 4.3422346115112305,
      "learning_rate": 4.148141863227558e-05,
      "loss": 0.5129,
      "step": 5520
    },
    {
      "epoch": 0.9227432004004672,
      "grad_norm": 4.82142972946167,
      "learning_rate": 4.1464449346682504e-05,
      "loss": 0.4887,
      "step": 5530
    },
    {
      "epoch": 0.9244118137827465,
      "grad_norm": 3.747398853302002,
      "learning_rate": 4.144748006108943e-05,
      "loss": 0.4191,
      "step": 5540
    },
    {
      "epoch": 0.9260804271650258,
      "grad_norm": 6.317661285400391,
      "learning_rate": 4.1430510775496356e-05,
      "loss": 0.6077,
      "step": 5550
    },
    {
      "epoch": 0.9277490405473052,
      "grad_norm": 8.354905128479004,
      "learning_rate": 4.141354148990328e-05,
      "loss": 0.5112,
      "step": 5560
    },
    {
      "epoch": 0.9294176539295845,
      "grad_norm": 4.525085926055908,
      "learning_rate": 4.13965722043102e-05,
      "loss": 0.6139,
      "step": 5570
    },
    {
      "epoch": 0.9310862673118638,
      "grad_norm": 8.851008415222168,
      "learning_rate": 4.137960291871713e-05,
      "loss": 0.6961,
      "step": 5580
    },
    {
      "epoch": 0.9327548806941431,
      "grad_norm": 1.5388422012329102,
      "learning_rate": 4.1362633633124046e-05,
      "loss": 0.437,
      "step": 5590
    },
    {
      "epoch": 0.9344234940764224,
      "grad_norm": 12.384132385253906,
      "learning_rate": 4.134566434753097e-05,
      "loss": 0.6892,
      "step": 5600
    },
    {
      "epoch": 0.9360921074587019,
      "grad_norm": 5.3065714836120605,
      "learning_rate": 4.132869506193789e-05,
      "loss": 0.6548,
      "step": 5610
    },
    {
      "epoch": 0.9377607208409812,
      "grad_norm": 8.042698860168457,
      "learning_rate": 4.131172577634482e-05,
      "loss": 0.8408,
      "step": 5620
    },
    {
      "epoch": 0.9394293342232605,
      "grad_norm": 1.0562492609024048,
      "learning_rate": 4.129475649075174e-05,
      "loss": 0.6352,
      "step": 5630
    },
    {
      "epoch": 0.9410979476055398,
      "grad_norm": 5.534671306610107,
      "learning_rate": 4.127778720515866e-05,
      "loss": 0.6412,
      "step": 5640
    },
    {
      "epoch": 0.9427665609878191,
      "grad_norm": 7.715068817138672,
      "learning_rate": 4.126081791956559e-05,
      "loss": 0.6922,
      "step": 5650
    },
    {
      "epoch": 0.9444351743700985,
      "grad_norm": 3.7539029121398926,
      "learning_rate": 4.124384863397251e-05,
      "loss": 0.6979,
      "step": 5660
    },
    {
      "epoch": 0.9461037877523778,
      "grad_norm": 6.7038350105285645,
      "learning_rate": 4.122687934837943e-05,
      "loss": 0.838,
      "step": 5670
    },
    {
      "epoch": 0.9477724011346571,
      "grad_norm": 1.6392005681991577,
      "learning_rate": 4.120991006278636e-05,
      "loss": 0.4468,
      "step": 5680
    },
    {
      "epoch": 0.9494410145169364,
      "grad_norm": 3.4735050201416016,
      "learning_rate": 4.1192940777193284e-05,
      "loss": 0.8158,
      "step": 5690
    },
    {
      "epoch": 0.9511096278992157,
      "grad_norm": 8.991637229919434,
      "learning_rate": 4.117597149160021e-05,
      "loss": 0.8683,
      "step": 5700
    },
    {
      "epoch": 0.9527782412814951,
      "grad_norm": 7.031501770019531,
      "learning_rate": 4.115900220600713e-05,
      "loss": 0.7253,
      "step": 5710
    },
    {
      "epoch": 0.9544468546637744,
      "grad_norm": 7.107928276062012,
      "learning_rate": 4.1142032920414055e-05,
      "loss": 0.5106,
      "step": 5720
    },
    {
      "epoch": 0.9561154680460537,
      "grad_norm": 7.524052143096924,
      "learning_rate": 4.1125063634820974e-05,
      "loss": 0.52,
      "step": 5730
    },
    {
      "epoch": 0.957784081428333,
      "grad_norm": 3.145481586456299,
      "learning_rate": 4.11080943492279e-05,
      "loss": 0.5373,
      "step": 5740
    },
    {
      "epoch": 0.9594526948106123,
      "grad_norm": 2.202536106109619,
      "learning_rate": 4.109112506363482e-05,
      "loss": 0.4803,
      "step": 5750
    },
    {
      "epoch": 0.9611213081928917,
      "grad_norm": 11.449325561523438,
      "learning_rate": 4.1074155778041745e-05,
      "loss": 0.5958,
      "step": 5760
    },
    {
      "epoch": 0.9627899215751711,
      "grad_norm": 6.558907985687256,
      "learning_rate": 4.105718649244867e-05,
      "loss": 0.7473,
      "step": 5770
    },
    {
      "epoch": 0.9644585349574504,
      "grad_norm": 3.457144021987915,
      "learning_rate": 4.104021720685559e-05,
      "loss": 0.8943,
      "step": 5780
    },
    {
      "epoch": 0.9661271483397297,
      "grad_norm": 4.960923194885254,
      "learning_rate": 4.1023247921262516e-05,
      "loss": 0.7275,
      "step": 5790
    },
    {
      "epoch": 0.967795761722009,
      "grad_norm": 4.399231433868408,
      "learning_rate": 4.100627863566944e-05,
      "loss": 1.1137,
      "step": 5800
    },
    {
      "epoch": 0.9694643751042883,
      "grad_norm": 7.2484517097473145,
      "learning_rate": 4.098930935007637e-05,
      "loss": 0.6629,
      "step": 5810
    },
    {
      "epoch": 0.9711329884865677,
      "grad_norm": 6.6189961433410645,
      "learning_rate": 4.097234006448329e-05,
      "loss": 0.6317,
      "step": 5820
    },
    {
      "epoch": 0.972801601868847,
      "grad_norm": 14.54278564453125,
      "learning_rate": 4.095537077889021e-05,
      "loss": 0.9016,
      "step": 5830
    },
    {
      "epoch": 0.9744702152511263,
      "grad_norm": 7.602426528930664,
      "learning_rate": 4.093840149329713e-05,
      "loss": 0.4989,
      "step": 5840
    },
    {
      "epoch": 0.9761388286334056,
      "grad_norm": 1.0486760139465332,
      "learning_rate": 4.092143220770406e-05,
      "loss": 0.5774,
      "step": 5850
    },
    {
      "epoch": 0.9778074420156849,
      "grad_norm": 3.768935203552246,
      "learning_rate": 4.0904462922110984e-05,
      "loss": 0.3698,
      "step": 5860
    },
    {
      "epoch": 0.9794760553979643,
      "grad_norm": 7.159753322601318,
      "learning_rate": 4.08874936365179e-05,
      "loss": 0.6508,
      "step": 5870
    },
    {
      "epoch": 0.9811446687802436,
      "grad_norm": 7.644687175750732,
      "learning_rate": 4.087052435092483e-05,
      "loss": 0.4545,
      "step": 5880
    },
    {
      "epoch": 0.982813282162523,
      "grad_norm": 9.999479293823242,
      "learning_rate": 4.085355506533175e-05,
      "loss": 0.6054,
      "step": 5890
    },
    {
      "epoch": 0.9844818955448023,
      "grad_norm": 5.687373161315918,
      "learning_rate": 4.0836585779738674e-05,
      "loss": 0.7532,
      "step": 5900
    },
    {
      "epoch": 0.9861505089270816,
      "grad_norm": 4.953110218048096,
      "learning_rate": 4.08196164941456e-05,
      "loss": 0.5566,
      "step": 5910
    },
    {
      "epoch": 0.987819122309361,
      "grad_norm": 2.7320237159729004,
      "learning_rate": 4.0802647208552525e-05,
      "loss": 0.614,
      "step": 5920
    },
    {
      "epoch": 0.9894877356916403,
      "grad_norm": 7.805532932281494,
      "learning_rate": 4.0785677922959444e-05,
      "loss": 0.5588,
      "step": 5930
    },
    {
      "epoch": 0.9911563490739196,
      "grad_norm": 7.908514976501465,
      "learning_rate": 4.076870863736637e-05,
      "loss": 0.8623,
      "step": 5940
    },
    {
      "epoch": 0.9928249624561989,
      "grad_norm": 1.8355716466903687,
      "learning_rate": 4.0751739351773296e-05,
      "loss": 0.4996,
      "step": 5950
    },
    {
      "epoch": 0.9944935758384782,
      "grad_norm": 2.027433156967163,
      "learning_rate": 4.0734770066180215e-05,
      "loss": 0.8997,
      "step": 5960
    },
    {
      "epoch": 0.9961621892207575,
      "grad_norm": 8.070856094360352,
      "learning_rate": 4.071780078058714e-05,
      "loss": 0.8567,
      "step": 5970
    },
    {
      "epoch": 0.9978308026030369,
      "grad_norm": 5.994241714477539,
      "learning_rate": 4.070083149499406e-05,
      "loss": 0.9064,
      "step": 5980
    },
    {
      "epoch": 0.9994994159853162,
      "grad_norm": 4.204895496368408,
      "learning_rate": 4.0683862209400986e-05,
      "loss": 0.703,
      "step": 5990
    }
  ],
  "logging_steps": 10,
  "max_steps": 29965,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 2997,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.487996619039744e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
