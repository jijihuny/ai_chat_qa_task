{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.20002372760707082,
  "eval_steps": 500,
  "global_step": 3372,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 5.931901767706727e-05,
      "grad_norm": 18.21153450012207,
      "learning_rate": 1.1862396204033216e-08,
      "loss": 1.5562,
      "step": 1
    },
    {
      "epoch": 0.00011863803535413454,
      "grad_norm": 60.55733871459961,
      "learning_rate": 2.3724792408066433e-08,
      "loss": 4.3784,
      "step": 2
    },
    {
      "epoch": 0.00017795705303120182,
      "grad_norm": 23.13866424560547,
      "learning_rate": 3.5587188612099644e-08,
      "loss": 1.0293,
      "step": 3
    },
    {
      "epoch": 0.00023727607070826907,
      "grad_norm": 19.787689208984375,
      "learning_rate": 4.7449584816132866e-08,
      "loss": 0.5862,
      "step": 4
    },
    {
      "epoch": 0.00029659508838533635,
      "grad_norm": 42.488956451416016,
      "learning_rate": 5.931198102016608e-08,
      "loss": 0.9095,
      "step": 5
    },
    {
      "epoch": 0.00035591410606240363,
      "grad_norm": 39.30983352661133,
      "learning_rate": 7.117437722419929e-08,
      "loss": 1.9888,
      "step": 6
    },
    {
      "epoch": 0.00041523312373947086,
      "grad_norm": 51.29161834716797,
      "learning_rate": 8.30367734282325e-08,
      "loss": 1.8357,
      "step": 7
    },
    {
      "epoch": 0.00047455214141653814,
      "grad_norm": 78.7551498413086,
      "learning_rate": 9.489916963226573e-08,
      "loss": 2.282,
      "step": 8
    },
    {
      "epoch": 0.0005338711590936054,
      "grad_norm": 15.863694190979004,
      "learning_rate": 1.0676156583629895e-07,
      "loss": 2.2656,
      "step": 9
    },
    {
      "epoch": 0.0005931901767706727,
      "grad_norm": 89.81600189208984,
      "learning_rate": 1.1862396204033216e-07,
      "loss": 1.8711,
      "step": 10
    },
    {
      "epoch": 0.0006525091944477399,
      "grad_norm": 26.953248977661133,
      "learning_rate": 1.3048635824436538e-07,
      "loss": 1.6096,
      "step": 11
    },
    {
      "epoch": 0.0007118282121248073,
      "grad_norm": 32.546722412109375,
      "learning_rate": 1.4234875444839858e-07,
      "loss": 1.4163,
      "step": 12
    },
    {
      "epoch": 0.0007711472298018745,
      "grad_norm": 67.22864532470703,
      "learning_rate": 1.542111506524318e-07,
      "loss": 2.1648,
      "step": 13
    },
    {
      "epoch": 0.0008304662474789417,
      "grad_norm": 32.48191833496094,
      "learning_rate": 1.66073546856465e-07,
      "loss": 0.9824,
      "step": 14
    },
    {
      "epoch": 0.000889785265156009,
      "grad_norm": 45.35607147216797,
      "learning_rate": 1.7793594306049826e-07,
      "loss": 3.1515,
      "step": 15
    },
    {
      "epoch": 0.0009491042828330763,
      "grad_norm": 49.304046630859375,
      "learning_rate": 1.8979833926453146e-07,
      "loss": 1.6586,
      "step": 16
    },
    {
      "epoch": 0.0010084233005101435,
      "grad_norm": 77.31291198730469,
      "learning_rate": 2.0166073546856466e-07,
      "loss": 3.3181,
      "step": 17
    },
    {
      "epoch": 0.0010677423181872107,
      "grad_norm": 77.35077667236328,
      "learning_rate": 2.135231316725979e-07,
      "loss": 1.5873,
      "step": 18
    },
    {
      "epoch": 0.0011270613358642782,
      "grad_norm": 101.73736572265625,
      "learning_rate": 2.253855278766311e-07,
      "loss": 2.8063,
      "step": 19
    },
    {
      "epoch": 0.0011863803535413454,
      "grad_norm": 22.593318939208984,
      "learning_rate": 2.3724792408066432e-07,
      "loss": 1.5909,
      "step": 20
    },
    {
      "epoch": 0.0012456993712184126,
      "grad_norm": 18.1798152923584,
      "learning_rate": 2.4911032028469755e-07,
      "loss": 1.1987,
      "step": 21
    },
    {
      "epoch": 0.0013050183888954799,
      "grad_norm": 20.846908569335938,
      "learning_rate": 2.6097271648873075e-07,
      "loss": 1.7476,
      "step": 22
    },
    {
      "epoch": 0.001364337406572547,
      "grad_norm": 68.06765747070312,
      "learning_rate": 2.7283511269276395e-07,
      "loss": 1.951,
      "step": 23
    },
    {
      "epoch": 0.0014236564242496145,
      "grad_norm": 93.73208618164062,
      "learning_rate": 2.8469750889679715e-07,
      "loss": 3.2063,
      "step": 24
    },
    {
      "epoch": 0.0014829754419266818,
      "grad_norm": 86.34806823730469,
      "learning_rate": 2.965599051008304e-07,
      "loss": 2.5356,
      "step": 25
    },
    {
      "epoch": 0.001542294459603749,
      "grad_norm": 89.12501525878906,
      "learning_rate": 3.084223013048636e-07,
      "loss": 2.6026,
      "step": 26
    },
    {
      "epoch": 0.0016016134772808162,
      "grad_norm": 48.73095703125,
      "learning_rate": 3.202846975088968e-07,
      "loss": 1.2887,
      "step": 27
    },
    {
      "epoch": 0.0016609324949578834,
      "grad_norm": 74.59615325927734,
      "learning_rate": 3.3214709371293e-07,
      "loss": 5.6495,
      "step": 28
    },
    {
      "epoch": 0.0017202515126349507,
      "grad_norm": 72.9882583618164,
      "learning_rate": 3.4400948991696327e-07,
      "loss": 1.8125,
      "step": 29
    },
    {
      "epoch": 0.001779570530312018,
      "grad_norm": 50.05543518066406,
      "learning_rate": 3.558718861209965e-07,
      "loss": 1.2885,
      "step": 30
    },
    {
      "epoch": 0.0018388895479890853,
      "grad_norm": 110.28607940673828,
      "learning_rate": 3.6773428232502967e-07,
      "loss": 3.5049,
      "step": 31
    },
    {
      "epoch": 0.0018982085656661526,
      "grad_norm": 49.59636306762695,
      "learning_rate": 3.795966785290629e-07,
      "loss": 2.0352,
      "step": 32
    },
    {
      "epoch": 0.00195752758334322,
      "grad_norm": 34.76351547241211,
      "learning_rate": 3.914590747330961e-07,
      "loss": 1.123,
      "step": 33
    },
    {
      "epoch": 0.002016846601020287,
      "grad_norm": 49.948875427246094,
      "learning_rate": 4.0332147093712933e-07,
      "loss": 2.1645,
      "step": 34
    },
    {
      "epoch": 0.0020761656186973542,
      "grad_norm": 42.01350402832031,
      "learning_rate": 4.1518386714116253e-07,
      "loss": 0.9738,
      "step": 35
    },
    {
      "epoch": 0.0021354846363744215,
      "grad_norm": 81.60076904296875,
      "learning_rate": 4.270462633451958e-07,
      "loss": 2.0477,
      "step": 36
    },
    {
      "epoch": 0.0021948036540514887,
      "grad_norm": 99.7342300415039,
      "learning_rate": 4.3890865954922893e-07,
      "loss": 2.2333,
      "step": 37
    },
    {
      "epoch": 0.0022541226717285564,
      "grad_norm": 69.1491470336914,
      "learning_rate": 4.507710557532622e-07,
      "loss": 2.1741,
      "step": 38
    },
    {
      "epoch": 0.0023134416894056236,
      "grad_norm": 38.881248474121094,
      "learning_rate": 4.626334519572954e-07,
      "loss": 0.7099,
      "step": 39
    },
    {
      "epoch": 0.002372760707082691,
      "grad_norm": 54.75040054321289,
      "learning_rate": 4.7449584816132864e-07,
      "loss": 2.1992,
      "step": 40
    },
    {
      "epoch": 0.002432079724759758,
      "grad_norm": 68.22157287597656,
      "learning_rate": 4.863582443653619e-07,
      "loss": 2.2203,
      "step": 41
    },
    {
      "epoch": 0.0024913987424368253,
      "grad_norm": 32.87198257446289,
      "learning_rate": 4.982206405693951e-07,
      "loss": 0.8024,
      "step": 42
    },
    {
      "epoch": 0.0025507177601138925,
      "grad_norm": 31.768178939819336,
      "learning_rate": 5.100830367734283e-07,
      "loss": 3.3163,
      "step": 43
    },
    {
      "epoch": 0.0026100367777909597,
      "grad_norm": 71.38982391357422,
      "learning_rate": 5.219454329774615e-07,
      "loss": 2.6593,
      "step": 44
    },
    {
      "epoch": 0.002669355795468027,
      "grad_norm": 99.39778900146484,
      "learning_rate": 5.338078291814947e-07,
      "loss": 3.1923,
      "step": 45
    },
    {
      "epoch": 0.002728674813145094,
      "grad_norm": 65.34956359863281,
      "learning_rate": 5.456702253855279e-07,
      "loss": 1.1707,
      "step": 46
    },
    {
      "epoch": 0.0027879938308221614,
      "grad_norm": 37.90747833251953,
      "learning_rate": 5.575326215895611e-07,
      "loss": 2.2722,
      "step": 47
    },
    {
      "epoch": 0.002847312848499229,
      "grad_norm": 50.04475402832031,
      "learning_rate": 5.693950177935943e-07,
      "loss": 1.62,
      "step": 48
    },
    {
      "epoch": 0.0029066318661762963,
      "grad_norm": 37.20331954956055,
      "learning_rate": 5.812574139976276e-07,
      "loss": 1.0425,
      "step": 49
    },
    {
      "epoch": 0.0029659508838533635,
      "grad_norm": 51.78990936279297,
      "learning_rate": 5.931198102016608e-07,
      "loss": 2.641,
      "step": 50
    },
    {
      "epoch": 0.0030252699015304307,
      "grad_norm": 105.18022155761719,
      "learning_rate": 6.04982206405694e-07,
      "loss": 1.9288,
      "step": 51
    },
    {
      "epoch": 0.003084588919207498,
      "grad_norm": 94.53858184814453,
      "learning_rate": 6.168446026097272e-07,
      "loss": 1.6381,
      "step": 52
    },
    {
      "epoch": 0.003143907936884565,
      "grad_norm": 76.86193084716797,
      "learning_rate": 6.287069988137605e-07,
      "loss": 1.8284,
      "step": 53
    },
    {
      "epoch": 0.0032032269545616324,
      "grad_norm": 72.60234832763672,
      "learning_rate": 6.405693950177936e-07,
      "loss": 2.1931,
      "step": 54
    },
    {
      "epoch": 0.0032625459722386996,
      "grad_norm": 53.2988166809082,
      "learning_rate": 6.524317912218268e-07,
      "loss": 1.2328,
      "step": 55
    },
    {
      "epoch": 0.003321864989915767,
      "grad_norm": 83.8056640625,
      "learning_rate": 6.6429418742586e-07,
      "loss": 1.4645,
      "step": 56
    },
    {
      "epoch": 0.003381184007592834,
      "grad_norm": 51.68008041381836,
      "learning_rate": 6.761565836298933e-07,
      "loss": 1.4716,
      "step": 57
    },
    {
      "epoch": 0.0034405030252699013,
      "grad_norm": 52.028282165527344,
      "learning_rate": 6.880189798339265e-07,
      "loss": 1.3537,
      "step": 58
    },
    {
      "epoch": 0.003499822042946969,
      "grad_norm": 90.3622817993164,
      "learning_rate": 6.998813760379597e-07,
      "loss": 2.3241,
      "step": 59
    },
    {
      "epoch": 0.003559141060624036,
      "grad_norm": 51.33265686035156,
      "learning_rate": 7.11743772241993e-07,
      "loss": 2.4135,
      "step": 60
    },
    {
      "epoch": 0.0036184600783011034,
      "grad_norm": 109.73255157470703,
      "learning_rate": 7.236061684460261e-07,
      "loss": 1.727,
      "step": 61
    },
    {
      "epoch": 0.0036777790959781707,
      "grad_norm": 65.5224380493164,
      "learning_rate": 7.354685646500593e-07,
      "loss": 2.0379,
      "step": 62
    },
    {
      "epoch": 0.003737098113655238,
      "grad_norm": 68.57380676269531,
      "learning_rate": 7.473309608540925e-07,
      "loss": 2.0604,
      "step": 63
    },
    {
      "epoch": 0.003796417131332305,
      "grad_norm": 46.95387649536133,
      "learning_rate": 7.591933570581258e-07,
      "loss": 2.3004,
      "step": 64
    },
    {
      "epoch": 0.0038557361490093723,
      "grad_norm": 80.76876068115234,
      "learning_rate": 7.71055753262159e-07,
      "loss": 1.6454,
      "step": 65
    },
    {
      "epoch": 0.00391505516668644,
      "grad_norm": 40.535743713378906,
      "learning_rate": 7.829181494661923e-07,
      "loss": 2.2061,
      "step": 66
    },
    {
      "epoch": 0.003974374184363507,
      "grad_norm": 41.85569381713867,
      "learning_rate": 7.947805456702253e-07,
      "loss": 0.7279,
      "step": 67
    },
    {
      "epoch": 0.004033693202040574,
      "grad_norm": 81.51060485839844,
      "learning_rate": 8.066429418742587e-07,
      "loss": 2.7386,
      "step": 68
    },
    {
      "epoch": 0.004093012219717642,
      "grad_norm": 116.39146423339844,
      "learning_rate": 8.185053380782919e-07,
      "loss": 3.7165,
      "step": 69
    },
    {
      "epoch": 0.0041523312373947085,
      "grad_norm": 50.37773513793945,
      "learning_rate": 8.303677342823251e-07,
      "loss": 2.8676,
      "step": 70
    },
    {
      "epoch": 0.004211650255071776,
      "grad_norm": 17.810115814208984,
      "learning_rate": 8.422301304863584e-07,
      "loss": 0.3161,
      "step": 71
    },
    {
      "epoch": 0.004270969272748843,
      "grad_norm": 101.27669525146484,
      "learning_rate": 8.540925266903916e-07,
      "loss": 2.4632,
      "step": 72
    },
    {
      "epoch": 0.004330288290425911,
      "grad_norm": 177.4308319091797,
      "learning_rate": 8.659549228944248e-07,
      "loss": 2.7872,
      "step": 73
    },
    {
      "epoch": 0.004389607308102977,
      "grad_norm": 52.13441467285156,
      "learning_rate": 8.778173190984579e-07,
      "loss": 1.2523,
      "step": 74
    },
    {
      "epoch": 0.004448926325780045,
      "grad_norm": 15.817767143249512,
      "learning_rate": 8.896797153024913e-07,
      "loss": 0.5112,
      "step": 75
    },
    {
      "epoch": 0.004508245343457113,
      "grad_norm": 32.692054748535156,
      "learning_rate": 9.015421115065244e-07,
      "loss": 1.0485,
      "step": 76
    },
    {
      "epoch": 0.0045675643611341795,
      "grad_norm": 26.497270584106445,
      "learning_rate": 9.134045077105576e-07,
      "loss": 1.5465,
      "step": 77
    },
    {
      "epoch": 0.004626883378811247,
      "grad_norm": 23.131364822387695,
      "learning_rate": 9.252669039145908e-07,
      "loss": 0.4174,
      "step": 78
    },
    {
      "epoch": 0.004686202396488314,
      "grad_norm": 76.77631378173828,
      "learning_rate": 9.371293001186241e-07,
      "loss": 0.6882,
      "step": 79
    },
    {
      "epoch": 0.004745521414165382,
      "grad_norm": 32.91665267944336,
      "learning_rate": 9.489916963226573e-07,
      "loss": 0.622,
      "step": 80
    },
    {
      "epoch": 0.004804840431842448,
      "grad_norm": 53.5190315246582,
      "learning_rate": 9.608540925266905e-07,
      "loss": 1.678,
      "step": 81
    },
    {
      "epoch": 0.004864159449519516,
      "grad_norm": 47.63593292236328,
      "learning_rate": 9.727164887307238e-07,
      "loss": 1.6328,
      "step": 82
    },
    {
      "epoch": 0.004923478467196583,
      "grad_norm": 46.98574447631836,
      "learning_rate": 9.845788849347569e-07,
      "loss": 1.3039,
      "step": 83
    },
    {
      "epoch": 0.0049827974848736505,
      "grad_norm": 32.09832000732422,
      "learning_rate": 9.964412811387902e-07,
      "loss": 0.7703,
      "step": 84
    },
    {
      "epoch": 0.005042116502550718,
      "grad_norm": 14.068093299865723,
      "learning_rate": 1.0083036773428233e-06,
      "loss": 0.9673,
      "step": 85
    },
    {
      "epoch": 0.005101435520227785,
      "grad_norm": 109.47574615478516,
      "learning_rate": 1.0201660735468566e-06,
      "loss": 0.8631,
      "step": 86
    },
    {
      "epoch": 0.005160754537904853,
      "grad_norm": 25.765417098999023,
      "learning_rate": 1.0320284697508897e-06,
      "loss": 0.3716,
      "step": 87
    },
    {
      "epoch": 0.005220073555581919,
      "grad_norm": 332.5910339355469,
      "learning_rate": 1.043890865954923e-06,
      "loss": 1.702,
      "step": 88
    },
    {
      "epoch": 0.005279392573258987,
      "grad_norm": 14.339134216308594,
      "learning_rate": 1.055753262158956e-06,
      "loss": 0.2289,
      "step": 89
    },
    {
      "epoch": 0.005338711590936054,
      "grad_norm": 21.796384811401367,
      "learning_rate": 1.0676156583629894e-06,
      "loss": 0.2974,
      "step": 90
    },
    {
      "epoch": 0.0053980306086131215,
      "grad_norm": 56.36151123046875,
      "learning_rate": 1.0794780545670227e-06,
      "loss": 1.0407,
      "step": 91
    },
    {
      "epoch": 0.005457349626290188,
      "grad_norm": 69.94447326660156,
      "learning_rate": 1.0913404507710558e-06,
      "loss": 0.2829,
      "step": 92
    },
    {
      "epoch": 0.005516668643967256,
      "grad_norm": 10.895770072937012,
      "learning_rate": 1.1032028469750891e-06,
      "loss": 0.6561,
      "step": 93
    },
    {
      "epoch": 0.005575987661644323,
      "grad_norm": 30.37158966064453,
      "learning_rate": 1.1150652431791222e-06,
      "loss": 0.6053,
      "step": 94
    },
    {
      "epoch": 0.0056353066793213905,
      "grad_norm": 29.332706451416016,
      "learning_rate": 1.1269276393831555e-06,
      "loss": 0.6045,
      "step": 95
    },
    {
      "epoch": 0.005694625696998458,
      "grad_norm": 24.956544876098633,
      "learning_rate": 1.1387900355871886e-06,
      "loss": 0.6386,
      "step": 96
    },
    {
      "epoch": 0.005753944714675525,
      "grad_norm": 16.534542083740234,
      "learning_rate": 1.150652431791222e-06,
      "loss": 0.4756,
      "step": 97
    },
    {
      "epoch": 0.005813263732352593,
      "grad_norm": 76.67335510253906,
      "learning_rate": 1.1625148279952552e-06,
      "loss": 2.2155,
      "step": 98
    },
    {
      "epoch": 0.005872582750029659,
      "grad_norm": 32.78414535522461,
      "learning_rate": 1.1743772241992883e-06,
      "loss": 0.5853,
      "step": 99
    },
    {
      "epoch": 0.005931901767706727,
      "grad_norm": 40.033470153808594,
      "learning_rate": 1.1862396204033216e-06,
      "loss": 0.2807,
      "step": 100
    },
    {
      "epoch": 0.005991220785383794,
      "grad_norm": 61.65553665161133,
      "learning_rate": 1.1981020166073547e-06,
      "loss": 0.6524,
      "step": 101
    },
    {
      "epoch": 0.0060505398030608615,
      "grad_norm": 20.528127670288086,
      "learning_rate": 1.209964412811388e-06,
      "loss": 0.9628,
      "step": 102
    },
    {
      "epoch": 0.006109858820737928,
      "grad_norm": 18.94948959350586,
      "learning_rate": 1.2218268090154211e-06,
      "loss": 0.5905,
      "step": 103
    },
    {
      "epoch": 0.006169177838414996,
      "grad_norm": 60.77953338623047,
      "learning_rate": 1.2336892052194544e-06,
      "loss": 1.4975,
      "step": 104
    },
    {
      "epoch": 0.006228496856092063,
      "grad_norm": 15.832735061645508,
      "learning_rate": 1.2455516014234877e-06,
      "loss": 0.3097,
      "step": 105
    },
    {
      "epoch": 0.00628781587376913,
      "grad_norm": 28.50030517578125,
      "learning_rate": 1.257413997627521e-06,
      "loss": 0.5964,
      "step": 106
    },
    {
      "epoch": 0.006347134891446198,
      "grad_norm": 21.516468048095703,
      "learning_rate": 1.2692763938315541e-06,
      "loss": 0.6704,
      "step": 107
    },
    {
      "epoch": 0.006406453909123265,
      "grad_norm": 28.318147659301758,
      "learning_rate": 1.2811387900355872e-06,
      "loss": 0.5377,
      "step": 108
    },
    {
      "epoch": 0.0064657729268003325,
      "grad_norm": 13.896384239196777,
      "learning_rate": 1.2930011862396206e-06,
      "loss": 0.3127,
      "step": 109
    },
    {
      "epoch": 0.006525091944477399,
      "grad_norm": 7.5415263175964355,
      "learning_rate": 1.3048635824436536e-06,
      "loss": 0.4258,
      "step": 110
    },
    {
      "epoch": 0.006584410962154467,
      "grad_norm": 32.90008544921875,
      "learning_rate": 1.316725978647687e-06,
      "loss": 0.6629,
      "step": 111
    },
    {
      "epoch": 0.006643729979831534,
      "grad_norm": 27.36944007873535,
      "learning_rate": 1.32858837485172e-06,
      "loss": 0.7083,
      "step": 112
    },
    {
      "epoch": 0.006703048997508601,
      "grad_norm": 22.981767654418945,
      "learning_rate": 1.3404507710557536e-06,
      "loss": 0.7959,
      "step": 113
    },
    {
      "epoch": 0.006762368015185668,
      "grad_norm": 63.309547424316406,
      "learning_rate": 1.3523131672597867e-06,
      "loss": 0.4014,
      "step": 114
    },
    {
      "epoch": 0.006821687032862736,
      "grad_norm": 32.34077453613281,
      "learning_rate": 1.3641755634638198e-06,
      "loss": 1.0936,
      "step": 115
    },
    {
      "epoch": 0.006881006050539803,
      "grad_norm": 14.484752655029297,
      "learning_rate": 1.376037959667853e-06,
      "loss": 0.4515,
      "step": 116
    },
    {
      "epoch": 0.00694032506821687,
      "grad_norm": 6.370797157287598,
      "learning_rate": 1.3879003558718862e-06,
      "loss": 0.0635,
      "step": 117
    },
    {
      "epoch": 0.006999644085893938,
      "grad_norm": 188.53309631347656,
      "learning_rate": 1.3997627520759195e-06,
      "loss": 0.8154,
      "step": 118
    },
    {
      "epoch": 0.007058963103571005,
      "grad_norm": 51.5035400390625,
      "learning_rate": 1.4116251482799526e-06,
      "loss": 0.8029,
      "step": 119
    },
    {
      "epoch": 0.007118282121248072,
      "grad_norm": 20.599512100219727,
      "learning_rate": 1.423487544483986e-06,
      "loss": 0.7136,
      "step": 120
    },
    {
      "epoch": 0.007177601138925139,
      "grad_norm": 35.200252532958984,
      "learning_rate": 1.4353499406880192e-06,
      "loss": 0.1694,
      "step": 121
    },
    {
      "epoch": 0.007236920156602207,
      "grad_norm": 47.10051345825195,
      "learning_rate": 1.4472123368920523e-06,
      "loss": 1.007,
      "step": 122
    },
    {
      "epoch": 0.007296239174279274,
      "grad_norm": 35.89066696166992,
      "learning_rate": 1.4590747330960856e-06,
      "loss": 0.8836,
      "step": 123
    },
    {
      "epoch": 0.007355558191956341,
      "grad_norm": 19.53022575378418,
      "learning_rate": 1.4709371293001187e-06,
      "loss": 1.0838,
      "step": 124
    },
    {
      "epoch": 0.007414877209633408,
      "grad_norm": 17.390295028686523,
      "learning_rate": 1.482799525504152e-06,
      "loss": 0.7298,
      "step": 125
    },
    {
      "epoch": 0.007474196227310476,
      "grad_norm": 40.79446029663086,
      "learning_rate": 1.494661921708185e-06,
      "loss": 0.833,
      "step": 126
    },
    {
      "epoch": 0.007533515244987543,
      "grad_norm": 42.40686798095703,
      "learning_rate": 1.5065243179122182e-06,
      "loss": 1.1949,
      "step": 127
    },
    {
      "epoch": 0.00759283426266461,
      "grad_norm": 13.021488189697266,
      "learning_rate": 1.5183867141162517e-06,
      "loss": 0.427,
      "step": 128
    },
    {
      "epoch": 0.007652153280341678,
      "grad_norm": 93.64767456054688,
      "learning_rate": 1.5302491103202848e-06,
      "loss": 0.45,
      "step": 129
    },
    {
      "epoch": 0.007711472298018745,
      "grad_norm": 26.62034797668457,
      "learning_rate": 1.542111506524318e-06,
      "loss": 1.4897,
      "step": 130
    },
    {
      "epoch": 0.007770791315695812,
      "grad_norm": 5.791799545288086,
      "learning_rate": 1.5539739027283512e-06,
      "loss": 0.0789,
      "step": 131
    },
    {
      "epoch": 0.00783011033337288,
      "grad_norm": 45.0703010559082,
      "learning_rate": 1.5658362989323845e-06,
      "loss": 0.8315,
      "step": 132
    },
    {
      "epoch": 0.007889429351049946,
      "grad_norm": 19.701461791992188,
      "learning_rate": 1.5776986951364176e-06,
      "loss": 1.6172,
      "step": 133
    },
    {
      "epoch": 0.007948748368727014,
      "grad_norm": 17.200759887695312,
      "learning_rate": 1.5895610913404507e-06,
      "loss": 0.1106,
      "step": 134
    },
    {
      "epoch": 0.008008067386404081,
      "grad_norm": 49.57864761352539,
      "learning_rate": 1.6014234875444842e-06,
      "loss": 0.4637,
      "step": 135
    },
    {
      "epoch": 0.008067386404081148,
      "grad_norm": 8.914223670959473,
      "learning_rate": 1.6132858837485173e-06,
      "loss": 0.3755,
      "step": 136
    },
    {
      "epoch": 0.008126705421758215,
      "grad_norm": 32.548248291015625,
      "learning_rate": 1.6251482799525506e-06,
      "loss": 0.1224,
      "step": 137
    },
    {
      "epoch": 0.008186024439435283,
      "grad_norm": 9.139509201049805,
      "learning_rate": 1.6370106761565837e-06,
      "loss": 0.1059,
      "step": 138
    },
    {
      "epoch": 0.00824534345711235,
      "grad_norm": 61.039878845214844,
      "learning_rate": 1.648873072360617e-06,
      "loss": 1.2518,
      "step": 139
    },
    {
      "epoch": 0.008304662474789417,
      "grad_norm": 10.652982711791992,
      "learning_rate": 1.6607354685646501e-06,
      "loss": 1.2194,
      "step": 140
    },
    {
      "epoch": 0.008363981492466485,
      "grad_norm": 22.739835739135742,
      "learning_rate": 1.6725978647686832e-06,
      "loss": 0.3085,
      "step": 141
    },
    {
      "epoch": 0.008423300510143552,
      "grad_norm": 39.32793045043945,
      "learning_rate": 1.6844602609727167e-06,
      "loss": 0.9252,
      "step": 142
    },
    {
      "epoch": 0.008482619527820619,
      "grad_norm": 23.002059936523438,
      "learning_rate": 1.69632265717675e-06,
      "loss": 0.3482,
      "step": 143
    },
    {
      "epoch": 0.008541938545497686,
      "grad_norm": 24.187681198120117,
      "learning_rate": 1.7081850533807831e-06,
      "loss": 0.3614,
      "step": 144
    },
    {
      "epoch": 0.008601257563174754,
      "grad_norm": 13.541705131530762,
      "learning_rate": 1.7200474495848162e-06,
      "loss": 0.1397,
      "step": 145
    },
    {
      "epoch": 0.008660576580851821,
      "grad_norm": 21.693809509277344,
      "learning_rate": 1.7319098457888495e-06,
      "loss": 0.3884,
      "step": 146
    },
    {
      "epoch": 0.008719895598528888,
      "grad_norm": 36.69486618041992,
      "learning_rate": 1.7437722419928826e-06,
      "loss": 0.4732,
      "step": 147
    },
    {
      "epoch": 0.008779214616205955,
      "grad_norm": 66.3919906616211,
      "learning_rate": 1.7556346381969157e-06,
      "loss": 1.2716,
      "step": 148
    },
    {
      "epoch": 0.008838533633883023,
      "grad_norm": 2.669985294342041,
      "learning_rate": 1.7674970344009492e-06,
      "loss": 0.0197,
      "step": 149
    },
    {
      "epoch": 0.00889785265156009,
      "grad_norm": 17.826412200927734,
      "learning_rate": 1.7793594306049826e-06,
      "loss": 0.4452,
      "step": 150
    },
    {
      "epoch": 0.008957171669237157,
      "grad_norm": 46.85899353027344,
      "learning_rate": 1.7912218268090156e-06,
      "loss": 1.4407,
      "step": 151
    },
    {
      "epoch": 0.009016490686914225,
      "grad_norm": 29.015804290771484,
      "learning_rate": 1.8030842230130487e-06,
      "loss": 0.8162,
      "step": 152
    },
    {
      "epoch": 0.009075809704591292,
      "grad_norm": 22.666688919067383,
      "learning_rate": 1.814946619217082e-06,
      "loss": 0.103,
      "step": 153
    },
    {
      "epoch": 0.009135128722268359,
      "grad_norm": 15.232190132141113,
      "learning_rate": 1.8268090154211151e-06,
      "loss": 0.1729,
      "step": 154
    },
    {
      "epoch": 0.009194447739945426,
      "grad_norm": 20.400596618652344,
      "learning_rate": 1.8386714116251482e-06,
      "loss": 0.3859,
      "step": 155
    },
    {
      "epoch": 0.009253766757622494,
      "grad_norm": 10.00393009185791,
      "learning_rate": 1.8505338078291815e-06,
      "loss": 0.2085,
      "step": 156
    },
    {
      "epoch": 0.009313085775299561,
      "grad_norm": 74.92560577392578,
      "learning_rate": 1.862396204033215e-06,
      "loss": 0.8395,
      "step": 157
    },
    {
      "epoch": 0.009372404792976628,
      "grad_norm": 16.837371826171875,
      "learning_rate": 1.8742586002372482e-06,
      "loss": 0.4283,
      "step": 158
    },
    {
      "epoch": 0.009431723810653695,
      "grad_norm": 16.395610809326172,
      "learning_rate": 1.8861209964412813e-06,
      "loss": 0.2252,
      "step": 159
    },
    {
      "epoch": 0.009491042828330763,
      "grad_norm": 30.830352783203125,
      "learning_rate": 1.8979833926453146e-06,
      "loss": 0.5286,
      "step": 160
    },
    {
      "epoch": 0.00955036184600783,
      "grad_norm": 13.937146186828613,
      "learning_rate": 1.909845788849348e-06,
      "loss": 0.2031,
      "step": 161
    },
    {
      "epoch": 0.009609680863684897,
      "grad_norm": 35.293121337890625,
      "learning_rate": 1.921708185053381e-06,
      "loss": 0.8141,
      "step": 162
    },
    {
      "epoch": 0.009668999881361965,
      "grad_norm": 25.00118064880371,
      "learning_rate": 1.933570581257414e-06,
      "loss": 0.4485,
      "step": 163
    },
    {
      "epoch": 0.009728318899039032,
      "grad_norm": 14.270784378051758,
      "learning_rate": 1.9454329774614476e-06,
      "loss": 0.4613,
      "step": 164
    },
    {
      "epoch": 0.009787637916716099,
      "grad_norm": 29.038301467895508,
      "learning_rate": 1.9572953736654807e-06,
      "loss": 0.81,
      "step": 165
    },
    {
      "epoch": 0.009846956934393166,
      "grad_norm": 9.776226997375488,
      "learning_rate": 1.9691577698695138e-06,
      "loss": 0.1892,
      "step": 166
    },
    {
      "epoch": 0.009906275952070234,
      "grad_norm": 1.7161364555358887,
      "learning_rate": 1.981020166073547e-06,
      "loss": 0.0226,
      "step": 167
    },
    {
      "epoch": 0.009965594969747301,
      "grad_norm": 24.420392990112305,
      "learning_rate": 1.9928825622775804e-06,
      "loss": 0.3319,
      "step": 168
    },
    {
      "epoch": 0.010024913987424368,
      "grad_norm": 36.88396453857422,
      "learning_rate": 2.0047449584816135e-06,
      "loss": 0.4658,
      "step": 169
    },
    {
      "epoch": 0.010084233005101436,
      "grad_norm": 29.969589233398438,
      "learning_rate": 2.0166073546856466e-06,
      "loss": 0.2696,
      "step": 170
    },
    {
      "epoch": 0.010143552022778503,
      "grad_norm": 18.385299682617188,
      "learning_rate": 2.02846975088968e-06,
      "loss": 0.3636,
      "step": 171
    },
    {
      "epoch": 0.01020287104045557,
      "grad_norm": 31.880414962768555,
      "learning_rate": 2.040332147093713e-06,
      "loss": 0.3482,
      "step": 172
    },
    {
      "epoch": 0.010262190058132637,
      "grad_norm": 28.632469177246094,
      "learning_rate": 2.0521945432977463e-06,
      "loss": 0.5212,
      "step": 173
    },
    {
      "epoch": 0.010321509075809705,
      "grad_norm": 46.48494338989258,
      "learning_rate": 2.0640569395017794e-06,
      "loss": 0.609,
      "step": 174
    },
    {
      "epoch": 0.010380828093486772,
      "grad_norm": 17.78241539001465,
      "learning_rate": 2.075919335705813e-06,
      "loss": 0.1559,
      "step": 175
    },
    {
      "epoch": 0.010440147111163839,
      "grad_norm": 53.983882904052734,
      "learning_rate": 2.087781731909846e-06,
      "loss": 2.5247,
      "step": 176
    },
    {
      "epoch": 0.010499466128840906,
      "grad_norm": 35.81573486328125,
      "learning_rate": 2.099644128113879e-06,
      "loss": 0.7034,
      "step": 177
    },
    {
      "epoch": 0.010558785146517974,
      "grad_norm": 20.41644287109375,
      "learning_rate": 2.111506524317912e-06,
      "loss": 0.6194,
      "step": 178
    },
    {
      "epoch": 0.010618104164195041,
      "grad_norm": 36.88861846923828,
      "learning_rate": 2.1233689205219457e-06,
      "loss": 0.5353,
      "step": 179
    },
    {
      "epoch": 0.010677423181872108,
      "grad_norm": 2.2670767307281494,
      "learning_rate": 2.135231316725979e-06,
      "loss": 0.0316,
      "step": 180
    },
    {
      "epoch": 0.010736742199549176,
      "grad_norm": 5.573067665100098,
      "learning_rate": 2.147093712930012e-06,
      "loss": 0.1052,
      "step": 181
    },
    {
      "epoch": 0.010796061217226243,
      "grad_norm": 19.03733253479004,
      "learning_rate": 2.1589561091340454e-06,
      "loss": 0.1763,
      "step": 182
    },
    {
      "epoch": 0.01085538023490331,
      "grad_norm": 15.619942665100098,
      "learning_rate": 2.1708185053380785e-06,
      "loss": 0.1793,
      "step": 183
    },
    {
      "epoch": 0.010914699252580377,
      "grad_norm": 32.225074768066406,
      "learning_rate": 2.1826809015421116e-06,
      "loss": 0.1697,
      "step": 184
    },
    {
      "epoch": 0.010974018270257445,
      "grad_norm": 16.86585235595703,
      "learning_rate": 2.1945432977461447e-06,
      "loss": 0.3968,
      "step": 185
    },
    {
      "epoch": 0.011033337287934512,
      "grad_norm": 3.2731096744537354,
      "learning_rate": 2.2064056939501782e-06,
      "loss": 0.0371,
      "step": 186
    },
    {
      "epoch": 0.011092656305611579,
      "grad_norm": 60.51643371582031,
      "learning_rate": 2.2182680901542113e-06,
      "loss": 0.656,
      "step": 187
    },
    {
      "epoch": 0.011151975323288646,
      "grad_norm": 32.551273345947266,
      "learning_rate": 2.2301304863582444e-06,
      "loss": 0.3008,
      "step": 188
    },
    {
      "epoch": 0.011211294340965714,
      "grad_norm": 26.363264083862305,
      "learning_rate": 2.241992882562278e-06,
      "loss": 0.4331,
      "step": 189
    },
    {
      "epoch": 0.011270613358642781,
      "grad_norm": 35.42939376831055,
      "learning_rate": 2.253855278766311e-06,
      "loss": 0.406,
      "step": 190
    },
    {
      "epoch": 0.011329932376319848,
      "grad_norm": 25.44184684753418,
      "learning_rate": 2.265717674970344e-06,
      "loss": 0.499,
      "step": 191
    },
    {
      "epoch": 0.011389251393996916,
      "grad_norm": 11.333423614501953,
      "learning_rate": 2.2775800711743772e-06,
      "loss": 0.2562,
      "step": 192
    },
    {
      "epoch": 0.011448570411673983,
      "grad_norm": 5.15677547454834,
      "learning_rate": 2.2894424673784107e-06,
      "loss": 0.0469,
      "step": 193
    },
    {
      "epoch": 0.01150788942935105,
      "grad_norm": 76.04222869873047,
      "learning_rate": 2.301304863582444e-06,
      "loss": 0.6752,
      "step": 194
    },
    {
      "epoch": 0.011567208447028117,
      "grad_norm": 1.8089264631271362,
      "learning_rate": 2.313167259786477e-06,
      "loss": 0.0234,
      "step": 195
    },
    {
      "epoch": 0.011626527464705185,
      "grad_norm": 61.26433563232422,
      "learning_rate": 2.3250296559905105e-06,
      "loss": 1.7952,
      "step": 196
    },
    {
      "epoch": 0.011685846482382252,
      "grad_norm": 25.42932891845703,
      "learning_rate": 2.3368920521945436e-06,
      "loss": 0.165,
      "step": 197
    },
    {
      "epoch": 0.011745165500059319,
      "grad_norm": 2.3779232501983643,
      "learning_rate": 2.3487544483985766e-06,
      "loss": 0.0298,
      "step": 198
    },
    {
      "epoch": 0.011804484517736386,
      "grad_norm": 21.06014060974121,
      "learning_rate": 2.3606168446026097e-06,
      "loss": 0.3358,
      "step": 199
    },
    {
      "epoch": 0.011863803535413454,
      "grad_norm": 56.88042449951172,
      "learning_rate": 2.3724792408066433e-06,
      "loss": 0.7482,
      "step": 200
    },
    {
      "epoch": 0.01192312255309052,
      "grad_norm": 27.384723663330078,
      "learning_rate": 2.3843416370106764e-06,
      "loss": 0.9022,
      "step": 201
    },
    {
      "epoch": 0.011982441570767588,
      "grad_norm": 27.021595001220703,
      "learning_rate": 2.3962040332147095e-06,
      "loss": 0.5832,
      "step": 202
    },
    {
      "epoch": 0.012041760588444656,
      "grad_norm": 19.63344955444336,
      "learning_rate": 2.408066429418743e-06,
      "loss": 0.1846,
      "step": 203
    },
    {
      "epoch": 0.012101079606121723,
      "grad_norm": 20.44239044189453,
      "learning_rate": 2.419928825622776e-06,
      "loss": 0.1031,
      "step": 204
    },
    {
      "epoch": 0.01216039862379879,
      "grad_norm": 10.51576042175293,
      "learning_rate": 2.431791221826809e-06,
      "loss": 0.314,
      "step": 205
    },
    {
      "epoch": 0.012219717641475857,
      "grad_norm": 24.19548225402832,
      "learning_rate": 2.4436536180308423e-06,
      "loss": 0.3095,
      "step": 206
    },
    {
      "epoch": 0.012279036659152925,
      "grad_norm": 84.45732116699219,
      "learning_rate": 2.4555160142348754e-06,
      "loss": 1.3594,
      "step": 207
    },
    {
      "epoch": 0.012338355676829992,
      "grad_norm": 5.653714179992676,
      "learning_rate": 2.467378410438909e-06,
      "loss": 0.083,
      "step": 208
    },
    {
      "epoch": 0.012397674694507059,
      "grad_norm": 2.0013551712036133,
      "learning_rate": 2.479240806642942e-06,
      "loss": 0.0182,
      "step": 209
    },
    {
      "epoch": 0.012456993712184125,
      "grad_norm": 1.34891939163208,
      "learning_rate": 2.4911032028469755e-06,
      "loss": 0.0126,
      "step": 210
    },
    {
      "epoch": 0.012516312729861194,
      "grad_norm": 2.7314023971557617,
      "learning_rate": 2.5029655990510086e-06,
      "loss": 0.0605,
      "step": 211
    },
    {
      "epoch": 0.01257563174753826,
      "grad_norm": 24.924911499023438,
      "learning_rate": 2.514827995255042e-06,
      "loss": 0.1376,
      "step": 212
    },
    {
      "epoch": 0.012634950765215328,
      "grad_norm": 35.85737609863281,
      "learning_rate": 2.526690391459075e-06,
      "loss": 0.0892,
      "step": 213
    },
    {
      "epoch": 0.012694269782892396,
      "grad_norm": 27.998605728149414,
      "learning_rate": 2.5385527876631083e-06,
      "loss": 0.6387,
      "step": 214
    },
    {
      "epoch": 0.012753588800569463,
      "grad_norm": 33.4176139831543,
      "learning_rate": 2.5504151838671414e-06,
      "loss": 0.704,
      "step": 215
    },
    {
      "epoch": 0.01281290781824653,
      "grad_norm": 28.455434799194336,
      "learning_rate": 2.5622775800711745e-06,
      "loss": 0.4288,
      "step": 216
    },
    {
      "epoch": 0.012872226835923596,
      "grad_norm": 18.30112648010254,
      "learning_rate": 2.574139976275208e-06,
      "loss": 0.1971,
      "step": 217
    },
    {
      "epoch": 0.012931545853600665,
      "grad_norm": 1.3666890859603882,
      "learning_rate": 2.586002372479241e-06,
      "loss": 0.0172,
      "step": 218
    },
    {
      "epoch": 0.012990864871277732,
      "grad_norm": 36.8421630859375,
      "learning_rate": 2.597864768683274e-06,
      "loss": 1.2668,
      "step": 219
    },
    {
      "epoch": 0.013050183888954799,
      "grad_norm": 16.2437801361084,
      "learning_rate": 2.6097271648873073e-06,
      "loss": 0.2747,
      "step": 220
    },
    {
      "epoch": 0.013109502906631865,
      "grad_norm": 12.883844375610352,
      "learning_rate": 2.6215895610913404e-06,
      "loss": 0.7401,
      "step": 221
    },
    {
      "epoch": 0.013168821924308934,
      "grad_norm": 33.96721267700195,
      "learning_rate": 2.633451957295374e-06,
      "loss": 0.9313,
      "step": 222
    },
    {
      "epoch": 0.013228140941986,
      "grad_norm": 61.47676086425781,
      "learning_rate": 2.645314353499407e-06,
      "loss": 2.9222,
      "step": 223
    },
    {
      "epoch": 0.013287459959663067,
      "grad_norm": 3.9019455909729004,
      "learning_rate": 2.65717674970344e-06,
      "loss": 0.0332,
      "step": 224
    },
    {
      "epoch": 0.013346778977340136,
      "grad_norm": 12.74038028717041,
      "learning_rate": 2.669039145907473e-06,
      "loss": 0.2131,
      "step": 225
    },
    {
      "epoch": 0.013406097995017203,
      "grad_norm": 41.14720916748047,
      "learning_rate": 2.680901542111507e-06,
      "loss": 1.7224,
      "step": 226
    },
    {
      "epoch": 0.01346541701269427,
      "grad_norm": 30.8498477935791,
      "learning_rate": 2.6927639383155402e-06,
      "loss": 0.3343,
      "step": 227
    },
    {
      "epoch": 0.013524736030371336,
      "grad_norm": 4.7135910987854,
      "learning_rate": 2.7046263345195733e-06,
      "loss": 0.05,
      "step": 228
    },
    {
      "epoch": 0.013584055048048405,
      "grad_norm": 31.424684524536133,
      "learning_rate": 2.7164887307236064e-06,
      "loss": 0.6413,
      "step": 229
    },
    {
      "epoch": 0.013643374065725472,
      "grad_norm": 23.154830932617188,
      "learning_rate": 2.7283511269276395e-06,
      "loss": 0.1569,
      "step": 230
    },
    {
      "epoch": 0.013702693083402539,
      "grad_norm": 43.15333557128906,
      "learning_rate": 2.740213523131673e-06,
      "loss": 0.5052,
      "step": 231
    },
    {
      "epoch": 0.013762012101079605,
      "grad_norm": 47.13294219970703,
      "learning_rate": 2.752075919335706e-06,
      "loss": 0.2459,
      "step": 232
    },
    {
      "epoch": 0.013821331118756674,
      "grad_norm": 12.980045318603516,
      "learning_rate": 2.7639383155397392e-06,
      "loss": 0.2657,
      "step": 233
    },
    {
      "epoch": 0.01388065013643374,
      "grad_norm": 36.940128326416016,
      "learning_rate": 2.7758007117437723e-06,
      "loss": 0.6207,
      "step": 234
    },
    {
      "epoch": 0.013939969154110807,
      "grad_norm": 16.513761520385742,
      "learning_rate": 2.7876631079478054e-06,
      "loss": 0.2994,
      "step": 235
    },
    {
      "epoch": 0.013999288171787876,
      "grad_norm": 75.68594360351562,
      "learning_rate": 2.799525504151839e-06,
      "loss": 0.7756,
      "step": 236
    },
    {
      "epoch": 0.014058607189464943,
      "grad_norm": 89.40553283691406,
      "learning_rate": 2.811387900355872e-06,
      "loss": 0.8364,
      "step": 237
    },
    {
      "epoch": 0.01411792620714201,
      "grad_norm": 22.8454532623291,
      "learning_rate": 2.823250296559905e-06,
      "loss": 0.3833,
      "step": 238
    },
    {
      "epoch": 0.014177245224819076,
      "grad_norm": 33.39584732055664,
      "learning_rate": 2.8351126927639382e-06,
      "loss": 0.3642,
      "step": 239
    },
    {
      "epoch": 0.014236564242496145,
      "grad_norm": 9.252045631408691,
      "learning_rate": 2.846975088967972e-06,
      "loss": 0.1105,
      "step": 240
    },
    {
      "epoch": 0.014295883260173212,
      "grad_norm": 26.20060157775879,
      "learning_rate": 2.8588374851720053e-06,
      "loss": 1.181,
      "step": 241
    },
    {
      "epoch": 0.014355202277850278,
      "grad_norm": 24.104169845581055,
      "learning_rate": 2.8706998813760384e-06,
      "loss": 0.9153,
      "step": 242
    },
    {
      "epoch": 0.014414521295527345,
      "grad_norm": 11.22342300415039,
      "learning_rate": 2.8825622775800715e-06,
      "loss": 0.0622,
      "step": 243
    },
    {
      "epoch": 0.014473840313204414,
      "grad_norm": 25.888505935668945,
      "learning_rate": 2.8944246737841046e-06,
      "loss": 0.8295,
      "step": 244
    },
    {
      "epoch": 0.01453315933088148,
      "grad_norm": 33.810585021972656,
      "learning_rate": 2.906287069988138e-06,
      "loss": 0.6895,
      "step": 245
    },
    {
      "epoch": 0.014592478348558547,
      "grad_norm": 26.957345962524414,
      "learning_rate": 2.918149466192171e-06,
      "loss": 0.4937,
      "step": 246
    },
    {
      "epoch": 0.014651797366235616,
      "grad_norm": 15.080723762512207,
      "learning_rate": 2.9300118623962043e-06,
      "loss": 0.4153,
      "step": 247
    },
    {
      "epoch": 0.014711116383912683,
      "grad_norm": 5.338090896606445,
      "learning_rate": 2.9418742586002374e-06,
      "loss": 0.0371,
      "step": 248
    },
    {
      "epoch": 0.01477043540158975,
      "grad_norm": 5.7739973068237305,
      "learning_rate": 2.9537366548042705e-06,
      "loss": 0.0639,
      "step": 249
    },
    {
      "epoch": 0.014829754419266816,
      "grad_norm": 39.10357666015625,
      "learning_rate": 2.965599051008304e-06,
      "loss": 0.3755,
      "step": 250
    },
    {
      "epoch": 0.014889073436943885,
      "grad_norm": 19.85245132446289,
      "learning_rate": 2.977461447212337e-06,
      "loss": 0.1217,
      "step": 251
    },
    {
      "epoch": 0.014948392454620952,
      "grad_norm": 1.926372766494751,
      "learning_rate": 2.98932384341637e-06,
      "loss": 0.018,
      "step": 252
    },
    {
      "epoch": 0.015007711472298018,
      "grad_norm": 19.442176818847656,
      "learning_rate": 3.0011862396204033e-06,
      "loss": 0.2821,
      "step": 253
    },
    {
      "epoch": 0.015067030489975085,
      "grad_norm": 113.26595306396484,
      "learning_rate": 3.0130486358244364e-06,
      "loss": 2.7842,
      "step": 254
    },
    {
      "epoch": 0.015126349507652154,
      "grad_norm": 17.0638484954834,
      "learning_rate": 3.0249110320284703e-06,
      "loss": 0.1368,
      "step": 255
    },
    {
      "epoch": 0.01518566852532922,
      "grad_norm": 11.766704559326172,
      "learning_rate": 3.0367734282325034e-06,
      "loss": 0.0689,
      "step": 256
    },
    {
      "epoch": 0.015244987543006287,
      "grad_norm": 3.803536891937256,
      "learning_rate": 3.0486358244365365e-06,
      "loss": 0.0511,
      "step": 257
    },
    {
      "epoch": 0.015304306560683356,
      "grad_norm": 51.72078323364258,
      "learning_rate": 3.0604982206405696e-06,
      "loss": 1.0352,
      "step": 258
    },
    {
      "epoch": 0.015363625578360423,
      "grad_norm": 21.064504623413086,
      "learning_rate": 3.072360616844603e-06,
      "loss": 1.5483,
      "step": 259
    },
    {
      "epoch": 0.01542294459603749,
      "grad_norm": 10.774333000183105,
      "learning_rate": 3.084223013048636e-06,
      "loss": 0.0444,
      "step": 260
    },
    {
      "epoch": 0.015482263613714556,
      "grad_norm": 15.908637046813965,
      "learning_rate": 3.0960854092526693e-06,
      "loss": 0.2588,
      "step": 261
    },
    {
      "epoch": 0.015541582631391625,
      "grad_norm": 37.355289459228516,
      "learning_rate": 3.1079478054567024e-06,
      "loss": 0.6238,
      "step": 262
    },
    {
      "epoch": 0.015600901649068692,
      "grad_norm": 80.33399200439453,
      "learning_rate": 3.1198102016607355e-06,
      "loss": 0.468,
      "step": 263
    },
    {
      "epoch": 0.01566022066674576,
      "grad_norm": 61.06782150268555,
      "learning_rate": 3.131672597864769e-06,
      "loss": 0.7477,
      "step": 264
    },
    {
      "epoch": 0.015719539684422825,
      "grad_norm": 16.136411666870117,
      "learning_rate": 3.143534994068802e-06,
      "loss": 0.1311,
      "step": 265
    },
    {
      "epoch": 0.015778858702099892,
      "grad_norm": 38.80807113647461,
      "learning_rate": 3.155397390272835e-06,
      "loss": 0.6732,
      "step": 266
    },
    {
      "epoch": 0.015838177719776962,
      "grad_norm": 1.432082176208496,
      "learning_rate": 3.1672597864768683e-06,
      "loss": 0.0107,
      "step": 267
    },
    {
      "epoch": 0.01589749673745403,
      "grad_norm": 10.857145309448242,
      "learning_rate": 3.1791221826809014e-06,
      "loss": 0.1299,
      "step": 268
    },
    {
      "epoch": 0.015956815755131096,
      "grad_norm": 3.3091769218444824,
      "learning_rate": 3.1909845788849353e-06,
      "loss": 0.0305,
      "step": 269
    },
    {
      "epoch": 0.016016134772808163,
      "grad_norm": 3.513911008834839,
      "learning_rate": 3.2028469750889684e-06,
      "loss": 0.0341,
      "step": 270
    },
    {
      "epoch": 0.01607545379048523,
      "grad_norm": 10.538867950439453,
      "learning_rate": 3.2147093712930015e-06,
      "loss": 0.1392,
      "step": 271
    },
    {
      "epoch": 0.016134772808162296,
      "grad_norm": 4.503777503967285,
      "learning_rate": 3.2265717674970346e-06,
      "loss": 0.0286,
      "step": 272
    },
    {
      "epoch": 0.016194091825839363,
      "grad_norm": 59.96050262451172,
      "learning_rate": 3.238434163701068e-06,
      "loss": 0.7089,
      "step": 273
    },
    {
      "epoch": 0.01625341084351643,
      "grad_norm": 37.533531188964844,
      "learning_rate": 3.2502965599051012e-06,
      "loss": 0.7658,
      "step": 274
    },
    {
      "epoch": 0.0163127298611935,
      "grad_norm": 41.40412139892578,
      "learning_rate": 3.2621589561091343e-06,
      "loss": 0.5386,
      "step": 275
    },
    {
      "epoch": 0.016372048878870567,
      "grad_norm": 11.515114784240723,
      "learning_rate": 3.2740213523131674e-06,
      "loss": 0.4656,
      "step": 276
    },
    {
      "epoch": 0.016431367896547634,
      "grad_norm": 16.591602325439453,
      "learning_rate": 3.2858837485172005e-06,
      "loss": 0.441,
      "step": 277
    },
    {
      "epoch": 0.0164906869142247,
      "grad_norm": 11.709247589111328,
      "learning_rate": 3.297746144721234e-06,
      "loss": 1.147,
      "step": 278
    },
    {
      "epoch": 0.016550005931901767,
      "grad_norm": 5.741352081298828,
      "learning_rate": 3.309608540925267e-06,
      "loss": 0.0794,
      "step": 279
    },
    {
      "epoch": 0.016609324949578834,
      "grad_norm": 4.3433146476745605,
      "learning_rate": 3.3214709371293002e-06,
      "loss": 0.0383,
      "step": 280
    },
    {
      "epoch": 0.0166686439672559,
      "grad_norm": 18.72660255432129,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.5869,
      "step": 281
    },
    {
      "epoch": 0.01672796298493297,
      "grad_norm": 5.562717914581299,
      "learning_rate": 3.3451957295373664e-06,
      "loss": 0.0383,
      "step": 282
    },
    {
      "epoch": 0.016787282002610038,
      "grad_norm": 36.744285583496094,
      "learning_rate": 3.3570581257414e-06,
      "loss": 0.6589,
      "step": 283
    },
    {
      "epoch": 0.016846601020287105,
      "grad_norm": 30.9134521484375,
      "learning_rate": 3.3689205219454335e-06,
      "loss": 0.6273,
      "step": 284
    },
    {
      "epoch": 0.01690592003796417,
      "grad_norm": 13.87418270111084,
      "learning_rate": 3.3807829181494666e-06,
      "loss": 0.15,
      "step": 285
    },
    {
      "epoch": 0.016965239055641238,
      "grad_norm": 21.795852661132812,
      "learning_rate": 3.3926453143535e-06,
      "loss": 0.4788,
      "step": 286
    },
    {
      "epoch": 0.017024558073318305,
      "grad_norm": 33.53510284423828,
      "learning_rate": 3.404507710557533e-06,
      "loss": 0.2789,
      "step": 287
    },
    {
      "epoch": 0.017083877090995372,
      "grad_norm": 48.63550567626953,
      "learning_rate": 3.4163701067615663e-06,
      "loss": 0.7778,
      "step": 288
    },
    {
      "epoch": 0.017143196108672442,
      "grad_norm": 27.542850494384766,
      "learning_rate": 3.4282325029655994e-06,
      "loss": 0.4126,
      "step": 289
    },
    {
      "epoch": 0.01720251512634951,
      "grad_norm": 9.147525787353516,
      "learning_rate": 3.4400948991696325e-06,
      "loss": 0.1297,
      "step": 290
    },
    {
      "epoch": 0.017261834144026576,
      "grad_norm": 13.290396690368652,
      "learning_rate": 3.4519572953736656e-06,
      "loss": 0.2147,
      "step": 291
    },
    {
      "epoch": 0.017321153161703642,
      "grad_norm": 14.49785327911377,
      "learning_rate": 3.463819691577699e-06,
      "loss": 0.0737,
      "step": 292
    },
    {
      "epoch": 0.01738047217938071,
      "grad_norm": 49.71638488769531,
      "learning_rate": 3.475682087781732e-06,
      "loss": 0.5189,
      "step": 293
    },
    {
      "epoch": 0.017439791197057776,
      "grad_norm": 43.69709777832031,
      "learning_rate": 3.4875444839857653e-06,
      "loss": 0.5232,
      "step": 294
    },
    {
      "epoch": 0.017499110214734843,
      "grad_norm": 16.400053024291992,
      "learning_rate": 3.4994068801897984e-06,
      "loss": 0.4424,
      "step": 295
    },
    {
      "epoch": 0.01755842923241191,
      "grad_norm": 44.083168029785156,
      "learning_rate": 3.5112692763938315e-06,
      "loss": 0.2941,
      "step": 296
    },
    {
      "epoch": 0.01761774825008898,
      "grad_norm": 16.044960021972656,
      "learning_rate": 3.523131672597865e-06,
      "loss": 0.3248,
      "step": 297
    },
    {
      "epoch": 0.017677067267766047,
      "grad_norm": 13.611616134643555,
      "learning_rate": 3.5349940688018985e-06,
      "loss": 0.5154,
      "step": 298
    },
    {
      "epoch": 0.017736386285443113,
      "grad_norm": 3.828232526779175,
      "learning_rate": 3.5468564650059316e-06,
      "loss": 0.0546,
      "step": 299
    },
    {
      "epoch": 0.01779570530312018,
      "grad_norm": 15.169906616210938,
      "learning_rate": 3.558718861209965e-06,
      "loss": 0.2607,
      "step": 300
    },
    {
      "epoch": 0.017855024320797247,
      "grad_norm": 1.344543695449829,
      "learning_rate": 3.570581257413998e-06,
      "loss": 0.0091,
      "step": 301
    },
    {
      "epoch": 0.017914343338474314,
      "grad_norm": 38.31380844116211,
      "learning_rate": 3.5824436536180313e-06,
      "loss": 1.5884,
      "step": 302
    },
    {
      "epoch": 0.01797366235615138,
      "grad_norm": 36.779361724853516,
      "learning_rate": 3.5943060498220644e-06,
      "loss": 0.272,
      "step": 303
    },
    {
      "epoch": 0.01803298137382845,
      "grad_norm": 31.5524959564209,
      "learning_rate": 3.6061684460260975e-06,
      "loss": 0.4616,
      "step": 304
    },
    {
      "epoch": 0.018092300391505518,
      "grad_norm": 99.38426208496094,
      "learning_rate": 3.6180308422301306e-06,
      "loss": 1.6149,
      "step": 305
    },
    {
      "epoch": 0.018151619409182584,
      "grad_norm": 27.373435974121094,
      "learning_rate": 3.629893238434164e-06,
      "loss": 0.5038,
      "step": 306
    },
    {
      "epoch": 0.01821093842685965,
      "grad_norm": 24.156408309936523,
      "learning_rate": 3.641755634638197e-06,
      "loss": 0.7945,
      "step": 307
    },
    {
      "epoch": 0.018270257444536718,
      "grad_norm": 11.020200729370117,
      "learning_rate": 3.6536180308422303e-06,
      "loss": 0.333,
      "step": 308
    },
    {
      "epoch": 0.018329576462213785,
      "grad_norm": 13.830069541931152,
      "learning_rate": 3.6654804270462634e-06,
      "loss": 0.2732,
      "step": 309
    },
    {
      "epoch": 0.01838889547989085,
      "grad_norm": 27.35219955444336,
      "learning_rate": 3.6773428232502965e-06,
      "loss": 1.0108,
      "step": 310
    },
    {
      "epoch": 0.018448214497567922,
      "grad_norm": 14.183393478393555,
      "learning_rate": 3.68920521945433e-06,
      "loss": 0.4616,
      "step": 311
    },
    {
      "epoch": 0.01850753351524499,
      "grad_norm": 22.015954971313477,
      "learning_rate": 3.701067615658363e-06,
      "loss": 0.3403,
      "step": 312
    },
    {
      "epoch": 0.018566852532922055,
      "grad_norm": 20.875816345214844,
      "learning_rate": 3.7129300118623966e-06,
      "loss": 1.3939,
      "step": 313
    },
    {
      "epoch": 0.018626171550599122,
      "grad_norm": 0.5591548681259155,
      "learning_rate": 3.72479240806643e-06,
      "loss": 0.0118,
      "step": 314
    },
    {
      "epoch": 0.01868549056827619,
      "grad_norm": 19.980257034301758,
      "learning_rate": 3.7366548042704632e-06,
      "loss": 0.4552,
      "step": 315
    },
    {
      "epoch": 0.018744809585953256,
      "grad_norm": 1.2171714305877686,
      "learning_rate": 3.7485172004744963e-06,
      "loss": 0.0158,
      "step": 316
    },
    {
      "epoch": 0.018804128603630323,
      "grad_norm": 21.071542739868164,
      "learning_rate": 3.7603795966785294e-06,
      "loss": 0.2359,
      "step": 317
    },
    {
      "epoch": 0.01886344762130739,
      "grad_norm": 10.512090682983398,
      "learning_rate": 3.7722419928825625e-06,
      "loss": 0.25,
      "step": 318
    },
    {
      "epoch": 0.01892276663898446,
      "grad_norm": 24.257400512695312,
      "learning_rate": 3.784104389086596e-06,
      "loss": 0.258,
      "step": 319
    },
    {
      "epoch": 0.018982085656661526,
      "grad_norm": 25.846656799316406,
      "learning_rate": 3.795966785290629e-06,
      "loss": 0.1953,
      "step": 320
    },
    {
      "epoch": 0.019041404674338593,
      "grad_norm": 18.676658630371094,
      "learning_rate": 3.8078291814946622e-06,
      "loss": 0.3095,
      "step": 321
    },
    {
      "epoch": 0.01910072369201566,
      "grad_norm": 21.771339416503906,
      "learning_rate": 3.819691577698696e-06,
      "loss": 0.6922,
      "step": 322
    },
    {
      "epoch": 0.019160042709692727,
      "grad_norm": 1.901534080505371,
      "learning_rate": 3.831553973902728e-06,
      "loss": 0.0317,
      "step": 323
    },
    {
      "epoch": 0.019219361727369794,
      "grad_norm": 50.81045150756836,
      "learning_rate": 3.843416370106762e-06,
      "loss": 0.7865,
      "step": 324
    },
    {
      "epoch": 0.01927868074504686,
      "grad_norm": 5.654763221740723,
      "learning_rate": 3.855278766310795e-06,
      "loss": 0.0723,
      "step": 325
    },
    {
      "epoch": 0.01933799976272393,
      "grad_norm": 30.57217025756836,
      "learning_rate": 3.867141162514828e-06,
      "loss": 0.3189,
      "step": 326
    },
    {
      "epoch": 0.019397318780400997,
      "grad_norm": 0.913116991519928,
      "learning_rate": 3.879003558718862e-06,
      "loss": 0.0158,
      "step": 327
    },
    {
      "epoch": 0.019456637798078064,
      "grad_norm": 23.236278533935547,
      "learning_rate": 3.890865954922895e-06,
      "loss": 0.1286,
      "step": 328
    },
    {
      "epoch": 0.01951595681575513,
      "grad_norm": 53.033050537109375,
      "learning_rate": 3.902728351126928e-06,
      "loss": 0.2703,
      "step": 329
    },
    {
      "epoch": 0.019575275833432198,
      "grad_norm": 6.1879563331604,
      "learning_rate": 3.914590747330961e-06,
      "loss": 0.1885,
      "step": 330
    },
    {
      "epoch": 0.019634594851109265,
      "grad_norm": 13.236085891723633,
      "learning_rate": 3.926453143534995e-06,
      "loss": 0.2333,
      "step": 331
    },
    {
      "epoch": 0.01969391386878633,
      "grad_norm": 43.68854522705078,
      "learning_rate": 3.9383155397390276e-06,
      "loss": 1.0965,
      "step": 332
    },
    {
      "epoch": 0.0197532328864634,
      "grad_norm": 1.2685322761535645,
      "learning_rate": 3.950177935943061e-06,
      "loss": 0.0158,
      "step": 333
    },
    {
      "epoch": 0.01981255190414047,
      "grad_norm": 8.670794486999512,
      "learning_rate": 3.962040332147094e-06,
      "loss": 0.0994,
      "step": 334
    },
    {
      "epoch": 0.019871870921817535,
      "grad_norm": 7.495683670043945,
      "learning_rate": 3.973902728351127e-06,
      "loss": 0.0712,
      "step": 335
    },
    {
      "epoch": 0.019931189939494602,
      "grad_norm": 23.613725662231445,
      "learning_rate": 3.985765124555161e-06,
      "loss": 0.4037,
      "step": 336
    },
    {
      "epoch": 0.01999050895717167,
      "grad_norm": 3.2578365802764893,
      "learning_rate": 3.9976275207591935e-06,
      "loss": 0.0189,
      "step": 337
    },
    {
      "epoch": 0.020049827974848736,
      "grad_norm": 22.93348503112793,
      "learning_rate": 4.009489916963227e-06,
      "loss": 0.5356,
      "step": 338
    },
    {
      "epoch": 0.020109146992525802,
      "grad_norm": 40.78229904174805,
      "learning_rate": 4.02135231316726e-06,
      "loss": 0.5034,
      "step": 339
    },
    {
      "epoch": 0.020168466010202873,
      "grad_norm": 3.311279535293579,
      "learning_rate": 4.033214709371293e-06,
      "loss": 0.0254,
      "step": 340
    },
    {
      "epoch": 0.02022778502787994,
      "grad_norm": 36.30038070678711,
      "learning_rate": 4.045077105575327e-06,
      "loss": 0.8259,
      "step": 341
    },
    {
      "epoch": 0.020287104045557006,
      "grad_norm": 100.17850494384766,
      "learning_rate": 4.05693950177936e-06,
      "loss": 0.6997,
      "step": 342
    },
    {
      "epoch": 0.020346423063234073,
      "grad_norm": 31.999422073364258,
      "learning_rate": 4.068801897983393e-06,
      "loss": 0.3901,
      "step": 343
    },
    {
      "epoch": 0.02040574208091114,
      "grad_norm": 14.074980735778809,
      "learning_rate": 4.080664294187426e-06,
      "loss": 0.1819,
      "step": 344
    },
    {
      "epoch": 0.020465061098588207,
      "grad_norm": 3.4235565662384033,
      "learning_rate": 4.09252669039146e-06,
      "loss": 0.029,
      "step": 345
    },
    {
      "epoch": 0.020524380116265274,
      "grad_norm": 3.3697216510772705,
      "learning_rate": 4.104389086595493e-06,
      "loss": 0.0228,
      "step": 346
    },
    {
      "epoch": 0.02058369913394234,
      "grad_norm": 36.917118072509766,
      "learning_rate": 4.116251482799526e-06,
      "loss": 0.4915,
      "step": 347
    },
    {
      "epoch": 0.02064301815161941,
      "grad_norm": 79.53916931152344,
      "learning_rate": 4.128113879003559e-06,
      "loss": 1.3515,
      "step": 348
    },
    {
      "epoch": 0.020702337169296477,
      "grad_norm": 12.419602394104004,
      "learning_rate": 4.139976275207592e-06,
      "loss": 0.2074,
      "step": 349
    },
    {
      "epoch": 0.020761656186973544,
      "grad_norm": 56.59999465942383,
      "learning_rate": 4.151838671411626e-06,
      "loss": 0.3049,
      "step": 350
    },
    {
      "epoch": 0.02082097520465061,
      "grad_norm": 15.864811897277832,
      "learning_rate": 4.1637010676156585e-06,
      "loss": 0.9252,
      "step": 351
    },
    {
      "epoch": 0.020880294222327678,
      "grad_norm": 50.469215393066406,
      "learning_rate": 4.175563463819692e-06,
      "loss": 0.3773,
      "step": 352
    },
    {
      "epoch": 0.020939613240004745,
      "grad_norm": 14.335256576538086,
      "learning_rate": 4.187425860023725e-06,
      "loss": 0.3383,
      "step": 353
    },
    {
      "epoch": 0.02099893225768181,
      "grad_norm": 1.4576228857040405,
      "learning_rate": 4.199288256227758e-06,
      "loss": 0.013,
      "step": 354
    },
    {
      "epoch": 0.02105825127535888,
      "grad_norm": 11.569210052490234,
      "learning_rate": 4.211150652431792e-06,
      "loss": 1.0119,
      "step": 355
    },
    {
      "epoch": 0.02111757029303595,
      "grad_norm": 2.7433581352233887,
      "learning_rate": 4.223013048635824e-06,
      "loss": 0.0207,
      "step": 356
    },
    {
      "epoch": 0.021176889310713015,
      "grad_norm": 4.296839714050293,
      "learning_rate": 4.234875444839858e-06,
      "loss": 0.0413,
      "step": 357
    },
    {
      "epoch": 0.021236208328390082,
      "grad_norm": 43.312950134277344,
      "learning_rate": 4.2467378410438914e-06,
      "loss": 0.7749,
      "step": 358
    },
    {
      "epoch": 0.02129552734606715,
      "grad_norm": 25.598430633544922,
      "learning_rate": 4.258600237247925e-06,
      "loss": 1.2024,
      "step": 359
    },
    {
      "epoch": 0.021354846363744216,
      "grad_norm": 35.993343353271484,
      "learning_rate": 4.270462633451958e-06,
      "loss": 0.2212,
      "step": 360
    },
    {
      "epoch": 0.021414165381421282,
      "grad_norm": 4.499322414398193,
      "learning_rate": 4.282325029655991e-06,
      "loss": 0.0322,
      "step": 361
    },
    {
      "epoch": 0.021473484399098353,
      "grad_norm": 52.19636917114258,
      "learning_rate": 4.294187425860024e-06,
      "loss": 0.9175,
      "step": 362
    },
    {
      "epoch": 0.02153280341677542,
      "grad_norm": 563.0819091796875,
      "learning_rate": 4.306049822064057e-06,
      "loss": 0.6958,
      "step": 363
    },
    {
      "epoch": 0.021592122434452486,
      "grad_norm": 1.6662224531173706,
      "learning_rate": 4.317912218268091e-06,
      "loss": 0.0151,
      "step": 364
    },
    {
      "epoch": 0.021651441452129553,
      "grad_norm": 50.47195053100586,
      "learning_rate": 4.3297746144721235e-06,
      "loss": 1.4061,
      "step": 365
    },
    {
      "epoch": 0.02171076046980662,
      "grad_norm": 22.977746963500977,
      "learning_rate": 4.341637010676157e-06,
      "loss": 0.7272,
      "step": 366
    },
    {
      "epoch": 0.021770079487483687,
      "grad_norm": 42.62785339355469,
      "learning_rate": 4.35349940688019e-06,
      "loss": 1.2326,
      "step": 367
    },
    {
      "epoch": 0.021829398505160753,
      "grad_norm": 0.8876022100448608,
      "learning_rate": 4.365361803084223e-06,
      "loss": 0.0065,
      "step": 368
    },
    {
      "epoch": 0.02188871752283782,
      "grad_norm": 58.492164611816406,
      "learning_rate": 4.377224199288257e-06,
      "loss": 0.1667,
      "step": 369
    },
    {
      "epoch": 0.02194803654051489,
      "grad_norm": 26.588428497314453,
      "learning_rate": 4.389086595492289e-06,
      "loss": 0.2713,
      "step": 370
    },
    {
      "epoch": 0.022007355558191957,
      "grad_norm": 41.60142517089844,
      "learning_rate": 4.400948991696323e-06,
      "loss": 0.2405,
      "step": 371
    },
    {
      "epoch": 0.022066674575869024,
      "grad_norm": 23.66971206665039,
      "learning_rate": 4.4128113879003565e-06,
      "loss": 0.2179,
      "step": 372
    },
    {
      "epoch": 0.02212599359354609,
      "grad_norm": 8.879677772521973,
      "learning_rate": 4.42467378410439e-06,
      "loss": 0.0409,
      "step": 373
    },
    {
      "epoch": 0.022185312611223158,
      "grad_norm": 2.4198482036590576,
      "learning_rate": 4.436536180308423e-06,
      "loss": 0.0263,
      "step": 374
    },
    {
      "epoch": 0.022244631628900224,
      "grad_norm": 18.61821746826172,
      "learning_rate": 4.448398576512456e-06,
      "loss": 0.0844,
      "step": 375
    },
    {
      "epoch": 0.02230395064657729,
      "grad_norm": 0.2476583570241928,
      "learning_rate": 4.460260972716489e-06,
      "loss": 0.0025,
      "step": 376
    },
    {
      "epoch": 0.02236326966425436,
      "grad_norm": 7.290757656097412,
      "learning_rate": 4.472123368920522e-06,
      "loss": 0.0934,
      "step": 377
    },
    {
      "epoch": 0.022422588681931428,
      "grad_norm": 11.085472106933594,
      "learning_rate": 4.483985765124556e-06,
      "loss": 0.2579,
      "step": 378
    },
    {
      "epoch": 0.022481907699608495,
      "grad_norm": 98.82109069824219,
      "learning_rate": 4.4958481613285886e-06,
      "loss": 1.0618,
      "step": 379
    },
    {
      "epoch": 0.022541226717285562,
      "grad_norm": 29.299882888793945,
      "learning_rate": 4.507710557532622e-06,
      "loss": 0.5959,
      "step": 380
    },
    {
      "epoch": 0.02260054573496263,
      "grad_norm": 0.863355815410614,
      "learning_rate": 4.519572953736655e-06,
      "loss": 0.0079,
      "step": 381
    },
    {
      "epoch": 0.022659864752639695,
      "grad_norm": 11.506941795349121,
      "learning_rate": 4.531435349940688e-06,
      "loss": 0.3151,
      "step": 382
    },
    {
      "epoch": 0.022719183770316762,
      "grad_norm": 0.7523003816604614,
      "learning_rate": 4.543297746144722e-06,
      "loss": 0.0043,
      "step": 383
    },
    {
      "epoch": 0.022778502787993832,
      "grad_norm": 27.391460418701172,
      "learning_rate": 4.5551601423487545e-06,
      "loss": 0.5575,
      "step": 384
    },
    {
      "epoch": 0.0228378218056709,
      "grad_norm": 27.681787490844727,
      "learning_rate": 4.567022538552788e-06,
      "loss": 0.389,
      "step": 385
    },
    {
      "epoch": 0.022897140823347966,
      "grad_norm": 23.04844093322754,
      "learning_rate": 4.5788849347568215e-06,
      "loss": 0.2498,
      "step": 386
    },
    {
      "epoch": 0.022956459841025033,
      "grad_norm": 24.591554641723633,
      "learning_rate": 4.590747330960855e-06,
      "loss": 0.3716,
      "step": 387
    },
    {
      "epoch": 0.0230157788587021,
      "grad_norm": 86.09770965576172,
      "learning_rate": 4.602609727164888e-06,
      "loss": 0.9713,
      "step": 388
    },
    {
      "epoch": 0.023075097876379166,
      "grad_norm": 54.55867004394531,
      "learning_rate": 4.614472123368921e-06,
      "loss": 2.038,
      "step": 389
    },
    {
      "epoch": 0.023134416894056233,
      "grad_norm": 11.225473403930664,
      "learning_rate": 4.626334519572954e-06,
      "loss": 0.0836,
      "step": 390
    },
    {
      "epoch": 0.0231937359117333,
      "grad_norm": 7.734654903411865,
      "learning_rate": 4.638196915776987e-06,
      "loss": 0.0619,
      "step": 391
    },
    {
      "epoch": 0.02325305492941037,
      "grad_norm": 6.840951919555664,
      "learning_rate": 4.650059311981021e-06,
      "loss": 0.056,
      "step": 392
    },
    {
      "epoch": 0.023312373947087437,
      "grad_norm": 28.903472900390625,
      "learning_rate": 4.661921708185054e-06,
      "loss": 0.1628,
      "step": 393
    },
    {
      "epoch": 0.023371692964764504,
      "grad_norm": 23.15361976623535,
      "learning_rate": 4.673784104389087e-06,
      "loss": 1.7242,
      "step": 394
    },
    {
      "epoch": 0.02343101198244157,
      "grad_norm": 51.580074310302734,
      "learning_rate": 4.68564650059312e-06,
      "loss": 0.2953,
      "step": 395
    },
    {
      "epoch": 0.023490331000118637,
      "grad_norm": 28.856813430786133,
      "learning_rate": 4.697508896797153e-06,
      "loss": 0.7261,
      "step": 396
    },
    {
      "epoch": 0.023549650017795704,
      "grad_norm": 13.548829078674316,
      "learning_rate": 4.709371293001187e-06,
      "loss": 0.0966,
      "step": 397
    },
    {
      "epoch": 0.02360896903547277,
      "grad_norm": 17.42716407775879,
      "learning_rate": 4.7212336892052195e-06,
      "loss": 0.0746,
      "step": 398
    },
    {
      "epoch": 0.02366828805314984,
      "grad_norm": 37.84424591064453,
      "learning_rate": 4.733096085409253e-06,
      "loss": 0.3589,
      "step": 399
    },
    {
      "epoch": 0.023727607070826908,
      "grad_norm": 10.89127254486084,
      "learning_rate": 4.7449584816132865e-06,
      "loss": 0.0619,
      "step": 400
    },
    {
      "epoch": 0.023786926088503975,
      "grad_norm": 0.48342201113700867,
      "learning_rate": 4.75682087781732e-06,
      "loss": 0.005,
      "step": 401
    },
    {
      "epoch": 0.02384624510618104,
      "grad_norm": 8.16068172454834,
      "learning_rate": 4.768683274021353e-06,
      "loss": 0.0798,
      "step": 402
    },
    {
      "epoch": 0.02390556412385811,
      "grad_norm": 1.5532094240188599,
      "learning_rate": 4.780545670225386e-06,
      "loss": 0.0147,
      "step": 403
    },
    {
      "epoch": 0.023964883141535175,
      "grad_norm": 91.38591766357422,
      "learning_rate": 4.792408066429419e-06,
      "loss": 1.778,
      "step": 404
    },
    {
      "epoch": 0.024024202159212242,
      "grad_norm": 27.30093002319336,
      "learning_rate": 4.8042704626334524e-06,
      "loss": 0.4091,
      "step": 405
    },
    {
      "epoch": 0.024083521176889312,
      "grad_norm": 19.575464248657227,
      "learning_rate": 4.816132858837486e-06,
      "loss": 0.0848,
      "step": 406
    },
    {
      "epoch": 0.02414284019456638,
      "grad_norm": 0.28234443068504333,
      "learning_rate": 4.827995255041519e-06,
      "loss": 0.0046,
      "step": 407
    },
    {
      "epoch": 0.024202159212243446,
      "grad_norm": 89.24227142333984,
      "learning_rate": 4.839857651245552e-06,
      "loss": 1.4643,
      "step": 408
    },
    {
      "epoch": 0.024261478229920513,
      "grad_norm": 27.388225555419922,
      "learning_rate": 4.851720047449585e-06,
      "loss": 0.3584,
      "step": 409
    },
    {
      "epoch": 0.02432079724759758,
      "grad_norm": 3.933063507080078,
      "learning_rate": 4.863582443653618e-06,
      "loss": 0.0294,
      "step": 410
    },
    {
      "epoch": 0.024380116265274646,
      "grad_norm": 15.265995979309082,
      "learning_rate": 4.875444839857652e-06,
      "loss": 0.2139,
      "step": 411
    },
    {
      "epoch": 0.024439435282951713,
      "grad_norm": 16.251354217529297,
      "learning_rate": 4.8873072360616845e-06,
      "loss": 0.1984,
      "step": 412
    },
    {
      "epoch": 0.02449875430062878,
      "grad_norm": 2.8407113552093506,
      "learning_rate": 4.899169632265718e-06,
      "loss": 0.0199,
      "step": 413
    },
    {
      "epoch": 0.02455807331830585,
      "grad_norm": 28.828147888183594,
      "learning_rate": 4.911032028469751e-06,
      "loss": 0.3343,
      "step": 414
    },
    {
      "epoch": 0.024617392335982917,
      "grad_norm": 38.379539489746094,
      "learning_rate": 4.922894424673785e-06,
      "loss": 0.1045,
      "step": 415
    },
    {
      "epoch": 0.024676711353659984,
      "grad_norm": 16.21827507019043,
      "learning_rate": 4.934756820877818e-06,
      "loss": 1.4535,
      "step": 416
    },
    {
      "epoch": 0.02473603037133705,
      "grad_norm": 7.460943698883057,
      "learning_rate": 4.946619217081851e-06,
      "loss": 0.2359,
      "step": 417
    },
    {
      "epoch": 0.024795349389014117,
      "grad_norm": 14.497017860412598,
      "learning_rate": 4.958481613285884e-06,
      "loss": 0.4507,
      "step": 418
    },
    {
      "epoch": 0.024854668406691184,
      "grad_norm": 53.220062255859375,
      "learning_rate": 4.9703440094899175e-06,
      "loss": 0.364,
      "step": 419
    },
    {
      "epoch": 0.02491398742436825,
      "grad_norm": 18.87822914123535,
      "learning_rate": 4.982206405693951e-06,
      "loss": 0.1927,
      "step": 420
    },
    {
      "epoch": 0.02497330644204532,
      "grad_norm": 19.281494140625,
      "learning_rate": 4.994068801897984e-06,
      "loss": 0.0955,
      "step": 421
    },
    {
      "epoch": 0.025032625459722388,
      "grad_norm": 35.270748138427734,
      "learning_rate": 5.005931198102017e-06,
      "loss": 0.2354,
      "step": 422
    },
    {
      "epoch": 0.025091944477399455,
      "grad_norm": 4.161923885345459,
      "learning_rate": 5.017793594306051e-06,
      "loss": 0.0182,
      "step": 423
    },
    {
      "epoch": 0.02515126349507652,
      "grad_norm": 34.13665771484375,
      "learning_rate": 5.029655990510084e-06,
      "loss": 1.4643,
      "step": 424
    },
    {
      "epoch": 0.02521058251275359,
      "grad_norm": 0.37689685821533203,
      "learning_rate": 5.041518386714117e-06,
      "loss": 0.0044,
      "step": 425
    },
    {
      "epoch": 0.025269901530430655,
      "grad_norm": 0.7206357717514038,
      "learning_rate": 5.05338078291815e-06,
      "loss": 0.0091,
      "step": 426
    },
    {
      "epoch": 0.025329220548107722,
      "grad_norm": 43.16693115234375,
      "learning_rate": 5.065243179122183e-06,
      "loss": 0.1187,
      "step": 427
    },
    {
      "epoch": 0.025388539565784792,
      "grad_norm": 38.01136016845703,
      "learning_rate": 5.077105575326217e-06,
      "loss": 0.344,
      "step": 428
    },
    {
      "epoch": 0.02544785858346186,
      "grad_norm": 5.614851474761963,
      "learning_rate": 5.08896797153025e-06,
      "loss": 0.0622,
      "step": 429
    },
    {
      "epoch": 0.025507177601138926,
      "grad_norm": 26.903339385986328,
      "learning_rate": 5.100830367734283e-06,
      "loss": 0.6652,
      "step": 430
    },
    {
      "epoch": 0.025566496618815993,
      "grad_norm": 4.11208438873291,
      "learning_rate": 5.112692763938316e-06,
      "loss": 0.0375,
      "step": 431
    },
    {
      "epoch": 0.02562581563649306,
      "grad_norm": 6.739086627960205,
      "learning_rate": 5.124555160142349e-06,
      "loss": 0.1061,
      "step": 432
    },
    {
      "epoch": 0.025685134654170126,
      "grad_norm": 69.28042602539062,
      "learning_rate": 5.1364175563463825e-06,
      "loss": 1.5632,
      "step": 433
    },
    {
      "epoch": 0.025744453671847193,
      "grad_norm": 3.1042397022247314,
      "learning_rate": 5.148279952550416e-06,
      "loss": 0.103,
      "step": 434
    },
    {
      "epoch": 0.025803772689524263,
      "grad_norm": 10.203184127807617,
      "learning_rate": 5.160142348754449e-06,
      "loss": 0.1756,
      "step": 435
    },
    {
      "epoch": 0.02586309170720133,
      "grad_norm": 19.857433319091797,
      "learning_rate": 5.172004744958482e-06,
      "loss": 0.2326,
      "step": 436
    },
    {
      "epoch": 0.025922410724878397,
      "grad_norm": 2.7567594051361084,
      "learning_rate": 5.183867141162515e-06,
      "loss": 0.0278,
      "step": 437
    },
    {
      "epoch": 0.025981729742555464,
      "grad_norm": 30.444719314575195,
      "learning_rate": 5.195729537366548e-06,
      "loss": 0.2391,
      "step": 438
    },
    {
      "epoch": 0.02604104876023253,
      "grad_norm": 79.04061889648438,
      "learning_rate": 5.207591933570582e-06,
      "loss": 0.6306,
      "step": 439
    },
    {
      "epoch": 0.026100367777909597,
      "grad_norm": 24.096309661865234,
      "learning_rate": 5.219454329774615e-06,
      "loss": 0.1968,
      "step": 440
    },
    {
      "epoch": 0.026159686795586664,
      "grad_norm": 42.54764938354492,
      "learning_rate": 5.231316725978648e-06,
      "loss": 0.2969,
      "step": 441
    },
    {
      "epoch": 0.02621900581326373,
      "grad_norm": 27.4951171875,
      "learning_rate": 5.243179122182681e-06,
      "loss": 0.3914,
      "step": 442
    },
    {
      "epoch": 0.0262783248309408,
      "grad_norm": 67.53387451171875,
      "learning_rate": 5.255041518386714e-06,
      "loss": 0.5784,
      "step": 443
    },
    {
      "epoch": 0.026337643848617868,
      "grad_norm": 1.667177438735962,
      "learning_rate": 5.266903914590748e-06,
      "loss": 0.0105,
      "step": 444
    },
    {
      "epoch": 0.026396962866294935,
      "grad_norm": 12.531370162963867,
      "learning_rate": 5.2787663107947805e-06,
      "loss": 0.1507,
      "step": 445
    },
    {
      "epoch": 0.026456281883972,
      "grad_norm": 4.362466812133789,
      "learning_rate": 5.290628706998814e-06,
      "loss": 0.0455,
      "step": 446
    },
    {
      "epoch": 0.026515600901649068,
      "grad_norm": 11.447038650512695,
      "learning_rate": 5.302491103202847e-06,
      "loss": 0.1925,
      "step": 447
    },
    {
      "epoch": 0.026574919919326135,
      "grad_norm": 42.80162811279297,
      "learning_rate": 5.31435349940688e-06,
      "loss": 0.2744,
      "step": 448
    },
    {
      "epoch": 0.026634238937003202,
      "grad_norm": 3.3882386684417725,
      "learning_rate": 5.326215895610914e-06,
      "loss": 0.0217,
      "step": 449
    },
    {
      "epoch": 0.026693557954680272,
      "grad_norm": 1.1061683893203735,
      "learning_rate": 5.338078291814946e-06,
      "loss": 0.0108,
      "step": 450
    },
    {
      "epoch": 0.02675287697235734,
      "grad_norm": 61.4359245300293,
      "learning_rate": 5.349940688018981e-06,
      "loss": 0.449,
      "step": 451
    },
    {
      "epoch": 0.026812195990034406,
      "grad_norm": 5.2446818351745605,
      "learning_rate": 5.361803084223014e-06,
      "loss": 0.0655,
      "step": 452
    },
    {
      "epoch": 0.026871515007711472,
      "grad_norm": 16.501888275146484,
      "learning_rate": 5.373665480427047e-06,
      "loss": 1.9903,
      "step": 453
    },
    {
      "epoch": 0.02693083402538854,
      "grad_norm": 0.6132614612579346,
      "learning_rate": 5.3855278766310805e-06,
      "loss": 0.0058,
      "step": 454
    },
    {
      "epoch": 0.026990153043065606,
      "grad_norm": 3.248063325881958,
      "learning_rate": 5.397390272835113e-06,
      "loss": 0.0358,
      "step": 455
    },
    {
      "epoch": 0.027049472060742673,
      "grad_norm": 144.2313995361328,
      "learning_rate": 5.409252669039147e-06,
      "loss": 0.6238,
      "step": 456
    },
    {
      "epoch": 0.027108791078419743,
      "grad_norm": 2.10817551612854,
      "learning_rate": 5.42111506524318e-06,
      "loss": 0.0194,
      "step": 457
    },
    {
      "epoch": 0.02716811009609681,
      "grad_norm": 27.03932762145996,
      "learning_rate": 5.432977461447213e-06,
      "loss": 0.6017,
      "step": 458
    },
    {
      "epoch": 0.027227429113773877,
      "grad_norm": 80.41546630859375,
      "learning_rate": 5.444839857651246e-06,
      "loss": 1.1224,
      "step": 459
    },
    {
      "epoch": 0.027286748131450943,
      "grad_norm": 5.565119743347168,
      "learning_rate": 5.456702253855279e-06,
      "loss": 0.0311,
      "step": 460
    },
    {
      "epoch": 0.02734606714912801,
      "grad_norm": 16.207368850708008,
      "learning_rate": 5.4685646500593126e-06,
      "loss": 0.273,
      "step": 461
    },
    {
      "epoch": 0.027405386166805077,
      "grad_norm": 15.337684631347656,
      "learning_rate": 5.480427046263346e-06,
      "loss": 0.0987,
      "step": 462
    },
    {
      "epoch": 0.027464705184482144,
      "grad_norm": 39.43831253051758,
      "learning_rate": 5.492289442467379e-06,
      "loss": 0.6719,
      "step": 463
    },
    {
      "epoch": 0.02752402420215921,
      "grad_norm": 45.23306655883789,
      "learning_rate": 5.504151838671412e-06,
      "loss": 0.5731,
      "step": 464
    },
    {
      "epoch": 0.02758334321983628,
      "grad_norm": 11.618595123291016,
      "learning_rate": 5.516014234875445e-06,
      "loss": 0.171,
      "step": 465
    },
    {
      "epoch": 0.027642662237513348,
      "grad_norm": 3.571790933609009,
      "learning_rate": 5.5278766310794785e-06,
      "loss": 0.019,
      "step": 466
    },
    {
      "epoch": 0.027701981255190414,
      "grad_norm": 58.88029479980469,
      "learning_rate": 5.539739027283512e-06,
      "loss": 0.6387,
      "step": 467
    },
    {
      "epoch": 0.02776130027286748,
      "grad_norm": 0.9503989219665527,
      "learning_rate": 5.551601423487545e-06,
      "loss": 0.0079,
      "step": 468
    },
    {
      "epoch": 0.027820619290544548,
      "grad_norm": 8.97628402709961,
      "learning_rate": 5.563463819691578e-06,
      "loss": 0.2937,
      "step": 469
    },
    {
      "epoch": 0.027879938308221615,
      "grad_norm": 56.35631561279297,
      "learning_rate": 5.575326215895611e-06,
      "loss": 1.724,
      "step": 470
    },
    {
      "epoch": 0.02793925732589868,
      "grad_norm": 0.7859665751457214,
      "learning_rate": 5.587188612099644e-06,
      "loss": 0.0061,
      "step": 471
    },
    {
      "epoch": 0.027998576343575752,
      "grad_norm": 5.7547430992126465,
      "learning_rate": 5.599051008303678e-06,
      "loss": 0.0578,
      "step": 472
    },
    {
      "epoch": 0.02805789536125282,
      "grad_norm": 6.784146785736084,
      "learning_rate": 5.6109134045077106e-06,
      "loss": 0.0899,
      "step": 473
    },
    {
      "epoch": 0.028117214378929885,
      "grad_norm": 13.41358757019043,
      "learning_rate": 5.622775800711744e-06,
      "loss": 0.068,
      "step": 474
    },
    {
      "epoch": 0.028176533396606952,
      "grad_norm": 1.4162429571151733,
      "learning_rate": 5.634638196915777e-06,
      "loss": 0.0055,
      "step": 475
    },
    {
      "epoch": 0.02823585241428402,
      "grad_norm": 11.567426681518555,
      "learning_rate": 5.64650059311981e-06,
      "loss": 0.2956,
      "step": 476
    },
    {
      "epoch": 0.028295171431961086,
      "grad_norm": 39.92967224121094,
      "learning_rate": 5.658362989323844e-06,
      "loss": 0.5062,
      "step": 477
    },
    {
      "epoch": 0.028354490449638153,
      "grad_norm": 2.0051004886627197,
      "learning_rate": 5.6702253855278765e-06,
      "loss": 0.0148,
      "step": 478
    },
    {
      "epoch": 0.028413809467315223,
      "grad_norm": 59.80725860595703,
      "learning_rate": 5.68208778173191e-06,
      "loss": 2.1171,
      "step": 479
    },
    {
      "epoch": 0.02847312848499229,
      "grad_norm": 34.02106857299805,
      "learning_rate": 5.693950177935944e-06,
      "loss": 0.5772,
      "step": 480
    },
    {
      "epoch": 0.028532447502669357,
      "grad_norm": 13.271178245544434,
      "learning_rate": 5.705812574139977e-06,
      "loss": 0.6776,
      "step": 481
    },
    {
      "epoch": 0.028591766520346423,
      "grad_norm": 47.57286834716797,
      "learning_rate": 5.7176749703440105e-06,
      "loss": 0.4492,
      "step": 482
    },
    {
      "epoch": 0.02865108553802349,
      "grad_norm": 24.322189331054688,
      "learning_rate": 5.729537366548043e-06,
      "loss": 0.1445,
      "step": 483
    },
    {
      "epoch": 0.028710404555700557,
      "grad_norm": 70.58203887939453,
      "learning_rate": 5.741399762752077e-06,
      "loss": 1.8687,
      "step": 484
    },
    {
      "epoch": 0.028769723573377624,
      "grad_norm": 11.68777084350586,
      "learning_rate": 5.75326215895611e-06,
      "loss": 0.889,
      "step": 485
    },
    {
      "epoch": 0.02882904259105469,
      "grad_norm": 1.5925242900848389,
      "learning_rate": 5.765124555160143e-06,
      "loss": 0.0081,
      "step": 486
    },
    {
      "epoch": 0.02888836160873176,
      "grad_norm": 13.196725845336914,
      "learning_rate": 5.7769869513641764e-06,
      "loss": 0.1634,
      "step": 487
    },
    {
      "epoch": 0.028947680626408828,
      "grad_norm": 14.901154518127441,
      "learning_rate": 5.788849347568209e-06,
      "loss": 0.3209,
      "step": 488
    },
    {
      "epoch": 0.029006999644085894,
      "grad_norm": 90.0256118774414,
      "learning_rate": 5.800711743772243e-06,
      "loss": 2.2225,
      "step": 489
    },
    {
      "epoch": 0.02906631866176296,
      "grad_norm": 20.48738670349121,
      "learning_rate": 5.812574139976276e-06,
      "loss": 0.3111,
      "step": 490
    },
    {
      "epoch": 0.029125637679440028,
      "grad_norm": 30.302928924560547,
      "learning_rate": 5.824436536180309e-06,
      "loss": 0.7738,
      "step": 491
    },
    {
      "epoch": 0.029184956697117095,
      "grad_norm": 60.763954162597656,
      "learning_rate": 5.836298932384342e-06,
      "loss": 0.3535,
      "step": 492
    },
    {
      "epoch": 0.02924427571479416,
      "grad_norm": 0.7529153823852539,
      "learning_rate": 5.848161328588375e-06,
      "loss": 0.0074,
      "step": 493
    },
    {
      "epoch": 0.029303594732471232,
      "grad_norm": 15.407999992370605,
      "learning_rate": 5.8600237247924085e-06,
      "loss": 0.0898,
      "step": 494
    },
    {
      "epoch": 0.0293629137501483,
      "grad_norm": 16.342559814453125,
      "learning_rate": 5.871886120996442e-06,
      "loss": 0.2782,
      "step": 495
    },
    {
      "epoch": 0.029422232767825365,
      "grad_norm": 34.81489944458008,
      "learning_rate": 5.883748517200475e-06,
      "loss": 0.3509,
      "step": 496
    },
    {
      "epoch": 0.029481551785502432,
      "grad_norm": 58.58232116699219,
      "learning_rate": 5.895610913404508e-06,
      "loss": 1.5476,
      "step": 497
    },
    {
      "epoch": 0.0295408708031795,
      "grad_norm": 27.615598678588867,
      "learning_rate": 5.907473309608541e-06,
      "loss": 0.2685,
      "step": 498
    },
    {
      "epoch": 0.029600189820856566,
      "grad_norm": 0.6218074560165405,
      "learning_rate": 5.9193357058125744e-06,
      "loss": 0.0048,
      "step": 499
    },
    {
      "epoch": 0.029659508838533633,
      "grad_norm": 2.0886263847351074,
      "learning_rate": 5.931198102016608e-06,
      "loss": 0.0142,
      "step": 500
    },
    {
      "epoch": 0.029718827856210703,
      "grad_norm": 32.145626068115234,
      "learning_rate": 5.943060498220641e-06,
      "loss": 0.4246,
      "step": 501
    },
    {
      "epoch": 0.02977814687388777,
      "grad_norm": 25.664382934570312,
      "learning_rate": 5.954922894424674e-06,
      "loss": 1.0177,
      "step": 502
    },
    {
      "epoch": 0.029837465891564836,
      "grad_norm": 18.669567108154297,
      "learning_rate": 5.966785290628707e-06,
      "loss": 0.3633,
      "step": 503
    },
    {
      "epoch": 0.029896784909241903,
      "grad_norm": 46.1468391418457,
      "learning_rate": 5.97864768683274e-06,
      "loss": 1.5601,
      "step": 504
    },
    {
      "epoch": 0.02995610392691897,
      "grad_norm": 13.371537208557129,
      "learning_rate": 5.990510083036774e-06,
      "loss": 0.2941,
      "step": 505
    },
    {
      "epoch": 0.030015422944596037,
      "grad_norm": 2.451674222946167,
      "learning_rate": 6.0023724792408065e-06,
      "loss": 0.0194,
      "step": 506
    },
    {
      "epoch": 0.030074741962273104,
      "grad_norm": 32.10176086425781,
      "learning_rate": 6.01423487544484e-06,
      "loss": 0.8468,
      "step": 507
    },
    {
      "epoch": 0.03013406097995017,
      "grad_norm": 34.78166961669922,
      "learning_rate": 6.026097271648873e-06,
      "loss": 0.2432,
      "step": 508
    },
    {
      "epoch": 0.03019337999762724,
      "grad_norm": 4.358023643493652,
      "learning_rate": 6.037959667852907e-06,
      "loss": 0.0427,
      "step": 509
    },
    {
      "epoch": 0.030252699015304307,
      "grad_norm": 3.6387369632720947,
      "learning_rate": 6.049822064056941e-06,
      "loss": 0.0418,
      "step": 510
    },
    {
      "epoch": 0.030312018032981374,
      "grad_norm": 32.75173568725586,
      "learning_rate": 6.061684460260973e-06,
      "loss": 0.3172,
      "step": 511
    },
    {
      "epoch": 0.03037133705065844,
      "grad_norm": 5.468038082122803,
      "learning_rate": 6.073546856465007e-06,
      "loss": 0.0243,
      "step": 512
    },
    {
      "epoch": 0.030430656068335508,
      "grad_norm": 22.59092140197754,
      "learning_rate": 6.08540925266904e-06,
      "loss": 0.1823,
      "step": 513
    },
    {
      "epoch": 0.030489975086012575,
      "grad_norm": 18.085132598876953,
      "learning_rate": 6.097271648873073e-06,
      "loss": 0.2731,
      "step": 514
    },
    {
      "epoch": 0.03054929410368964,
      "grad_norm": 14.785921096801758,
      "learning_rate": 6.1091340450771065e-06,
      "loss": 0.6697,
      "step": 515
    },
    {
      "epoch": 0.03060861312136671,
      "grad_norm": 23.78458023071289,
      "learning_rate": 6.120996441281139e-06,
      "loss": 0.4963,
      "step": 516
    },
    {
      "epoch": 0.03066793213904378,
      "grad_norm": 0.24468249082565308,
      "learning_rate": 6.132858837485173e-06,
      "loss": 0.0026,
      "step": 517
    },
    {
      "epoch": 0.030727251156720845,
      "grad_norm": 23.85887336730957,
      "learning_rate": 6.144721233689206e-06,
      "loss": 0.5052,
      "step": 518
    },
    {
      "epoch": 0.030786570174397912,
      "grad_norm": 14.31017017364502,
      "learning_rate": 6.156583629893239e-06,
      "loss": 0.3632,
      "step": 519
    },
    {
      "epoch": 0.03084588919207498,
      "grad_norm": 0.08873153477907181,
      "learning_rate": 6.168446026097272e-06,
      "loss": 0.0016,
      "step": 520
    },
    {
      "epoch": 0.030905208209752046,
      "grad_norm": 13.749438285827637,
      "learning_rate": 6.180308422301305e-06,
      "loss": 0.0851,
      "step": 521
    },
    {
      "epoch": 0.030964527227429112,
      "grad_norm": 26.48379135131836,
      "learning_rate": 6.192170818505339e-06,
      "loss": 0.3618,
      "step": 522
    },
    {
      "epoch": 0.031023846245106183,
      "grad_norm": 22.272207260131836,
      "learning_rate": 6.204033214709372e-06,
      "loss": 0.7964,
      "step": 523
    },
    {
      "epoch": 0.03108316526278325,
      "grad_norm": 11.380992889404297,
      "learning_rate": 6.215895610913405e-06,
      "loss": 0.1358,
      "step": 524
    },
    {
      "epoch": 0.031142484280460316,
      "grad_norm": 20.790430068969727,
      "learning_rate": 6.227758007117438e-06,
      "loss": 0.2589,
      "step": 525
    },
    {
      "epoch": 0.031201803298137383,
      "grad_norm": 42.938575744628906,
      "learning_rate": 6.239620403321471e-06,
      "loss": 0.6157,
      "step": 526
    },
    {
      "epoch": 0.03126112231581445,
      "grad_norm": 77.5995101928711,
      "learning_rate": 6.2514827995255045e-06,
      "loss": 0.3481,
      "step": 527
    },
    {
      "epoch": 0.03132044133349152,
      "grad_norm": 12.254205703735352,
      "learning_rate": 6.263345195729538e-06,
      "loss": 0.3122,
      "step": 528
    },
    {
      "epoch": 0.03137976035116859,
      "grad_norm": 1.2098002433776855,
      "learning_rate": 6.275207591933571e-06,
      "loss": 0.0107,
      "step": 529
    },
    {
      "epoch": 0.03143907936884565,
      "grad_norm": 25.904346466064453,
      "learning_rate": 6.287069988137604e-06,
      "loss": 0.1683,
      "step": 530
    },
    {
      "epoch": 0.03149839838652272,
      "grad_norm": 25.032089233398438,
      "learning_rate": 6.298932384341637e-06,
      "loss": 0.0838,
      "step": 531
    },
    {
      "epoch": 0.031557717404199784,
      "grad_norm": 14.226330757141113,
      "learning_rate": 6.31079478054567e-06,
      "loss": 0.3432,
      "step": 532
    },
    {
      "epoch": 0.031617036421876854,
      "grad_norm": 4.383443355560303,
      "learning_rate": 6.322657176749704e-06,
      "loss": 0.0164,
      "step": 533
    },
    {
      "epoch": 0.031676355439553924,
      "grad_norm": 1.4674363136291504,
      "learning_rate": 6.334519572953737e-06,
      "loss": 0.0141,
      "step": 534
    },
    {
      "epoch": 0.03173567445723099,
      "grad_norm": 0.10709460079669952,
      "learning_rate": 6.34638196915777e-06,
      "loss": 0.0019,
      "step": 535
    },
    {
      "epoch": 0.03179499347490806,
      "grad_norm": 26.73877716064453,
      "learning_rate": 6.358244365361803e-06,
      "loss": 1.0931,
      "step": 536
    },
    {
      "epoch": 0.03185431249258512,
      "grad_norm": 0.46972495317459106,
      "learning_rate": 6.370106761565836e-06,
      "loss": 0.0033,
      "step": 537
    },
    {
      "epoch": 0.03191363151026219,
      "grad_norm": 10.007389068603516,
      "learning_rate": 6.381969157769871e-06,
      "loss": 0.0363,
      "step": 538
    },
    {
      "epoch": 0.031972950527939255,
      "grad_norm": 1.743033528327942,
      "learning_rate": 6.393831553973904e-06,
      "loss": 0.0109,
      "step": 539
    },
    {
      "epoch": 0.032032269545616325,
      "grad_norm": 118.57125854492188,
      "learning_rate": 6.405693950177937e-06,
      "loss": 1.4501,
      "step": 540
    },
    {
      "epoch": 0.032091588563293395,
      "grad_norm": 1.8559935092926025,
      "learning_rate": 6.41755634638197e-06,
      "loss": 0.0166,
      "step": 541
    },
    {
      "epoch": 0.03215090758097046,
      "grad_norm": 319.94891357421875,
      "learning_rate": 6.429418742586003e-06,
      "loss": 0.6142,
      "step": 542
    },
    {
      "epoch": 0.03221022659864753,
      "grad_norm": 47.4030876159668,
      "learning_rate": 6.4412811387900366e-06,
      "loss": 0.1861,
      "step": 543
    },
    {
      "epoch": 0.03226954561632459,
      "grad_norm": 4.000736236572266,
      "learning_rate": 6.453143534994069e-06,
      "loss": 0.037,
      "step": 544
    },
    {
      "epoch": 0.03232886463400166,
      "grad_norm": 5.937483310699463,
      "learning_rate": 6.465005931198103e-06,
      "loss": 0.027,
      "step": 545
    },
    {
      "epoch": 0.032388183651678726,
      "grad_norm": 13.473496437072754,
      "learning_rate": 6.476868327402136e-06,
      "loss": 0.0827,
      "step": 546
    },
    {
      "epoch": 0.032447502669355796,
      "grad_norm": 8.410700798034668,
      "learning_rate": 6.488730723606169e-06,
      "loss": 0.0788,
      "step": 547
    },
    {
      "epoch": 0.03250682168703286,
      "grad_norm": 14.92020034790039,
      "learning_rate": 6.5005931198102025e-06,
      "loss": 0.1129,
      "step": 548
    },
    {
      "epoch": 0.03256614070470993,
      "grad_norm": 31.521621704101562,
      "learning_rate": 6.512455516014235e-06,
      "loss": 0.2728,
      "step": 549
    },
    {
      "epoch": 0.032625459722387,
      "grad_norm": 9.112065315246582,
      "learning_rate": 6.524317912218269e-06,
      "loss": 0.0785,
      "step": 550
    },
    {
      "epoch": 0.03268477874006406,
      "grad_norm": 26.495683670043945,
      "learning_rate": 6.536180308422302e-06,
      "loss": 0.3832,
      "step": 551
    },
    {
      "epoch": 0.032744097757741134,
      "grad_norm": 25.934003829956055,
      "learning_rate": 6.548042704626335e-06,
      "loss": 0.6167,
      "step": 552
    },
    {
      "epoch": 0.0328034167754182,
      "grad_norm": 13.694177627563477,
      "learning_rate": 6.559905100830368e-06,
      "loss": 0.2437,
      "step": 553
    },
    {
      "epoch": 0.03286273579309527,
      "grad_norm": 13.91048526763916,
      "learning_rate": 6.571767497034401e-06,
      "loss": 0.1258,
      "step": 554
    },
    {
      "epoch": 0.03292205481077233,
      "grad_norm": 24.44048500061035,
      "learning_rate": 6.5836298932384346e-06,
      "loss": 1.0382,
      "step": 555
    },
    {
      "epoch": 0.0329813738284494,
      "grad_norm": 9.146708488464355,
      "learning_rate": 6.595492289442468e-06,
      "loss": 0.2057,
      "step": 556
    },
    {
      "epoch": 0.03304069284612647,
      "grad_norm": 77.26551818847656,
      "learning_rate": 6.607354685646501e-06,
      "loss": 1.65,
      "step": 557
    },
    {
      "epoch": 0.033100011863803534,
      "grad_norm": 1.4736500978469849,
      "learning_rate": 6.619217081850534e-06,
      "loss": 0.0114,
      "step": 558
    },
    {
      "epoch": 0.033159330881480605,
      "grad_norm": 55.7794189453125,
      "learning_rate": 6.631079478054567e-06,
      "loss": 0.4553,
      "step": 559
    },
    {
      "epoch": 0.03321864989915767,
      "grad_norm": 41.61618423461914,
      "learning_rate": 6.6429418742586005e-06,
      "loss": 0.0773,
      "step": 560
    },
    {
      "epoch": 0.03327796891683474,
      "grad_norm": 8.870129585266113,
      "learning_rate": 6.654804270462634e-06,
      "loss": 0.0793,
      "step": 561
    },
    {
      "epoch": 0.0333372879345118,
      "grad_norm": 9.174796104431152,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.0865,
      "step": 562
    },
    {
      "epoch": 0.03339660695218887,
      "grad_norm": 32.07326889038086,
      "learning_rate": 6.6785290628707e-06,
      "loss": 0.404,
      "step": 563
    },
    {
      "epoch": 0.03345592596986594,
      "grad_norm": 12.44846248626709,
      "learning_rate": 6.690391459074733e-06,
      "loss": 0.0825,
      "step": 564
    },
    {
      "epoch": 0.033515244987543005,
      "grad_norm": 3.018965005874634,
      "learning_rate": 6.702253855278766e-06,
      "loss": 0.0272,
      "step": 565
    },
    {
      "epoch": 0.033574564005220076,
      "grad_norm": 63.912086486816406,
      "learning_rate": 6.7141162514828e-06,
      "loss": 0.8736,
      "step": 566
    },
    {
      "epoch": 0.03363388302289714,
      "grad_norm": 5.909724235534668,
      "learning_rate": 6.725978647686834e-06,
      "loss": 0.0399,
      "step": 567
    },
    {
      "epoch": 0.03369320204057421,
      "grad_norm": 9.065934181213379,
      "learning_rate": 6.737841043890867e-06,
      "loss": 0.0391,
      "step": 568
    },
    {
      "epoch": 0.03375252105825127,
      "grad_norm": 21.517475128173828,
      "learning_rate": 6.7497034400949004e-06,
      "loss": 0.1714,
      "step": 569
    },
    {
      "epoch": 0.03381184007592834,
      "grad_norm": 3.174170732498169,
      "learning_rate": 6.761565836298933e-06,
      "loss": 0.0188,
      "step": 570
    },
    {
      "epoch": 0.03387115909360541,
      "grad_norm": 83.66744232177734,
      "learning_rate": 6.773428232502967e-06,
      "loss": 1.1353,
      "step": 571
    },
    {
      "epoch": 0.033930478111282476,
      "grad_norm": 12.04195785522461,
      "learning_rate": 6.785290628707e-06,
      "loss": 0.7466,
      "step": 572
    },
    {
      "epoch": 0.03398979712895955,
      "grad_norm": 13.937100410461426,
      "learning_rate": 6.797153024911033e-06,
      "loss": 0.167,
      "step": 573
    },
    {
      "epoch": 0.03404911614663661,
      "grad_norm": 21.24671173095703,
      "learning_rate": 6.809015421115066e-06,
      "loss": 0.1093,
      "step": 574
    },
    {
      "epoch": 0.03410843516431368,
      "grad_norm": 15.625717163085938,
      "learning_rate": 6.820877817319099e-06,
      "loss": 0.079,
      "step": 575
    },
    {
      "epoch": 0.034167754181990743,
      "grad_norm": 26.08768081665039,
      "learning_rate": 6.8327402135231325e-06,
      "loss": 0.7419,
      "step": 576
    },
    {
      "epoch": 0.034227073199667814,
      "grad_norm": 15.016480445861816,
      "learning_rate": 6.844602609727165e-06,
      "loss": 0.2981,
      "step": 577
    },
    {
      "epoch": 0.034286392217344884,
      "grad_norm": 37.7904052734375,
      "learning_rate": 6.856465005931199e-06,
      "loss": 1.571,
      "step": 578
    },
    {
      "epoch": 0.03434571123502195,
      "grad_norm": 0.6501741409301758,
      "learning_rate": 6.868327402135232e-06,
      "loss": 0.0045,
      "step": 579
    },
    {
      "epoch": 0.03440503025269902,
      "grad_norm": 2.831965684890747,
      "learning_rate": 6.880189798339265e-06,
      "loss": 0.0392,
      "step": 580
    },
    {
      "epoch": 0.03446434927037608,
      "grad_norm": 65.36515045166016,
      "learning_rate": 6.8920521945432984e-06,
      "loss": 1.5449,
      "step": 581
    },
    {
      "epoch": 0.03452366828805315,
      "grad_norm": 15.28355598449707,
      "learning_rate": 6.903914590747331e-06,
      "loss": 0.387,
      "step": 582
    },
    {
      "epoch": 0.034582987305730215,
      "grad_norm": 15.43011474609375,
      "learning_rate": 6.915776986951365e-06,
      "loss": 0.5571,
      "step": 583
    },
    {
      "epoch": 0.034642306323407285,
      "grad_norm": 22.471908569335938,
      "learning_rate": 6.927639383155398e-06,
      "loss": 0.615,
      "step": 584
    },
    {
      "epoch": 0.034701625341084355,
      "grad_norm": 138.518310546875,
      "learning_rate": 6.939501779359431e-06,
      "loss": 1.3301,
      "step": 585
    },
    {
      "epoch": 0.03476094435876142,
      "grad_norm": 28.936952590942383,
      "learning_rate": 6.951364175563464e-06,
      "loss": 0.2442,
      "step": 586
    },
    {
      "epoch": 0.03482026337643849,
      "grad_norm": 0.91312575340271,
      "learning_rate": 6.963226571767497e-06,
      "loss": 0.0055,
      "step": 587
    },
    {
      "epoch": 0.03487958239411555,
      "grad_norm": 7.8998894691467285,
      "learning_rate": 6.9750889679715305e-06,
      "loss": 0.1763,
      "step": 588
    },
    {
      "epoch": 0.03493890141179262,
      "grad_norm": 11.277544021606445,
      "learning_rate": 6.986951364175564e-06,
      "loss": 0.1441,
      "step": 589
    },
    {
      "epoch": 0.034998220429469686,
      "grad_norm": 0.6486137509346008,
      "learning_rate": 6.998813760379597e-06,
      "loss": 0.0065,
      "step": 590
    },
    {
      "epoch": 0.035057539447146756,
      "grad_norm": 17.797426223754883,
      "learning_rate": 7.01067615658363e-06,
      "loss": 0.3192,
      "step": 591
    },
    {
      "epoch": 0.03511685846482382,
      "grad_norm": 14.91891860961914,
      "learning_rate": 7.022538552787663e-06,
      "loss": 0.3691,
      "step": 592
    },
    {
      "epoch": 0.03517617748250089,
      "grad_norm": 73.55413055419922,
      "learning_rate": 7.034400948991696e-06,
      "loss": 0.8677,
      "step": 593
    },
    {
      "epoch": 0.03523549650017796,
      "grad_norm": 17.64323616027832,
      "learning_rate": 7.04626334519573e-06,
      "loss": 0.3899,
      "step": 594
    },
    {
      "epoch": 0.03529481551785502,
      "grad_norm": 64.40029907226562,
      "learning_rate": 7.058125741399763e-06,
      "loss": 1.5063,
      "step": 595
    },
    {
      "epoch": 0.03535413453553209,
      "grad_norm": 2.762934923171997,
      "learning_rate": 7.069988137603797e-06,
      "loss": 0.0176,
      "step": 596
    },
    {
      "epoch": 0.03541345355320916,
      "grad_norm": 8.762001991271973,
      "learning_rate": 7.0818505338078305e-06,
      "loss": 0.0934,
      "step": 597
    },
    {
      "epoch": 0.03547277257088623,
      "grad_norm": 16.56723403930664,
      "learning_rate": 7.093712930011863e-06,
      "loss": 0.4287,
      "step": 598
    },
    {
      "epoch": 0.03553209158856329,
      "grad_norm": 0.13016295433044434,
      "learning_rate": 7.105575326215897e-06,
      "loss": 0.0015,
      "step": 599
    },
    {
      "epoch": 0.03559141060624036,
      "grad_norm": 17.690706253051758,
      "learning_rate": 7.11743772241993e-06,
      "loss": 0.152,
      "step": 600
    },
    {
      "epoch": 0.03565072962391743,
      "grad_norm": 0.38170596957206726,
      "learning_rate": 7.129300118623963e-06,
      "loss": 0.0028,
      "step": 601
    },
    {
      "epoch": 0.035710048641594494,
      "grad_norm": 2.481839895248413,
      "learning_rate": 7.141162514827996e-06,
      "loss": 0.0174,
      "step": 602
    },
    {
      "epoch": 0.035769367659271564,
      "grad_norm": 5.18834924697876,
      "learning_rate": 7.153024911032029e-06,
      "loss": 0.0273,
      "step": 603
    },
    {
      "epoch": 0.03582868667694863,
      "grad_norm": 16.89133071899414,
      "learning_rate": 7.164887307236063e-06,
      "loss": 0.2458,
      "step": 604
    },
    {
      "epoch": 0.0358880056946257,
      "grad_norm": 16.47635841369629,
      "learning_rate": 7.176749703440096e-06,
      "loss": 0.0624,
      "step": 605
    },
    {
      "epoch": 0.03594732471230276,
      "grad_norm": 1.945164680480957,
      "learning_rate": 7.188612099644129e-06,
      "loss": 0.0127,
      "step": 606
    },
    {
      "epoch": 0.03600664372997983,
      "grad_norm": 21.27509307861328,
      "learning_rate": 7.200474495848162e-06,
      "loss": 0.5869,
      "step": 607
    },
    {
      "epoch": 0.0360659627476569,
      "grad_norm": 16.15771484375,
      "learning_rate": 7.212336892052195e-06,
      "loss": 0.6334,
      "step": 608
    },
    {
      "epoch": 0.036125281765333965,
      "grad_norm": 7.321718215942383,
      "learning_rate": 7.2241992882562285e-06,
      "loss": 0.1188,
      "step": 609
    },
    {
      "epoch": 0.036184600783011035,
      "grad_norm": 2.6607465744018555,
      "learning_rate": 7.236061684460261e-06,
      "loss": 0.0234,
      "step": 610
    },
    {
      "epoch": 0.0362439198006881,
      "grad_norm": 15.613584518432617,
      "learning_rate": 7.247924080664295e-06,
      "loss": 0.3484,
      "step": 611
    },
    {
      "epoch": 0.03630323881836517,
      "grad_norm": 34.32089614868164,
      "learning_rate": 7.259786476868328e-06,
      "loss": 0.5635,
      "step": 612
    },
    {
      "epoch": 0.03636255783604223,
      "grad_norm": 49.01021957397461,
      "learning_rate": 7.271648873072361e-06,
      "loss": 0.0736,
      "step": 613
    },
    {
      "epoch": 0.0364218768537193,
      "grad_norm": 2.1086905002593994,
      "learning_rate": 7.283511269276394e-06,
      "loss": 0.0108,
      "step": 614
    },
    {
      "epoch": 0.03648119587139637,
      "grad_norm": 10.799666404724121,
      "learning_rate": 7.295373665480427e-06,
      "loss": 0.0777,
      "step": 615
    },
    {
      "epoch": 0.036540514889073436,
      "grad_norm": 0.3843843638896942,
      "learning_rate": 7.307236061684461e-06,
      "loss": 0.0025,
      "step": 616
    },
    {
      "epoch": 0.036599833906750506,
      "grad_norm": 0.18714800477027893,
      "learning_rate": 7.319098457888494e-06,
      "loss": 0.0016,
      "step": 617
    },
    {
      "epoch": 0.03665915292442757,
      "grad_norm": 22.517414093017578,
      "learning_rate": 7.330960854092527e-06,
      "loss": 0.4379,
      "step": 618
    },
    {
      "epoch": 0.03671847194210464,
      "grad_norm": 69.37561798095703,
      "learning_rate": 7.34282325029656e-06,
      "loss": 2.7413,
      "step": 619
    },
    {
      "epoch": 0.0367777909597817,
      "grad_norm": 23.13831901550293,
      "learning_rate": 7.354685646500593e-06,
      "loss": 0.161,
      "step": 620
    },
    {
      "epoch": 0.03683710997745877,
      "grad_norm": 28.334495544433594,
      "learning_rate": 7.3665480427046265e-06,
      "loss": 0.3435,
      "step": 621
    },
    {
      "epoch": 0.036896428995135844,
      "grad_norm": 44.19321823120117,
      "learning_rate": 7.37841043890866e-06,
      "loss": 0.6866,
      "step": 622
    },
    {
      "epoch": 0.03695574801281291,
      "grad_norm": 1.45368492603302,
      "learning_rate": 7.390272835112693e-06,
      "loss": 0.0107,
      "step": 623
    },
    {
      "epoch": 0.03701506703048998,
      "grad_norm": 15.82340145111084,
      "learning_rate": 7.402135231316726e-06,
      "loss": 0.356,
      "step": 624
    },
    {
      "epoch": 0.03707438604816704,
      "grad_norm": 5.290409564971924,
      "learning_rate": 7.4139976275207606e-06,
      "loss": 0.0556,
      "step": 625
    },
    {
      "epoch": 0.03713370506584411,
      "grad_norm": 16.669511795043945,
      "learning_rate": 7.425860023724793e-06,
      "loss": 0.0421,
      "step": 626
    },
    {
      "epoch": 0.037193024083521174,
      "grad_norm": 38.86491775512695,
      "learning_rate": 7.437722419928827e-06,
      "loss": 0.5087,
      "step": 627
    },
    {
      "epoch": 0.037252343101198244,
      "grad_norm": 0.4353680908679962,
      "learning_rate": 7.44958481613286e-06,
      "loss": 0.0051,
      "step": 628
    },
    {
      "epoch": 0.037311662118875315,
      "grad_norm": 0.2570100426673889,
      "learning_rate": 7.461447212336893e-06,
      "loss": 0.0036,
      "step": 629
    },
    {
      "epoch": 0.03737098113655238,
      "grad_norm": 15.117358207702637,
      "learning_rate": 7.4733096085409265e-06,
      "loss": 0.311,
      "step": 630
    },
    {
      "epoch": 0.03743030015422945,
      "grad_norm": 10.541757583618164,
      "learning_rate": 7.485172004744959e-06,
      "loss": 0.0582,
      "step": 631
    },
    {
      "epoch": 0.03748961917190651,
      "grad_norm": 29.22658920288086,
      "learning_rate": 7.497034400948993e-06,
      "loss": 0.1824,
      "step": 632
    },
    {
      "epoch": 0.03754893818958358,
      "grad_norm": 106.69937896728516,
      "learning_rate": 7.508896797153026e-06,
      "loss": 1.04,
      "step": 633
    },
    {
      "epoch": 0.037608257207260645,
      "grad_norm": 124.40401458740234,
      "learning_rate": 7.520759193357059e-06,
      "loss": 2.7493,
      "step": 634
    },
    {
      "epoch": 0.037667576224937716,
      "grad_norm": 13.450066566467285,
      "learning_rate": 7.532621589561092e-06,
      "loss": 0.132,
      "step": 635
    },
    {
      "epoch": 0.03772689524261478,
      "grad_norm": 17.956396102905273,
      "learning_rate": 7.544483985765125e-06,
      "loss": 0.2659,
      "step": 636
    },
    {
      "epoch": 0.03778621426029185,
      "grad_norm": 27.713830947875977,
      "learning_rate": 7.5563463819691586e-06,
      "loss": 0.4564,
      "step": 637
    },
    {
      "epoch": 0.03784553327796892,
      "grad_norm": 0.6121253967285156,
      "learning_rate": 7.568208778173192e-06,
      "loss": 0.0049,
      "step": 638
    },
    {
      "epoch": 0.03790485229564598,
      "grad_norm": 8.77171802520752,
      "learning_rate": 7.580071174377225e-06,
      "loss": 0.0675,
      "step": 639
    },
    {
      "epoch": 0.03796417131332305,
      "grad_norm": 8.929503440856934,
      "learning_rate": 7.591933570581258e-06,
      "loss": 0.0399,
      "step": 640
    },
    {
      "epoch": 0.038023490331000116,
      "grad_norm": 3.237545967102051,
      "learning_rate": 7.603795966785291e-06,
      "loss": 0.0212,
      "step": 641
    },
    {
      "epoch": 0.03808280934867719,
      "grad_norm": 6.148073673248291,
      "learning_rate": 7.6156583629893245e-06,
      "loss": 0.0313,
      "step": 642
    },
    {
      "epoch": 0.03814212836635425,
      "grad_norm": 55.05499267578125,
      "learning_rate": 7.627520759193357e-06,
      "loss": 1.2853,
      "step": 643
    },
    {
      "epoch": 0.03820144738403132,
      "grad_norm": 25.3015079498291,
      "learning_rate": 7.639383155397391e-06,
      "loss": 0.3391,
      "step": 644
    },
    {
      "epoch": 0.03826076640170839,
      "grad_norm": 11.769736289978027,
      "learning_rate": 7.651245551601423e-06,
      "loss": 0.1808,
      "step": 645
    },
    {
      "epoch": 0.038320085419385454,
      "grad_norm": 6.463439464569092,
      "learning_rate": 7.663107947805457e-06,
      "loss": 0.0256,
      "step": 646
    },
    {
      "epoch": 0.038379404437062524,
      "grad_norm": 0.16575264930725098,
      "learning_rate": 7.67497034400949e-06,
      "loss": 0.0017,
      "step": 647
    },
    {
      "epoch": 0.03843872345473959,
      "grad_norm": 3.7176740169525146,
      "learning_rate": 7.686832740213524e-06,
      "loss": 0.0388,
      "step": 648
    },
    {
      "epoch": 0.03849804247241666,
      "grad_norm": 80.80826568603516,
      "learning_rate": 7.698695136417557e-06,
      "loss": 1.9867,
      "step": 649
    },
    {
      "epoch": 0.03855736149009372,
      "grad_norm": 0.8623340725898743,
      "learning_rate": 7.71055753262159e-06,
      "loss": 0.0043,
      "step": 650
    },
    {
      "epoch": 0.03861668050777079,
      "grad_norm": 11.536831855773926,
      "learning_rate": 7.722419928825623e-06,
      "loss": 0.1636,
      "step": 651
    },
    {
      "epoch": 0.03867599952544786,
      "grad_norm": 27.25775909423828,
      "learning_rate": 7.734282325029656e-06,
      "loss": 0.3095,
      "step": 652
    },
    {
      "epoch": 0.038735318543124925,
      "grad_norm": 41.42608642578125,
      "learning_rate": 7.74614472123369e-06,
      "loss": 0.2338,
      "step": 653
    },
    {
      "epoch": 0.038794637560801995,
      "grad_norm": 24.221435546875,
      "learning_rate": 7.758007117437723e-06,
      "loss": 0.1977,
      "step": 654
    },
    {
      "epoch": 0.03885395657847906,
      "grad_norm": 29.01109504699707,
      "learning_rate": 7.769869513641757e-06,
      "loss": 0.6873,
      "step": 655
    },
    {
      "epoch": 0.03891327559615613,
      "grad_norm": 40.28892135620117,
      "learning_rate": 7.78173190984579e-06,
      "loss": 0.3059,
      "step": 656
    },
    {
      "epoch": 0.03897259461383319,
      "grad_norm": 13.635993957519531,
      "learning_rate": 7.793594306049824e-06,
      "loss": 0.1361,
      "step": 657
    },
    {
      "epoch": 0.03903191363151026,
      "grad_norm": 11.890140533447266,
      "learning_rate": 7.805456702253856e-06,
      "loss": 0.046,
      "step": 658
    },
    {
      "epoch": 0.03909123264918733,
      "grad_norm": 0.34017428755760193,
      "learning_rate": 7.81731909845789e-06,
      "loss": 0.0053,
      "step": 659
    },
    {
      "epoch": 0.039150551666864396,
      "grad_norm": 13.004120826721191,
      "learning_rate": 7.829181494661923e-06,
      "loss": 0.0944,
      "step": 660
    },
    {
      "epoch": 0.039209870684541466,
      "grad_norm": 48.25987243652344,
      "learning_rate": 7.841043890865956e-06,
      "loss": 1.1219,
      "step": 661
    },
    {
      "epoch": 0.03926918970221853,
      "grad_norm": 4.135448932647705,
      "learning_rate": 7.85290628706999e-06,
      "loss": 0.0325,
      "step": 662
    },
    {
      "epoch": 0.0393285087198956,
      "grad_norm": 30.09149169921875,
      "learning_rate": 7.864768683274022e-06,
      "loss": 0.376,
      "step": 663
    },
    {
      "epoch": 0.03938782773757266,
      "grad_norm": 5.216513156890869,
      "learning_rate": 7.876631079478055e-06,
      "loss": 0.0984,
      "step": 664
    },
    {
      "epoch": 0.03944714675524973,
      "grad_norm": 31.890317916870117,
      "learning_rate": 7.888493475682089e-06,
      "loss": 0.6261,
      "step": 665
    },
    {
      "epoch": 0.0395064657729268,
      "grad_norm": 5.630151271820068,
      "learning_rate": 7.900355871886122e-06,
      "loss": 0.0415,
      "step": 666
    },
    {
      "epoch": 0.03956578479060387,
      "grad_norm": 7.972771644592285,
      "learning_rate": 7.912218268090156e-06,
      "loss": 0.041,
      "step": 667
    },
    {
      "epoch": 0.03962510380828094,
      "grad_norm": 10.900495529174805,
      "learning_rate": 7.924080664294187e-06,
      "loss": 0.0971,
      "step": 668
    },
    {
      "epoch": 0.039684422825958,
      "grad_norm": 3.5389742851257324,
      "learning_rate": 7.935943060498221e-06,
      "loss": 0.0315,
      "step": 669
    },
    {
      "epoch": 0.03974374184363507,
      "grad_norm": 3.237287759780884,
      "learning_rate": 7.947805456702255e-06,
      "loss": 0.0276,
      "step": 670
    },
    {
      "epoch": 0.039803060861312134,
      "grad_norm": 12.628244400024414,
      "learning_rate": 7.959667852906288e-06,
      "loss": 0.0406,
      "step": 671
    },
    {
      "epoch": 0.039862379878989204,
      "grad_norm": 3.7026195526123047,
      "learning_rate": 7.971530249110322e-06,
      "loss": 0.0224,
      "step": 672
    },
    {
      "epoch": 0.039921698896666274,
      "grad_norm": 27.74074935913086,
      "learning_rate": 7.983392645314353e-06,
      "loss": 0.42,
      "step": 673
    },
    {
      "epoch": 0.03998101791434334,
      "grad_norm": 12.265288352966309,
      "learning_rate": 7.995255041518387e-06,
      "loss": 0.0881,
      "step": 674
    },
    {
      "epoch": 0.04004033693202041,
      "grad_norm": 9.398446083068848,
      "learning_rate": 8.00711743772242e-06,
      "loss": 0.0708,
      "step": 675
    },
    {
      "epoch": 0.04009965594969747,
      "grad_norm": 4.8679280281066895,
      "learning_rate": 8.018979833926454e-06,
      "loss": 0.0495,
      "step": 676
    },
    {
      "epoch": 0.04015897496737454,
      "grad_norm": 241.45542907714844,
      "learning_rate": 8.030842230130487e-06,
      "loss": 1.5581,
      "step": 677
    },
    {
      "epoch": 0.040218293985051605,
      "grad_norm": 42.05617904663086,
      "learning_rate": 8.04270462633452e-06,
      "loss": 0.3672,
      "step": 678
    },
    {
      "epoch": 0.040277613002728675,
      "grad_norm": 8.976506233215332,
      "learning_rate": 8.054567022538553e-06,
      "loss": 0.0688,
      "step": 679
    },
    {
      "epoch": 0.040336932020405745,
      "grad_norm": 2.501288414001465,
      "learning_rate": 8.066429418742586e-06,
      "loss": 0.0187,
      "step": 680
    },
    {
      "epoch": 0.04039625103808281,
      "grad_norm": 3.9058713912963867,
      "learning_rate": 8.07829181494662e-06,
      "loss": 0.0313,
      "step": 681
    },
    {
      "epoch": 0.04045557005575988,
      "grad_norm": 1.798220157623291,
      "learning_rate": 8.090154211150653e-06,
      "loss": 0.013,
      "step": 682
    },
    {
      "epoch": 0.04051488907343694,
      "grad_norm": 24.623504638671875,
      "learning_rate": 8.102016607354685e-06,
      "loss": 0.1378,
      "step": 683
    },
    {
      "epoch": 0.04057420809111401,
      "grad_norm": 70.70077514648438,
      "learning_rate": 8.11387900355872e-06,
      "loss": 0.502,
      "step": 684
    },
    {
      "epoch": 0.040633527108791076,
      "grad_norm": 42.39970016479492,
      "learning_rate": 8.125741399762754e-06,
      "loss": 0.2395,
      "step": 685
    },
    {
      "epoch": 0.040692846126468146,
      "grad_norm": 0.3163612186908722,
      "learning_rate": 8.137603795966786e-06,
      "loss": 0.0031,
      "step": 686
    },
    {
      "epoch": 0.04075216514414521,
      "grad_norm": 1.7792351245880127,
      "learning_rate": 8.14946619217082e-06,
      "loss": 0.0068,
      "step": 687
    },
    {
      "epoch": 0.04081148416182228,
      "grad_norm": 10.890973091125488,
      "learning_rate": 8.161328588374853e-06,
      "loss": 0.118,
      "step": 688
    },
    {
      "epoch": 0.04087080317949935,
      "grad_norm": 8.056952476501465,
      "learning_rate": 8.173190984578886e-06,
      "loss": 0.0153,
      "step": 689
    },
    {
      "epoch": 0.04093012219717641,
      "grad_norm": 31.073698043823242,
      "learning_rate": 8.18505338078292e-06,
      "loss": 1.0099,
      "step": 690
    },
    {
      "epoch": 0.040989441214853484,
      "grad_norm": 36.81930160522461,
      "learning_rate": 8.196915776986952e-06,
      "loss": 1.003,
      "step": 691
    },
    {
      "epoch": 0.04104876023253055,
      "grad_norm": 3.0140466690063477,
      "learning_rate": 8.208778173190985e-06,
      "loss": 0.0196,
      "step": 692
    },
    {
      "epoch": 0.04110807925020762,
      "grad_norm": 0.6654105186462402,
      "learning_rate": 8.220640569395019e-06,
      "loss": 0.0032,
      "step": 693
    },
    {
      "epoch": 0.04116739826788468,
      "grad_norm": 42.92753601074219,
      "learning_rate": 8.232502965599052e-06,
      "loss": 0.8502,
      "step": 694
    },
    {
      "epoch": 0.04122671728556175,
      "grad_norm": 4.848782062530518,
      "learning_rate": 8.244365361803086e-06,
      "loss": 0.0213,
      "step": 695
    },
    {
      "epoch": 0.04128603630323882,
      "grad_norm": 19.889726638793945,
      "learning_rate": 8.256227758007118e-06,
      "loss": 0.0828,
      "step": 696
    },
    {
      "epoch": 0.041345355320915884,
      "grad_norm": 84.79753112792969,
      "learning_rate": 8.268090154211151e-06,
      "loss": 0.8874,
      "step": 697
    },
    {
      "epoch": 0.041404674338592955,
      "grad_norm": 5.100963115692139,
      "learning_rate": 8.279952550415185e-06,
      "loss": 0.0421,
      "step": 698
    },
    {
      "epoch": 0.04146399335627002,
      "grad_norm": 14.32669734954834,
      "learning_rate": 8.291814946619218e-06,
      "loss": 0.4117,
      "step": 699
    },
    {
      "epoch": 0.04152331237394709,
      "grad_norm": 20.12917137145996,
      "learning_rate": 8.303677342823252e-06,
      "loss": 0.1961,
      "step": 700
    },
    {
      "epoch": 0.04158263139162415,
      "grad_norm": 18.835294723510742,
      "learning_rate": 8.315539739027283e-06,
      "loss": 0.4809,
      "step": 701
    },
    {
      "epoch": 0.04164195040930122,
      "grad_norm": 0.2161027491092682,
      "learning_rate": 8.327402135231317e-06,
      "loss": 0.0029,
      "step": 702
    },
    {
      "epoch": 0.04170126942697829,
      "grad_norm": 8.784311294555664,
      "learning_rate": 8.33926453143535e-06,
      "loss": 0.0101,
      "step": 703
    },
    {
      "epoch": 0.041760588444655355,
      "grad_norm": 24.180673599243164,
      "learning_rate": 8.351126927639384e-06,
      "loss": 1.0418,
      "step": 704
    },
    {
      "epoch": 0.041819907462332426,
      "grad_norm": 5.245011806488037,
      "learning_rate": 8.362989323843418e-06,
      "loss": 0.0293,
      "step": 705
    },
    {
      "epoch": 0.04187922648000949,
      "grad_norm": 52.665382385253906,
      "learning_rate": 8.37485172004745e-06,
      "loss": 1.6497,
      "step": 706
    },
    {
      "epoch": 0.04193854549768656,
      "grad_norm": 0.3399477005004883,
      "learning_rate": 8.386714116251483e-06,
      "loss": 0.0034,
      "step": 707
    },
    {
      "epoch": 0.04199786451536362,
      "grad_norm": 4.636553764343262,
      "learning_rate": 8.398576512455516e-06,
      "loss": 0.0229,
      "step": 708
    },
    {
      "epoch": 0.04205718353304069,
      "grad_norm": 39.8346061706543,
      "learning_rate": 8.41043890865955e-06,
      "loss": 2.6167,
      "step": 709
    },
    {
      "epoch": 0.04211650255071776,
      "grad_norm": 33.05677795410156,
      "learning_rate": 8.422301304863583e-06,
      "loss": 0.2782,
      "step": 710
    },
    {
      "epoch": 0.042175821568394826,
      "grad_norm": 0.8372069597244263,
      "learning_rate": 8.434163701067615e-06,
      "loss": 0.0072,
      "step": 711
    },
    {
      "epoch": 0.0422351405860719,
      "grad_norm": 23.25339698791504,
      "learning_rate": 8.446026097271649e-06,
      "loss": 0.743,
      "step": 712
    },
    {
      "epoch": 0.04229445960374896,
      "grad_norm": 5.428773403167725,
      "learning_rate": 8.457888493475684e-06,
      "loss": 0.0833,
      "step": 713
    },
    {
      "epoch": 0.04235377862142603,
      "grad_norm": 30.969778060913086,
      "learning_rate": 8.469750889679716e-06,
      "loss": 0.776,
      "step": 714
    },
    {
      "epoch": 0.042413097639103094,
      "grad_norm": 27.32459831237793,
      "learning_rate": 8.48161328588375e-06,
      "loss": 0.2691,
      "step": 715
    },
    {
      "epoch": 0.042472416656780164,
      "grad_norm": 19.78827667236328,
      "learning_rate": 8.493475682087783e-06,
      "loss": 0.1584,
      "step": 716
    },
    {
      "epoch": 0.042531735674457234,
      "grad_norm": 9.620295524597168,
      "learning_rate": 8.505338078291816e-06,
      "loss": 1.2862,
      "step": 717
    },
    {
      "epoch": 0.0425910546921343,
      "grad_norm": 30.16031265258789,
      "learning_rate": 8.51720047449585e-06,
      "loss": 0.1809,
      "step": 718
    },
    {
      "epoch": 0.04265037370981137,
      "grad_norm": 0.49280041456222534,
      "learning_rate": 8.529062870699882e-06,
      "loss": 0.0037,
      "step": 719
    },
    {
      "epoch": 0.04270969272748843,
      "grad_norm": 15.601734161376953,
      "learning_rate": 8.540925266903915e-06,
      "loss": 0.4419,
      "step": 720
    },
    {
      "epoch": 0.0427690117451655,
      "grad_norm": 0.2725735902786255,
      "learning_rate": 8.552787663107949e-06,
      "loss": 0.004,
      "step": 721
    },
    {
      "epoch": 0.042828330762842565,
      "grad_norm": 7.299377918243408,
      "learning_rate": 8.564650059311982e-06,
      "loss": 0.072,
      "step": 722
    },
    {
      "epoch": 0.042887649780519635,
      "grad_norm": 133.84869384765625,
      "learning_rate": 8.576512455516016e-06,
      "loss": 0.1614,
      "step": 723
    },
    {
      "epoch": 0.042946968798196705,
      "grad_norm": 23.382909774780273,
      "learning_rate": 8.588374851720048e-06,
      "loss": 0.0456,
      "step": 724
    },
    {
      "epoch": 0.04300628781587377,
      "grad_norm": 21.97574234008789,
      "learning_rate": 8.600237247924081e-06,
      "loss": 0.0504,
      "step": 725
    },
    {
      "epoch": 0.04306560683355084,
      "grad_norm": 25.026670455932617,
      "learning_rate": 8.612099644128115e-06,
      "loss": 1.0046,
      "step": 726
    },
    {
      "epoch": 0.0431249258512279,
      "grad_norm": 0.6980884075164795,
      "learning_rate": 8.623962040332148e-06,
      "loss": 0.0023,
      "step": 727
    },
    {
      "epoch": 0.04318424486890497,
      "grad_norm": 34.39231872558594,
      "learning_rate": 8.635824436536182e-06,
      "loss": 0.1498,
      "step": 728
    },
    {
      "epoch": 0.043243563886582036,
      "grad_norm": 5.918300151824951,
      "learning_rate": 8.647686832740214e-06,
      "loss": 0.042,
      "step": 729
    },
    {
      "epoch": 0.043302882904259106,
      "grad_norm": 0.7783824801445007,
      "learning_rate": 8.659549228944247e-06,
      "loss": 0.0051,
      "step": 730
    },
    {
      "epoch": 0.04336220192193617,
      "grad_norm": 20.845651626586914,
      "learning_rate": 8.67141162514828e-06,
      "loss": 0.1943,
      "step": 731
    },
    {
      "epoch": 0.04342152093961324,
      "grad_norm": 2.902158737182617,
      "learning_rate": 8.683274021352314e-06,
      "loss": 0.0203,
      "step": 732
    },
    {
      "epoch": 0.04348083995729031,
      "grad_norm": 21.435888290405273,
      "learning_rate": 8.695136417556348e-06,
      "loss": 0.4202,
      "step": 733
    },
    {
      "epoch": 0.04354015897496737,
      "grad_norm": 24.025035858154297,
      "learning_rate": 8.70699881376038e-06,
      "loss": 1.2793,
      "step": 734
    },
    {
      "epoch": 0.04359947799264444,
      "grad_norm": 80.78585052490234,
      "learning_rate": 8.718861209964413e-06,
      "loss": 0.4517,
      "step": 735
    },
    {
      "epoch": 0.04365879701032151,
      "grad_norm": 12.758453369140625,
      "learning_rate": 8.730723606168446e-06,
      "loss": 0.0628,
      "step": 736
    },
    {
      "epoch": 0.04371811602799858,
      "grad_norm": 19.541767120361328,
      "learning_rate": 8.74258600237248e-06,
      "loss": 0.1875,
      "step": 737
    },
    {
      "epoch": 0.04377743504567564,
      "grad_norm": 13.864410400390625,
      "learning_rate": 8.754448398576513e-06,
      "loss": 0.0821,
      "step": 738
    },
    {
      "epoch": 0.04383675406335271,
      "grad_norm": 40.70596694946289,
      "learning_rate": 8.766310794780545e-06,
      "loss": 1.3377,
      "step": 739
    },
    {
      "epoch": 0.04389607308102978,
      "grad_norm": 29.64586639404297,
      "learning_rate": 8.778173190984579e-06,
      "loss": 0.4093,
      "step": 740
    },
    {
      "epoch": 0.043955392098706844,
      "grad_norm": 18.975603103637695,
      "learning_rate": 8.790035587188612e-06,
      "loss": 0.4944,
      "step": 741
    },
    {
      "epoch": 0.044014711116383914,
      "grad_norm": 54.60427474975586,
      "learning_rate": 8.801897983392646e-06,
      "loss": 0.2447,
      "step": 742
    },
    {
      "epoch": 0.04407403013406098,
      "grad_norm": 61.67592239379883,
      "learning_rate": 8.81376037959668e-06,
      "loss": 1.0987,
      "step": 743
    },
    {
      "epoch": 0.04413334915173805,
      "grad_norm": 25.218774795532227,
      "learning_rate": 8.825622775800713e-06,
      "loss": 0.1724,
      "step": 744
    },
    {
      "epoch": 0.04419266816941511,
      "grad_norm": 49.26459503173828,
      "learning_rate": 8.837485172004746e-06,
      "loss": 1.4766,
      "step": 745
    },
    {
      "epoch": 0.04425198718709218,
      "grad_norm": 24.970417022705078,
      "learning_rate": 8.84934756820878e-06,
      "loss": 0.1768,
      "step": 746
    },
    {
      "epoch": 0.04431130620476925,
      "grad_norm": 51.5588264465332,
      "learning_rate": 8.861209964412812e-06,
      "loss": 1.0591,
      "step": 747
    },
    {
      "epoch": 0.044370625222446315,
      "grad_norm": 27.942705154418945,
      "learning_rate": 8.873072360616845e-06,
      "loss": 0.4986,
      "step": 748
    },
    {
      "epoch": 0.044429944240123385,
      "grad_norm": 62.245338439941406,
      "learning_rate": 8.884934756820879e-06,
      "loss": 0.8757,
      "step": 749
    },
    {
      "epoch": 0.04448926325780045,
      "grad_norm": 20.391864776611328,
      "learning_rate": 8.896797153024912e-06,
      "loss": 0.1296,
      "step": 750
    },
    {
      "epoch": 0.04454858227547752,
      "grad_norm": 9.212031364440918,
      "learning_rate": 8.908659549228946e-06,
      "loss": 0.0832,
      "step": 751
    },
    {
      "epoch": 0.04460790129315458,
      "grad_norm": 39.5468635559082,
      "learning_rate": 8.920521945432978e-06,
      "loss": 0.0413,
      "step": 752
    },
    {
      "epoch": 0.04466722031083165,
      "grad_norm": 26.318767547607422,
      "learning_rate": 8.932384341637011e-06,
      "loss": 0.3616,
      "step": 753
    },
    {
      "epoch": 0.04472653932850872,
      "grad_norm": 1.3068032264709473,
      "learning_rate": 8.944246737841045e-06,
      "loss": 0.0101,
      "step": 754
    },
    {
      "epoch": 0.044785858346185786,
      "grad_norm": 7.105989456176758,
      "learning_rate": 8.956109134045078e-06,
      "loss": 0.0251,
      "step": 755
    },
    {
      "epoch": 0.044845177363862856,
      "grad_norm": 23.823932647705078,
      "learning_rate": 8.967971530249112e-06,
      "loss": 0.4416,
      "step": 756
    },
    {
      "epoch": 0.04490449638153992,
      "grad_norm": 22.42474365234375,
      "learning_rate": 8.979833926453144e-06,
      "loss": 0.1435,
      "step": 757
    },
    {
      "epoch": 0.04496381539921699,
      "grad_norm": 25.977075576782227,
      "learning_rate": 8.991696322657177e-06,
      "loss": 0.0796,
      "step": 758
    },
    {
      "epoch": 0.04502313441689405,
      "grad_norm": 33.07231140136719,
      "learning_rate": 9.00355871886121e-06,
      "loss": 0.1721,
      "step": 759
    },
    {
      "epoch": 0.045082453434571124,
      "grad_norm": 36.801856994628906,
      "learning_rate": 9.015421115065244e-06,
      "loss": 0.2849,
      "step": 760
    },
    {
      "epoch": 0.045141772452248194,
      "grad_norm": 14.680794715881348,
      "learning_rate": 9.027283511269278e-06,
      "loss": 0.11,
      "step": 761
    },
    {
      "epoch": 0.04520109146992526,
      "grad_norm": 33.11042022705078,
      "learning_rate": 9.03914590747331e-06,
      "loss": 1.3755,
      "step": 762
    },
    {
      "epoch": 0.04526041048760233,
      "grad_norm": 14.911454200744629,
      "learning_rate": 9.051008303677343e-06,
      "loss": 0.1171,
      "step": 763
    },
    {
      "epoch": 0.04531972950527939,
      "grad_norm": 307.574462890625,
      "learning_rate": 9.062870699881377e-06,
      "loss": 0.5444,
      "step": 764
    },
    {
      "epoch": 0.04537904852295646,
      "grad_norm": 25.81522560119629,
      "learning_rate": 9.07473309608541e-06,
      "loss": 0.1124,
      "step": 765
    },
    {
      "epoch": 0.045438367540633524,
      "grad_norm": 49.93372344970703,
      "learning_rate": 9.086595492289444e-06,
      "loss": 0.6058,
      "step": 766
    },
    {
      "epoch": 0.045497686558310595,
      "grad_norm": 28.964420318603516,
      "learning_rate": 9.098457888493475e-06,
      "loss": 0.0772,
      "step": 767
    },
    {
      "epoch": 0.045557005575987665,
      "grad_norm": 3.2547757625579834,
      "learning_rate": 9.110320284697509e-06,
      "loss": 0.0304,
      "step": 768
    },
    {
      "epoch": 0.04561632459366473,
      "grad_norm": 21.786407470703125,
      "learning_rate": 9.122182680901542e-06,
      "loss": 0.2354,
      "step": 769
    },
    {
      "epoch": 0.0456756436113418,
      "grad_norm": 24.739620208740234,
      "learning_rate": 9.134045077105576e-06,
      "loss": 0.2956,
      "step": 770
    },
    {
      "epoch": 0.04573496262901886,
      "grad_norm": 11.691837310791016,
      "learning_rate": 9.14590747330961e-06,
      "loss": 0.2249,
      "step": 771
    },
    {
      "epoch": 0.04579428164669593,
      "grad_norm": 0.6380642056465149,
      "learning_rate": 9.157769869513643e-06,
      "loss": 0.0044,
      "step": 772
    },
    {
      "epoch": 0.045853600664372995,
      "grad_norm": 76.87090301513672,
      "learning_rate": 9.169632265717677e-06,
      "loss": 1.4134,
      "step": 773
    },
    {
      "epoch": 0.045912919682050066,
      "grad_norm": 68.49305725097656,
      "learning_rate": 9.18149466192171e-06,
      "loss": 0.2335,
      "step": 774
    },
    {
      "epoch": 0.045972238699727136,
      "grad_norm": 17.85025405883789,
      "learning_rate": 9.193357058125742e-06,
      "loss": 0.1013,
      "step": 775
    },
    {
      "epoch": 0.0460315577174042,
      "grad_norm": 1.6760845184326172,
      "learning_rate": 9.205219454329775e-06,
      "loss": 0.0221,
      "step": 776
    },
    {
      "epoch": 0.04609087673508127,
      "grad_norm": 2.498687744140625,
      "learning_rate": 9.217081850533809e-06,
      "loss": 0.016,
      "step": 777
    },
    {
      "epoch": 0.04615019575275833,
      "grad_norm": 0.5918596386909485,
      "learning_rate": 9.228944246737842e-06,
      "loss": 0.0066,
      "step": 778
    },
    {
      "epoch": 0.0462095147704354,
      "grad_norm": 22.599138259887695,
      "learning_rate": 9.240806642941876e-06,
      "loss": 0.5691,
      "step": 779
    },
    {
      "epoch": 0.046268833788112466,
      "grad_norm": 3.529221296310425,
      "learning_rate": 9.252669039145908e-06,
      "loss": 0.0456,
      "step": 780
    },
    {
      "epoch": 0.04632815280578954,
      "grad_norm": 45.79658889770508,
      "learning_rate": 9.264531435349941e-06,
      "loss": 1.0673,
      "step": 781
    },
    {
      "epoch": 0.0463874718234666,
      "grad_norm": 5.142307758331299,
      "learning_rate": 9.276393831553975e-06,
      "loss": 0.0251,
      "step": 782
    },
    {
      "epoch": 0.04644679084114367,
      "grad_norm": 0.4891352355480194,
      "learning_rate": 9.288256227758008e-06,
      "loss": 0.0031,
      "step": 783
    },
    {
      "epoch": 0.04650610985882074,
      "grad_norm": 5.78629207611084,
      "learning_rate": 9.300118623962042e-06,
      "loss": 0.0809,
      "step": 784
    },
    {
      "epoch": 0.046565428876497804,
      "grad_norm": 10.061049461364746,
      "learning_rate": 9.311981020166074e-06,
      "loss": 0.1097,
      "step": 785
    },
    {
      "epoch": 0.046624747894174874,
      "grad_norm": 21.80876922607422,
      "learning_rate": 9.323843416370107e-06,
      "loss": 1.0082,
      "step": 786
    },
    {
      "epoch": 0.04668406691185194,
      "grad_norm": 2.6491408348083496,
      "learning_rate": 9.33570581257414e-06,
      "loss": 0.0195,
      "step": 787
    },
    {
      "epoch": 0.04674338592952901,
      "grad_norm": 33.79179382324219,
      "learning_rate": 9.347568208778174e-06,
      "loss": 0.2097,
      "step": 788
    },
    {
      "epoch": 0.04680270494720607,
      "grad_norm": 11.839632034301758,
      "learning_rate": 9.359430604982208e-06,
      "loss": 0.2663,
      "step": 789
    },
    {
      "epoch": 0.04686202396488314,
      "grad_norm": 45.06708908081055,
      "learning_rate": 9.37129300118624e-06,
      "loss": 0.9288,
      "step": 790
    },
    {
      "epoch": 0.04692134298256021,
      "grad_norm": 14.91472339630127,
      "learning_rate": 9.383155397390273e-06,
      "loss": 0.0745,
      "step": 791
    },
    {
      "epoch": 0.046980662000237275,
      "grad_norm": 0.5310548543930054,
      "learning_rate": 9.395017793594307e-06,
      "loss": 0.0048,
      "step": 792
    },
    {
      "epoch": 0.047039981017914345,
      "grad_norm": 8.152522087097168,
      "learning_rate": 9.40688018979834e-06,
      "loss": 0.06,
      "step": 793
    },
    {
      "epoch": 0.04709930003559141,
      "grad_norm": 16.505725860595703,
      "learning_rate": 9.418742586002374e-06,
      "loss": 0.0513,
      "step": 794
    },
    {
      "epoch": 0.04715861905326848,
      "grad_norm": 2.7966485023498535,
      "learning_rate": 9.430604982206405e-06,
      "loss": 0.0192,
      "step": 795
    },
    {
      "epoch": 0.04721793807094554,
      "grad_norm": 29.612550735473633,
      "learning_rate": 9.442467378410439e-06,
      "loss": 0.0808,
      "step": 796
    },
    {
      "epoch": 0.04727725708862261,
      "grad_norm": 27.459535598754883,
      "learning_rate": 9.454329774614472e-06,
      "loss": 0.3538,
      "step": 797
    },
    {
      "epoch": 0.04733657610629968,
      "grad_norm": 24.624967575073242,
      "learning_rate": 9.466192170818506e-06,
      "loss": 0.4023,
      "step": 798
    },
    {
      "epoch": 0.047395895123976746,
      "grad_norm": 6.4704179763793945,
      "learning_rate": 9.47805456702254e-06,
      "loss": 0.0382,
      "step": 799
    },
    {
      "epoch": 0.047455214141653816,
      "grad_norm": 24.134374618530273,
      "learning_rate": 9.489916963226573e-06,
      "loss": 0.05,
      "step": 800
    },
    {
      "epoch": 0.04751453315933088,
      "grad_norm": 110.41009521484375,
      "learning_rate": 9.501779359430607e-06,
      "loss": 0.659,
      "step": 801
    },
    {
      "epoch": 0.04757385217700795,
      "grad_norm": 28.885698318481445,
      "learning_rate": 9.51364175563464e-06,
      "loss": 2.0588,
      "step": 802
    },
    {
      "epoch": 0.04763317119468501,
      "grad_norm": 10.667218208312988,
      "learning_rate": 9.525504151838672e-06,
      "loss": 0.2735,
      "step": 803
    },
    {
      "epoch": 0.04769249021236208,
      "grad_norm": 11.483024597167969,
      "learning_rate": 9.537366548042705e-06,
      "loss": 0.0574,
      "step": 804
    },
    {
      "epoch": 0.047751809230039154,
      "grad_norm": 35.85350036621094,
      "learning_rate": 9.549228944246739e-06,
      "loss": 0.388,
      "step": 805
    },
    {
      "epoch": 0.04781112824771622,
      "grad_norm": 13.471258163452148,
      "learning_rate": 9.561091340450772e-06,
      "loss": 0.0405,
      "step": 806
    },
    {
      "epoch": 0.04787044726539329,
      "grad_norm": 17.730636596679688,
      "learning_rate": 9.572953736654806e-06,
      "loss": 0.2056,
      "step": 807
    },
    {
      "epoch": 0.04792976628307035,
      "grad_norm": 2.3427841663360596,
      "learning_rate": 9.584816132858838e-06,
      "loss": 0.0088,
      "step": 808
    },
    {
      "epoch": 0.04798908530074742,
      "grad_norm": 8.981420516967773,
      "learning_rate": 9.596678529062871e-06,
      "loss": 0.1786,
      "step": 809
    },
    {
      "epoch": 0.048048404318424484,
      "grad_norm": 1.7298816442489624,
      "learning_rate": 9.608540925266905e-06,
      "loss": 0.0094,
      "step": 810
    },
    {
      "epoch": 0.048107723336101554,
      "grad_norm": 8.489082336425781,
      "learning_rate": 9.620403321470938e-06,
      "loss": 0.6462,
      "step": 811
    },
    {
      "epoch": 0.048167042353778625,
      "grad_norm": 0.3342575430870056,
      "learning_rate": 9.632265717674972e-06,
      "loss": 0.0025,
      "step": 812
    },
    {
      "epoch": 0.04822636137145569,
      "grad_norm": 17.98828125,
      "learning_rate": 9.644128113879004e-06,
      "loss": 0.4819,
      "step": 813
    },
    {
      "epoch": 0.04828568038913276,
      "grad_norm": 17.288305282592773,
      "learning_rate": 9.655990510083037e-06,
      "loss": 0.1618,
      "step": 814
    },
    {
      "epoch": 0.04834499940680982,
      "grad_norm": 0.07694462686777115,
      "learning_rate": 9.66785290628707e-06,
      "loss": 0.0007,
      "step": 815
    },
    {
      "epoch": 0.04840431842448689,
      "grad_norm": 20.668710708618164,
      "learning_rate": 9.679715302491104e-06,
      "loss": 0.1436,
      "step": 816
    },
    {
      "epoch": 0.048463637442163955,
      "grad_norm": 1.3192903995513916,
      "learning_rate": 9.691577698695138e-06,
      "loss": 0.0042,
      "step": 817
    },
    {
      "epoch": 0.048522956459841025,
      "grad_norm": 10.429515838623047,
      "learning_rate": 9.70344009489917e-06,
      "loss": 0.0257,
      "step": 818
    },
    {
      "epoch": 0.048582275477518096,
      "grad_norm": 0.5979418158531189,
      "learning_rate": 9.715302491103203e-06,
      "loss": 0.006,
      "step": 819
    },
    {
      "epoch": 0.04864159449519516,
      "grad_norm": 10.240522384643555,
      "learning_rate": 9.727164887307237e-06,
      "loss": 0.0314,
      "step": 820
    },
    {
      "epoch": 0.04870091351287223,
      "grad_norm": 19.857187271118164,
      "learning_rate": 9.73902728351127e-06,
      "loss": 0.1498,
      "step": 821
    },
    {
      "epoch": 0.04876023253054929,
      "grad_norm": 6.680696487426758,
      "learning_rate": 9.750889679715304e-06,
      "loss": 0.0218,
      "step": 822
    },
    {
      "epoch": 0.04881955154822636,
      "grad_norm": 17.226591110229492,
      "learning_rate": 9.762752075919336e-06,
      "loss": 0.6234,
      "step": 823
    },
    {
      "epoch": 0.048878870565903426,
      "grad_norm": 19.541484832763672,
      "learning_rate": 9.774614472123369e-06,
      "loss": 0.1609,
      "step": 824
    },
    {
      "epoch": 0.048938189583580496,
      "grad_norm": 46.395599365234375,
      "learning_rate": 9.786476868327403e-06,
      "loss": 0.2078,
      "step": 825
    },
    {
      "epoch": 0.04899750860125756,
      "grad_norm": 24.150651931762695,
      "learning_rate": 9.798339264531436e-06,
      "loss": 0.1417,
      "step": 826
    },
    {
      "epoch": 0.04905682761893463,
      "grad_norm": 0.1326362043619156,
      "learning_rate": 9.81020166073547e-06,
      "loss": 0.0023,
      "step": 827
    },
    {
      "epoch": 0.0491161466366117,
      "grad_norm": 76.5613784790039,
      "learning_rate": 9.822064056939501e-06,
      "loss": 0.5314,
      "step": 828
    },
    {
      "epoch": 0.049175465654288764,
      "grad_norm": 8.506689071655273,
      "learning_rate": 9.833926453143537e-06,
      "loss": 0.0464,
      "step": 829
    },
    {
      "epoch": 0.049234784671965834,
      "grad_norm": 22.201995849609375,
      "learning_rate": 9.84578884934757e-06,
      "loss": 0.4057,
      "step": 830
    },
    {
      "epoch": 0.0492941036896429,
      "grad_norm": 0.27395954728126526,
      "learning_rate": 9.857651245551602e-06,
      "loss": 0.0017,
      "step": 831
    },
    {
      "epoch": 0.04935342270731997,
      "grad_norm": 9.031987190246582,
      "learning_rate": 9.869513641755635e-06,
      "loss": 0.2443,
      "step": 832
    },
    {
      "epoch": 0.04941274172499703,
      "grad_norm": 1.484173059463501,
      "learning_rate": 9.881376037959669e-06,
      "loss": 0.0126,
      "step": 833
    },
    {
      "epoch": 0.0494720607426741,
      "grad_norm": 0.5021127462387085,
      "learning_rate": 9.893238434163703e-06,
      "loss": 0.0076,
      "step": 834
    },
    {
      "epoch": 0.04953137976035117,
      "grad_norm": 23.8610897064209,
      "learning_rate": 9.905100830367736e-06,
      "loss": 0.0769,
      "step": 835
    },
    {
      "epoch": 0.049590698778028235,
      "grad_norm": 4.6731414794921875,
      "learning_rate": 9.916963226571768e-06,
      "loss": 0.0846,
      "step": 836
    },
    {
      "epoch": 0.049650017795705305,
      "grad_norm": 1.5037034749984741,
      "learning_rate": 9.928825622775801e-06,
      "loss": 0.0124,
      "step": 837
    },
    {
      "epoch": 0.04970933681338237,
      "grad_norm": 18.679290771484375,
      "learning_rate": 9.940688018979835e-06,
      "loss": 0.0771,
      "step": 838
    },
    {
      "epoch": 0.04976865583105944,
      "grad_norm": 18.829063415527344,
      "learning_rate": 9.952550415183868e-06,
      "loss": 0.4064,
      "step": 839
    },
    {
      "epoch": 0.0498279748487365,
      "grad_norm": 3.988823175430298,
      "learning_rate": 9.964412811387902e-06,
      "loss": 0.0251,
      "step": 840
    },
    {
      "epoch": 0.04988729386641357,
      "grad_norm": 0.3949398994445801,
      "learning_rate": 9.976275207591934e-06,
      "loss": 0.0038,
      "step": 841
    },
    {
      "epoch": 0.04994661288409064,
      "grad_norm": 6.787082195281982,
      "learning_rate": 9.988137603795967e-06,
      "loss": 0.0404,
      "step": 842
    },
    {
      "epoch": 0.050005931901767706,
      "grad_norm": 30.504274368286133,
      "learning_rate": 1e-05,
      "loss": 0.4323,
      "step": 843
    },
    {
      "epoch": 0.050065250919444776,
      "grad_norm": 15.882794380187988,
      "learning_rate": 1.0011862396204034e-05,
      "loss": 0.3883,
      "step": 844
    },
    {
      "epoch": 0.05012456993712184,
      "grad_norm": 23.94886016845703,
      "learning_rate": 1.0023724792408068e-05,
      "loss": 0.4492,
      "step": 845
    },
    {
      "epoch": 0.05018388895479891,
      "grad_norm": 3.9921133518218994,
      "learning_rate": 1.0035587188612101e-05,
      "loss": 0.0327,
      "step": 846
    },
    {
      "epoch": 0.05024320797247597,
      "grad_norm": 25.73537826538086,
      "learning_rate": 1.0047449584816133e-05,
      "loss": 0.1866,
      "step": 847
    },
    {
      "epoch": 0.05030252699015304,
      "grad_norm": 91.80765533447266,
      "learning_rate": 1.0059311981020168e-05,
      "loss": 0.601,
      "step": 848
    },
    {
      "epoch": 0.05036184600783011,
      "grad_norm": 30.08064842224121,
      "learning_rate": 1.00711743772242e-05,
      "loss": 0.2625,
      "step": 849
    },
    {
      "epoch": 0.05042116502550718,
      "grad_norm": 0.8778267502784729,
      "learning_rate": 1.0083036773428234e-05,
      "loss": 0.0065,
      "step": 850
    },
    {
      "epoch": 0.05048048404318425,
      "grad_norm": 21.25672721862793,
      "learning_rate": 1.0094899169632266e-05,
      "loss": 0.2005,
      "step": 851
    },
    {
      "epoch": 0.05053980306086131,
      "grad_norm": 0.21478280425071716,
      "learning_rate": 1.01067615658363e-05,
      "loss": 0.0023,
      "step": 852
    },
    {
      "epoch": 0.05059912207853838,
      "grad_norm": 40.69270324707031,
      "learning_rate": 1.0118623962040333e-05,
      "loss": 1.0537,
      "step": 853
    },
    {
      "epoch": 0.050658441096215444,
      "grad_norm": 57.49634552001953,
      "learning_rate": 1.0130486358244366e-05,
      "loss": 0.6056,
      "step": 854
    },
    {
      "epoch": 0.050717760113892514,
      "grad_norm": 0.3839806914329529,
      "learning_rate": 1.01423487544484e-05,
      "loss": 0.0035,
      "step": 855
    },
    {
      "epoch": 0.050777079131569584,
      "grad_norm": 0.9462630748748779,
      "learning_rate": 1.0154211150652433e-05,
      "loss": 0.0061,
      "step": 856
    },
    {
      "epoch": 0.05083639814924665,
      "grad_norm": 6.799269676208496,
      "learning_rate": 1.0166073546856465e-05,
      "loss": 0.062,
      "step": 857
    },
    {
      "epoch": 0.05089571716692372,
      "grad_norm": 41.322574615478516,
      "learning_rate": 1.01779359430605e-05,
      "loss": 0.6828,
      "step": 858
    },
    {
      "epoch": 0.05095503618460078,
      "grad_norm": 33.78376388549805,
      "learning_rate": 1.0189798339264532e-05,
      "loss": 0.7522,
      "step": 859
    },
    {
      "epoch": 0.05101435520227785,
      "grad_norm": 3.7466797828674316,
      "learning_rate": 1.0201660735468566e-05,
      "loss": 0.0269,
      "step": 860
    },
    {
      "epoch": 0.051073674219954915,
      "grad_norm": 4.425155162811279,
      "learning_rate": 1.0213523131672597e-05,
      "loss": 0.0388,
      "step": 861
    },
    {
      "epoch": 0.051132993237631985,
      "grad_norm": 9.139689445495605,
      "learning_rate": 1.0225385527876633e-05,
      "loss": 0.0565,
      "step": 862
    },
    {
      "epoch": 0.051192312255309055,
      "grad_norm": 83.28683471679688,
      "learning_rate": 1.0237247924080664e-05,
      "loss": 1.216,
      "step": 863
    },
    {
      "epoch": 0.05125163127298612,
      "grad_norm": 122.18856811523438,
      "learning_rate": 1.0249110320284698e-05,
      "loss": 0.283,
      "step": 864
    },
    {
      "epoch": 0.05131095029066319,
      "grad_norm": 24.61338996887207,
      "learning_rate": 1.0260972716488731e-05,
      "loss": 0.1057,
      "step": 865
    },
    {
      "epoch": 0.05137026930834025,
      "grad_norm": 0.1422336846590042,
      "learning_rate": 1.0272835112692765e-05,
      "loss": 0.0014,
      "step": 866
    },
    {
      "epoch": 0.05142958832601732,
      "grad_norm": 20.569591522216797,
      "learning_rate": 1.0284697508896797e-05,
      "loss": 0.0588,
      "step": 867
    },
    {
      "epoch": 0.051488907343694386,
      "grad_norm": 0.1580682098865509,
      "learning_rate": 1.0296559905100832e-05,
      "loss": 0.0028,
      "step": 868
    },
    {
      "epoch": 0.051548226361371456,
      "grad_norm": 0.8620279431343079,
      "learning_rate": 1.0308422301304864e-05,
      "loss": 0.0051,
      "step": 869
    },
    {
      "epoch": 0.051607545379048526,
      "grad_norm": 6.585638046264648,
      "learning_rate": 1.0320284697508897e-05,
      "loss": 0.0305,
      "step": 870
    },
    {
      "epoch": 0.05166686439672559,
      "grad_norm": 75.3847885131836,
      "learning_rate": 1.033214709371293e-05,
      "loss": 1.5384,
      "step": 871
    },
    {
      "epoch": 0.05172618341440266,
      "grad_norm": 1.1435441970825195,
      "learning_rate": 1.0344009489916964e-05,
      "loss": 0.0091,
      "step": 872
    },
    {
      "epoch": 0.05178550243207972,
      "grad_norm": 34.62624740600586,
      "learning_rate": 1.0355871886120998e-05,
      "loss": 1.0466,
      "step": 873
    },
    {
      "epoch": 0.051844821449756794,
      "grad_norm": 12.504073143005371,
      "learning_rate": 1.036773428232503e-05,
      "loss": 0.5376,
      "step": 874
    },
    {
      "epoch": 0.05190414046743386,
      "grad_norm": 0.9891764521598816,
      "learning_rate": 1.0379596678529065e-05,
      "loss": 0.0065,
      "step": 875
    },
    {
      "epoch": 0.05196345948511093,
      "grad_norm": 0.33230772614479065,
      "learning_rate": 1.0391459074733097e-05,
      "loss": 0.0025,
      "step": 876
    },
    {
      "epoch": 0.05202277850278799,
      "grad_norm": 0.28564363718032837,
      "learning_rate": 1.040332147093713e-05,
      "loss": 0.0018,
      "step": 877
    },
    {
      "epoch": 0.05208209752046506,
      "grad_norm": 8.986251831054688,
      "learning_rate": 1.0415183867141164e-05,
      "loss": 0.0822,
      "step": 878
    },
    {
      "epoch": 0.05214141653814213,
      "grad_norm": 28.941326141357422,
      "learning_rate": 1.0427046263345197e-05,
      "loss": 0.37,
      "step": 879
    },
    {
      "epoch": 0.052200735555819194,
      "grad_norm": 0.29156944155693054,
      "learning_rate": 1.043890865954923e-05,
      "loss": 0.0029,
      "step": 880
    },
    {
      "epoch": 0.052260054573496265,
      "grad_norm": 18.322315216064453,
      "learning_rate": 1.0450771055753264e-05,
      "loss": 0.107,
      "step": 881
    },
    {
      "epoch": 0.05231937359117333,
      "grad_norm": 7.973185062408447,
      "learning_rate": 1.0462633451957296e-05,
      "loss": 0.0772,
      "step": 882
    },
    {
      "epoch": 0.0523786926088504,
      "grad_norm": 198.51380920410156,
      "learning_rate": 1.047449584816133e-05,
      "loss": 0.4144,
      "step": 883
    },
    {
      "epoch": 0.05243801162652746,
      "grad_norm": 0.22183959186077118,
      "learning_rate": 1.0486358244365362e-05,
      "loss": 0.0021,
      "step": 884
    },
    {
      "epoch": 0.05249733064420453,
      "grad_norm": 56.79983139038086,
      "learning_rate": 1.0498220640569397e-05,
      "loss": 0.8753,
      "step": 885
    },
    {
      "epoch": 0.0525566496618816,
      "grad_norm": 3.440328359603882,
      "learning_rate": 1.0510083036773429e-05,
      "loss": 0.0177,
      "step": 886
    },
    {
      "epoch": 0.052615968679558665,
      "grad_norm": 9.61678409576416,
      "learning_rate": 1.0521945432977462e-05,
      "loss": 0.0644,
      "step": 887
    },
    {
      "epoch": 0.052675287697235736,
      "grad_norm": 0.03253112733364105,
      "learning_rate": 1.0533807829181496e-05,
      "loss": 0.0005,
      "step": 888
    },
    {
      "epoch": 0.0527346067149128,
      "grad_norm": 9.023521423339844,
      "learning_rate": 1.0545670225385529e-05,
      "loss": 0.0484,
      "step": 889
    },
    {
      "epoch": 0.05279392573258987,
      "grad_norm": 0.7013326287269592,
      "learning_rate": 1.0557532621589561e-05,
      "loss": 0.0048,
      "step": 890
    },
    {
      "epoch": 0.05285324475026693,
      "grad_norm": 33.02549362182617,
      "learning_rate": 1.0569395017793596e-05,
      "loss": 0.1552,
      "step": 891
    },
    {
      "epoch": 0.052912563767944,
      "grad_norm": 0.8775694966316223,
      "learning_rate": 1.0581257413997628e-05,
      "loss": 0.0054,
      "step": 892
    },
    {
      "epoch": 0.05297188278562107,
      "grad_norm": 11.554401397705078,
      "learning_rate": 1.0593119810201662e-05,
      "loss": 0.0615,
      "step": 893
    },
    {
      "epoch": 0.053031201803298136,
      "grad_norm": 44.71805953979492,
      "learning_rate": 1.0604982206405693e-05,
      "loss": 0.29,
      "step": 894
    },
    {
      "epoch": 0.05309052082097521,
      "grad_norm": 15.68670654296875,
      "learning_rate": 1.0616844602609729e-05,
      "loss": 0.4706,
      "step": 895
    },
    {
      "epoch": 0.05314983983865227,
      "grad_norm": 47.51082229614258,
      "learning_rate": 1.062870699881376e-05,
      "loss": 0.7414,
      "step": 896
    },
    {
      "epoch": 0.05320915885632934,
      "grad_norm": 39.706851959228516,
      "learning_rate": 1.0640569395017794e-05,
      "loss": 0.2821,
      "step": 897
    },
    {
      "epoch": 0.053268477874006404,
      "grad_norm": 9.89735221862793,
      "learning_rate": 1.0652431791221827e-05,
      "loss": 0.0497,
      "step": 898
    },
    {
      "epoch": 0.053327796891683474,
      "grad_norm": 5.79826021194458,
      "learning_rate": 1.0664294187425861e-05,
      "loss": 0.0278,
      "step": 899
    },
    {
      "epoch": 0.053387115909360544,
      "grad_norm": 31.471208572387695,
      "learning_rate": 1.0676156583629893e-05,
      "loss": 0.1812,
      "step": 900
    },
    {
      "epoch": 0.05344643492703761,
      "grad_norm": 0.06478448212146759,
      "learning_rate": 1.0688018979833928e-05,
      "loss": 0.0007,
      "step": 901
    },
    {
      "epoch": 0.05350575394471468,
      "grad_norm": 12.399961471557617,
      "learning_rate": 1.0699881376037962e-05,
      "loss": 0.2796,
      "step": 902
    },
    {
      "epoch": 0.05356507296239174,
      "grad_norm": 9.849668502807617,
      "learning_rate": 1.0711743772241993e-05,
      "loss": 0.0322,
      "step": 903
    },
    {
      "epoch": 0.05362439198006881,
      "grad_norm": 0.47581997513771057,
      "learning_rate": 1.0723606168446029e-05,
      "loss": 0.0021,
      "step": 904
    },
    {
      "epoch": 0.053683710997745875,
      "grad_norm": 10.531790733337402,
      "learning_rate": 1.073546856465006e-05,
      "loss": 0.0392,
      "step": 905
    },
    {
      "epoch": 0.053743030015422945,
      "grad_norm": 15.624112129211426,
      "learning_rate": 1.0747330960854094e-05,
      "loss": 0.1033,
      "step": 906
    },
    {
      "epoch": 0.053802349033100015,
      "grad_norm": 0.1516224443912506,
      "learning_rate": 1.0759193357058126e-05,
      "loss": 0.0015,
      "step": 907
    },
    {
      "epoch": 0.05386166805077708,
      "grad_norm": 0.03600407764315605,
      "learning_rate": 1.0771055753262161e-05,
      "loss": 0.0007,
      "step": 908
    },
    {
      "epoch": 0.05392098706845415,
      "grad_norm": 18.71119499206543,
      "learning_rate": 1.0782918149466193e-05,
      "loss": 0.7524,
      "step": 909
    },
    {
      "epoch": 0.05398030608613121,
      "grad_norm": 1.6667531728744507,
      "learning_rate": 1.0794780545670226e-05,
      "loss": 0.0062,
      "step": 910
    },
    {
      "epoch": 0.05403962510380828,
      "grad_norm": 0.366011381149292,
      "learning_rate": 1.080664294187426e-05,
      "loss": 0.002,
      "step": 911
    },
    {
      "epoch": 0.054098944121485346,
      "grad_norm": 0.006559840403497219,
      "learning_rate": 1.0818505338078293e-05,
      "loss": 0.0001,
      "step": 912
    },
    {
      "epoch": 0.054158263139162416,
      "grad_norm": 10.740619659423828,
      "learning_rate": 1.0830367734282325e-05,
      "loss": 0.1112,
      "step": 913
    },
    {
      "epoch": 0.054217582156839486,
      "grad_norm": 2.9949276447296143,
      "learning_rate": 1.084223013048636e-05,
      "loss": 0.0151,
      "step": 914
    },
    {
      "epoch": 0.05427690117451655,
      "grad_norm": 9.473036766052246,
      "learning_rate": 1.0854092526690392e-05,
      "loss": 0.0517,
      "step": 915
    },
    {
      "epoch": 0.05433622019219362,
      "grad_norm": 50.79595947265625,
      "learning_rate": 1.0865954922894426e-05,
      "loss": 0.6555,
      "step": 916
    },
    {
      "epoch": 0.05439553920987068,
      "grad_norm": 24.756601333618164,
      "learning_rate": 1.0877817319098458e-05,
      "loss": 0.184,
      "step": 917
    },
    {
      "epoch": 0.05445485822754775,
      "grad_norm": 0.015882911160588264,
      "learning_rate": 1.0889679715302493e-05,
      "loss": 0.0002,
      "step": 918
    },
    {
      "epoch": 0.05451417724522482,
      "grad_norm": 24.90085792541504,
      "learning_rate": 1.0901542111506525e-05,
      "loss": 1.0863,
      "step": 919
    },
    {
      "epoch": 0.05457349626290189,
      "grad_norm": 109.80657196044922,
      "learning_rate": 1.0913404507710558e-05,
      "loss": 0.4578,
      "step": 920
    },
    {
      "epoch": 0.05463281528057895,
      "grad_norm": 2.7340619564056396,
      "learning_rate": 1.0925266903914592e-05,
      "loss": 0.0129,
      "step": 921
    },
    {
      "epoch": 0.05469213429825602,
      "grad_norm": 2.6813716888427734,
      "learning_rate": 1.0937129300118625e-05,
      "loss": 0.0035,
      "step": 922
    },
    {
      "epoch": 0.05475145331593309,
      "grad_norm": 8.673164367675781,
      "learning_rate": 1.0948991696322657e-05,
      "loss": 0.0228,
      "step": 923
    },
    {
      "epoch": 0.054810772333610154,
      "grad_norm": 3.1165523529052734,
      "learning_rate": 1.0960854092526692e-05,
      "loss": 0.0332,
      "step": 924
    },
    {
      "epoch": 0.054870091351287224,
      "grad_norm": 21.537979125976562,
      "learning_rate": 1.0972716488730724e-05,
      "loss": 1.1462,
      "step": 925
    },
    {
      "epoch": 0.05492941036896429,
      "grad_norm": 0.36030688881874084,
      "learning_rate": 1.0984578884934757e-05,
      "loss": 0.0023,
      "step": 926
    },
    {
      "epoch": 0.05498872938664136,
      "grad_norm": 7.915945053100586,
      "learning_rate": 1.099644128113879e-05,
      "loss": 0.0208,
      "step": 927
    },
    {
      "epoch": 0.05504804840431842,
      "grad_norm": 101.720458984375,
      "learning_rate": 1.1008303677342825e-05,
      "loss": 2.1627,
      "step": 928
    },
    {
      "epoch": 0.05510736742199549,
      "grad_norm": 2.6518144607543945,
      "learning_rate": 1.1020166073546856e-05,
      "loss": 0.0213,
      "step": 929
    },
    {
      "epoch": 0.05516668643967256,
      "grad_norm": 0.18704870343208313,
      "learning_rate": 1.103202846975089e-05,
      "loss": 0.0016,
      "step": 930
    },
    {
      "epoch": 0.055226005457349625,
      "grad_norm": 50.66997528076172,
      "learning_rate": 1.1043890865954925e-05,
      "loss": 0.6668,
      "step": 931
    },
    {
      "epoch": 0.055285324475026695,
      "grad_norm": 0.24111083149909973,
      "learning_rate": 1.1055753262158957e-05,
      "loss": 0.0025,
      "step": 932
    },
    {
      "epoch": 0.05534464349270376,
      "grad_norm": 54.04998016357422,
      "learning_rate": 1.106761565836299e-05,
      "loss": 0.1175,
      "step": 933
    },
    {
      "epoch": 0.05540396251038083,
      "grad_norm": 0.016624270007014275,
      "learning_rate": 1.1079478054567024e-05,
      "loss": 0.0002,
      "step": 934
    },
    {
      "epoch": 0.05546328152805789,
      "grad_norm": 0.879493236541748,
      "learning_rate": 1.1091340450771057e-05,
      "loss": 0.0097,
      "step": 935
    },
    {
      "epoch": 0.05552260054573496,
      "grad_norm": 30.72482681274414,
      "learning_rate": 1.110320284697509e-05,
      "loss": 0.2442,
      "step": 936
    },
    {
      "epoch": 0.05558191956341203,
      "grad_norm": 32.08201599121094,
      "learning_rate": 1.1115065243179125e-05,
      "loss": 0.2026,
      "step": 937
    },
    {
      "epoch": 0.055641238581089096,
      "grad_norm": 0.026063187047839165,
      "learning_rate": 1.1126927639383156e-05,
      "loss": 0.0004,
      "step": 938
    },
    {
      "epoch": 0.055700557598766166,
      "grad_norm": 7.0774827003479,
      "learning_rate": 1.113879003558719e-05,
      "loss": 0.0373,
      "step": 939
    },
    {
      "epoch": 0.05575987661644323,
      "grad_norm": 11.6552152633667,
      "learning_rate": 1.1150652431791222e-05,
      "loss": 0.1523,
      "step": 940
    },
    {
      "epoch": 0.0558191956341203,
      "grad_norm": 0.7160587310791016,
      "learning_rate": 1.1162514827995257e-05,
      "loss": 0.0035,
      "step": 941
    },
    {
      "epoch": 0.05587851465179736,
      "grad_norm": 0.4723298251628876,
      "learning_rate": 1.1174377224199289e-05,
      "loss": 0.0042,
      "step": 942
    },
    {
      "epoch": 0.055937833669474434,
      "grad_norm": 5.42296838760376,
      "learning_rate": 1.1186239620403322e-05,
      "loss": 0.0204,
      "step": 943
    },
    {
      "epoch": 0.055997152687151504,
      "grad_norm": 2.9554128646850586,
      "learning_rate": 1.1198102016607356e-05,
      "loss": 0.0283,
      "step": 944
    },
    {
      "epoch": 0.05605647170482857,
      "grad_norm": 30.550735473632812,
      "learning_rate": 1.120996441281139e-05,
      "loss": 1.9377,
      "step": 945
    },
    {
      "epoch": 0.05611579072250564,
      "grad_norm": 53.35728454589844,
      "learning_rate": 1.1221826809015421e-05,
      "loss": 0.3738,
      "step": 946
    },
    {
      "epoch": 0.0561751097401827,
      "grad_norm": 45.512264251708984,
      "learning_rate": 1.1233689205219456e-05,
      "loss": 0.7352,
      "step": 947
    },
    {
      "epoch": 0.05623442875785977,
      "grad_norm": 0.21939876675605774,
      "learning_rate": 1.1245551601423488e-05,
      "loss": 0.0021,
      "step": 948
    },
    {
      "epoch": 0.056293747775536834,
      "grad_norm": 0.011403790675103664,
      "learning_rate": 1.1257413997627522e-05,
      "loss": 0.0003,
      "step": 949
    },
    {
      "epoch": 0.056353066793213905,
      "grad_norm": 0.11560017615556717,
      "learning_rate": 1.1269276393831553e-05,
      "loss": 0.0015,
      "step": 950
    },
    {
      "epoch": 0.056412385810890975,
      "grad_norm": 18.93577003479004,
      "learning_rate": 1.1281138790035589e-05,
      "loss": 0.1438,
      "step": 951
    },
    {
      "epoch": 0.05647170482856804,
      "grad_norm": 10.565010070800781,
      "learning_rate": 1.129300118623962e-05,
      "loss": 0.1635,
      "step": 952
    },
    {
      "epoch": 0.05653102384624511,
      "grad_norm": 1.1205800771713257,
      "learning_rate": 1.1304863582443654e-05,
      "loss": 0.0089,
      "step": 953
    },
    {
      "epoch": 0.05659034286392217,
      "grad_norm": 12.748688697814941,
      "learning_rate": 1.1316725978647688e-05,
      "loss": 0.1231,
      "step": 954
    },
    {
      "epoch": 0.05664966188159924,
      "grad_norm": 60.690216064453125,
      "learning_rate": 1.1328588374851721e-05,
      "loss": 1.0453,
      "step": 955
    },
    {
      "epoch": 0.056708980899276305,
      "grad_norm": 1.13016676902771,
      "learning_rate": 1.1340450771055753e-05,
      "loss": 0.0078,
      "step": 956
    },
    {
      "epoch": 0.056768299916953376,
      "grad_norm": 0.035989709198474884,
      "learning_rate": 1.1352313167259788e-05,
      "loss": 0.0008,
      "step": 957
    },
    {
      "epoch": 0.056827618934630446,
      "grad_norm": 2.2857511043548584,
      "learning_rate": 1.136417556346382e-05,
      "loss": 0.0147,
      "step": 958
    },
    {
      "epoch": 0.05688693795230751,
      "grad_norm": 15.498078346252441,
      "learning_rate": 1.1376037959667853e-05,
      "loss": 0.5,
      "step": 959
    },
    {
      "epoch": 0.05694625696998458,
      "grad_norm": 33.62834930419922,
      "learning_rate": 1.1387900355871889e-05,
      "loss": 0.4053,
      "step": 960
    },
    {
      "epoch": 0.05700557598766164,
      "grad_norm": 7.894740581512451,
      "learning_rate": 1.139976275207592e-05,
      "loss": 0.0501,
      "step": 961
    },
    {
      "epoch": 0.05706489500533871,
      "grad_norm": 1.8257237672805786,
      "learning_rate": 1.1411625148279954e-05,
      "loss": 0.0059,
      "step": 962
    },
    {
      "epoch": 0.057124214023015776,
      "grad_norm": 6.751984596252441,
      "learning_rate": 1.1423487544483986e-05,
      "loss": 0.0617,
      "step": 963
    },
    {
      "epoch": 0.05718353304069285,
      "grad_norm": 39.34041213989258,
      "learning_rate": 1.1435349940688021e-05,
      "loss": 0.3156,
      "step": 964
    },
    {
      "epoch": 0.05724285205836992,
      "grad_norm": 0.052190106362104416,
      "learning_rate": 1.1447212336892053e-05,
      "loss": 0.0005,
      "step": 965
    },
    {
      "epoch": 0.05730217107604698,
      "grad_norm": 64.17741394042969,
      "learning_rate": 1.1459074733096086e-05,
      "loss": 0.4081,
      "step": 966
    },
    {
      "epoch": 0.05736149009372405,
      "grad_norm": 8.884902954101562,
      "learning_rate": 1.147093712930012e-05,
      "loss": 0.03,
      "step": 967
    },
    {
      "epoch": 0.057420809111401114,
      "grad_norm": 15.809819221496582,
      "learning_rate": 1.1482799525504153e-05,
      "loss": 0.2382,
      "step": 968
    },
    {
      "epoch": 0.057480128129078184,
      "grad_norm": 2.1742067337036133,
      "learning_rate": 1.1494661921708185e-05,
      "loss": 0.0089,
      "step": 969
    },
    {
      "epoch": 0.05753944714675525,
      "grad_norm": 21.3780460357666,
      "learning_rate": 1.150652431791222e-05,
      "loss": 0.2041,
      "step": 970
    },
    {
      "epoch": 0.05759876616443232,
      "grad_norm": 0.023162508383393288,
      "learning_rate": 1.1518386714116252e-05,
      "loss": 0.0003,
      "step": 971
    },
    {
      "epoch": 0.05765808518210938,
      "grad_norm": 13.812826156616211,
      "learning_rate": 1.1530249110320286e-05,
      "loss": 0.098,
      "step": 972
    },
    {
      "epoch": 0.05771740419978645,
      "grad_norm": 43.100257873535156,
      "learning_rate": 1.1542111506524318e-05,
      "loss": 0.2688,
      "step": 973
    },
    {
      "epoch": 0.05777672321746352,
      "grad_norm": 7.965233325958252,
      "learning_rate": 1.1553973902728353e-05,
      "loss": 0.0491,
      "step": 974
    },
    {
      "epoch": 0.057836042235140585,
      "grad_norm": 34.10211944580078,
      "learning_rate": 1.1565836298932385e-05,
      "loss": 0.1002,
      "step": 975
    },
    {
      "epoch": 0.057895361252817655,
      "grad_norm": 13.35275936126709,
      "learning_rate": 1.1577698695136418e-05,
      "loss": 0.1359,
      "step": 976
    },
    {
      "epoch": 0.05795468027049472,
      "grad_norm": 1.6243833303451538,
      "learning_rate": 1.1589561091340452e-05,
      "loss": 0.0098,
      "step": 977
    },
    {
      "epoch": 0.05801399928817179,
      "grad_norm": 31.581626892089844,
      "learning_rate": 1.1601423487544485e-05,
      "loss": 0.461,
      "step": 978
    },
    {
      "epoch": 0.05807331830584885,
      "grad_norm": 5.176648139953613,
      "learning_rate": 1.1613285883748517e-05,
      "loss": 0.1049,
      "step": 979
    },
    {
      "epoch": 0.05813263732352592,
      "grad_norm": 0.15664337575435638,
      "learning_rate": 1.1625148279952552e-05,
      "loss": 0.0017,
      "step": 980
    },
    {
      "epoch": 0.05819195634120299,
      "grad_norm": 7.0908966064453125,
      "learning_rate": 1.1637010676156584e-05,
      "loss": 0.0294,
      "step": 981
    },
    {
      "epoch": 0.058251275358880056,
      "grad_norm": 0.6312547922134399,
      "learning_rate": 1.1648873072360618e-05,
      "loss": 0.0043,
      "step": 982
    },
    {
      "epoch": 0.058310594376557126,
      "grad_norm": 9.637360572814941,
      "learning_rate": 1.166073546856465e-05,
      "loss": 0.0392,
      "step": 983
    },
    {
      "epoch": 0.05836991339423419,
      "grad_norm": 56.40428924560547,
      "learning_rate": 1.1672597864768685e-05,
      "loss": 1.3541,
      "step": 984
    },
    {
      "epoch": 0.05842923241191126,
      "grad_norm": 15.249167442321777,
      "learning_rate": 1.1684460260972716e-05,
      "loss": 0.6683,
      "step": 985
    },
    {
      "epoch": 0.05848855142958832,
      "grad_norm": 26.16333770751953,
      "learning_rate": 1.169632265717675e-05,
      "loss": 0.2215,
      "step": 986
    },
    {
      "epoch": 0.05854787044726539,
      "grad_norm": 36.34090042114258,
      "learning_rate": 1.1708185053380784e-05,
      "loss": 0.9517,
      "step": 987
    },
    {
      "epoch": 0.058607189464942464,
      "grad_norm": 11.266629219055176,
      "learning_rate": 1.1720047449584817e-05,
      "loss": 0.1038,
      "step": 988
    },
    {
      "epoch": 0.05866650848261953,
      "grad_norm": 12.892695426940918,
      "learning_rate": 1.173190984578885e-05,
      "loss": 0.0371,
      "step": 989
    },
    {
      "epoch": 0.0587258275002966,
      "grad_norm": 53.75881576538086,
      "learning_rate": 1.1743772241992884e-05,
      "loss": 2.5139,
      "step": 990
    },
    {
      "epoch": 0.05878514651797366,
      "grad_norm": 12.117039680480957,
      "learning_rate": 1.1755634638196918e-05,
      "loss": 0.0515,
      "step": 991
    },
    {
      "epoch": 0.05884446553565073,
      "grad_norm": 26.41986656188965,
      "learning_rate": 1.176749703440095e-05,
      "loss": 0.2704,
      "step": 992
    },
    {
      "epoch": 0.058903784553327794,
      "grad_norm": 23.246074676513672,
      "learning_rate": 1.1779359430604985e-05,
      "loss": 0.1043,
      "step": 993
    },
    {
      "epoch": 0.058963103571004864,
      "grad_norm": 0.008642852306365967,
      "learning_rate": 1.1791221826809016e-05,
      "loss": 0.0002,
      "step": 994
    },
    {
      "epoch": 0.059022422588681935,
      "grad_norm": 16.78977394104004,
      "learning_rate": 1.180308422301305e-05,
      "loss": 0.0555,
      "step": 995
    },
    {
      "epoch": 0.059081741606359,
      "grad_norm": 50.80305480957031,
      "learning_rate": 1.1814946619217082e-05,
      "loss": 0.735,
      "step": 996
    },
    {
      "epoch": 0.05914106062403607,
      "grad_norm": 5.545618057250977,
      "learning_rate": 1.1826809015421117e-05,
      "loss": 0.0353,
      "step": 997
    },
    {
      "epoch": 0.05920037964171313,
      "grad_norm": 1.0744234323501587,
      "learning_rate": 1.1838671411625149e-05,
      "loss": 0.0105,
      "step": 998
    },
    {
      "epoch": 0.0592596986593902,
      "grad_norm": 0.2211807668209076,
      "learning_rate": 1.1850533807829182e-05,
      "loss": 0.0022,
      "step": 999
    },
    {
      "epoch": 0.059319017677067265,
      "grad_norm": 9.163850784301758,
      "learning_rate": 1.1862396204033216e-05,
      "loss": 0.0899,
      "step": 1000
    },
    {
      "epoch": 0.059378336694744335,
      "grad_norm": 5.665236949920654,
      "learning_rate": 1.187425860023725e-05,
      "loss": 0.0202,
      "step": 1001
    },
    {
      "epoch": 0.059437655712421406,
      "grad_norm": 26.701602935791016,
      "learning_rate": 1.1886120996441281e-05,
      "loss": 0.2624,
      "step": 1002
    },
    {
      "epoch": 0.05949697473009847,
      "grad_norm": 14.2296781539917,
      "learning_rate": 1.1897983392645316e-05,
      "loss": 0.1586,
      "step": 1003
    },
    {
      "epoch": 0.05955629374777554,
      "grad_norm": 53.828407287597656,
      "learning_rate": 1.1909845788849348e-05,
      "loss": 0.1429,
      "step": 1004
    },
    {
      "epoch": 0.0596156127654526,
      "grad_norm": 91.11255645751953,
      "learning_rate": 1.1921708185053382e-05,
      "loss": 0.6445,
      "step": 1005
    },
    {
      "epoch": 0.05967493178312967,
      "grad_norm": 42.85197067260742,
      "learning_rate": 1.1933570581257414e-05,
      "loss": 0.5227,
      "step": 1006
    },
    {
      "epoch": 0.059734250800806736,
      "grad_norm": 19.597469329833984,
      "learning_rate": 1.1945432977461449e-05,
      "loss": 0.5809,
      "step": 1007
    },
    {
      "epoch": 0.059793569818483806,
      "grad_norm": 65.0315933227539,
      "learning_rate": 1.195729537366548e-05,
      "loss": 0.4938,
      "step": 1008
    },
    {
      "epoch": 0.05985288883616088,
      "grad_norm": 1.7445578575134277,
      "learning_rate": 1.1969157769869514e-05,
      "loss": 0.0051,
      "step": 1009
    },
    {
      "epoch": 0.05991220785383794,
      "grad_norm": 0.7483154535293579,
      "learning_rate": 1.1981020166073548e-05,
      "loss": 0.0054,
      "step": 1010
    },
    {
      "epoch": 0.05997152687151501,
      "grad_norm": 8.005128860473633,
      "learning_rate": 1.1992882562277581e-05,
      "loss": 0.0584,
      "step": 1011
    },
    {
      "epoch": 0.060030845889192073,
      "grad_norm": 25.236547470092773,
      "learning_rate": 1.2004744958481613e-05,
      "loss": 0.4249,
      "step": 1012
    },
    {
      "epoch": 0.060090164906869144,
      "grad_norm": 0.22090274095535278,
      "learning_rate": 1.2016607354685648e-05,
      "loss": 0.0017,
      "step": 1013
    },
    {
      "epoch": 0.06014948392454621,
      "grad_norm": 1.0172638893127441,
      "learning_rate": 1.202846975088968e-05,
      "loss": 0.0112,
      "step": 1014
    },
    {
      "epoch": 0.06020880294222328,
      "grad_norm": 0.011793326586484909,
      "learning_rate": 1.2040332147093714e-05,
      "loss": 0.0002,
      "step": 1015
    },
    {
      "epoch": 0.06026812195990034,
      "grad_norm": 36.72350311279297,
      "learning_rate": 1.2052194543297745e-05,
      "loss": 0.5601,
      "step": 1016
    },
    {
      "epoch": 0.06032744097757741,
      "grad_norm": 14.741311073303223,
      "learning_rate": 1.206405693950178e-05,
      "loss": 0.0887,
      "step": 1017
    },
    {
      "epoch": 0.06038675999525448,
      "grad_norm": 0.5092244148254395,
      "learning_rate": 1.2075919335705814e-05,
      "loss": 0.0083,
      "step": 1018
    },
    {
      "epoch": 0.060446079012931545,
      "grad_norm": 18.82623291015625,
      "learning_rate": 1.2087781731909846e-05,
      "loss": 0.3753,
      "step": 1019
    },
    {
      "epoch": 0.060505398030608615,
      "grad_norm": 9.548155784606934,
      "learning_rate": 1.2099644128113881e-05,
      "loss": 0.1023,
      "step": 1020
    },
    {
      "epoch": 0.06056471704828568,
      "grad_norm": 6.780745029449463,
      "learning_rate": 1.2111506524317913e-05,
      "loss": 0.094,
      "step": 1021
    },
    {
      "epoch": 0.06062403606596275,
      "grad_norm": 1.386809229850769,
      "learning_rate": 1.2123368920521947e-05,
      "loss": 0.0073,
      "step": 1022
    },
    {
      "epoch": 0.06068335508363981,
      "grad_norm": 9.406003952026367,
      "learning_rate": 1.213523131672598e-05,
      "loss": 0.0376,
      "step": 1023
    },
    {
      "epoch": 0.06074267410131688,
      "grad_norm": 41.38134002685547,
      "learning_rate": 1.2147093712930014e-05,
      "loss": 0.7135,
      "step": 1024
    },
    {
      "epoch": 0.06080199311899395,
      "grad_norm": 20.247323989868164,
      "learning_rate": 1.2158956109134045e-05,
      "loss": 0.1733,
      "step": 1025
    },
    {
      "epoch": 0.060861312136671016,
      "grad_norm": 19.915969848632812,
      "learning_rate": 1.217081850533808e-05,
      "loss": 0.3197,
      "step": 1026
    },
    {
      "epoch": 0.060920631154348086,
      "grad_norm": 0.2774864435195923,
      "learning_rate": 1.2182680901542112e-05,
      "loss": 0.0033,
      "step": 1027
    },
    {
      "epoch": 0.06097995017202515,
      "grad_norm": 14.444863319396973,
      "learning_rate": 1.2194543297746146e-05,
      "loss": 0.0885,
      "step": 1028
    },
    {
      "epoch": 0.06103926918970222,
      "grad_norm": 4.182670593261719,
      "learning_rate": 1.2206405693950178e-05,
      "loss": 0.4011,
      "step": 1029
    },
    {
      "epoch": 0.06109858820737928,
      "grad_norm": 44.770477294921875,
      "learning_rate": 1.2218268090154213e-05,
      "loss": 0.8739,
      "step": 1030
    },
    {
      "epoch": 0.06115790722505635,
      "grad_norm": 21.466562271118164,
      "learning_rate": 1.2230130486358245e-05,
      "loss": 1.2953,
      "step": 1031
    },
    {
      "epoch": 0.06121722624273342,
      "grad_norm": 41.90333938598633,
      "learning_rate": 1.2241992882562278e-05,
      "loss": 0.2982,
      "step": 1032
    },
    {
      "epoch": 0.06127654526041049,
      "grad_norm": 0.2753341495990753,
      "learning_rate": 1.2253855278766312e-05,
      "loss": 0.0019,
      "step": 1033
    },
    {
      "epoch": 0.06133586427808756,
      "grad_norm": 50.61421585083008,
      "learning_rate": 1.2265717674970345e-05,
      "loss": 1.7491,
      "step": 1034
    },
    {
      "epoch": 0.06139518329576462,
      "grad_norm": 0.03458680212497711,
      "learning_rate": 1.2277580071174377e-05,
      "loss": 0.0007,
      "step": 1035
    },
    {
      "epoch": 0.06145450231344169,
      "grad_norm": 54.12687683105469,
      "learning_rate": 1.2289442467378412e-05,
      "loss": 1.9447,
      "step": 1036
    },
    {
      "epoch": 0.061513821331118754,
      "grad_norm": 13.222217559814453,
      "learning_rate": 1.2301304863582444e-05,
      "loss": 0.1312,
      "step": 1037
    },
    {
      "epoch": 0.061573140348795824,
      "grad_norm": 1.0223848819732666,
      "learning_rate": 1.2313167259786478e-05,
      "loss": 0.0067,
      "step": 1038
    },
    {
      "epoch": 0.061632459366472894,
      "grad_norm": 12.285061836242676,
      "learning_rate": 1.232502965599051e-05,
      "loss": 0.0299,
      "step": 1039
    },
    {
      "epoch": 0.06169177838414996,
      "grad_norm": 25.306915283203125,
      "learning_rate": 1.2336892052194545e-05,
      "loss": 0.3429,
      "step": 1040
    },
    {
      "epoch": 0.06175109740182703,
      "grad_norm": 10.674539566040039,
      "learning_rate": 1.2348754448398577e-05,
      "loss": 0.0829,
      "step": 1041
    },
    {
      "epoch": 0.06181041641950409,
      "grad_norm": 35.54343032836914,
      "learning_rate": 1.236061684460261e-05,
      "loss": 0.1105,
      "step": 1042
    },
    {
      "epoch": 0.06186973543718116,
      "grad_norm": 0.5902915596961975,
      "learning_rate": 1.2372479240806644e-05,
      "loss": 0.005,
      "step": 1043
    },
    {
      "epoch": 0.061929054454858225,
      "grad_norm": 46.11483383178711,
      "learning_rate": 1.2384341637010677e-05,
      "loss": 0.2172,
      "step": 1044
    },
    {
      "epoch": 0.061988373472535295,
      "grad_norm": 35.632415771484375,
      "learning_rate": 1.2396204033214709e-05,
      "loss": 0.7607,
      "step": 1045
    },
    {
      "epoch": 0.062047692490212365,
      "grad_norm": 3.7063684463500977,
      "learning_rate": 1.2408066429418744e-05,
      "loss": 0.009,
      "step": 1046
    },
    {
      "epoch": 0.06210701150788943,
      "grad_norm": 1.391041874885559,
      "learning_rate": 1.2419928825622778e-05,
      "loss": 0.0068,
      "step": 1047
    },
    {
      "epoch": 0.0621663305255665,
      "grad_norm": 0.2538388669490814,
      "learning_rate": 1.243179122182681e-05,
      "loss": 0.0015,
      "step": 1048
    },
    {
      "epoch": 0.06222564954324356,
      "grad_norm": 1.3726153373718262,
      "learning_rate": 1.2443653618030845e-05,
      "loss": 0.0081,
      "step": 1049
    },
    {
      "epoch": 0.06228496856092063,
      "grad_norm": 12.223569869995117,
      "learning_rate": 1.2455516014234877e-05,
      "loss": 0.1319,
      "step": 1050
    },
    {
      "epoch": 0.062344287578597696,
      "grad_norm": 19.387954711914062,
      "learning_rate": 1.246737841043891e-05,
      "loss": 0.2251,
      "step": 1051
    },
    {
      "epoch": 0.062403606596274766,
      "grad_norm": 27.30116081237793,
      "learning_rate": 1.2479240806642942e-05,
      "loss": 0.1092,
      "step": 1052
    },
    {
      "epoch": 0.062462925613951836,
      "grad_norm": 3.317941188812256,
      "learning_rate": 1.2491103202846977e-05,
      "loss": 0.0266,
      "step": 1053
    },
    {
      "epoch": 0.0625222446316289,
      "grad_norm": 0.028310786932706833,
      "learning_rate": 1.2502965599051009e-05,
      "loss": 0.0004,
      "step": 1054
    },
    {
      "epoch": 0.06258156364930596,
      "grad_norm": 7.222072601318359,
      "learning_rate": 1.2514827995255042e-05,
      "loss": 0.0556,
      "step": 1055
    },
    {
      "epoch": 0.06264088266698303,
      "grad_norm": 14.044188499450684,
      "learning_rate": 1.2526690391459076e-05,
      "loss": 0.3898,
      "step": 1056
    },
    {
      "epoch": 0.0627002016846601,
      "grad_norm": 0.5088880062103271,
      "learning_rate": 1.253855278766311e-05,
      "loss": 0.0035,
      "step": 1057
    },
    {
      "epoch": 0.06275952070233717,
      "grad_norm": 0.031241117045283318,
      "learning_rate": 1.2550415183867141e-05,
      "loss": 0.0004,
      "step": 1058
    },
    {
      "epoch": 0.06281883972001423,
      "grad_norm": 36.38539505004883,
      "learning_rate": 1.2562277580071177e-05,
      "loss": 0.3869,
      "step": 1059
    },
    {
      "epoch": 0.0628781587376913,
      "grad_norm": 8.546703338623047,
      "learning_rate": 1.2574139976275208e-05,
      "loss": 0.2214,
      "step": 1060
    },
    {
      "epoch": 0.06293747775536837,
      "grad_norm": 9.405561447143555,
      "learning_rate": 1.2586002372479242e-05,
      "loss": 0.0482,
      "step": 1061
    },
    {
      "epoch": 0.06299679677304544,
      "grad_norm": 26.552871704101562,
      "learning_rate": 1.2597864768683274e-05,
      "loss": 0.435,
      "step": 1062
    },
    {
      "epoch": 0.06305611579072251,
      "grad_norm": 8.75198745727539,
      "learning_rate": 1.2609727164887309e-05,
      "loss": 0.1086,
      "step": 1063
    },
    {
      "epoch": 0.06311543480839957,
      "grad_norm": 0.41411057114601135,
      "learning_rate": 1.262158956109134e-05,
      "loss": 0.0018,
      "step": 1064
    },
    {
      "epoch": 0.06317475382607664,
      "grad_norm": 42.52467727661133,
      "learning_rate": 1.2633451957295374e-05,
      "loss": 0.2618,
      "step": 1065
    },
    {
      "epoch": 0.06323407284375371,
      "grad_norm": 21.919998168945312,
      "learning_rate": 1.2645314353499408e-05,
      "loss": 0.6509,
      "step": 1066
    },
    {
      "epoch": 0.06329339186143078,
      "grad_norm": 6.686143398284912,
      "learning_rate": 1.2657176749703441e-05,
      "loss": 0.1554,
      "step": 1067
    },
    {
      "epoch": 0.06335271087910785,
      "grad_norm": 0.19598717987537384,
      "learning_rate": 1.2669039145907473e-05,
      "loss": 0.0013,
      "step": 1068
    },
    {
      "epoch": 0.0634120298967849,
      "grad_norm": 70.27210998535156,
      "learning_rate": 1.2680901542111508e-05,
      "loss": 1.3208,
      "step": 1069
    },
    {
      "epoch": 0.06347134891446198,
      "grad_norm": 1.2961064577102661,
      "learning_rate": 1.269276393831554e-05,
      "loss": 0.0064,
      "step": 1070
    },
    {
      "epoch": 0.06353066793213905,
      "grad_norm": 22.706878662109375,
      "learning_rate": 1.2704626334519574e-05,
      "loss": 0.2325,
      "step": 1071
    },
    {
      "epoch": 0.06358998694981612,
      "grad_norm": 0.47221705317497253,
      "learning_rate": 1.2716488730723606e-05,
      "loss": 0.0029,
      "step": 1072
    },
    {
      "epoch": 0.06364930596749317,
      "grad_norm": 2.8553061485290527,
      "learning_rate": 1.272835112692764e-05,
      "loss": 0.011,
      "step": 1073
    },
    {
      "epoch": 0.06370862498517024,
      "grad_norm": 65.05521392822266,
      "learning_rate": 1.2740213523131673e-05,
      "loss": 2.0549,
      "step": 1074
    },
    {
      "epoch": 0.06376794400284731,
      "grad_norm": 1.3543545007705688,
      "learning_rate": 1.2752075919335706e-05,
      "loss": 0.0037,
      "step": 1075
    },
    {
      "epoch": 0.06382726302052438,
      "grad_norm": 33.82251739501953,
      "learning_rate": 1.2763938315539741e-05,
      "loss": 0.4239,
      "step": 1076
    },
    {
      "epoch": 0.06388658203820145,
      "grad_norm": 0.18449833989143372,
      "learning_rate": 1.2775800711743773e-05,
      "loss": 0.0007,
      "step": 1077
    },
    {
      "epoch": 0.06394590105587851,
      "grad_norm": 5.264753818511963,
      "learning_rate": 1.2787663107947808e-05,
      "loss": 0.0824,
      "step": 1078
    },
    {
      "epoch": 0.06400522007355558,
      "grad_norm": 16.152793884277344,
      "learning_rate": 1.279952550415184e-05,
      "loss": 0.1587,
      "step": 1079
    },
    {
      "epoch": 0.06406453909123265,
      "grad_norm": 0.07442248612642288,
      "learning_rate": 1.2811387900355874e-05,
      "loss": 0.0003,
      "step": 1080
    },
    {
      "epoch": 0.06412385810890972,
      "grad_norm": 29.42265510559082,
      "learning_rate": 1.2823250296559906e-05,
      "loss": 0.7723,
      "step": 1081
    },
    {
      "epoch": 0.06418317712658679,
      "grad_norm": 10.678155899047852,
      "learning_rate": 1.283511269276394e-05,
      "loss": 0.1483,
      "step": 1082
    },
    {
      "epoch": 0.06424249614426385,
      "grad_norm": 13.05994701385498,
      "learning_rate": 1.2846975088967973e-05,
      "loss": 0.3819,
      "step": 1083
    },
    {
      "epoch": 0.06430181516194092,
      "grad_norm": 17.808639526367188,
      "learning_rate": 1.2858837485172006e-05,
      "loss": 0.0662,
      "step": 1084
    },
    {
      "epoch": 0.06436113417961799,
      "grad_norm": 11.28715991973877,
      "learning_rate": 1.2870699881376038e-05,
      "loss": 0.0478,
      "step": 1085
    },
    {
      "epoch": 0.06442045319729506,
      "grad_norm": 2.0546176433563232,
      "learning_rate": 1.2882562277580073e-05,
      "loss": 0.0147,
      "step": 1086
    },
    {
      "epoch": 0.06447977221497211,
      "grad_norm": 2.3910436630249023,
      "learning_rate": 1.2894424673784105e-05,
      "loss": 0.0245,
      "step": 1087
    },
    {
      "epoch": 0.06453909123264918,
      "grad_norm": 49.98996353149414,
      "learning_rate": 1.2906287069988138e-05,
      "loss": 1.1902,
      "step": 1088
    },
    {
      "epoch": 0.06459841025032625,
      "grad_norm": 14.734504699707031,
      "learning_rate": 1.2918149466192172e-05,
      "loss": 0.0976,
      "step": 1089
    },
    {
      "epoch": 0.06465772926800332,
      "grad_norm": 88.35468292236328,
      "learning_rate": 1.2930011862396206e-05,
      "loss": 1.886,
      "step": 1090
    },
    {
      "epoch": 0.0647170482856804,
      "grad_norm": 7.738844871520996,
      "learning_rate": 1.2941874258600237e-05,
      "loss": 0.0475,
      "step": 1091
    },
    {
      "epoch": 0.06477636730335745,
      "grad_norm": 0.12414203584194183,
      "learning_rate": 1.2953736654804273e-05,
      "loss": 0.0014,
      "step": 1092
    },
    {
      "epoch": 0.06483568632103452,
      "grad_norm": 14.184544563293457,
      "learning_rate": 1.2965599051008304e-05,
      "loss": 0.3693,
      "step": 1093
    },
    {
      "epoch": 0.06489500533871159,
      "grad_norm": 18.179821014404297,
      "learning_rate": 1.2977461447212338e-05,
      "loss": 0.1369,
      "step": 1094
    },
    {
      "epoch": 0.06495432435638866,
      "grad_norm": 0.05091811344027519,
      "learning_rate": 1.298932384341637e-05,
      "loss": 0.0008,
      "step": 1095
    },
    {
      "epoch": 0.06501364337406572,
      "grad_norm": 0.7354139089584351,
      "learning_rate": 1.3001186239620405e-05,
      "loss": 0.0075,
      "step": 1096
    },
    {
      "epoch": 0.06507296239174279,
      "grad_norm": 17.20938491821289,
      "learning_rate": 1.3013048635824437e-05,
      "loss": 0.3154,
      "step": 1097
    },
    {
      "epoch": 0.06513228140941986,
      "grad_norm": 2.4011805057525635,
      "learning_rate": 1.302491103202847e-05,
      "loss": 0.0091,
      "step": 1098
    },
    {
      "epoch": 0.06519160042709693,
      "grad_norm": 5.734222888946533,
      "learning_rate": 1.3036773428232504e-05,
      "loss": 0.0331,
      "step": 1099
    },
    {
      "epoch": 0.065250919444774,
      "grad_norm": 0.5543069839477539,
      "learning_rate": 1.3048635824436537e-05,
      "loss": 0.0033,
      "step": 1100
    },
    {
      "epoch": 0.06531023846245106,
      "grad_norm": 0.026419125497341156,
      "learning_rate": 1.3060498220640569e-05,
      "loss": 0.0003,
      "step": 1101
    },
    {
      "epoch": 0.06536955748012813,
      "grad_norm": 0.5263641476631165,
      "learning_rate": 1.3072360616844604e-05,
      "loss": 0.0027,
      "step": 1102
    },
    {
      "epoch": 0.0654288764978052,
      "grad_norm": 10.538402557373047,
      "learning_rate": 1.3084223013048636e-05,
      "loss": 0.0614,
      "step": 1103
    },
    {
      "epoch": 0.06548819551548227,
      "grad_norm": 4.559321880340576,
      "learning_rate": 1.309608540925267e-05,
      "loss": 0.0539,
      "step": 1104
    },
    {
      "epoch": 0.06554751453315934,
      "grad_norm": 97.12171936035156,
      "learning_rate": 1.3107947805456705e-05,
      "loss": 2.1584,
      "step": 1105
    },
    {
      "epoch": 0.0656068335508364,
      "grad_norm": 41.39286422729492,
      "learning_rate": 1.3119810201660737e-05,
      "loss": 0.7612,
      "step": 1106
    },
    {
      "epoch": 0.06566615256851346,
      "grad_norm": 38.488773345947266,
      "learning_rate": 1.313167259786477e-05,
      "loss": 0.9165,
      "step": 1107
    },
    {
      "epoch": 0.06572547158619053,
      "grad_norm": 5.535702705383301,
      "learning_rate": 1.3143534994068802e-05,
      "loss": 0.0135,
      "step": 1108
    },
    {
      "epoch": 0.0657847906038676,
      "grad_norm": 11.785985946655273,
      "learning_rate": 1.3155397390272837e-05,
      "loss": 0.1864,
      "step": 1109
    },
    {
      "epoch": 0.06584410962154466,
      "grad_norm": 71.10602569580078,
      "learning_rate": 1.3167259786476869e-05,
      "loss": 0.268,
      "step": 1110
    },
    {
      "epoch": 0.06590342863922173,
      "grad_norm": 23.46035385131836,
      "learning_rate": 1.3179122182680904e-05,
      "loss": 1.0346,
      "step": 1111
    },
    {
      "epoch": 0.0659627476568988,
      "grad_norm": 58.64242935180664,
      "learning_rate": 1.3190984578884936e-05,
      "loss": 0.7725,
      "step": 1112
    },
    {
      "epoch": 0.06602206667457587,
      "grad_norm": 2.8226747512817383,
      "learning_rate": 1.320284697508897e-05,
      "loss": 0.0114,
      "step": 1113
    },
    {
      "epoch": 0.06608138569225294,
      "grad_norm": 1.7100204229354858,
      "learning_rate": 1.3214709371293001e-05,
      "loss": 0.0077,
      "step": 1114
    },
    {
      "epoch": 0.06614070470993,
      "grad_norm": 24.421493530273438,
      "learning_rate": 1.3226571767497037e-05,
      "loss": 0.5949,
      "step": 1115
    },
    {
      "epoch": 0.06620002372760707,
      "grad_norm": 30.718549728393555,
      "learning_rate": 1.3238434163701069e-05,
      "loss": 0.2156,
      "step": 1116
    },
    {
      "epoch": 0.06625934274528414,
      "grad_norm": 46.977264404296875,
      "learning_rate": 1.3250296559905102e-05,
      "loss": 0.3582,
      "step": 1117
    },
    {
      "epoch": 0.06631866176296121,
      "grad_norm": 20.542280197143555,
      "learning_rate": 1.3262158956109134e-05,
      "loss": 0.372,
      "step": 1118
    },
    {
      "epoch": 0.06637798078063828,
      "grad_norm": 25.435266494750977,
      "learning_rate": 1.3274021352313169e-05,
      "loss": 0.1983,
      "step": 1119
    },
    {
      "epoch": 0.06643729979831534,
      "grad_norm": 0.009741134010255337,
      "learning_rate": 1.3285883748517201e-05,
      "loss": 0.0001,
      "step": 1120
    },
    {
      "epoch": 0.0664966188159924,
      "grad_norm": 1.224016785621643,
      "learning_rate": 1.3297746144721234e-05,
      "loss": 0.005,
      "step": 1121
    },
    {
      "epoch": 0.06655593783366948,
      "grad_norm": 4.937636375427246,
      "learning_rate": 1.3309608540925268e-05,
      "loss": 0.0301,
      "step": 1122
    },
    {
      "epoch": 0.06661525685134655,
      "grad_norm": 9.79858112335205,
      "learning_rate": 1.3321470937129301e-05,
      "loss": 0.08,
      "step": 1123
    },
    {
      "epoch": 0.0666745758690236,
      "grad_norm": 0.3364149034023285,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0017,
      "step": 1124
    },
    {
      "epoch": 0.06673389488670067,
      "grad_norm": 36.0853157043457,
      "learning_rate": 1.3345195729537369e-05,
      "loss": 0.7784,
      "step": 1125
    },
    {
      "epoch": 0.06679321390437774,
      "grad_norm": 0.04374495521187782,
      "learning_rate": 1.33570581257414e-05,
      "loss": 0.0009,
      "step": 1126
    },
    {
      "epoch": 0.06685253292205481,
      "grad_norm": 4.915780067443848,
      "learning_rate": 1.3368920521945434e-05,
      "loss": 0.0389,
      "step": 1127
    },
    {
      "epoch": 0.06691185193973188,
      "grad_norm": 21.67680549621582,
      "learning_rate": 1.3380782918149466e-05,
      "loss": 0.3018,
      "step": 1128
    },
    {
      "epoch": 0.06697117095740894,
      "grad_norm": 0.025487391278147697,
      "learning_rate": 1.3392645314353501e-05,
      "loss": 0.0004,
      "step": 1129
    },
    {
      "epoch": 0.06703048997508601,
      "grad_norm": 31.76581382751465,
      "learning_rate": 1.3404507710557533e-05,
      "loss": 0.2004,
      "step": 1130
    },
    {
      "epoch": 0.06708980899276308,
      "grad_norm": 32.86215591430664,
      "learning_rate": 1.3416370106761566e-05,
      "loss": 0.7539,
      "step": 1131
    },
    {
      "epoch": 0.06714912801044015,
      "grad_norm": 2.6647191047668457,
      "learning_rate": 1.34282325029656e-05,
      "loss": 0.0102,
      "step": 1132
    },
    {
      "epoch": 0.06720844702811721,
      "grad_norm": 1.4695531129837036,
      "learning_rate": 1.3440094899169633e-05,
      "loss": 0.0086,
      "step": 1133
    },
    {
      "epoch": 0.06726776604579428,
      "grad_norm": 31.924362182617188,
      "learning_rate": 1.3451957295373668e-05,
      "loss": 0.8739,
      "step": 1134
    },
    {
      "epoch": 0.06732708506347135,
      "grad_norm": 30.37970733642578,
      "learning_rate": 1.34638196915777e-05,
      "loss": 0.8539,
      "step": 1135
    },
    {
      "epoch": 0.06738640408114842,
      "grad_norm": 19.332502365112305,
      "learning_rate": 1.3475682087781734e-05,
      "loss": 0.4385,
      "step": 1136
    },
    {
      "epoch": 0.06744572309882549,
      "grad_norm": 12.895374298095703,
      "learning_rate": 1.3487544483985766e-05,
      "loss": 0.425,
      "step": 1137
    },
    {
      "epoch": 0.06750504211650254,
      "grad_norm": 0.16020239889621735,
      "learning_rate": 1.3499406880189801e-05,
      "loss": 0.0014,
      "step": 1138
    },
    {
      "epoch": 0.06756436113417962,
      "grad_norm": 28.946962356567383,
      "learning_rate": 1.3511269276393833e-05,
      "loss": 0.4264,
      "step": 1139
    },
    {
      "epoch": 0.06762368015185669,
      "grad_norm": 0.2207198292016983,
      "learning_rate": 1.3523131672597866e-05,
      "loss": 0.0017,
      "step": 1140
    },
    {
      "epoch": 0.06768299916953376,
      "grad_norm": 0.11091805249452591,
      "learning_rate": 1.3534994068801898e-05,
      "loss": 0.0009,
      "step": 1141
    },
    {
      "epoch": 0.06774231818721083,
      "grad_norm": 35.636817932128906,
      "learning_rate": 1.3546856465005933e-05,
      "loss": 1.2244,
      "step": 1142
    },
    {
      "epoch": 0.06780163720488788,
      "grad_norm": 18.466856002807617,
      "learning_rate": 1.3558718861209965e-05,
      "loss": 0.0634,
      "step": 1143
    },
    {
      "epoch": 0.06786095622256495,
      "grad_norm": 2.399308681488037,
      "learning_rate": 1.3570581257414e-05,
      "loss": 0.0158,
      "step": 1144
    },
    {
      "epoch": 0.06792027524024202,
      "grad_norm": 2.2688610553741455,
      "learning_rate": 1.3582443653618032e-05,
      "loss": 0.0094,
      "step": 1145
    },
    {
      "epoch": 0.0679795942579191,
      "grad_norm": 18.918787002563477,
      "learning_rate": 1.3594306049822066e-05,
      "loss": 0.7667,
      "step": 1146
    },
    {
      "epoch": 0.06803891327559615,
      "grad_norm": 55.25878143310547,
      "learning_rate": 1.3606168446026097e-05,
      "loss": 0.4689,
      "step": 1147
    },
    {
      "epoch": 0.06809823229327322,
      "grad_norm": 1.4306855201721191,
      "learning_rate": 1.3618030842230133e-05,
      "loss": 0.0082,
      "step": 1148
    },
    {
      "epoch": 0.06815755131095029,
      "grad_norm": 29.1612606048584,
      "learning_rate": 1.3629893238434164e-05,
      "loss": 0.0673,
      "step": 1149
    },
    {
      "epoch": 0.06821687032862736,
      "grad_norm": 0.31273481249809265,
      "learning_rate": 1.3641755634638198e-05,
      "loss": 0.002,
      "step": 1150
    },
    {
      "epoch": 0.06827618934630443,
      "grad_norm": 26.352834701538086,
      "learning_rate": 1.365361803084223e-05,
      "loss": 0.1531,
      "step": 1151
    },
    {
      "epoch": 0.06833550836398149,
      "grad_norm": 13.8046293258667,
      "learning_rate": 1.3665480427046265e-05,
      "loss": 0.3559,
      "step": 1152
    },
    {
      "epoch": 0.06839482738165856,
      "grad_norm": 19.30605125427246,
      "learning_rate": 1.3677342823250297e-05,
      "loss": 0.3072,
      "step": 1153
    },
    {
      "epoch": 0.06845414639933563,
      "grad_norm": 15.266265869140625,
      "learning_rate": 1.368920521945433e-05,
      "loss": 0.3141,
      "step": 1154
    },
    {
      "epoch": 0.0685134654170127,
      "grad_norm": 9.313015937805176,
      "learning_rate": 1.3701067615658364e-05,
      "loss": 0.0663,
      "step": 1155
    },
    {
      "epoch": 0.06857278443468977,
      "grad_norm": 0.7865702509880066,
      "learning_rate": 1.3712930011862397e-05,
      "loss": 0.0049,
      "step": 1156
    },
    {
      "epoch": 0.06863210345236682,
      "grad_norm": 74.43518829345703,
      "learning_rate": 1.372479240806643e-05,
      "loss": 1.025,
      "step": 1157
    },
    {
      "epoch": 0.0686914224700439,
      "grad_norm": 1.6650134325027466,
      "learning_rate": 1.3736654804270464e-05,
      "loss": 0.0122,
      "step": 1158
    },
    {
      "epoch": 0.06875074148772096,
      "grad_norm": 7.033146381378174,
      "learning_rate": 1.3748517200474496e-05,
      "loss": 0.1625,
      "step": 1159
    },
    {
      "epoch": 0.06881006050539804,
      "grad_norm": 35.98696517944336,
      "learning_rate": 1.376037959667853e-05,
      "loss": 0.8348,
      "step": 1160
    },
    {
      "epoch": 0.06886937952307509,
      "grad_norm": 7.075418949127197,
      "learning_rate": 1.3772241992882562e-05,
      "loss": 0.1267,
      "step": 1161
    },
    {
      "epoch": 0.06892869854075216,
      "grad_norm": 30.811878204345703,
      "learning_rate": 1.3784104389086597e-05,
      "loss": 0.3946,
      "step": 1162
    },
    {
      "epoch": 0.06898801755842923,
      "grad_norm": 38.13654708862305,
      "learning_rate": 1.379596678529063e-05,
      "loss": 0.1162,
      "step": 1163
    },
    {
      "epoch": 0.0690473365761063,
      "grad_norm": 11.084602355957031,
      "learning_rate": 1.3807829181494662e-05,
      "loss": 0.2628,
      "step": 1164
    },
    {
      "epoch": 0.06910665559378337,
      "grad_norm": 70.57081604003906,
      "learning_rate": 1.3819691577698697e-05,
      "loss": 0.4732,
      "step": 1165
    },
    {
      "epoch": 0.06916597461146043,
      "grad_norm": 17.11402130126953,
      "learning_rate": 1.383155397390273e-05,
      "loss": 0.4614,
      "step": 1166
    },
    {
      "epoch": 0.0692252936291375,
      "grad_norm": 0.1747501641511917,
      "learning_rate": 1.3843416370106764e-05,
      "loss": 0.0023,
      "step": 1167
    },
    {
      "epoch": 0.06928461264681457,
      "grad_norm": 13.279068946838379,
      "learning_rate": 1.3855278766310796e-05,
      "loss": 0.0965,
      "step": 1168
    },
    {
      "epoch": 0.06934393166449164,
      "grad_norm": 12.943321228027344,
      "learning_rate": 1.386714116251483e-05,
      "loss": 0.1925,
      "step": 1169
    },
    {
      "epoch": 0.06940325068216871,
      "grad_norm": 33.74226760864258,
      "learning_rate": 1.3879003558718862e-05,
      "loss": 0.3467,
      "step": 1170
    },
    {
      "epoch": 0.06946256969984577,
      "grad_norm": 19.861425399780273,
      "learning_rate": 1.3890865954922897e-05,
      "loss": 0.6685,
      "step": 1171
    },
    {
      "epoch": 0.06952188871752284,
      "grad_norm": 0.026700694113969803,
      "learning_rate": 1.3902728351126929e-05,
      "loss": 0.0006,
      "step": 1172
    },
    {
      "epoch": 0.06958120773519991,
      "grad_norm": 7.246057033538818,
      "learning_rate": 1.3914590747330962e-05,
      "loss": 0.0521,
      "step": 1173
    },
    {
      "epoch": 0.06964052675287698,
      "grad_norm": 54.509971618652344,
      "learning_rate": 1.3926453143534994e-05,
      "loss": 0.4507,
      "step": 1174
    },
    {
      "epoch": 0.06969984577055403,
      "grad_norm": 17.391643524169922,
      "learning_rate": 1.393831553973903e-05,
      "loss": 0.7704,
      "step": 1175
    },
    {
      "epoch": 0.0697591647882311,
      "grad_norm": 8.638718605041504,
      "learning_rate": 1.3950177935943061e-05,
      "loss": 0.0348,
      "step": 1176
    },
    {
      "epoch": 0.06981848380590817,
      "grad_norm": 21.639057159423828,
      "learning_rate": 1.3962040332147096e-05,
      "loss": 0.3427,
      "step": 1177
    },
    {
      "epoch": 0.06987780282358524,
      "grad_norm": 0.2944852113723755,
      "learning_rate": 1.3973902728351128e-05,
      "loss": 0.0016,
      "step": 1178
    },
    {
      "epoch": 0.06993712184126231,
      "grad_norm": 15.193737030029297,
      "learning_rate": 1.3985765124555162e-05,
      "loss": 0.3846,
      "step": 1179
    },
    {
      "epoch": 0.06999644085893937,
      "grad_norm": 5.137587070465088,
      "learning_rate": 1.3997627520759193e-05,
      "loss": 0.0691,
      "step": 1180
    },
    {
      "epoch": 0.07005575987661644,
      "grad_norm": 20.324914932250977,
      "learning_rate": 1.4009489916963229e-05,
      "loss": 0.3418,
      "step": 1181
    },
    {
      "epoch": 0.07011507889429351,
      "grad_norm": 2.5594515800476074,
      "learning_rate": 1.402135231316726e-05,
      "loss": 0.0298,
      "step": 1182
    },
    {
      "epoch": 0.07017439791197058,
      "grad_norm": 12.176438331604004,
      "learning_rate": 1.4033214709371294e-05,
      "loss": 0.275,
      "step": 1183
    },
    {
      "epoch": 0.07023371692964764,
      "grad_norm": 47.56638717651367,
      "learning_rate": 1.4045077105575326e-05,
      "loss": 3.167,
      "step": 1184
    },
    {
      "epoch": 0.07029303594732471,
      "grad_norm": 42.5908317565918,
      "learning_rate": 1.4056939501779361e-05,
      "loss": 0.453,
      "step": 1185
    },
    {
      "epoch": 0.07035235496500178,
      "grad_norm": 18.918556213378906,
      "learning_rate": 1.4068801897983393e-05,
      "loss": 0.1249,
      "step": 1186
    },
    {
      "epoch": 0.07041167398267885,
      "grad_norm": 2.2635624408721924,
      "learning_rate": 1.4080664294187426e-05,
      "loss": 0.0146,
      "step": 1187
    },
    {
      "epoch": 0.07047099300035592,
      "grad_norm": 2.3133206367492676,
      "learning_rate": 1.409252669039146e-05,
      "loss": 0.0208,
      "step": 1188
    },
    {
      "epoch": 0.07053031201803298,
      "grad_norm": 21.78610610961914,
      "learning_rate": 1.4104389086595493e-05,
      "loss": 0.243,
      "step": 1189
    },
    {
      "epoch": 0.07058963103571005,
      "grad_norm": 41.196189880371094,
      "learning_rate": 1.4116251482799525e-05,
      "loss": 0.2771,
      "step": 1190
    },
    {
      "epoch": 0.07064895005338712,
      "grad_norm": 24.24260139465332,
      "learning_rate": 1.412811387900356e-05,
      "loss": 0.2599,
      "step": 1191
    },
    {
      "epoch": 0.07070826907106419,
      "grad_norm": 4.9162421226501465,
      "learning_rate": 1.4139976275207594e-05,
      "loss": 0.0574,
      "step": 1192
    },
    {
      "epoch": 0.07076758808874126,
      "grad_norm": 29.07131004333496,
      "learning_rate": 1.4151838671411626e-05,
      "loss": 0.3784,
      "step": 1193
    },
    {
      "epoch": 0.07082690710641831,
      "grad_norm": 7.597085952758789,
      "learning_rate": 1.4163701067615661e-05,
      "loss": 0.7278,
      "step": 1194
    },
    {
      "epoch": 0.07088622612409538,
      "grad_norm": 23.768644332885742,
      "learning_rate": 1.4175563463819693e-05,
      "loss": 0.4455,
      "step": 1195
    },
    {
      "epoch": 0.07094554514177245,
      "grad_norm": 1.4908325672149658,
      "learning_rate": 1.4187425860023726e-05,
      "loss": 0.0087,
      "step": 1196
    },
    {
      "epoch": 0.07100486415944952,
      "grad_norm": 2.9982380867004395,
      "learning_rate": 1.4199288256227758e-05,
      "loss": 0.0294,
      "step": 1197
    },
    {
      "epoch": 0.07106418317712658,
      "grad_norm": 6.2091193199157715,
      "learning_rate": 1.4211150652431793e-05,
      "loss": 0.0727,
      "step": 1198
    },
    {
      "epoch": 0.07112350219480365,
      "grad_norm": 0.15154658257961273,
      "learning_rate": 1.4223013048635825e-05,
      "loss": 0.0022,
      "step": 1199
    },
    {
      "epoch": 0.07118282121248072,
      "grad_norm": 9.691481590270996,
      "learning_rate": 1.423487544483986e-05,
      "loss": 0.1278,
      "step": 1200
    },
    {
      "epoch": 0.07124214023015779,
      "grad_norm": 0.4294542968273163,
      "learning_rate": 1.4246737841043892e-05,
      "loss": 0.0029,
      "step": 1201
    },
    {
      "epoch": 0.07130145924783486,
      "grad_norm": 10.641602516174316,
      "learning_rate": 1.4258600237247926e-05,
      "loss": 0.2386,
      "step": 1202
    },
    {
      "epoch": 0.07136077826551192,
      "grad_norm": 13.761210441589355,
      "learning_rate": 1.4270462633451958e-05,
      "loss": 0.2122,
      "step": 1203
    },
    {
      "epoch": 0.07142009728318899,
      "grad_norm": 1.3764088153839111,
      "learning_rate": 1.4282325029655993e-05,
      "loss": 0.012,
      "step": 1204
    },
    {
      "epoch": 0.07147941630086606,
      "grad_norm": 14.673005104064941,
      "learning_rate": 1.4294187425860025e-05,
      "loss": 0.2434,
      "step": 1205
    },
    {
      "epoch": 0.07153873531854313,
      "grad_norm": 0.11755573004484177,
      "learning_rate": 1.4306049822064058e-05,
      "loss": 0.0014,
      "step": 1206
    },
    {
      "epoch": 0.0715980543362202,
      "grad_norm": 2.8179852962493896,
      "learning_rate": 1.431791221826809e-05,
      "loss": 0.0153,
      "step": 1207
    },
    {
      "epoch": 0.07165737335389726,
      "grad_norm": 14.966882705688477,
      "learning_rate": 1.4329774614472125e-05,
      "loss": 0.0662,
      "step": 1208
    },
    {
      "epoch": 0.07171669237157433,
      "grad_norm": 13.370076179504395,
      "learning_rate": 1.4341637010676157e-05,
      "loss": 0.0711,
      "step": 1209
    },
    {
      "epoch": 0.0717760113892514,
      "grad_norm": 3.6105270385742188,
      "learning_rate": 1.4353499406880192e-05,
      "loss": 0.0066,
      "step": 1210
    },
    {
      "epoch": 0.07183533040692847,
      "grad_norm": 33.99266815185547,
      "learning_rate": 1.4365361803084224e-05,
      "loss": 0.9107,
      "step": 1211
    },
    {
      "epoch": 0.07189464942460552,
      "grad_norm": 0.32129889726638794,
      "learning_rate": 1.4377224199288258e-05,
      "loss": 0.0036,
      "step": 1212
    },
    {
      "epoch": 0.07195396844228259,
      "grad_norm": 4.407817840576172,
      "learning_rate": 1.438908659549229e-05,
      "loss": 0.0232,
      "step": 1213
    },
    {
      "epoch": 0.07201328745995966,
      "grad_norm": 27.925962448120117,
      "learning_rate": 1.4400948991696325e-05,
      "loss": 0.1493,
      "step": 1214
    },
    {
      "epoch": 0.07207260647763673,
      "grad_norm": 32.5975227355957,
      "learning_rate": 1.4412811387900356e-05,
      "loss": 0.144,
      "step": 1215
    },
    {
      "epoch": 0.0721319254953138,
      "grad_norm": 0.28401467204093933,
      "learning_rate": 1.442467378410439e-05,
      "loss": 0.0023,
      "step": 1216
    },
    {
      "epoch": 0.07219124451299086,
      "grad_norm": 2.159151554107666,
      "learning_rate": 1.4436536180308422e-05,
      "loss": 0.0189,
      "step": 1217
    },
    {
      "epoch": 0.07225056353066793,
      "grad_norm": 7.5704426765441895,
      "learning_rate": 1.4448398576512457e-05,
      "loss": 0.1183,
      "step": 1218
    },
    {
      "epoch": 0.072309882548345,
      "grad_norm": 1.086067795753479,
      "learning_rate": 1.4460260972716489e-05,
      "loss": 0.0045,
      "step": 1219
    },
    {
      "epoch": 0.07236920156602207,
      "grad_norm": 0.9133253693580627,
      "learning_rate": 1.4472123368920522e-05,
      "loss": 0.0067,
      "step": 1220
    },
    {
      "epoch": 0.07242852058369914,
      "grad_norm": 0.1508202850818634,
      "learning_rate": 1.4483985765124558e-05,
      "loss": 0.002,
      "step": 1221
    },
    {
      "epoch": 0.0724878396013762,
      "grad_norm": 0.5536344647407532,
      "learning_rate": 1.449584816132859e-05,
      "loss": 0.0064,
      "step": 1222
    },
    {
      "epoch": 0.07254715861905327,
      "grad_norm": 18.906814575195312,
      "learning_rate": 1.4507710557532625e-05,
      "loss": 0.3975,
      "step": 1223
    },
    {
      "epoch": 0.07260647763673034,
      "grad_norm": 13.05229377746582,
      "learning_rate": 1.4519572953736656e-05,
      "loss": 0.1706,
      "step": 1224
    },
    {
      "epoch": 0.07266579665440741,
      "grad_norm": 3.419961929321289,
      "learning_rate": 1.453143534994069e-05,
      "loss": 0.0319,
      "step": 1225
    },
    {
      "epoch": 0.07272511567208446,
      "grad_norm": 2.4905848503112793,
      "learning_rate": 1.4543297746144722e-05,
      "loss": 0.0197,
      "step": 1226
    },
    {
      "epoch": 0.07278443468976153,
      "grad_norm": 0.05550121143460274,
      "learning_rate": 1.4555160142348757e-05,
      "loss": 0.001,
      "step": 1227
    },
    {
      "epoch": 0.0728437537074386,
      "grad_norm": 51.9495849609375,
      "learning_rate": 1.4567022538552789e-05,
      "loss": 1.05,
      "step": 1228
    },
    {
      "epoch": 0.07290307272511568,
      "grad_norm": 0.9373720288276672,
      "learning_rate": 1.4578884934756822e-05,
      "loss": 0.0042,
      "step": 1229
    },
    {
      "epoch": 0.07296239174279275,
      "grad_norm": 22.950876235961914,
      "learning_rate": 1.4590747330960854e-05,
      "loss": 0.3772,
      "step": 1230
    },
    {
      "epoch": 0.0730217107604698,
      "grad_norm": 24.57500457763672,
      "learning_rate": 1.460260972716489e-05,
      "loss": 0.4983,
      "step": 1231
    },
    {
      "epoch": 0.07308102977814687,
      "grad_norm": 63.34393310546875,
      "learning_rate": 1.4614472123368921e-05,
      "loss": 1.8531,
      "step": 1232
    },
    {
      "epoch": 0.07314034879582394,
      "grad_norm": 28.009387969970703,
      "learning_rate": 1.4626334519572956e-05,
      "loss": 0.4707,
      "step": 1233
    },
    {
      "epoch": 0.07319966781350101,
      "grad_norm": 6.492431163787842,
      "learning_rate": 1.4638196915776988e-05,
      "loss": 0.0248,
      "step": 1234
    },
    {
      "epoch": 0.07325898683117807,
      "grad_norm": 0.03460098057985306,
      "learning_rate": 1.4650059311981022e-05,
      "loss": 0.0004,
      "step": 1235
    },
    {
      "epoch": 0.07331830584885514,
      "grad_norm": 32.09788131713867,
      "learning_rate": 1.4661921708185054e-05,
      "loss": 0.1245,
      "step": 1236
    },
    {
      "epoch": 0.07337762486653221,
      "grad_norm": 1.3260977268218994,
      "learning_rate": 1.4673784104389089e-05,
      "loss": 0.0113,
      "step": 1237
    },
    {
      "epoch": 0.07343694388420928,
      "grad_norm": 18.662446975708008,
      "learning_rate": 1.468564650059312e-05,
      "loss": 0.228,
      "step": 1238
    },
    {
      "epoch": 0.07349626290188635,
      "grad_norm": 12.030985832214355,
      "learning_rate": 1.4697508896797154e-05,
      "loss": 0.3566,
      "step": 1239
    },
    {
      "epoch": 0.0735555819195634,
      "grad_norm": 15.130560874938965,
      "learning_rate": 1.4709371293001186e-05,
      "loss": 0.3652,
      "step": 1240
    },
    {
      "epoch": 0.07361490093724048,
      "grad_norm": 3.0091843605041504,
      "learning_rate": 1.4721233689205221e-05,
      "loss": 0.007,
      "step": 1241
    },
    {
      "epoch": 0.07367421995491755,
      "grad_norm": 8.641072273254395,
      "learning_rate": 1.4733096085409253e-05,
      "loss": 0.0388,
      "step": 1242
    },
    {
      "epoch": 0.07373353897259462,
      "grad_norm": 0.15007099509239197,
      "learning_rate": 1.4744958481613288e-05,
      "loss": 0.001,
      "step": 1243
    },
    {
      "epoch": 0.07379285799027169,
      "grad_norm": 27.703744888305664,
      "learning_rate": 1.475682087781732e-05,
      "loss": 0.1293,
      "step": 1244
    },
    {
      "epoch": 0.07385217700794874,
      "grad_norm": 53.598976135253906,
      "learning_rate": 1.4768683274021354e-05,
      "loss": 0.7767,
      "step": 1245
    },
    {
      "epoch": 0.07391149602562581,
      "grad_norm": 30.8078670501709,
      "learning_rate": 1.4780545670225385e-05,
      "loss": 3.1509,
      "step": 1246
    },
    {
      "epoch": 0.07397081504330288,
      "grad_norm": 13.666648864746094,
      "learning_rate": 1.479240806642942e-05,
      "loss": 0.1048,
      "step": 1247
    },
    {
      "epoch": 0.07403013406097995,
      "grad_norm": 47.492366790771484,
      "learning_rate": 1.4804270462633452e-05,
      "loss": 1.2951,
      "step": 1248
    },
    {
      "epoch": 0.07408945307865701,
      "grad_norm": 20.63587760925293,
      "learning_rate": 1.4816132858837486e-05,
      "loss": 0.0525,
      "step": 1249
    },
    {
      "epoch": 0.07414877209633408,
      "grad_norm": 0.1464616060256958,
      "learning_rate": 1.4827995255041521e-05,
      "loss": 0.0011,
      "step": 1250
    },
    {
      "epoch": 0.07420809111401115,
      "grad_norm": 21.72698402404785,
      "learning_rate": 1.4839857651245553e-05,
      "loss": 0.2136,
      "step": 1251
    },
    {
      "epoch": 0.07426741013168822,
      "grad_norm": 0.6801286935806274,
      "learning_rate": 1.4851720047449586e-05,
      "loss": 0.0043,
      "step": 1252
    },
    {
      "epoch": 0.07432672914936529,
      "grad_norm": 5.9513773918151855,
      "learning_rate": 1.4863582443653618e-05,
      "loss": 0.0566,
      "step": 1253
    },
    {
      "epoch": 0.07438604816704235,
      "grad_norm": 33.964412689208984,
      "learning_rate": 1.4875444839857654e-05,
      "loss": 0.433,
      "step": 1254
    },
    {
      "epoch": 0.07444536718471942,
      "grad_norm": 0.04784053936600685,
      "learning_rate": 1.4887307236061685e-05,
      "loss": 0.0007,
      "step": 1255
    },
    {
      "epoch": 0.07450468620239649,
      "grad_norm": 6.907476425170898,
      "learning_rate": 1.489916963226572e-05,
      "loss": 0.0439,
      "step": 1256
    },
    {
      "epoch": 0.07456400522007356,
      "grad_norm": 25.305925369262695,
      "learning_rate": 1.4911032028469752e-05,
      "loss": 0.6256,
      "step": 1257
    },
    {
      "epoch": 0.07462332423775063,
      "grad_norm": 2.9289157390594482,
      "learning_rate": 1.4922894424673786e-05,
      "loss": 0.0178,
      "step": 1258
    },
    {
      "epoch": 0.07468264325542769,
      "grad_norm": 0.3703809678554535,
      "learning_rate": 1.4934756820877818e-05,
      "loss": 0.0022,
      "step": 1259
    },
    {
      "epoch": 0.07474196227310476,
      "grad_norm": 17.28885269165039,
      "learning_rate": 1.4946619217081853e-05,
      "loss": 0.3104,
      "step": 1260
    },
    {
      "epoch": 0.07480128129078183,
      "grad_norm": 6.777275085449219,
      "learning_rate": 1.4958481613285885e-05,
      "loss": 0.0562,
      "step": 1261
    },
    {
      "epoch": 0.0748606003084589,
      "grad_norm": 8.243762969970703,
      "learning_rate": 1.4970344009489918e-05,
      "loss": 0.0416,
      "step": 1262
    },
    {
      "epoch": 0.07491991932613595,
      "grad_norm": 13.416831970214844,
      "learning_rate": 1.498220640569395e-05,
      "loss": 0.1524,
      "step": 1263
    },
    {
      "epoch": 0.07497923834381302,
      "grad_norm": 9.723966598510742,
      "learning_rate": 1.4994068801897985e-05,
      "loss": 0.1255,
      "step": 1264
    },
    {
      "epoch": 0.0750385573614901,
      "grad_norm": 38.358909606933594,
      "learning_rate": 1.5005931198102017e-05,
      "loss": 0.211,
      "step": 1265
    },
    {
      "epoch": 0.07509787637916716,
      "grad_norm": 53.683982849121094,
      "learning_rate": 1.5017793594306052e-05,
      "loss": 0.208,
      "step": 1266
    },
    {
      "epoch": 0.07515719539684423,
      "grad_norm": 1.2907249927520752,
      "learning_rate": 1.5029655990510084e-05,
      "loss": 0.0106,
      "step": 1267
    },
    {
      "epoch": 0.07521651441452129,
      "grad_norm": 16.85954475402832,
      "learning_rate": 1.5041518386714118e-05,
      "loss": 0.1699,
      "step": 1268
    },
    {
      "epoch": 0.07527583343219836,
      "grad_norm": 32.080631256103516,
      "learning_rate": 1.505338078291815e-05,
      "loss": 0.2798,
      "step": 1269
    },
    {
      "epoch": 0.07533515244987543,
      "grad_norm": 12.638262748718262,
      "learning_rate": 1.5065243179122185e-05,
      "loss": 0.0788,
      "step": 1270
    },
    {
      "epoch": 0.0753944714675525,
      "grad_norm": 3.1483259201049805,
      "learning_rate": 1.5077105575326217e-05,
      "loss": 0.0112,
      "step": 1271
    },
    {
      "epoch": 0.07545379048522956,
      "grad_norm": 2.6410415172576904,
      "learning_rate": 1.508896797153025e-05,
      "loss": 0.0288,
      "step": 1272
    },
    {
      "epoch": 0.07551310950290663,
      "grad_norm": 40.959774017333984,
      "learning_rate": 1.5100830367734282e-05,
      "loss": 0.5087,
      "step": 1273
    },
    {
      "epoch": 0.0755724285205837,
      "grad_norm": 8.112497329711914,
      "learning_rate": 1.5112692763938317e-05,
      "loss": 0.073,
      "step": 1274
    },
    {
      "epoch": 0.07563174753826077,
      "grad_norm": 0.02300712838768959,
      "learning_rate": 1.5124555160142349e-05,
      "loss": 0.0003,
      "step": 1275
    },
    {
      "epoch": 0.07569106655593784,
      "grad_norm": 0.11022944748401642,
      "learning_rate": 1.5136417556346384e-05,
      "loss": 0.0012,
      "step": 1276
    },
    {
      "epoch": 0.0757503855736149,
      "grad_norm": 32.57449722290039,
      "learning_rate": 1.5148279952550416e-05,
      "loss": 0.2738,
      "step": 1277
    },
    {
      "epoch": 0.07580970459129197,
      "grad_norm": 0.059677161276340485,
      "learning_rate": 1.516014234875445e-05,
      "loss": 0.0009,
      "step": 1278
    },
    {
      "epoch": 0.07586902360896904,
      "grad_norm": 0.11898139119148254,
      "learning_rate": 1.5172004744958481e-05,
      "loss": 0.0019,
      "step": 1279
    },
    {
      "epoch": 0.0759283426266461,
      "grad_norm": 19.906143188476562,
      "learning_rate": 1.5183867141162517e-05,
      "loss": 0.8108,
      "step": 1280
    },
    {
      "epoch": 0.07598766164432318,
      "grad_norm": 48.81956100463867,
      "learning_rate": 1.519572953736655e-05,
      "loss": 0.8844,
      "step": 1281
    },
    {
      "epoch": 0.07604698066200023,
      "grad_norm": 0.058698419481515884,
      "learning_rate": 1.5207591933570582e-05,
      "loss": 0.0014,
      "step": 1282
    },
    {
      "epoch": 0.0761062996796773,
      "grad_norm": 0.31658056378364563,
      "learning_rate": 1.5219454329774617e-05,
      "loss": 0.0033,
      "step": 1283
    },
    {
      "epoch": 0.07616561869735437,
      "grad_norm": 30.506547927856445,
      "learning_rate": 1.5231316725978649e-05,
      "loss": 0.6108,
      "step": 1284
    },
    {
      "epoch": 0.07622493771503144,
      "grad_norm": 2.7121076583862305,
      "learning_rate": 1.5243179122182682e-05,
      "loss": 0.0164,
      "step": 1285
    },
    {
      "epoch": 0.0762842567327085,
      "grad_norm": 6.341916561126709,
      "learning_rate": 1.5255041518386714e-05,
      "loss": 0.0446,
      "step": 1286
    },
    {
      "epoch": 0.07634357575038557,
      "grad_norm": 27.696849822998047,
      "learning_rate": 1.526690391459075e-05,
      "loss": 2.4432,
      "step": 1287
    },
    {
      "epoch": 0.07640289476806264,
      "grad_norm": 3.4660208225250244,
      "learning_rate": 1.5278766310794783e-05,
      "loss": 0.0198,
      "step": 1288
    },
    {
      "epoch": 0.07646221378573971,
      "grad_norm": 0.7649022936820984,
      "learning_rate": 1.5290628706998817e-05,
      "loss": 0.0054,
      "step": 1289
    },
    {
      "epoch": 0.07652153280341678,
      "grad_norm": 22.172121047973633,
      "learning_rate": 1.5302491103202847e-05,
      "loss": 0.5822,
      "step": 1290
    },
    {
      "epoch": 0.07658085182109384,
      "grad_norm": 33.457237243652344,
      "learning_rate": 1.5314353499406884e-05,
      "loss": 1.4996,
      "step": 1291
    },
    {
      "epoch": 0.07664017083877091,
      "grad_norm": 9.875329971313477,
      "learning_rate": 1.5326215895610914e-05,
      "loss": 0.0236,
      "step": 1292
    },
    {
      "epoch": 0.07669948985644798,
      "grad_norm": 15.106829643249512,
      "learning_rate": 1.5338078291814947e-05,
      "loss": 0.15,
      "step": 1293
    },
    {
      "epoch": 0.07675880887412505,
      "grad_norm": 20.3529052734375,
      "learning_rate": 1.534994068801898e-05,
      "loss": 0.1923,
      "step": 1294
    },
    {
      "epoch": 0.07681812789180212,
      "grad_norm": 11.930947303771973,
      "learning_rate": 1.5361803084223014e-05,
      "loss": 0.2043,
      "step": 1295
    },
    {
      "epoch": 0.07687744690947917,
      "grad_norm": 23.845746994018555,
      "learning_rate": 1.5373665480427048e-05,
      "loss": 0.6127,
      "step": 1296
    },
    {
      "epoch": 0.07693676592715624,
      "grad_norm": 19.01056480407715,
      "learning_rate": 1.538552787663108e-05,
      "loss": 0.8448,
      "step": 1297
    },
    {
      "epoch": 0.07699608494483332,
      "grad_norm": 1.2873464822769165,
      "learning_rate": 1.5397390272835115e-05,
      "loss": 0.0095,
      "step": 1298
    },
    {
      "epoch": 0.07705540396251039,
      "grad_norm": 59.970638275146484,
      "learning_rate": 1.540925266903915e-05,
      "loss": 0.9174,
      "step": 1299
    },
    {
      "epoch": 0.07711472298018744,
      "grad_norm": 21.17597770690918,
      "learning_rate": 1.542111506524318e-05,
      "loss": 0.2616,
      "step": 1300
    },
    {
      "epoch": 0.07717404199786451,
      "grad_norm": 0.47597113251686096,
      "learning_rate": 1.5432977461447215e-05,
      "loss": 0.0042,
      "step": 1301
    },
    {
      "epoch": 0.07723336101554158,
      "grad_norm": 40.908653259277344,
      "learning_rate": 1.5444839857651245e-05,
      "loss": 0.4106,
      "step": 1302
    },
    {
      "epoch": 0.07729268003321865,
      "grad_norm": 38.855995178222656,
      "learning_rate": 1.545670225385528e-05,
      "loss": 0.5717,
      "step": 1303
    },
    {
      "epoch": 0.07735199905089572,
      "grad_norm": 12.362601280212402,
      "learning_rate": 1.5468564650059313e-05,
      "loss": 0.212,
      "step": 1304
    },
    {
      "epoch": 0.07741131806857278,
      "grad_norm": 5.099349021911621,
      "learning_rate": 1.5480427046263346e-05,
      "loss": 0.0095,
      "step": 1305
    },
    {
      "epoch": 0.07747063708624985,
      "grad_norm": 64.48108673095703,
      "learning_rate": 1.549228944246738e-05,
      "loss": 0.8937,
      "step": 1306
    },
    {
      "epoch": 0.07752995610392692,
      "grad_norm": 1.6135669946670532,
      "learning_rate": 1.5504151838671413e-05,
      "loss": 0.0094,
      "step": 1307
    },
    {
      "epoch": 0.07758927512160399,
      "grad_norm": 24.235734939575195,
      "learning_rate": 1.5516014234875447e-05,
      "loss": 0.2981,
      "step": 1308
    },
    {
      "epoch": 0.07764859413928106,
      "grad_norm": 15.651697158813477,
      "learning_rate": 1.552787663107948e-05,
      "loss": 0.2443,
      "step": 1309
    },
    {
      "epoch": 0.07770791315695812,
      "grad_norm": 4.259941101074219,
      "learning_rate": 1.5539739027283514e-05,
      "loss": 0.2556,
      "step": 1310
    },
    {
      "epoch": 0.07776723217463519,
      "grad_norm": 1.929581642150879,
      "learning_rate": 1.5551601423487547e-05,
      "loss": 0.01,
      "step": 1311
    },
    {
      "epoch": 0.07782655119231226,
      "grad_norm": 36.721736907958984,
      "learning_rate": 1.556346381969158e-05,
      "loss": 0.8798,
      "step": 1312
    },
    {
      "epoch": 0.07788587020998933,
      "grad_norm": 5.359382629394531,
      "learning_rate": 1.557532621589561e-05,
      "loss": 0.0477,
      "step": 1313
    },
    {
      "epoch": 0.07794518922766638,
      "grad_norm": 1.2580050230026245,
      "learning_rate": 1.5587188612099648e-05,
      "loss": 0.0076,
      "step": 1314
    },
    {
      "epoch": 0.07800450824534345,
      "grad_norm": 26.761337280273438,
      "learning_rate": 1.5599051008303678e-05,
      "loss": 0.6661,
      "step": 1315
    },
    {
      "epoch": 0.07806382726302052,
      "grad_norm": 23.205562591552734,
      "learning_rate": 1.561091340450771e-05,
      "loss": 0.2623,
      "step": 1316
    },
    {
      "epoch": 0.0781231462806976,
      "grad_norm": 16.657752990722656,
      "learning_rate": 1.5622775800711745e-05,
      "loss": 0.1799,
      "step": 1317
    },
    {
      "epoch": 0.07818246529837466,
      "grad_norm": 11.744302749633789,
      "learning_rate": 1.563463819691578e-05,
      "loss": 0.3734,
      "step": 1318
    },
    {
      "epoch": 0.07824178431605172,
      "grad_norm": 0.03563367575407028,
      "learning_rate": 1.5646500593119812e-05,
      "loss": 0.0006,
      "step": 1319
    },
    {
      "epoch": 0.07830110333372879,
      "grad_norm": 77.43659973144531,
      "learning_rate": 1.5658362989323845e-05,
      "loss": 1.0697,
      "step": 1320
    },
    {
      "epoch": 0.07836042235140586,
      "grad_norm": 12.180349349975586,
      "learning_rate": 1.567022538552788e-05,
      "loss": 0.3492,
      "step": 1321
    },
    {
      "epoch": 0.07841974136908293,
      "grad_norm": 0.2501111924648285,
      "learning_rate": 1.5682087781731912e-05,
      "loss": 0.0016,
      "step": 1322
    },
    {
      "epoch": 0.07847906038675999,
      "grad_norm": 4.796742916107178,
      "learning_rate": 1.5693950177935943e-05,
      "loss": 0.025,
      "step": 1323
    },
    {
      "epoch": 0.07853837940443706,
      "grad_norm": 29.180137634277344,
      "learning_rate": 1.570581257413998e-05,
      "loss": 1.5159,
      "step": 1324
    },
    {
      "epoch": 0.07859769842211413,
      "grad_norm": 0.3324487805366516,
      "learning_rate": 1.571767497034401e-05,
      "loss": 0.0037,
      "step": 1325
    },
    {
      "epoch": 0.0786570174397912,
      "grad_norm": 7.7234296798706055,
      "learning_rate": 1.5729537366548043e-05,
      "loss": 0.0614,
      "step": 1326
    },
    {
      "epoch": 0.07871633645746827,
      "grad_norm": 23.98431968688965,
      "learning_rate": 1.5741399762752077e-05,
      "loss": 0.459,
      "step": 1327
    },
    {
      "epoch": 0.07877565547514533,
      "grad_norm": 15.544511795043945,
      "learning_rate": 1.575326215895611e-05,
      "loss": 0.0726,
      "step": 1328
    },
    {
      "epoch": 0.0788349744928224,
      "grad_norm": 16.368793487548828,
      "learning_rate": 1.5765124555160144e-05,
      "loss": 1.2675,
      "step": 1329
    },
    {
      "epoch": 0.07889429351049947,
      "grad_norm": 0.44685786962509155,
      "learning_rate": 1.5776986951364177e-05,
      "loss": 0.0026,
      "step": 1330
    },
    {
      "epoch": 0.07895361252817654,
      "grad_norm": 37.221744537353516,
      "learning_rate": 1.578884934756821e-05,
      "loss": 0.2644,
      "step": 1331
    },
    {
      "epoch": 0.0790129315458536,
      "grad_norm": 0.2915516495704651,
      "learning_rate": 1.5800711743772244e-05,
      "loss": 0.0054,
      "step": 1332
    },
    {
      "epoch": 0.07907225056353066,
      "grad_norm": 1.5015426874160767,
      "learning_rate": 1.5812574139976274e-05,
      "loss": 0.0127,
      "step": 1333
    },
    {
      "epoch": 0.07913156958120773,
      "grad_norm": 46.326988220214844,
      "learning_rate": 1.582443653618031e-05,
      "loss": 0.321,
      "step": 1334
    },
    {
      "epoch": 0.0791908885988848,
      "grad_norm": 17.6676025390625,
      "learning_rate": 1.583629893238434e-05,
      "loss": 0.4164,
      "step": 1335
    },
    {
      "epoch": 0.07925020761656187,
      "grad_norm": 3.7917258739471436,
      "learning_rate": 1.5848161328588375e-05,
      "loss": 0.0328,
      "step": 1336
    },
    {
      "epoch": 0.07930952663423893,
      "grad_norm": 0.2841365933418274,
      "learning_rate": 1.586002372479241e-05,
      "loss": 0.0038,
      "step": 1337
    },
    {
      "epoch": 0.079368845651916,
      "grad_norm": 30.840700149536133,
      "learning_rate": 1.5871886120996442e-05,
      "loss": 0.069,
      "step": 1338
    },
    {
      "epoch": 0.07942816466959307,
      "grad_norm": 0.12397275120019913,
      "learning_rate": 1.5883748517200476e-05,
      "loss": 0.002,
      "step": 1339
    },
    {
      "epoch": 0.07948748368727014,
      "grad_norm": 16.876747131347656,
      "learning_rate": 1.589561091340451e-05,
      "loss": 0.5535,
      "step": 1340
    },
    {
      "epoch": 0.07954680270494721,
      "grad_norm": 15.828529357910156,
      "learning_rate": 1.5907473309608543e-05,
      "loss": 0.0983,
      "step": 1341
    },
    {
      "epoch": 0.07960612172262427,
      "grad_norm": 56.72171401977539,
      "learning_rate": 1.5919335705812576e-05,
      "loss": 0.275,
      "step": 1342
    },
    {
      "epoch": 0.07966544074030134,
      "grad_norm": 0.08106275647878647,
      "learning_rate": 1.593119810201661e-05,
      "loss": 0.0008,
      "step": 1343
    },
    {
      "epoch": 0.07972475975797841,
      "grad_norm": 14.11219596862793,
      "learning_rate": 1.5943060498220643e-05,
      "loss": 0.2733,
      "step": 1344
    },
    {
      "epoch": 0.07978407877565548,
      "grad_norm": 3.5140883922576904,
      "learning_rate": 1.5954922894424677e-05,
      "loss": 0.0297,
      "step": 1345
    },
    {
      "epoch": 0.07984339779333255,
      "grad_norm": 0.686621367931366,
      "learning_rate": 1.5966785290628707e-05,
      "loss": 0.0049,
      "step": 1346
    },
    {
      "epoch": 0.0799027168110096,
      "grad_norm": 1.1304664611816406,
      "learning_rate": 1.5978647686832744e-05,
      "loss": 0.0085,
      "step": 1347
    },
    {
      "epoch": 0.07996203582868668,
      "grad_norm": 0.03262392804026604,
      "learning_rate": 1.5990510083036774e-05,
      "loss": 0.0006,
      "step": 1348
    },
    {
      "epoch": 0.08002135484636375,
      "grad_norm": 8.81772518157959,
      "learning_rate": 1.6002372479240807e-05,
      "loss": 0.4071,
      "step": 1349
    },
    {
      "epoch": 0.08008067386404082,
      "grad_norm": 7.615694046020508,
      "learning_rate": 1.601423487544484e-05,
      "loss": 0.185,
      "step": 1350
    },
    {
      "epoch": 0.08013999288171787,
      "grad_norm": 0.2555987238883972,
      "learning_rate": 1.6026097271648874e-05,
      "loss": 0.0022,
      "step": 1351
    },
    {
      "epoch": 0.08019931189939494,
      "grad_norm": 5.536379337310791,
      "learning_rate": 1.6037959667852908e-05,
      "loss": 0.0348,
      "step": 1352
    },
    {
      "epoch": 0.08025863091707201,
      "grad_norm": 14.838717460632324,
      "learning_rate": 1.604982206405694e-05,
      "loss": 0.3475,
      "step": 1353
    },
    {
      "epoch": 0.08031794993474908,
      "grad_norm": 14.674931526184082,
      "learning_rate": 1.6061684460260975e-05,
      "loss": 0.7277,
      "step": 1354
    },
    {
      "epoch": 0.08037726895242615,
      "grad_norm": 0.6068735122680664,
      "learning_rate": 1.607354685646501e-05,
      "loss": 0.0061,
      "step": 1355
    },
    {
      "epoch": 0.08043658797010321,
      "grad_norm": 1.1393228769302368,
      "learning_rate": 1.608540925266904e-05,
      "loss": 0.0092,
      "step": 1356
    },
    {
      "epoch": 0.08049590698778028,
      "grad_norm": 10.069403648376465,
      "learning_rate": 1.6097271648873075e-05,
      "loss": 0.0308,
      "step": 1357
    },
    {
      "epoch": 0.08055522600545735,
      "grad_norm": 11.253527641296387,
      "learning_rate": 1.6109134045077106e-05,
      "loss": 0.1225,
      "step": 1358
    },
    {
      "epoch": 0.08061454502313442,
      "grad_norm": 0.3317014276981354,
      "learning_rate": 1.612099644128114e-05,
      "loss": 0.0032,
      "step": 1359
    },
    {
      "epoch": 0.08067386404081149,
      "grad_norm": 0.11758706718683243,
      "learning_rate": 1.6132858837485173e-05,
      "loss": 0.002,
      "step": 1360
    },
    {
      "epoch": 0.08073318305848855,
      "grad_norm": 33.58361053466797,
      "learning_rate": 1.6144721233689206e-05,
      "loss": 1.1691,
      "step": 1361
    },
    {
      "epoch": 0.08079250207616562,
      "grad_norm": 1.136061191558838,
      "learning_rate": 1.615658362989324e-05,
      "loss": 0.0085,
      "step": 1362
    },
    {
      "epoch": 0.08085182109384269,
      "grad_norm": 62.17837142944336,
      "learning_rate": 1.6168446026097273e-05,
      "loss": 0.3306,
      "step": 1363
    },
    {
      "epoch": 0.08091114011151976,
      "grad_norm": 36.586429595947266,
      "learning_rate": 1.6180308422301307e-05,
      "loss": 0.7473,
      "step": 1364
    },
    {
      "epoch": 0.08097045912919681,
      "grad_norm": 14.333422660827637,
      "learning_rate": 1.619217081850534e-05,
      "loss": 0.3276,
      "step": 1365
    },
    {
      "epoch": 0.08102977814687388,
      "grad_norm": 27.28018569946289,
      "learning_rate": 1.620403321470937e-05,
      "loss": 1.2975,
      "step": 1366
    },
    {
      "epoch": 0.08108909716455096,
      "grad_norm": 4.022741317749023,
      "learning_rate": 1.6215895610913407e-05,
      "loss": 0.0254,
      "step": 1367
    },
    {
      "epoch": 0.08114841618222803,
      "grad_norm": 11.384842872619629,
      "learning_rate": 1.622775800711744e-05,
      "loss": 0.149,
      "step": 1368
    },
    {
      "epoch": 0.0812077351999051,
      "grad_norm": 3.9177050590515137,
      "learning_rate": 1.623962040332147e-05,
      "loss": 0.0251,
      "step": 1369
    },
    {
      "epoch": 0.08126705421758215,
      "grad_norm": 22.706317901611328,
      "learning_rate": 1.6251482799525508e-05,
      "loss": 0.1312,
      "step": 1370
    },
    {
      "epoch": 0.08132637323525922,
      "grad_norm": 0.25760555267333984,
      "learning_rate": 1.6263345195729538e-05,
      "loss": 0.0042,
      "step": 1371
    },
    {
      "epoch": 0.08138569225293629,
      "grad_norm": 0.12682083249092102,
      "learning_rate": 1.627520759193357e-05,
      "loss": 0.0017,
      "step": 1372
    },
    {
      "epoch": 0.08144501127061336,
      "grad_norm": 0.4941398799419403,
      "learning_rate": 1.6287069988137605e-05,
      "loss": 0.0067,
      "step": 1373
    },
    {
      "epoch": 0.08150433028829042,
      "grad_norm": 0.3227140009403229,
      "learning_rate": 1.629893238434164e-05,
      "loss": 0.003,
      "step": 1374
    },
    {
      "epoch": 0.08156364930596749,
      "grad_norm": 30.722877502441406,
      "learning_rate": 1.6310794780545672e-05,
      "loss": 0.6072,
      "step": 1375
    },
    {
      "epoch": 0.08162296832364456,
      "grad_norm": 21.03399085998535,
      "learning_rate": 1.6322657176749706e-05,
      "loss": 0.4847,
      "step": 1376
    },
    {
      "epoch": 0.08168228734132163,
      "grad_norm": 10.967559814453125,
      "learning_rate": 1.633451957295374e-05,
      "loss": 0.0792,
      "step": 1377
    },
    {
      "epoch": 0.0817416063589987,
      "grad_norm": 6.243692398071289,
      "learning_rate": 1.6346381969157773e-05,
      "loss": 0.0413,
      "step": 1378
    },
    {
      "epoch": 0.08180092537667576,
      "grad_norm": 20.163318634033203,
      "learning_rate": 1.6358244365361803e-05,
      "loss": 0.1139,
      "step": 1379
    },
    {
      "epoch": 0.08186024439435283,
      "grad_norm": 55.12126159667969,
      "learning_rate": 1.637010676156584e-05,
      "loss": 1.173,
      "step": 1380
    },
    {
      "epoch": 0.0819195634120299,
      "grad_norm": 17.338436126708984,
      "learning_rate": 1.638196915776987e-05,
      "loss": 0.301,
      "step": 1381
    },
    {
      "epoch": 0.08197888242970697,
      "grad_norm": 29.925979614257812,
      "learning_rate": 1.6393831553973903e-05,
      "loss": 0.5851,
      "step": 1382
    },
    {
      "epoch": 0.08203820144738404,
      "grad_norm": 0.3071501553058624,
      "learning_rate": 1.6405693950177937e-05,
      "loss": 0.0035,
      "step": 1383
    },
    {
      "epoch": 0.0820975204650611,
      "grad_norm": 3.931917190551758,
      "learning_rate": 1.641755634638197e-05,
      "loss": 0.0233,
      "step": 1384
    },
    {
      "epoch": 0.08215683948273816,
      "grad_norm": 3.978389263153076,
      "learning_rate": 1.6429418742586004e-05,
      "loss": 0.0373,
      "step": 1385
    },
    {
      "epoch": 0.08221615850041523,
      "grad_norm": 0.1476312130689621,
      "learning_rate": 1.6441281138790037e-05,
      "loss": 0.0017,
      "step": 1386
    },
    {
      "epoch": 0.0822754775180923,
      "grad_norm": 0.12608087062835693,
      "learning_rate": 1.645314353499407e-05,
      "loss": 0.0025,
      "step": 1387
    },
    {
      "epoch": 0.08233479653576936,
      "grad_norm": 0.1761361062526703,
      "learning_rate": 1.6465005931198104e-05,
      "loss": 0.0011,
      "step": 1388
    },
    {
      "epoch": 0.08239411555344643,
      "grad_norm": 6.1986589431762695,
      "learning_rate": 1.6476868327402135e-05,
      "loss": 0.0724,
      "step": 1389
    },
    {
      "epoch": 0.0824534345711235,
      "grad_norm": 5.750641345977783,
      "learning_rate": 1.648873072360617e-05,
      "loss": 0.0622,
      "step": 1390
    },
    {
      "epoch": 0.08251275358880057,
      "grad_norm": 17.110126495361328,
      "learning_rate": 1.65005931198102e-05,
      "loss": 0.3679,
      "step": 1391
    },
    {
      "epoch": 0.08257207260647764,
      "grad_norm": 32.56341552734375,
      "learning_rate": 1.6512455516014235e-05,
      "loss": 1.4581,
      "step": 1392
    },
    {
      "epoch": 0.0826313916241547,
      "grad_norm": 38.118568420410156,
      "learning_rate": 1.652431791221827e-05,
      "loss": 1.1514,
      "step": 1393
    },
    {
      "epoch": 0.08269071064183177,
      "grad_norm": 0.21723169088363647,
      "learning_rate": 1.6536180308422302e-05,
      "loss": 0.002,
      "step": 1394
    },
    {
      "epoch": 0.08275002965950884,
      "grad_norm": 0.05001446604728699,
      "learning_rate": 1.6548042704626336e-05,
      "loss": 0.0008,
      "step": 1395
    },
    {
      "epoch": 0.08280934867718591,
      "grad_norm": 6.0511155128479,
      "learning_rate": 1.655990510083037e-05,
      "loss": 0.0501,
      "step": 1396
    },
    {
      "epoch": 0.08286866769486298,
      "grad_norm": 1.149936556816101,
      "learning_rate": 1.6571767497034403e-05,
      "loss": 0.0117,
      "step": 1397
    },
    {
      "epoch": 0.08292798671254004,
      "grad_norm": 13.991877555847168,
      "learning_rate": 1.6583629893238436e-05,
      "loss": 0.1983,
      "step": 1398
    },
    {
      "epoch": 0.0829873057302171,
      "grad_norm": 12.481038093566895,
      "learning_rate": 1.659549228944247e-05,
      "loss": 0.4337,
      "step": 1399
    },
    {
      "epoch": 0.08304662474789418,
      "grad_norm": 6.512905120849609,
      "learning_rate": 1.6607354685646503e-05,
      "loss": 0.0936,
      "step": 1400
    },
    {
      "epoch": 0.08310594376557125,
      "grad_norm": 4.430333614349365,
      "learning_rate": 1.6619217081850537e-05,
      "loss": 0.0658,
      "step": 1401
    },
    {
      "epoch": 0.0831652627832483,
      "grad_norm": 0.29709136486053467,
      "learning_rate": 1.6631079478054567e-05,
      "loss": 0.0041,
      "step": 1402
    },
    {
      "epoch": 0.08322458180092537,
      "grad_norm": 8.820489883422852,
      "learning_rate": 1.6642941874258604e-05,
      "loss": 0.1036,
      "step": 1403
    },
    {
      "epoch": 0.08328390081860244,
      "grad_norm": 10.66254997253418,
      "learning_rate": 1.6654804270462634e-05,
      "loss": 0.181,
      "step": 1404
    },
    {
      "epoch": 0.08334321983627951,
      "grad_norm": 19.5769100189209,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.2801,
      "step": 1405
    },
    {
      "epoch": 0.08340253885395658,
      "grad_norm": 77.5624771118164,
      "learning_rate": 1.66785290628707e-05,
      "loss": 1.9346,
      "step": 1406
    },
    {
      "epoch": 0.08346185787163364,
      "grad_norm": 3.0345728397369385,
      "learning_rate": 1.6690391459074735e-05,
      "loss": 0.0233,
      "step": 1407
    },
    {
      "epoch": 0.08352117688931071,
      "grad_norm": 0.46794891357421875,
      "learning_rate": 1.6702253855278768e-05,
      "loss": 0.0035,
      "step": 1408
    },
    {
      "epoch": 0.08358049590698778,
      "grad_norm": 0.012665621004998684,
      "learning_rate": 1.67141162514828e-05,
      "loss": 0.0002,
      "step": 1409
    },
    {
      "epoch": 0.08363981492466485,
      "grad_norm": 16.8698787689209,
      "learning_rate": 1.6725978647686835e-05,
      "loss": 0.4018,
      "step": 1410
    },
    {
      "epoch": 0.08369913394234192,
      "grad_norm": 0.02239040657877922,
      "learning_rate": 1.673784104389087e-05,
      "loss": 0.0006,
      "step": 1411
    },
    {
      "epoch": 0.08375845296001898,
      "grad_norm": 0.703155517578125,
      "learning_rate": 1.67497034400949e-05,
      "loss": 0.0056,
      "step": 1412
    },
    {
      "epoch": 0.08381777197769605,
      "grad_norm": 0.9392409324645996,
      "learning_rate": 1.6761565836298936e-05,
      "loss": 0.0035,
      "step": 1413
    },
    {
      "epoch": 0.08387709099537312,
      "grad_norm": 23.832542419433594,
      "learning_rate": 1.6773428232502966e-05,
      "loss": 0.3101,
      "step": 1414
    },
    {
      "epoch": 0.08393641001305019,
      "grad_norm": 6.020526885986328,
      "learning_rate": 1.6785290628707e-05,
      "loss": 0.4358,
      "step": 1415
    },
    {
      "epoch": 0.08399572903072725,
      "grad_norm": 15.8089599609375,
      "learning_rate": 1.6797153024911033e-05,
      "loss": 0.0908,
      "step": 1416
    },
    {
      "epoch": 0.08405504804840432,
      "grad_norm": 24.33116340637207,
      "learning_rate": 1.6809015421115066e-05,
      "loss": 0.2305,
      "step": 1417
    },
    {
      "epoch": 0.08411436706608139,
      "grad_norm": 0.23441648483276367,
      "learning_rate": 1.68208778173191e-05,
      "loss": 0.0038,
      "step": 1418
    },
    {
      "epoch": 0.08417368608375846,
      "grad_norm": 11.875021934509277,
      "learning_rate": 1.6832740213523133e-05,
      "loss": 0.1312,
      "step": 1419
    },
    {
      "epoch": 0.08423300510143553,
      "grad_norm": 7.752817630767822,
      "learning_rate": 1.6844602609727167e-05,
      "loss": 0.1018,
      "step": 1420
    },
    {
      "epoch": 0.08429232411911258,
      "grad_norm": 0.09054528176784515,
      "learning_rate": 1.68564650059312e-05,
      "loss": 0.0015,
      "step": 1421
    },
    {
      "epoch": 0.08435164313678965,
      "grad_norm": 31.19590950012207,
      "learning_rate": 1.686832740213523e-05,
      "loss": 0.4416,
      "step": 1422
    },
    {
      "epoch": 0.08441096215446672,
      "grad_norm": 3.9847586154937744,
      "learning_rate": 1.6880189798339267e-05,
      "loss": 0.0264,
      "step": 1423
    },
    {
      "epoch": 0.0844702811721438,
      "grad_norm": 39.647457122802734,
      "learning_rate": 1.6892052194543298e-05,
      "loss": 0.8452,
      "step": 1424
    },
    {
      "epoch": 0.08452960018982085,
      "grad_norm": 0.14843367040157318,
      "learning_rate": 1.690391459074733e-05,
      "loss": 0.0014,
      "step": 1425
    },
    {
      "epoch": 0.08458891920749792,
      "grad_norm": 17.47109603881836,
      "learning_rate": 1.6915776986951368e-05,
      "loss": 0.2771,
      "step": 1426
    },
    {
      "epoch": 0.08464823822517499,
      "grad_norm": 1.521654725074768,
      "learning_rate": 1.6927639383155398e-05,
      "loss": 0.0106,
      "step": 1427
    },
    {
      "epoch": 0.08470755724285206,
      "grad_norm": 11.617274284362793,
      "learning_rate": 1.693950177935943e-05,
      "loss": 0.0894,
      "step": 1428
    },
    {
      "epoch": 0.08476687626052913,
      "grad_norm": 0.08130993694067001,
      "learning_rate": 1.6951364175563465e-05,
      "loss": 0.0007,
      "step": 1429
    },
    {
      "epoch": 0.08482619527820619,
      "grad_norm": 0.9832998514175415,
      "learning_rate": 1.69632265717675e-05,
      "loss": 0.0072,
      "step": 1430
    },
    {
      "epoch": 0.08488551429588326,
      "grad_norm": 13.244735717773438,
      "learning_rate": 1.6975088967971532e-05,
      "loss": 0.3435,
      "step": 1431
    },
    {
      "epoch": 0.08494483331356033,
      "grad_norm": 1.7909623384475708,
      "learning_rate": 1.6986951364175566e-05,
      "loss": 0.0097,
      "step": 1432
    },
    {
      "epoch": 0.0850041523312374,
      "grad_norm": 17.142253875732422,
      "learning_rate": 1.69988137603796e-05,
      "loss": 0.0938,
      "step": 1433
    },
    {
      "epoch": 0.08506347134891447,
      "grad_norm": 0.48574191331863403,
      "learning_rate": 1.7010676156583633e-05,
      "loss": 0.0021,
      "step": 1434
    },
    {
      "epoch": 0.08512279036659152,
      "grad_norm": 21.268102645874023,
      "learning_rate": 1.7022538552787663e-05,
      "loss": 0.4579,
      "step": 1435
    },
    {
      "epoch": 0.0851821093842686,
      "grad_norm": 4.850008487701416,
      "learning_rate": 1.70344009489917e-05,
      "loss": 0.0713,
      "step": 1436
    },
    {
      "epoch": 0.08524142840194567,
      "grad_norm": 15.165351867675781,
      "learning_rate": 1.704626334519573e-05,
      "loss": 1.0853,
      "step": 1437
    },
    {
      "epoch": 0.08530074741962274,
      "grad_norm": 0.08300740271806717,
      "learning_rate": 1.7058125741399763e-05,
      "loss": 0.0011,
      "step": 1438
    },
    {
      "epoch": 0.08536006643729979,
      "grad_norm": 23.336362838745117,
      "learning_rate": 1.7069988137603797e-05,
      "loss": 0.3386,
      "step": 1439
    },
    {
      "epoch": 0.08541938545497686,
      "grad_norm": 172.85350036621094,
      "learning_rate": 1.708185053380783e-05,
      "loss": 0.8291,
      "step": 1440
    },
    {
      "epoch": 0.08547870447265393,
      "grad_norm": 8.309820175170898,
      "learning_rate": 1.7093712930011864e-05,
      "loss": 0.1871,
      "step": 1441
    },
    {
      "epoch": 0.085538023490331,
      "grad_norm": 4.5683183670043945,
      "learning_rate": 1.7105575326215898e-05,
      "loss": 0.1141,
      "step": 1442
    },
    {
      "epoch": 0.08559734250800807,
      "grad_norm": 11.291071891784668,
      "learning_rate": 1.711743772241993e-05,
      "loss": 0.1479,
      "step": 1443
    },
    {
      "epoch": 0.08565666152568513,
      "grad_norm": 0.807441234588623,
      "learning_rate": 1.7129300118623965e-05,
      "loss": 0.0078,
      "step": 1444
    },
    {
      "epoch": 0.0857159805433622,
      "grad_norm": 0.20232856273651123,
      "learning_rate": 1.7141162514827995e-05,
      "loss": 0.0016,
      "step": 1445
    },
    {
      "epoch": 0.08577529956103927,
      "grad_norm": 33.591949462890625,
      "learning_rate": 1.715302491103203e-05,
      "loss": 1.2988,
      "step": 1446
    },
    {
      "epoch": 0.08583461857871634,
      "grad_norm": 10.82215404510498,
      "learning_rate": 1.7164887307236062e-05,
      "loss": 0.2971,
      "step": 1447
    },
    {
      "epoch": 0.08589393759639341,
      "grad_norm": 0.049761101603507996,
      "learning_rate": 1.7176749703440095e-05,
      "loss": 0.001,
      "step": 1448
    },
    {
      "epoch": 0.08595325661407047,
      "grad_norm": 0.14870424568653107,
      "learning_rate": 1.718861209964413e-05,
      "loss": 0.0034,
      "step": 1449
    },
    {
      "epoch": 0.08601257563174754,
      "grad_norm": 30.798538208007812,
      "learning_rate": 1.7200474495848162e-05,
      "loss": 0.5554,
      "step": 1450
    },
    {
      "epoch": 0.08607189464942461,
      "grad_norm": 24.774812698364258,
      "learning_rate": 1.7212336892052196e-05,
      "loss": 0.2934,
      "step": 1451
    },
    {
      "epoch": 0.08613121366710168,
      "grad_norm": 17.81491470336914,
      "learning_rate": 1.722419928825623e-05,
      "loss": 0.1738,
      "step": 1452
    },
    {
      "epoch": 0.08619053268477873,
      "grad_norm": 56.667911529541016,
      "learning_rate": 1.7236061684460263e-05,
      "loss": 0.3297,
      "step": 1453
    },
    {
      "epoch": 0.0862498517024558,
      "grad_norm": 18.597761154174805,
      "learning_rate": 1.7247924080664296e-05,
      "loss": 1.3927,
      "step": 1454
    },
    {
      "epoch": 0.08630917072013287,
      "grad_norm": 2.8981130123138428,
      "learning_rate": 1.725978647686833e-05,
      "loss": 0.0119,
      "step": 1455
    },
    {
      "epoch": 0.08636848973780994,
      "grad_norm": 21.546859741210938,
      "learning_rate": 1.7271648873072363e-05,
      "loss": 1.2718,
      "step": 1456
    },
    {
      "epoch": 0.08642780875548702,
      "grad_norm": 4.238037586212158,
      "learning_rate": 1.7283511269276397e-05,
      "loss": 0.0268,
      "step": 1457
    },
    {
      "epoch": 0.08648712777316407,
      "grad_norm": 10.701045036315918,
      "learning_rate": 1.7295373665480427e-05,
      "loss": 0.3301,
      "step": 1458
    },
    {
      "epoch": 0.08654644679084114,
      "grad_norm": 15.507437705993652,
      "learning_rate": 1.7307236061684464e-05,
      "loss": 0.1971,
      "step": 1459
    },
    {
      "epoch": 0.08660576580851821,
      "grad_norm": 3.0848283767700195,
      "learning_rate": 1.7319098457888494e-05,
      "loss": 0.0419,
      "step": 1460
    },
    {
      "epoch": 0.08666508482619528,
      "grad_norm": 37.58503341674805,
      "learning_rate": 1.7330960854092528e-05,
      "loss": 1.9368,
      "step": 1461
    },
    {
      "epoch": 0.08672440384387234,
      "grad_norm": 7.260544776916504,
      "learning_rate": 1.734282325029656e-05,
      "loss": 0.1668,
      "step": 1462
    },
    {
      "epoch": 0.08678372286154941,
      "grad_norm": 1.2020487785339355,
      "learning_rate": 1.7354685646500595e-05,
      "loss": 0.0115,
      "step": 1463
    },
    {
      "epoch": 0.08684304187922648,
      "grad_norm": 1.5683385133743286,
      "learning_rate": 1.7366548042704628e-05,
      "loss": 0.0103,
      "step": 1464
    },
    {
      "epoch": 0.08690236089690355,
      "grad_norm": 2.7946743965148926,
      "learning_rate": 1.737841043890866e-05,
      "loss": 0.018,
      "step": 1465
    },
    {
      "epoch": 0.08696167991458062,
      "grad_norm": 6.198267936706543,
      "learning_rate": 1.7390272835112695e-05,
      "loss": 0.3223,
      "step": 1466
    },
    {
      "epoch": 0.08702099893225768,
      "grad_norm": 10.761144638061523,
      "learning_rate": 1.740213523131673e-05,
      "loss": 0.0915,
      "step": 1467
    },
    {
      "epoch": 0.08708031794993475,
      "grad_norm": 12.172670364379883,
      "learning_rate": 1.741399762752076e-05,
      "loss": 0.2899,
      "step": 1468
    },
    {
      "epoch": 0.08713963696761182,
      "grad_norm": 1.2943528890609741,
      "learning_rate": 1.7425860023724796e-05,
      "loss": 0.012,
      "step": 1469
    },
    {
      "epoch": 0.08719895598528889,
      "grad_norm": 0.2490861415863037,
      "learning_rate": 1.7437722419928826e-05,
      "loss": 0.004,
      "step": 1470
    },
    {
      "epoch": 0.08725827500296596,
      "grad_norm": 0.13737402856349945,
      "learning_rate": 1.744958481613286e-05,
      "loss": 0.0018,
      "step": 1471
    },
    {
      "epoch": 0.08731759402064301,
      "grad_norm": 0.014497684314846992,
      "learning_rate": 1.7461447212336893e-05,
      "loss": 0.0004,
      "step": 1472
    },
    {
      "epoch": 0.08737691303832008,
      "grad_norm": 28.442256927490234,
      "learning_rate": 1.7473309608540926e-05,
      "loss": 0.4808,
      "step": 1473
    },
    {
      "epoch": 0.08743623205599715,
      "grad_norm": 2.594871997833252,
      "learning_rate": 1.748517200474496e-05,
      "loss": 0.0265,
      "step": 1474
    },
    {
      "epoch": 0.08749555107367422,
      "grad_norm": 36.25309753417969,
      "learning_rate": 1.7497034400948993e-05,
      "loss": 1.3276,
      "step": 1475
    },
    {
      "epoch": 0.08755487009135128,
      "grad_norm": 5.467010021209717,
      "learning_rate": 1.7508896797153027e-05,
      "loss": 0.126,
      "step": 1476
    },
    {
      "epoch": 0.08761418910902835,
      "grad_norm": 11.147406578063965,
      "learning_rate": 1.752075919335706e-05,
      "loss": 0.1215,
      "step": 1477
    },
    {
      "epoch": 0.08767350812670542,
      "grad_norm": 18.546913146972656,
      "learning_rate": 1.753262158956109e-05,
      "loss": 0.1854,
      "step": 1478
    },
    {
      "epoch": 0.08773282714438249,
      "grad_norm": 29.768585205078125,
      "learning_rate": 1.7544483985765128e-05,
      "loss": 0.135,
      "step": 1479
    },
    {
      "epoch": 0.08779214616205956,
      "grad_norm": 39.18537902832031,
      "learning_rate": 1.7556346381969158e-05,
      "loss": 0.8275,
      "step": 1480
    },
    {
      "epoch": 0.08785146517973662,
      "grad_norm": 43.045440673828125,
      "learning_rate": 1.756820877817319e-05,
      "loss": 0.1975,
      "step": 1481
    },
    {
      "epoch": 0.08791078419741369,
      "grad_norm": 0.058604199439287186,
      "learning_rate": 1.7580071174377225e-05,
      "loss": 0.001,
      "step": 1482
    },
    {
      "epoch": 0.08797010321509076,
      "grad_norm": 7.131133556365967,
      "learning_rate": 1.7591933570581258e-05,
      "loss": 0.0337,
      "step": 1483
    },
    {
      "epoch": 0.08802942223276783,
      "grad_norm": 1.926883578300476,
      "learning_rate": 1.7603795966785292e-05,
      "loss": 0.0127,
      "step": 1484
    },
    {
      "epoch": 0.0880887412504449,
      "grad_norm": 10.195069313049316,
      "learning_rate": 1.7615658362989325e-05,
      "loss": 0.0924,
      "step": 1485
    },
    {
      "epoch": 0.08814806026812196,
      "grad_norm": 13.915552139282227,
      "learning_rate": 1.762752075919336e-05,
      "loss": 0.3916,
      "step": 1486
    },
    {
      "epoch": 0.08820737928579903,
      "grad_norm": 0.036177728325128555,
      "learning_rate": 1.7639383155397392e-05,
      "loss": 0.0009,
      "step": 1487
    },
    {
      "epoch": 0.0882666983034761,
      "grad_norm": 17.64438247680664,
      "learning_rate": 1.7651245551601426e-05,
      "loss": 0.377,
      "step": 1488
    },
    {
      "epoch": 0.08832601732115317,
      "grad_norm": 1.8000324964523315,
      "learning_rate": 1.766310794780546e-05,
      "loss": 0.0119,
      "step": 1489
    },
    {
      "epoch": 0.08838533633883022,
      "grad_norm": 3.939382791519165,
      "learning_rate": 1.7674970344009493e-05,
      "loss": 0.1319,
      "step": 1490
    },
    {
      "epoch": 0.08844465535650729,
      "grad_norm": 1.4927421808242798,
      "learning_rate": 1.7686832740213523e-05,
      "loss": 0.0171,
      "step": 1491
    },
    {
      "epoch": 0.08850397437418436,
      "grad_norm": 11.370841979980469,
      "learning_rate": 1.769869513641756e-05,
      "loss": 0.0978,
      "step": 1492
    },
    {
      "epoch": 0.08856329339186143,
      "grad_norm": 6.561331272125244,
      "learning_rate": 1.771055753262159e-05,
      "loss": 0.1126,
      "step": 1493
    },
    {
      "epoch": 0.0886226124095385,
      "grad_norm": 16.648529052734375,
      "learning_rate": 1.7722419928825624e-05,
      "loss": 0.7205,
      "step": 1494
    },
    {
      "epoch": 0.08868193142721556,
      "grad_norm": 24.68693733215332,
      "learning_rate": 1.7734282325029657e-05,
      "loss": 0.4554,
      "step": 1495
    },
    {
      "epoch": 0.08874125044489263,
      "grad_norm": 85.7117919921875,
      "learning_rate": 1.774614472123369e-05,
      "loss": 1.1654,
      "step": 1496
    },
    {
      "epoch": 0.0888005694625697,
      "grad_norm": 0.5174915194511414,
      "learning_rate": 1.7758007117437724e-05,
      "loss": 0.0067,
      "step": 1497
    },
    {
      "epoch": 0.08885988848024677,
      "grad_norm": 44.898345947265625,
      "learning_rate": 1.7769869513641758e-05,
      "loss": 0.4934,
      "step": 1498
    },
    {
      "epoch": 0.08891920749792384,
      "grad_norm": 8.671782493591309,
      "learning_rate": 1.778173190984579e-05,
      "loss": 0.0636,
      "step": 1499
    },
    {
      "epoch": 0.0889785265156009,
      "grad_norm": 10.578588485717773,
      "learning_rate": 1.7793594306049825e-05,
      "loss": 0.0745,
      "step": 1500
    },
    {
      "epoch": 0.08903784553327797,
      "grad_norm": 18.21291732788086,
      "learning_rate": 1.7805456702253855e-05,
      "loss": 0.6368,
      "step": 1501
    },
    {
      "epoch": 0.08909716455095504,
      "grad_norm": 1.0364267826080322,
      "learning_rate": 1.7817319098457892e-05,
      "loss": 0.0098,
      "step": 1502
    },
    {
      "epoch": 0.08915648356863211,
      "grad_norm": 0.22792945802211761,
      "learning_rate": 1.7829181494661922e-05,
      "loss": 0.0042,
      "step": 1503
    },
    {
      "epoch": 0.08921580258630916,
      "grad_norm": 9.437804222106934,
      "learning_rate": 1.7841043890865955e-05,
      "loss": 0.0224,
      "step": 1504
    },
    {
      "epoch": 0.08927512160398623,
      "grad_norm": 2.83115816116333,
      "learning_rate": 1.785290628706999e-05,
      "loss": 0.0196,
      "step": 1505
    },
    {
      "epoch": 0.0893344406216633,
      "grad_norm": 18.607574462890625,
      "learning_rate": 1.7864768683274022e-05,
      "loss": 1.3485,
      "step": 1506
    },
    {
      "epoch": 0.08939375963934038,
      "grad_norm": 25.58161163330078,
      "learning_rate": 1.7876631079478056e-05,
      "loss": 0.2837,
      "step": 1507
    },
    {
      "epoch": 0.08945307865701745,
      "grad_norm": 30.212848663330078,
      "learning_rate": 1.788849347568209e-05,
      "loss": 0.1528,
      "step": 1508
    },
    {
      "epoch": 0.0895123976746945,
      "grad_norm": 5.709510326385498,
      "learning_rate": 1.7900355871886123e-05,
      "loss": 0.22,
      "step": 1509
    },
    {
      "epoch": 0.08957171669237157,
      "grad_norm": 27.63766098022461,
      "learning_rate": 1.7912218268090156e-05,
      "loss": 0.1913,
      "step": 1510
    },
    {
      "epoch": 0.08963103571004864,
      "grad_norm": 3.076416254043579,
      "learning_rate": 1.7924080664294187e-05,
      "loss": 0.0371,
      "step": 1511
    },
    {
      "epoch": 0.08969035472772571,
      "grad_norm": 19.72402572631836,
      "learning_rate": 1.7935943060498224e-05,
      "loss": 1.2456,
      "step": 1512
    },
    {
      "epoch": 0.08974967374540277,
      "grad_norm": 16.93409538269043,
      "learning_rate": 1.7947805456702257e-05,
      "loss": 1.2878,
      "step": 1513
    },
    {
      "epoch": 0.08980899276307984,
      "grad_norm": 13.027085304260254,
      "learning_rate": 1.7959667852906287e-05,
      "loss": 0.0998,
      "step": 1514
    },
    {
      "epoch": 0.08986831178075691,
      "grad_norm": 1.9249799251556396,
      "learning_rate": 1.7971530249110324e-05,
      "loss": 0.0129,
      "step": 1515
    },
    {
      "epoch": 0.08992763079843398,
      "grad_norm": 25.725130081176758,
      "learning_rate": 1.7983392645314354e-05,
      "loss": 0.9077,
      "step": 1516
    },
    {
      "epoch": 0.08998694981611105,
      "grad_norm": 17.19769859313965,
      "learning_rate": 1.7995255041518388e-05,
      "loss": 0.0948,
      "step": 1517
    },
    {
      "epoch": 0.0900462688337881,
      "grad_norm": 12.85191822052002,
      "learning_rate": 1.800711743772242e-05,
      "loss": 1.1318,
      "step": 1518
    },
    {
      "epoch": 0.09010558785146518,
      "grad_norm": 4.928990364074707,
      "learning_rate": 1.8018979833926455e-05,
      "loss": 0.0594,
      "step": 1519
    },
    {
      "epoch": 0.09016490686914225,
      "grad_norm": 0.23634512722492218,
      "learning_rate": 1.8030842230130488e-05,
      "loss": 0.0035,
      "step": 1520
    },
    {
      "epoch": 0.09022422588681932,
      "grad_norm": 9.06618881225586,
      "learning_rate": 1.8042704626334522e-05,
      "loss": 0.0592,
      "step": 1521
    },
    {
      "epoch": 0.09028354490449639,
      "grad_norm": 0.3832463324069977,
      "learning_rate": 1.8054567022538555e-05,
      "loss": 0.0075,
      "step": 1522
    },
    {
      "epoch": 0.09034286392217344,
      "grad_norm": 24.251955032348633,
      "learning_rate": 1.806642941874259e-05,
      "loss": 0.1326,
      "step": 1523
    },
    {
      "epoch": 0.09040218293985051,
      "grad_norm": 1.8708393573760986,
      "learning_rate": 1.807829181494662e-05,
      "loss": 0.0118,
      "step": 1524
    },
    {
      "epoch": 0.09046150195752758,
      "grad_norm": 19.719892501831055,
      "learning_rate": 1.8090154211150656e-05,
      "loss": 0.1217,
      "step": 1525
    },
    {
      "epoch": 0.09052082097520465,
      "grad_norm": 0.025911465287208557,
      "learning_rate": 1.8102016607354686e-05,
      "loss": 0.0006,
      "step": 1526
    },
    {
      "epoch": 0.09058013999288171,
      "grad_norm": 8.005026817321777,
      "learning_rate": 1.811387900355872e-05,
      "loss": 0.5012,
      "step": 1527
    },
    {
      "epoch": 0.09063945901055878,
      "grad_norm": 0.8014708757400513,
      "learning_rate": 1.8125741399762753e-05,
      "loss": 0.0057,
      "step": 1528
    },
    {
      "epoch": 0.09069877802823585,
      "grad_norm": 17.997146606445312,
      "learning_rate": 1.8137603795966787e-05,
      "loss": 0.1513,
      "step": 1529
    },
    {
      "epoch": 0.09075809704591292,
      "grad_norm": 0.33291110396385193,
      "learning_rate": 1.814946619217082e-05,
      "loss": 0.0043,
      "step": 1530
    },
    {
      "epoch": 0.09081741606358999,
      "grad_norm": 15.016409873962402,
      "learning_rate": 1.8161328588374854e-05,
      "loss": 0.1004,
      "step": 1531
    },
    {
      "epoch": 0.09087673508126705,
      "grad_norm": 0.08719605952501297,
      "learning_rate": 1.8173190984578887e-05,
      "loss": 0.0014,
      "step": 1532
    },
    {
      "epoch": 0.09093605409894412,
      "grad_norm": 4.3764262199401855,
      "learning_rate": 1.818505338078292e-05,
      "loss": 0.0385,
      "step": 1533
    },
    {
      "epoch": 0.09099537311662119,
      "grad_norm": 6.816816329956055,
      "learning_rate": 1.819691577698695e-05,
      "loss": 0.2171,
      "step": 1534
    },
    {
      "epoch": 0.09105469213429826,
      "grad_norm": 0.01853402517735958,
      "learning_rate": 1.8208778173190988e-05,
      "loss": 0.0004,
      "step": 1535
    },
    {
      "epoch": 0.09111401115197533,
      "grad_norm": 11.167767524719238,
      "learning_rate": 1.8220640569395018e-05,
      "loss": 0.0877,
      "step": 1536
    },
    {
      "epoch": 0.09117333016965239,
      "grad_norm": 26.25453758239746,
      "learning_rate": 1.823250296559905e-05,
      "loss": 0.2009,
      "step": 1537
    },
    {
      "epoch": 0.09123264918732946,
      "grad_norm": 4.879428863525391,
      "learning_rate": 1.8244365361803085e-05,
      "loss": 0.0506,
      "step": 1538
    },
    {
      "epoch": 0.09129196820500653,
      "grad_norm": 44.720123291015625,
      "learning_rate": 1.825622775800712e-05,
      "loss": 1.5795,
      "step": 1539
    },
    {
      "epoch": 0.0913512872226836,
      "grad_norm": 0.845684826374054,
      "learning_rate": 1.8268090154211152e-05,
      "loss": 0.0092,
      "step": 1540
    },
    {
      "epoch": 0.09141060624036065,
      "grad_norm": 1.9723339080810547,
      "learning_rate": 1.8279952550415185e-05,
      "loss": 0.0067,
      "step": 1541
    },
    {
      "epoch": 0.09146992525803772,
      "grad_norm": 8.703948020935059,
      "learning_rate": 1.829181494661922e-05,
      "loss": 0.1101,
      "step": 1542
    },
    {
      "epoch": 0.0915292442757148,
      "grad_norm": 17.65921974182129,
      "learning_rate": 1.8303677342823252e-05,
      "loss": 0.2705,
      "step": 1543
    },
    {
      "epoch": 0.09158856329339186,
      "grad_norm": 18.65146255493164,
      "learning_rate": 1.8315539739027286e-05,
      "loss": 0.4243,
      "step": 1544
    },
    {
      "epoch": 0.09164788231106893,
      "grad_norm": 7.26701021194458,
      "learning_rate": 1.832740213523132e-05,
      "loss": 0.0618,
      "step": 1545
    },
    {
      "epoch": 0.09170720132874599,
      "grad_norm": 7.099400043487549,
      "learning_rate": 1.8339264531435353e-05,
      "loss": 0.4617,
      "step": 1546
    },
    {
      "epoch": 0.09176652034642306,
      "grad_norm": 27.3021297454834,
      "learning_rate": 1.8351126927639383e-05,
      "loss": 0.7189,
      "step": 1547
    },
    {
      "epoch": 0.09182583936410013,
      "grad_norm": 53.46974182128906,
      "learning_rate": 1.836298932384342e-05,
      "loss": 1.2494,
      "step": 1548
    },
    {
      "epoch": 0.0918851583817772,
      "grad_norm": 59.780879974365234,
      "learning_rate": 1.837485172004745e-05,
      "loss": 0.1985,
      "step": 1549
    },
    {
      "epoch": 0.09194447739945427,
      "grad_norm": 15.006464958190918,
      "learning_rate": 1.8386714116251484e-05,
      "loss": 0.0895,
      "step": 1550
    },
    {
      "epoch": 0.09200379641713133,
      "grad_norm": 0.03948763385415077,
      "learning_rate": 1.8398576512455517e-05,
      "loss": 0.0005,
      "step": 1551
    },
    {
      "epoch": 0.0920631154348084,
      "grad_norm": 4.914620399475098,
      "learning_rate": 1.841043890865955e-05,
      "loss": 0.0561,
      "step": 1552
    },
    {
      "epoch": 0.09212243445248547,
      "grad_norm": 11.49985122680664,
      "learning_rate": 1.8422301304863584e-05,
      "loss": 0.6105,
      "step": 1553
    },
    {
      "epoch": 0.09218175347016254,
      "grad_norm": 21.317747116088867,
      "learning_rate": 1.8434163701067618e-05,
      "loss": 0.36,
      "step": 1554
    },
    {
      "epoch": 0.0922410724878396,
      "grad_norm": 0.7502905130386353,
      "learning_rate": 1.844602609727165e-05,
      "loss": 0.0085,
      "step": 1555
    },
    {
      "epoch": 0.09230039150551667,
      "grad_norm": 1.0181519985198975,
      "learning_rate": 1.8457888493475685e-05,
      "loss": 0.0092,
      "step": 1556
    },
    {
      "epoch": 0.09235971052319374,
      "grad_norm": 0.3060894310474396,
      "learning_rate": 1.8469750889679715e-05,
      "loss": 0.0015,
      "step": 1557
    },
    {
      "epoch": 0.0924190295408708,
      "grad_norm": 0.6063419580459595,
      "learning_rate": 1.8481613285883752e-05,
      "loss": 0.0051,
      "step": 1558
    },
    {
      "epoch": 0.09247834855854788,
      "grad_norm": 0.1605157107114792,
      "learning_rate": 1.8493475682087782e-05,
      "loss": 0.002,
      "step": 1559
    },
    {
      "epoch": 0.09253766757622493,
      "grad_norm": 0.022661607712507248,
      "learning_rate": 1.8505338078291815e-05,
      "loss": 0.0004,
      "step": 1560
    },
    {
      "epoch": 0.092596986593902,
      "grad_norm": 10.64752197265625,
      "learning_rate": 1.851720047449585e-05,
      "loss": 0.0549,
      "step": 1561
    },
    {
      "epoch": 0.09265630561157907,
      "grad_norm": 8.957138061523438,
      "learning_rate": 1.8529062870699883e-05,
      "loss": 0.1014,
      "step": 1562
    },
    {
      "epoch": 0.09271562462925614,
      "grad_norm": 6.931691646575928,
      "learning_rate": 1.8540925266903916e-05,
      "loss": 0.0794,
      "step": 1563
    },
    {
      "epoch": 0.0927749436469332,
      "grad_norm": 57.1475944519043,
      "learning_rate": 1.855278766310795e-05,
      "loss": 1.1162,
      "step": 1564
    },
    {
      "epoch": 0.09283426266461027,
      "grad_norm": 5.147608757019043,
      "learning_rate": 1.8564650059311983e-05,
      "loss": 0.0419,
      "step": 1565
    },
    {
      "epoch": 0.09289358168228734,
      "grad_norm": 4.620843410491943,
      "learning_rate": 1.8576512455516017e-05,
      "loss": 0.6577,
      "step": 1566
    },
    {
      "epoch": 0.09295290069996441,
      "grad_norm": 35.42539596557617,
      "learning_rate": 1.8588374851720047e-05,
      "loss": 0.9987,
      "step": 1567
    },
    {
      "epoch": 0.09301221971764148,
      "grad_norm": 3.759934902191162,
      "learning_rate": 1.8600237247924084e-05,
      "loss": 0.0263,
      "step": 1568
    },
    {
      "epoch": 0.09307153873531854,
      "grad_norm": 2.2398123741149902,
      "learning_rate": 1.8612099644128114e-05,
      "loss": 0.0336,
      "step": 1569
    },
    {
      "epoch": 0.09313085775299561,
      "grad_norm": 16.99237632751465,
      "learning_rate": 1.8623962040332147e-05,
      "loss": 0.1866,
      "step": 1570
    },
    {
      "epoch": 0.09319017677067268,
      "grad_norm": 19.689361572265625,
      "learning_rate": 1.8635824436536184e-05,
      "loss": 0.4966,
      "step": 1571
    },
    {
      "epoch": 0.09324949578834975,
      "grad_norm": 37.55794906616211,
      "learning_rate": 1.8647686832740214e-05,
      "loss": 0.6085,
      "step": 1572
    },
    {
      "epoch": 0.09330881480602682,
      "grad_norm": 12.241118431091309,
      "learning_rate": 1.8659549228944248e-05,
      "loss": 0.1067,
      "step": 1573
    },
    {
      "epoch": 0.09336813382370387,
      "grad_norm": 0.16587364673614502,
      "learning_rate": 1.867141162514828e-05,
      "loss": 0.0029,
      "step": 1574
    },
    {
      "epoch": 0.09342745284138095,
      "grad_norm": 0.06950607895851135,
      "learning_rate": 1.8683274021352315e-05,
      "loss": 0.0012,
      "step": 1575
    },
    {
      "epoch": 0.09348677185905802,
      "grad_norm": 0.0612296536564827,
      "learning_rate": 1.869513641755635e-05,
      "loss": 0.0014,
      "step": 1576
    },
    {
      "epoch": 0.09354609087673509,
      "grad_norm": 7.7406392097473145,
      "learning_rate": 1.8706998813760382e-05,
      "loss": 0.0654,
      "step": 1577
    },
    {
      "epoch": 0.09360540989441214,
      "grad_norm": 36.7117805480957,
      "learning_rate": 1.8718861209964415e-05,
      "loss": 0.2059,
      "step": 1578
    },
    {
      "epoch": 0.09366472891208921,
      "grad_norm": 9.065834999084473,
      "learning_rate": 1.873072360616845e-05,
      "loss": 0.1695,
      "step": 1579
    },
    {
      "epoch": 0.09372404792976628,
      "grad_norm": 10.51366138458252,
      "learning_rate": 1.874258600237248e-05,
      "loss": 0.1288,
      "step": 1580
    },
    {
      "epoch": 0.09378336694744335,
      "grad_norm": 35.696876525878906,
      "learning_rate": 1.8754448398576516e-05,
      "loss": 0.1667,
      "step": 1581
    },
    {
      "epoch": 0.09384268596512042,
      "grad_norm": 30.858869552612305,
      "learning_rate": 1.8766310794780546e-05,
      "loss": 1.0975,
      "step": 1582
    },
    {
      "epoch": 0.09390200498279748,
      "grad_norm": 15.776329040527344,
      "learning_rate": 1.877817319098458e-05,
      "loss": 0.7294,
      "step": 1583
    },
    {
      "epoch": 0.09396132400047455,
      "grad_norm": 6.723967552185059,
      "learning_rate": 1.8790035587188613e-05,
      "loss": 0.3208,
      "step": 1584
    },
    {
      "epoch": 0.09402064301815162,
      "grad_norm": 168.49562072753906,
      "learning_rate": 1.8801897983392647e-05,
      "loss": 1.0311,
      "step": 1585
    },
    {
      "epoch": 0.09407996203582869,
      "grad_norm": 0.2647869288921356,
      "learning_rate": 1.881376037959668e-05,
      "loss": 0.0034,
      "step": 1586
    },
    {
      "epoch": 0.09413928105350576,
      "grad_norm": 12.385092735290527,
      "learning_rate": 1.8825622775800714e-05,
      "loss": 0.1777,
      "step": 1587
    },
    {
      "epoch": 0.09419860007118282,
      "grad_norm": 13.094401359558105,
      "learning_rate": 1.8837485172004747e-05,
      "loss": 0.196,
      "step": 1588
    },
    {
      "epoch": 0.09425791908885989,
      "grad_norm": 0.5262703895568848,
      "learning_rate": 1.884934756820878e-05,
      "loss": 0.0062,
      "step": 1589
    },
    {
      "epoch": 0.09431723810653696,
      "grad_norm": 16.74416160583496,
      "learning_rate": 1.886120996441281e-05,
      "loss": 0.2661,
      "step": 1590
    },
    {
      "epoch": 0.09437655712421403,
      "grad_norm": 3.3016414642333984,
      "learning_rate": 1.8873072360616848e-05,
      "loss": 0.0275,
      "step": 1591
    },
    {
      "epoch": 0.09443587614189108,
      "grad_norm": 17.263792037963867,
      "learning_rate": 1.8884934756820878e-05,
      "loss": 0.5009,
      "step": 1592
    },
    {
      "epoch": 0.09449519515956815,
      "grad_norm": 19.023380279541016,
      "learning_rate": 1.889679715302491e-05,
      "loss": 0.3926,
      "step": 1593
    },
    {
      "epoch": 0.09455451417724522,
      "grad_norm": 6.572348117828369,
      "learning_rate": 1.8908659549228945e-05,
      "loss": 0.1353,
      "step": 1594
    },
    {
      "epoch": 0.0946138331949223,
      "grad_norm": 3.662075996398926,
      "learning_rate": 1.892052194543298e-05,
      "loss": 0.0526,
      "step": 1595
    },
    {
      "epoch": 0.09467315221259937,
      "grad_norm": 18.64779281616211,
      "learning_rate": 1.8932384341637012e-05,
      "loss": 0.3974,
      "step": 1596
    },
    {
      "epoch": 0.09473247123027642,
      "grad_norm": 12.490171432495117,
      "learning_rate": 1.8944246737841046e-05,
      "loss": 0.2258,
      "step": 1597
    },
    {
      "epoch": 0.09479179024795349,
      "grad_norm": 12.94003677368164,
      "learning_rate": 1.895610913404508e-05,
      "loss": 0.0698,
      "step": 1598
    },
    {
      "epoch": 0.09485110926563056,
      "grad_norm": 0.49009066820144653,
      "learning_rate": 1.8967971530249113e-05,
      "loss": 0.0058,
      "step": 1599
    },
    {
      "epoch": 0.09491042828330763,
      "grad_norm": 12.673213958740234,
      "learning_rate": 1.8979833926453146e-05,
      "loss": 0.2495,
      "step": 1600
    },
    {
      "epoch": 0.0949697473009847,
      "grad_norm": 15.230789184570312,
      "learning_rate": 1.899169632265718e-05,
      "loss": 0.0807,
      "step": 1601
    },
    {
      "epoch": 0.09502906631866176,
      "grad_norm": 0.21257418394088745,
      "learning_rate": 1.9003558718861213e-05,
      "loss": 0.0021,
      "step": 1602
    },
    {
      "epoch": 0.09508838533633883,
      "grad_norm": 0.3535155951976776,
      "learning_rate": 1.9015421115065243e-05,
      "loss": 0.0039,
      "step": 1603
    },
    {
      "epoch": 0.0951477043540159,
      "grad_norm": 0.5232927203178406,
      "learning_rate": 1.902728351126928e-05,
      "loss": 0.0065,
      "step": 1604
    },
    {
      "epoch": 0.09520702337169297,
      "grad_norm": 4.338740825653076,
      "learning_rate": 1.903914590747331e-05,
      "loss": 0.0509,
      "step": 1605
    },
    {
      "epoch": 0.09526634238937003,
      "grad_norm": 9.972886085510254,
      "learning_rate": 1.9051008303677344e-05,
      "loss": 0.1548,
      "step": 1606
    },
    {
      "epoch": 0.0953256614070471,
      "grad_norm": 0.4020143151283264,
      "learning_rate": 1.9062870699881377e-05,
      "loss": 0.003,
      "step": 1607
    },
    {
      "epoch": 0.09538498042472417,
      "grad_norm": 2.2565982341766357,
      "learning_rate": 1.907473309608541e-05,
      "loss": 0.0159,
      "step": 1608
    },
    {
      "epoch": 0.09544429944240124,
      "grad_norm": 21.130401611328125,
      "learning_rate": 1.9086595492289444e-05,
      "loss": 0.6335,
      "step": 1609
    },
    {
      "epoch": 0.09550361846007831,
      "grad_norm": 19.788455963134766,
      "learning_rate": 1.9098457888493478e-05,
      "loss": 0.3522,
      "step": 1610
    },
    {
      "epoch": 0.09556293747775536,
      "grad_norm": 2.1954901218414307,
      "learning_rate": 1.911032028469751e-05,
      "loss": 0.0119,
      "step": 1611
    },
    {
      "epoch": 0.09562225649543243,
      "grad_norm": 1.7645670175552368,
      "learning_rate": 1.9122182680901545e-05,
      "loss": 0.0055,
      "step": 1612
    },
    {
      "epoch": 0.0956815755131095,
      "grad_norm": 7.163567543029785,
      "learning_rate": 1.9134045077105575e-05,
      "loss": 0.059,
      "step": 1613
    },
    {
      "epoch": 0.09574089453078657,
      "grad_norm": 38.13749313354492,
      "learning_rate": 1.9145907473309612e-05,
      "loss": 0.7208,
      "step": 1614
    },
    {
      "epoch": 0.09580021354846363,
      "grad_norm": 8.684967041015625,
      "learning_rate": 1.9157769869513642e-05,
      "loss": 0.119,
      "step": 1615
    },
    {
      "epoch": 0.0958595325661407,
      "grad_norm": 37.18331527709961,
      "learning_rate": 1.9169632265717676e-05,
      "loss": 0.3368,
      "step": 1616
    },
    {
      "epoch": 0.09591885158381777,
      "grad_norm": 0.04715137183666229,
      "learning_rate": 1.918149466192171e-05,
      "loss": 0.0008,
      "step": 1617
    },
    {
      "epoch": 0.09597817060149484,
      "grad_norm": 0.10006805509328842,
      "learning_rate": 1.9193357058125743e-05,
      "loss": 0.0026,
      "step": 1618
    },
    {
      "epoch": 0.09603748961917191,
      "grad_norm": 38.77457809448242,
      "learning_rate": 1.9205219454329776e-05,
      "loss": 1.2494,
      "step": 1619
    },
    {
      "epoch": 0.09609680863684897,
      "grad_norm": 41.89756774902344,
      "learning_rate": 1.921708185053381e-05,
      "loss": 0.1483,
      "step": 1620
    },
    {
      "epoch": 0.09615612765452604,
      "grad_norm": 31.30023193359375,
      "learning_rate": 1.9228944246737843e-05,
      "loss": 0.5688,
      "step": 1621
    },
    {
      "epoch": 0.09621544667220311,
      "grad_norm": 13.903547286987305,
      "learning_rate": 1.9240806642941877e-05,
      "loss": 0.054,
      "step": 1622
    },
    {
      "epoch": 0.09627476568988018,
      "grad_norm": 1.4389256238937378,
      "learning_rate": 1.9252669039145907e-05,
      "loss": 0.0083,
      "step": 1623
    },
    {
      "epoch": 0.09633408470755725,
      "grad_norm": 5.730507850646973,
      "learning_rate": 1.9264531435349944e-05,
      "loss": 0.0488,
      "step": 1624
    },
    {
      "epoch": 0.0963934037252343,
      "grad_norm": 0.6316882371902466,
      "learning_rate": 1.9276393831553974e-05,
      "loss": 0.0071,
      "step": 1625
    },
    {
      "epoch": 0.09645272274291138,
      "grad_norm": 39.49723815917969,
      "learning_rate": 1.9288256227758007e-05,
      "loss": 0.5726,
      "step": 1626
    },
    {
      "epoch": 0.09651204176058845,
      "grad_norm": 18.657114028930664,
      "learning_rate": 1.930011862396204e-05,
      "loss": 0.7822,
      "step": 1627
    },
    {
      "epoch": 0.09657136077826552,
      "grad_norm": 24.128414154052734,
      "learning_rate": 1.9311981020166074e-05,
      "loss": 0.407,
      "step": 1628
    },
    {
      "epoch": 0.09663067979594257,
      "grad_norm": 35.77015686035156,
      "learning_rate": 1.9323843416370108e-05,
      "loss": 0.9524,
      "step": 1629
    },
    {
      "epoch": 0.09668999881361964,
      "grad_norm": 2.350322961807251,
      "learning_rate": 1.933570581257414e-05,
      "loss": 0.0117,
      "step": 1630
    },
    {
      "epoch": 0.09674931783129671,
      "grad_norm": 18.030460357666016,
      "learning_rate": 1.9347568208778175e-05,
      "loss": 0.5604,
      "step": 1631
    },
    {
      "epoch": 0.09680863684897378,
      "grad_norm": 0.1336122751235962,
      "learning_rate": 1.935943060498221e-05,
      "loss": 0.0022,
      "step": 1632
    },
    {
      "epoch": 0.09686795586665085,
      "grad_norm": 0.13441821932792664,
      "learning_rate": 1.9371293001186242e-05,
      "loss": 0.0013,
      "step": 1633
    },
    {
      "epoch": 0.09692727488432791,
      "grad_norm": 34.24888229370117,
      "learning_rate": 1.9383155397390276e-05,
      "loss": 0.7477,
      "step": 1634
    },
    {
      "epoch": 0.09698659390200498,
      "grad_norm": 31.26367950439453,
      "learning_rate": 1.939501779359431e-05,
      "loss": 0.6627,
      "step": 1635
    },
    {
      "epoch": 0.09704591291968205,
      "grad_norm": 8.320521354675293,
      "learning_rate": 1.940688018979834e-05,
      "loss": 0.1652,
      "step": 1636
    },
    {
      "epoch": 0.09710523193735912,
      "grad_norm": 77.97953796386719,
      "learning_rate": 1.9418742586002376e-05,
      "loss": 1.1748,
      "step": 1637
    },
    {
      "epoch": 0.09716455095503619,
      "grad_norm": 2.319296360015869,
      "learning_rate": 1.9430604982206406e-05,
      "loss": 0.0154,
      "step": 1638
    },
    {
      "epoch": 0.09722386997271325,
      "grad_norm": 4.3137993812561035,
      "learning_rate": 1.944246737841044e-05,
      "loss": 0.0751,
      "step": 1639
    },
    {
      "epoch": 0.09728318899039032,
      "grad_norm": 10.239831924438477,
      "learning_rate": 1.9454329774614473e-05,
      "loss": 0.0782,
      "step": 1640
    },
    {
      "epoch": 0.09734250800806739,
      "grad_norm": 30.672439575195312,
      "learning_rate": 1.9466192170818507e-05,
      "loss": 0.0802,
      "step": 1641
    },
    {
      "epoch": 0.09740182702574446,
      "grad_norm": 40.17966079711914,
      "learning_rate": 1.947805456702254e-05,
      "loss": 1.1476,
      "step": 1642
    },
    {
      "epoch": 0.09746114604342151,
      "grad_norm": 7.1209187507629395,
      "learning_rate": 1.9489916963226574e-05,
      "loss": 0.0393,
      "step": 1643
    },
    {
      "epoch": 0.09752046506109859,
      "grad_norm": 35.256324768066406,
      "learning_rate": 1.9501779359430607e-05,
      "loss": 0.5326,
      "step": 1644
    },
    {
      "epoch": 0.09757978407877566,
      "grad_norm": 13.276054382324219,
      "learning_rate": 1.951364175563464e-05,
      "loss": 0.2911,
      "step": 1645
    },
    {
      "epoch": 0.09763910309645273,
      "grad_norm": 18.38847541809082,
      "learning_rate": 1.952550415183867e-05,
      "loss": 0.2489,
      "step": 1646
    },
    {
      "epoch": 0.0976984221141298,
      "grad_norm": 0.0691271647810936,
      "learning_rate": 1.9537366548042708e-05,
      "loss": 0.0011,
      "step": 1647
    },
    {
      "epoch": 0.09775774113180685,
      "grad_norm": 21.884414672851562,
      "learning_rate": 1.9549228944246738e-05,
      "loss": 0.8097,
      "step": 1648
    },
    {
      "epoch": 0.09781706014948392,
      "grad_norm": 1.2613375186920166,
      "learning_rate": 1.956109134045077e-05,
      "loss": 0.0043,
      "step": 1649
    },
    {
      "epoch": 0.09787637916716099,
      "grad_norm": 0.98648601770401,
      "learning_rate": 1.9572953736654805e-05,
      "loss": 0.0072,
      "step": 1650
    },
    {
      "epoch": 0.09793569818483806,
      "grad_norm": 0.3577084243297577,
      "learning_rate": 1.958481613285884e-05,
      "loss": 0.0037,
      "step": 1651
    },
    {
      "epoch": 0.09799501720251512,
      "grad_norm": 30.95888900756836,
      "learning_rate": 1.9596678529062872e-05,
      "loss": 0.2332,
      "step": 1652
    },
    {
      "epoch": 0.09805433622019219,
      "grad_norm": 8.981714248657227,
      "learning_rate": 1.9608540925266906e-05,
      "loss": 0.1494,
      "step": 1653
    },
    {
      "epoch": 0.09811365523786926,
      "grad_norm": 0.14130383729934692,
      "learning_rate": 1.962040332147094e-05,
      "loss": 0.0018,
      "step": 1654
    },
    {
      "epoch": 0.09817297425554633,
      "grad_norm": 13.03919792175293,
      "learning_rate": 1.9632265717674973e-05,
      "loss": 0.1305,
      "step": 1655
    },
    {
      "epoch": 0.0982322932732234,
      "grad_norm": 1.359484076499939,
      "learning_rate": 1.9644128113879003e-05,
      "loss": 0.0087,
      "step": 1656
    },
    {
      "epoch": 0.09829161229090046,
      "grad_norm": 27.384899139404297,
      "learning_rate": 1.965599051008304e-05,
      "loss": 0.4582,
      "step": 1657
    },
    {
      "epoch": 0.09835093130857753,
      "grad_norm": 7.5962395668029785,
      "learning_rate": 1.9667852906287073e-05,
      "loss": 0.2035,
      "step": 1658
    },
    {
      "epoch": 0.0984102503262546,
      "grad_norm": 6.316676139831543,
      "learning_rate": 1.9679715302491103e-05,
      "loss": 0.0564,
      "step": 1659
    },
    {
      "epoch": 0.09846956934393167,
      "grad_norm": 3.5953621864318848,
      "learning_rate": 1.969157769869514e-05,
      "loss": 0.0272,
      "step": 1660
    },
    {
      "epoch": 0.09852888836160874,
      "grad_norm": 26.724119186401367,
      "learning_rate": 1.970344009489917e-05,
      "loss": 0.3444,
      "step": 1661
    },
    {
      "epoch": 0.0985882073792858,
      "grad_norm": 0.7926079630851746,
      "learning_rate": 1.9715302491103204e-05,
      "loss": 0.0138,
      "step": 1662
    },
    {
      "epoch": 0.09864752639696286,
      "grad_norm": 0.09079133719205856,
      "learning_rate": 1.9727164887307237e-05,
      "loss": 0.0011,
      "step": 1663
    },
    {
      "epoch": 0.09870684541463993,
      "grad_norm": 8.96854019165039,
      "learning_rate": 1.973902728351127e-05,
      "loss": 0.2354,
      "step": 1664
    },
    {
      "epoch": 0.098766164432317,
      "grad_norm": 0.40886572003364563,
      "learning_rate": 1.9750889679715305e-05,
      "loss": 0.004,
      "step": 1665
    },
    {
      "epoch": 0.09882548344999406,
      "grad_norm": 33.21784210205078,
      "learning_rate": 1.9762752075919338e-05,
      "loss": 0.8915,
      "step": 1666
    },
    {
      "epoch": 0.09888480246767113,
      "grad_norm": 9.02245807647705,
      "learning_rate": 1.977461447212337e-05,
      "loss": 0.0914,
      "step": 1667
    },
    {
      "epoch": 0.0989441214853482,
      "grad_norm": 2.7627015113830566,
      "learning_rate": 1.9786476868327405e-05,
      "loss": 0.0243,
      "step": 1668
    },
    {
      "epoch": 0.09900344050302527,
      "grad_norm": 0.06227797269821167,
      "learning_rate": 1.9798339264531435e-05,
      "loss": 0.001,
      "step": 1669
    },
    {
      "epoch": 0.09906275952070234,
      "grad_norm": 0.18021513521671295,
      "learning_rate": 1.9810201660735472e-05,
      "loss": 0.0021,
      "step": 1670
    },
    {
      "epoch": 0.0991220785383794,
      "grad_norm": 8.076850891113281,
      "learning_rate": 1.9822064056939502e-05,
      "loss": 0.033,
      "step": 1671
    },
    {
      "epoch": 0.09918139755605647,
      "grad_norm": 0.5212966799736023,
      "learning_rate": 1.9833926453143536e-05,
      "loss": 0.0057,
      "step": 1672
    },
    {
      "epoch": 0.09924071657373354,
      "grad_norm": 14.180306434631348,
      "learning_rate": 1.984578884934757e-05,
      "loss": 0.1011,
      "step": 1673
    },
    {
      "epoch": 0.09930003559141061,
      "grad_norm": 1.6799087524414062,
      "learning_rate": 1.9857651245551603e-05,
      "loss": 0.0101,
      "step": 1674
    },
    {
      "epoch": 0.09935935460908768,
      "grad_norm": 33.6103401184082,
      "learning_rate": 1.9869513641755636e-05,
      "loss": 0.9522,
      "step": 1675
    },
    {
      "epoch": 0.09941867362676474,
      "grad_norm": 3.7950332164764404,
      "learning_rate": 1.988137603795967e-05,
      "loss": 0.0469,
      "step": 1676
    },
    {
      "epoch": 0.0994779926444418,
      "grad_norm": 37.28773880004883,
      "learning_rate": 1.9893238434163703e-05,
      "loss": 0.1533,
      "step": 1677
    },
    {
      "epoch": 0.09953731166211888,
      "grad_norm": 7.742271900177002,
      "learning_rate": 1.9905100830367737e-05,
      "loss": 0.038,
      "step": 1678
    },
    {
      "epoch": 0.09959663067979595,
      "grad_norm": 0.03470027446746826,
      "learning_rate": 1.9916963226571767e-05,
      "loss": 0.0008,
      "step": 1679
    },
    {
      "epoch": 0.099655949697473,
      "grad_norm": 0.40554627776145935,
      "learning_rate": 1.9928825622775804e-05,
      "loss": 0.0036,
      "step": 1680
    },
    {
      "epoch": 0.09971526871515007,
      "grad_norm": 25.975780487060547,
      "learning_rate": 1.9940688018979834e-05,
      "loss": 0.4039,
      "step": 1681
    },
    {
      "epoch": 0.09977458773282714,
      "grad_norm": 8.998363494873047,
      "learning_rate": 1.9952550415183868e-05,
      "loss": 0.1321,
      "step": 1682
    },
    {
      "epoch": 0.09983390675050421,
      "grad_norm": 12.284045219421387,
      "learning_rate": 1.99644128113879e-05,
      "loss": 0.0777,
      "step": 1683
    },
    {
      "epoch": 0.09989322576818128,
      "grad_norm": 37.73637008666992,
      "learning_rate": 1.9976275207591935e-05,
      "loss": 0.5216,
      "step": 1684
    },
    {
      "epoch": 0.09995254478585834,
      "grad_norm": 15.947797775268555,
      "learning_rate": 1.9988137603795968e-05,
      "loss": 0.5447,
      "step": 1685
    },
    {
      "epoch": 0.10001186380353541,
      "grad_norm": 0.4491789638996124,
      "learning_rate": 2e-05,
      "loss": 0.0041,
      "step": 1686
    },
    {
      "epoch": 0.10007118282121248,
      "grad_norm": 23.010095596313477,
      "learning_rate": 1.9998681782230428e-05,
      "loss": 0.0427,
      "step": 1687
    },
    {
      "epoch": 0.10013050183888955,
      "grad_norm": 2.1041717529296875,
      "learning_rate": 1.999736356446085e-05,
      "loss": 0.0085,
      "step": 1688
    },
    {
      "epoch": 0.10018982085656662,
      "grad_norm": 0.19603437185287476,
      "learning_rate": 1.9996045346691276e-05,
      "loss": 0.0019,
      "step": 1689
    },
    {
      "epoch": 0.10024913987424368,
      "grad_norm": 12.042930603027344,
      "learning_rate": 1.9994727128921702e-05,
      "loss": 0.2361,
      "step": 1690
    },
    {
      "epoch": 0.10030845889192075,
      "grad_norm": 14.96868896484375,
      "learning_rate": 1.9993408911152124e-05,
      "loss": 0.1692,
      "step": 1691
    },
    {
      "epoch": 0.10036777790959782,
      "grad_norm": 1.4282640218734741,
      "learning_rate": 1.999209069338255e-05,
      "loss": 0.0119,
      "step": 1692
    },
    {
      "epoch": 0.10042709692727489,
      "grad_norm": 9.787654876708984,
      "learning_rate": 1.9990772475612973e-05,
      "loss": 0.2278,
      "step": 1693
    },
    {
      "epoch": 0.10048641594495195,
      "grad_norm": 64.51529693603516,
      "learning_rate": 1.99894542578434e-05,
      "loss": 1.8736,
      "step": 1694
    },
    {
      "epoch": 0.10054573496262902,
      "grad_norm": 0.12196005135774612,
      "learning_rate": 1.998813604007382e-05,
      "loss": 0.0017,
      "step": 1695
    },
    {
      "epoch": 0.10060505398030609,
      "grad_norm": 3.0100817680358887,
      "learning_rate": 1.9986817822304247e-05,
      "loss": 0.0164,
      "step": 1696
    },
    {
      "epoch": 0.10066437299798316,
      "grad_norm": 24.990163803100586,
      "learning_rate": 1.998549960453467e-05,
      "loss": 0.661,
      "step": 1697
    },
    {
      "epoch": 0.10072369201566023,
      "grad_norm": 1.0910018682479858,
      "learning_rate": 1.9984181386765095e-05,
      "loss": 0.0062,
      "step": 1698
    },
    {
      "epoch": 0.10078301103333728,
      "grad_norm": 0.2790747880935669,
      "learning_rate": 1.9982863168995518e-05,
      "loss": 0.0019,
      "step": 1699
    },
    {
      "epoch": 0.10084233005101435,
      "grad_norm": 17.16112518310547,
      "learning_rate": 1.9981544951225944e-05,
      "loss": 0.1734,
      "step": 1700
    },
    {
      "epoch": 0.10090164906869142,
      "grad_norm": 22.926780700683594,
      "learning_rate": 1.998022673345637e-05,
      "loss": 0.2835,
      "step": 1701
    },
    {
      "epoch": 0.1009609680863685,
      "grad_norm": 31.924482345581055,
      "learning_rate": 1.9978908515686792e-05,
      "loss": 0.1919,
      "step": 1702
    },
    {
      "epoch": 0.10102028710404555,
      "grad_norm": 2.515512704849243,
      "learning_rate": 1.9977590297917218e-05,
      "loss": 0.0261,
      "step": 1703
    },
    {
      "epoch": 0.10107960612172262,
      "grad_norm": 0.3953113257884979,
      "learning_rate": 1.9976272080147644e-05,
      "loss": 0.0052,
      "step": 1704
    },
    {
      "epoch": 0.10113892513939969,
      "grad_norm": 14.12716007232666,
      "learning_rate": 1.9974953862378066e-05,
      "loss": 0.0793,
      "step": 1705
    },
    {
      "epoch": 0.10119824415707676,
      "grad_norm": 17.39898109436035,
      "learning_rate": 1.9973635644608492e-05,
      "loss": 0.1028,
      "step": 1706
    },
    {
      "epoch": 0.10125756317475383,
      "grad_norm": 26.088552474975586,
      "learning_rate": 1.9972317426838918e-05,
      "loss": 0.1402,
      "step": 1707
    },
    {
      "epoch": 0.10131688219243089,
      "grad_norm": 4.538229465484619,
      "learning_rate": 1.997099920906934e-05,
      "loss": 0.0234,
      "step": 1708
    },
    {
      "epoch": 0.10137620121010796,
      "grad_norm": 34.43851089477539,
      "learning_rate": 1.9969680991299763e-05,
      "loss": 0.644,
      "step": 1709
    },
    {
      "epoch": 0.10143552022778503,
      "grad_norm": 0.09676266461610794,
      "learning_rate": 1.996836277353019e-05,
      "loss": 0.0021,
      "step": 1710
    },
    {
      "epoch": 0.1014948392454621,
      "grad_norm": 5.2711262702941895,
      "learning_rate": 1.996704455576061e-05,
      "loss": 0.0268,
      "step": 1711
    },
    {
      "epoch": 0.10155415826313917,
      "grad_norm": 13.884523391723633,
      "learning_rate": 1.9965726337991037e-05,
      "loss": 0.2381,
      "step": 1712
    },
    {
      "epoch": 0.10161347728081623,
      "grad_norm": 18.467275619506836,
      "learning_rate": 1.996440812022146e-05,
      "loss": 0.2974,
      "step": 1713
    },
    {
      "epoch": 0.1016727962984933,
      "grad_norm": 2.9514031410217285,
      "learning_rate": 1.9963089902451886e-05,
      "loss": 0.0272,
      "step": 1714
    },
    {
      "epoch": 0.10173211531617037,
      "grad_norm": 9.983440399169922,
      "learning_rate": 1.996177168468231e-05,
      "loss": 0.1241,
      "step": 1715
    },
    {
      "epoch": 0.10179143433384744,
      "grad_norm": 0.6751769185066223,
      "learning_rate": 1.9960453466912734e-05,
      "loss": 0.0046,
      "step": 1716
    },
    {
      "epoch": 0.10185075335152449,
      "grad_norm": 7.9071197509765625,
      "learning_rate": 1.995913524914316e-05,
      "loss": 0.1045,
      "step": 1717
    },
    {
      "epoch": 0.10191007236920156,
      "grad_norm": 5.898563861846924,
      "learning_rate": 1.9957817031373586e-05,
      "loss": 0.0335,
      "step": 1718
    },
    {
      "epoch": 0.10196939138687863,
      "grad_norm": 5.486020565032959,
      "learning_rate": 1.9956498813604008e-05,
      "loss": 0.0247,
      "step": 1719
    },
    {
      "epoch": 0.1020287104045557,
      "grad_norm": 3.7670462131500244,
      "learning_rate": 1.9955180595834434e-05,
      "loss": 0.0141,
      "step": 1720
    },
    {
      "epoch": 0.10208802942223277,
      "grad_norm": 2.521373748779297,
      "learning_rate": 1.995386237806486e-05,
      "loss": 0.0157,
      "step": 1721
    },
    {
      "epoch": 0.10214734843990983,
      "grad_norm": 0.7598012089729309,
      "learning_rate": 1.9952544160295283e-05,
      "loss": 0.004,
      "step": 1722
    },
    {
      "epoch": 0.1022066674575869,
      "grad_norm": 1.218579888343811,
      "learning_rate": 1.995122594252571e-05,
      "loss": 0.0111,
      "step": 1723
    },
    {
      "epoch": 0.10226598647526397,
      "grad_norm": 27.425010681152344,
      "learning_rate": 1.994990772475613e-05,
      "loss": 1.0503,
      "step": 1724
    },
    {
      "epoch": 0.10232530549294104,
      "grad_norm": 0.11287716776132584,
      "learning_rate": 1.9948589506986557e-05,
      "loss": 0.0008,
      "step": 1725
    },
    {
      "epoch": 0.10238462451061811,
      "grad_norm": 21.652605056762695,
      "learning_rate": 1.994727128921698e-05,
      "loss": 0.1061,
      "step": 1726
    },
    {
      "epoch": 0.10244394352829517,
      "grad_norm": 22.011869430541992,
      "learning_rate": 1.9945953071447405e-05,
      "loss": 1.4206,
      "step": 1727
    },
    {
      "epoch": 0.10250326254597224,
      "grad_norm": 10.328645706176758,
      "learning_rate": 1.9944634853677828e-05,
      "loss": 0.1026,
      "step": 1728
    },
    {
      "epoch": 0.10256258156364931,
      "grad_norm": 3.4006736278533936,
      "learning_rate": 1.9943316635908254e-05,
      "loss": 0.0233,
      "step": 1729
    },
    {
      "epoch": 0.10262190058132638,
      "grad_norm": 1.3635032176971436,
      "learning_rate": 1.9941998418138676e-05,
      "loss": 0.007,
      "step": 1730
    },
    {
      "epoch": 0.10268121959900343,
      "grad_norm": 12.86635971069336,
      "learning_rate": 1.9940680200369102e-05,
      "loss": 0.8106,
      "step": 1731
    },
    {
      "epoch": 0.1027405386166805,
      "grad_norm": 17.894758224487305,
      "learning_rate": 1.9939361982599528e-05,
      "loss": 0.3847,
      "step": 1732
    },
    {
      "epoch": 0.10279985763435757,
      "grad_norm": 22.865337371826172,
      "learning_rate": 1.993804376482995e-05,
      "loss": 0.4537,
      "step": 1733
    },
    {
      "epoch": 0.10285917665203465,
      "grad_norm": 0.41756245493888855,
      "learning_rate": 1.9936725547060376e-05,
      "loss": 0.0039,
      "step": 1734
    },
    {
      "epoch": 0.10291849566971172,
      "grad_norm": 48.40582275390625,
      "learning_rate": 1.9935407329290802e-05,
      "loss": 0.2465,
      "step": 1735
    },
    {
      "epoch": 0.10297781468738877,
      "grad_norm": 8.10790729522705,
      "learning_rate": 1.9934089111521225e-05,
      "loss": 0.0701,
      "step": 1736
    },
    {
      "epoch": 0.10303713370506584,
      "grad_norm": 32.40191650390625,
      "learning_rate": 1.993277089375165e-05,
      "loss": 0.7542,
      "step": 1737
    },
    {
      "epoch": 0.10309645272274291,
      "grad_norm": 2.7689297199249268,
      "learning_rate": 1.9931452675982076e-05,
      "loss": 0.0253,
      "step": 1738
    },
    {
      "epoch": 0.10315577174041998,
      "grad_norm": 22.126367568969727,
      "learning_rate": 1.99301344582125e-05,
      "loss": 0.1422,
      "step": 1739
    },
    {
      "epoch": 0.10321509075809705,
      "grad_norm": 0.269359290599823,
      "learning_rate": 1.9928816240442925e-05,
      "loss": 0.004,
      "step": 1740
    },
    {
      "epoch": 0.10327440977577411,
      "grad_norm": 14.763608932495117,
      "learning_rate": 1.9927498022673347e-05,
      "loss": 0.3064,
      "step": 1741
    },
    {
      "epoch": 0.10333372879345118,
      "grad_norm": 14.117582321166992,
      "learning_rate": 1.992617980490377e-05,
      "loss": 0.1171,
      "step": 1742
    },
    {
      "epoch": 0.10339304781112825,
      "grad_norm": 11.64813232421875,
      "learning_rate": 1.9924861587134196e-05,
      "loss": 0.424,
      "step": 1743
    },
    {
      "epoch": 0.10345236682880532,
      "grad_norm": 2.3543248176574707,
      "learning_rate": 1.9923543369364618e-05,
      "loss": 0.0097,
      "step": 1744
    },
    {
      "epoch": 0.10351168584648238,
      "grad_norm": 17.154354095458984,
      "learning_rate": 1.9922225151595044e-05,
      "loss": 0.4289,
      "step": 1745
    },
    {
      "epoch": 0.10357100486415945,
      "grad_norm": 23.09157943725586,
      "learning_rate": 1.992090693382547e-05,
      "loss": 1.4948,
      "step": 1746
    },
    {
      "epoch": 0.10363032388183652,
      "grad_norm": 0.07694722712039948,
      "learning_rate": 1.9919588716055892e-05,
      "loss": 0.0016,
      "step": 1747
    },
    {
      "epoch": 0.10368964289951359,
      "grad_norm": 0.19600895047187805,
      "learning_rate": 1.9918270498286318e-05,
      "loss": 0.0017,
      "step": 1748
    },
    {
      "epoch": 0.10374896191719066,
      "grad_norm": 0.08407794684171677,
      "learning_rate": 1.9916952280516744e-05,
      "loss": 0.0015,
      "step": 1749
    },
    {
      "epoch": 0.10380828093486771,
      "grad_norm": 0.07424391806125641,
      "learning_rate": 1.9915634062747167e-05,
      "loss": 0.0013,
      "step": 1750
    },
    {
      "epoch": 0.10386759995254478,
      "grad_norm": 9.691080093383789,
      "learning_rate": 1.9914315844977592e-05,
      "loss": 0.0546,
      "step": 1751
    },
    {
      "epoch": 0.10392691897022185,
      "grad_norm": 22.57333755493164,
      "learning_rate": 1.991299762720802e-05,
      "loss": 0.2288,
      "step": 1752
    },
    {
      "epoch": 0.10398623798789892,
      "grad_norm": 1.0666528940200806,
      "learning_rate": 1.991167940943844e-05,
      "loss": 0.013,
      "step": 1753
    },
    {
      "epoch": 0.10404555700557598,
      "grad_norm": 16.606231689453125,
      "learning_rate": 1.9910361191668867e-05,
      "loss": 0.3303,
      "step": 1754
    },
    {
      "epoch": 0.10410487602325305,
      "grad_norm": 0.029582880437374115,
      "learning_rate": 1.990904297389929e-05,
      "loss": 0.0005,
      "step": 1755
    },
    {
      "epoch": 0.10416419504093012,
      "grad_norm": 0.10275774449110031,
      "learning_rate": 1.9907724756129715e-05,
      "loss": 0.0019,
      "step": 1756
    },
    {
      "epoch": 0.10422351405860719,
      "grad_norm": 1.37334144115448,
      "learning_rate": 1.9906406538360138e-05,
      "loss": 0.0124,
      "step": 1757
    },
    {
      "epoch": 0.10428283307628426,
      "grad_norm": 2.7454237937927246,
      "learning_rate": 1.9905088320590563e-05,
      "loss": 0.0255,
      "step": 1758
    },
    {
      "epoch": 0.10434215209396132,
      "grad_norm": 45.0133056640625,
      "learning_rate": 1.9903770102820986e-05,
      "loss": 0.3017,
      "step": 1759
    },
    {
      "epoch": 0.10440147111163839,
      "grad_norm": 25.851728439331055,
      "learning_rate": 1.9902451885051412e-05,
      "loss": 0.374,
      "step": 1760
    },
    {
      "epoch": 0.10446079012931546,
      "grad_norm": 1.459406852722168,
      "learning_rate": 1.9901133667281834e-05,
      "loss": 0.0188,
      "step": 1761
    },
    {
      "epoch": 0.10452010914699253,
      "grad_norm": 2.197798013687134,
      "learning_rate": 1.989981544951226e-05,
      "loss": 0.0148,
      "step": 1762
    },
    {
      "epoch": 0.1045794281646696,
      "grad_norm": 12.225265502929688,
      "learning_rate": 1.9898497231742686e-05,
      "loss": 0.1628,
      "step": 1763
    },
    {
      "epoch": 0.10463874718234666,
      "grad_norm": 4.319655895233154,
      "learning_rate": 1.989717901397311e-05,
      "loss": 0.0521,
      "step": 1764
    },
    {
      "epoch": 0.10469806620002373,
      "grad_norm": 23.78037452697754,
      "learning_rate": 1.9895860796203534e-05,
      "loss": 0.1988,
      "step": 1765
    },
    {
      "epoch": 0.1047573852177008,
      "grad_norm": 0.9272578358650208,
      "learning_rate": 1.989454257843396e-05,
      "loss": 0.0124,
      "step": 1766
    },
    {
      "epoch": 0.10481670423537787,
      "grad_norm": 8.76142692565918,
      "learning_rate": 1.9893224360664383e-05,
      "loss": 0.1669,
      "step": 1767
    },
    {
      "epoch": 0.10487602325305492,
      "grad_norm": 13.70595932006836,
      "learning_rate": 1.989190614289481e-05,
      "loss": 0.1137,
      "step": 1768
    },
    {
      "epoch": 0.104935342270732,
      "grad_norm": 1.016774296760559,
      "learning_rate": 1.9890587925125235e-05,
      "loss": 0.011,
      "step": 1769
    },
    {
      "epoch": 0.10499466128840906,
      "grad_norm": 1.9621285200119019,
      "learning_rate": 1.9889269707355657e-05,
      "loss": 0.0064,
      "step": 1770
    },
    {
      "epoch": 0.10505398030608613,
      "grad_norm": 11.744670867919922,
      "learning_rate": 1.9887951489586083e-05,
      "loss": 0.6245,
      "step": 1771
    },
    {
      "epoch": 0.1051132993237632,
      "grad_norm": 4.672901153564453,
      "learning_rate": 1.9886633271816505e-05,
      "loss": 0.0615,
      "step": 1772
    },
    {
      "epoch": 0.10517261834144026,
      "grad_norm": 16.419395446777344,
      "learning_rate": 1.988531505404693e-05,
      "loss": 0.1825,
      "step": 1773
    },
    {
      "epoch": 0.10523193735911733,
      "grad_norm": 4.933670997619629,
      "learning_rate": 1.9883996836277354e-05,
      "loss": 0.0136,
      "step": 1774
    },
    {
      "epoch": 0.1052912563767944,
      "grad_norm": 0.04479537159204483,
      "learning_rate": 1.988267861850778e-05,
      "loss": 0.0009,
      "step": 1775
    },
    {
      "epoch": 0.10535057539447147,
      "grad_norm": 0.05252082645893097,
      "learning_rate": 1.9881360400738202e-05,
      "loss": 0.0006,
      "step": 1776
    },
    {
      "epoch": 0.10540989441214854,
      "grad_norm": 0.6076812148094177,
      "learning_rate": 1.9880042182968628e-05,
      "loss": 0.0044,
      "step": 1777
    },
    {
      "epoch": 0.1054692134298256,
      "grad_norm": 42.138221740722656,
      "learning_rate": 1.987872396519905e-05,
      "loss": 0.3903,
      "step": 1778
    },
    {
      "epoch": 0.10552853244750267,
      "grad_norm": 7.173696517944336,
      "learning_rate": 1.9877405747429476e-05,
      "loss": 0.1433,
      "step": 1779
    },
    {
      "epoch": 0.10558785146517974,
      "grad_norm": 10.92909049987793,
      "learning_rate": 1.9876087529659902e-05,
      "loss": 0.1242,
      "step": 1780
    },
    {
      "epoch": 0.10564717048285681,
      "grad_norm": 5.557052135467529,
      "learning_rate": 1.9874769311890325e-05,
      "loss": 0.1127,
      "step": 1781
    },
    {
      "epoch": 0.10570648950053387,
      "grad_norm": 15.359785079956055,
      "learning_rate": 1.987345109412075e-05,
      "loss": 0.2122,
      "step": 1782
    },
    {
      "epoch": 0.10576580851821094,
      "grad_norm": 22.651464462280273,
      "learning_rate": 1.9872132876351177e-05,
      "loss": 0.6451,
      "step": 1783
    },
    {
      "epoch": 0.105825127535888,
      "grad_norm": 0.21800784766674042,
      "learning_rate": 1.98708146585816e-05,
      "loss": 0.0022,
      "step": 1784
    },
    {
      "epoch": 0.10588444655356508,
      "grad_norm": 0.03935680165886879,
      "learning_rate": 1.9869496440812025e-05,
      "loss": 0.0005,
      "step": 1785
    },
    {
      "epoch": 0.10594376557124215,
      "grad_norm": 0.839154064655304,
      "learning_rate": 1.9868178223042447e-05,
      "loss": 0.0085,
      "step": 1786
    },
    {
      "epoch": 0.1060030845889192,
      "grad_norm": 6.847257614135742,
      "learning_rate": 1.9866860005272873e-05,
      "loss": 0.1986,
      "step": 1787
    },
    {
      "epoch": 0.10606240360659627,
      "grad_norm": 7.2083916664123535,
      "learning_rate": 1.9865541787503296e-05,
      "loss": 0.3748,
      "step": 1788
    },
    {
      "epoch": 0.10612172262427334,
      "grad_norm": 36.09711456298828,
      "learning_rate": 1.9864223569733722e-05,
      "loss": 1.3668,
      "step": 1789
    },
    {
      "epoch": 0.10618104164195041,
      "grad_norm": 32.494361877441406,
      "learning_rate": 1.9862905351964144e-05,
      "loss": 0.9388,
      "step": 1790
    },
    {
      "epoch": 0.10624036065962747,
      "grad_norm": 0.1657538115978241,
      "learning_rate": 1.986158713419457e-05,
      "loss": 0.0017,
      "step": 1791
    },
    {
      "epoch": 0.10629967967730454,
      "grad_norm": 4.4232025146484375,
      "learning_rate": 1.9860268916424993e-05,
      "loss": 0.6456,
      "step": 1792
    },
    {
      "epoch": 0.10635899869498161,
      "grad_norm": 0.011130187660455704,
      "learning_rate": 1.985895069865542e-05,
      "loss": 0.0003,
      "step": 1793
    },
    {
      "epoch": 0.10641831771265868,
      "grad_norm": 13.137185096740723,
      "learning_rate": 1.9857632480885844e-05,
      "loss": 0.5985,
      "step": 1794
    },
    {
      "epoch": 0.10647763673033575,
      "grad_norm": 55.43513870239258,
      "learning_rate": 1.9856314263116267e-05,
      "loss": 0.6059,
      "step": 1795
    },
    {
      "epoch": 0.10653695574801281,
      "grad_norm": 17.917600631713867,
      "learning_rate": 1.9854996045346693e-05,
      "loss": 0.1797,
      "step": 1796
    },
    {
      "epoch": 0.10659627476568988,
      "grad_norm": 0.47782185673713684,
      "learning_rate": 1.985367782757712e-05,
      "loss": 0.0069,
      "step": 1797
    },
    {
      "epoch": 0.10665559378336695,
      "grad_norm": 0.005415050312876701,
      "learning_rate": 1.985235960980754e-05,
      "loss": 0.0001,
      "step": 1798
    },
    {
      "epoch": 0.10671491280104402,
      "grad_norm": 32.883235931396484,
      "learning_rate": 1.9851041392037967e-05,
      "loss": 0.9187,
      "step": 1799
    },
    {
      "epoch": 0.10677423181872109,
      "grad_norm": 13.310556411743164,
      "learning_rate": 1.9849723174268393e-05,
      "loss": 0.0961,
      "step": 1800
    },
    {
      "epoch": 0.10683355083639814,
      "grad_norm": 6.571265697479248,
      "learning_rate": 1.9848404956498815e-05,
      "loss": 0.0445,
      "step": 1801
    },
    {
      "epoch": 0.10689286985407521,
      "grad_norm": 0.5484558939933777,
      "learning_rate": 1.984708673872924e-05,
      "loss": 0.0056,
      "step": 1802
    },
    {
      "epoch": 0.10695218887175229,
      "grad_norm": 0.201209157705307,
      "learning_rate": 1.9845768520959664e-05,
      "loss": 0.0028,
      "step": 1803
    },
    {
      "epoch": 0.10701150788942936,
      "grad_norm": 16.960012435913086,
      "learning_rate": 1.984445030319009e-05,
      "loss": 0.6431,
      "step": 1804
    },
    {
      "epoch": 0.10707082690710641,
      "grad_norm": 10.042051315307617,
      "learning_rate": 1.9843132085420512e-05,
      "loss": 1.0169,
      "step": 1805
    },
    {
      "epoch": 0.10713014592478348,
      "grad_norm": 0.894257128238678,
      "learning_rate": 1.9841813867650938e-05,
      "loss": 0.0146,
      "step": 1806
    },
    {
      "epoch": 0.10718946494246055,
      "grad_norm": 103.31328582763672,
      "learning_rate": 1.984049564988136e-05,
      "loss": 0.9883,
      "step": 1807
    },
    {
      "epoch": 0.10724878396013762,
      "grad_norm": 0.1984342336654663,
      "learning_rate": 1.9839177432111786e-05,
      "loss": 0.0019,
      "step": 1808
    },
    {
      "epoch": 0.10730810297781469,
      "grad_norm": 14.100213050842285,
      "learning_rate": 1.983785921434221e-05,
      "loss": 0.202,
      "step": 1809
    },
    {
      "epoch": 0.10736742199549175,
      "grad_norm": 0.11815619468688965,
      "learning_rate": 1.9836540996572635e-05,
      "loss": 0.0025,
      "step": 1810
    },
    {
      "epoch": 0.10742674101316882,
      "grad_norm": 0.06766381114721298,
      "learning_rate": 1.983522277880306e-05,
      "loss": 0.0008,
      "step": 1811
    },
    {
      "epoch": 0.10748606003084589,
      "grad_norm": 13.119863510131836,
      "learning_rate": 1.9833904561033483e-05,
      "loss": 0.1211,
      "step": 1812
    },
    {
      "epoch": 0.10754537904852296,
      "grad_norm": 35.2301139831543,
      "learning_rate": 1.983258634326391e-05,
      "loss": 1.0547,
      "step": 1813
    },
    {
      "epoch": 0.10760469806620003,
      "grad_norm": 0.06288056075572968,
      "learning_rate": 1.9831268125494335e-05,
      "loss": 0.0011,
      "step": 1814
    },
    {
      "epoch": 0.10766401708387709,
      "grad_norm": 3.071992874145508,
      "learning_rate": 1.9829949907724757e-05,
      "loss": 0.0198,
      "step": 1815
    },
    {
      "epoch": 0.10772333610155416,
      "grad_norm": 9.138009071350098,
      "learning_rate": 1.9828631689955183e-05,
      "loss": 0.27,
      "step": 1816
    },
    {
      "epoch": 0.10778265511923123,
      "grad_norm": 13.693593978881836,
      "learning_rate": 1.982731347218561e-05,
      "loss": 0.3333,
      "step": 1817
    },
    {
      "epoch": 0.1078419741369083,
      "grad_norm": 26.90782928466797,
      "learning_rate": 1.982599525441603e-05,
      "loss": 0.8577,
      "step": 1818
    },
    {
      "epoch": 0.10790129315458535,
      "grad_norm": 4.585622787475586,
      "learning_rate": 1.9824677036646457e-05,
      "loss": 0.065,
      "step": 1819
    },
    {
      "epoch": 0.10796061217226242,
      "grad_norm": 7.663929462432861,
      "learning_rate": 1.982335881887688e-05,
      "loss": 0.0552,
      "step": 1820
    },
    {
      "epoch": 0.1080199311899395,
      "grad_norm": 2.9515626430511475,
      "learning_rate": 1.9822040601107302e-05,
      "loss": 0.0247,
      "step": 1821
    },
    {
      "epoch": 0.10807925020761656,
      "grad_norm": 1.0434576272964478,
      "learning_rate": 1.982072238333773e-05,
      "loss": 0.0213,
      "step": 1822
    },
    {
      "epoch": 0.10813856922529363,
      "grad_norm": 44.42057418823242,
      "learning_rate": 1.981940416556815e-05,
      "loss": 0.628,
      "step": 1823
    },
    {
      "epoch": 0.10819788824297069,
      "grad_norm": 18.72763442993164,
      "learning_rate": 1.9818085947798577e-05,
      "loss": 0.4041,
      "step": 1824
    },
    {
      "epoch": 0.10825720726064776,
      "grad_norm": 0.22971035540103912,
      "learning_rate": 1.9816767730029003e-05,
      "loss": 0.0044,
      "step": 1825
    },
    {
      "epoch": 0.10831652627832483,
      "grad_norm": 10.930180549621582,
      "learning_rate": 1.9815449512259425e-05,
      "loss": 0.1092,
      "step": 1826
    },
    {
      "epoch": 0.1083758452960019,
      "grad_norm": 17.770750045776367,
      "learning_rate": 1.981413129448985e-05,
      "loss": 0.638,
      "step": 1827
    },
    {
      "epoch": 0.10843516431367897,
      "grad_norm": 18.940858840942383,
      "learning_rate": 1.9812813076720277e-05,
      "loss": 0.5739,
      "step": 1828
    },
    {
      "epoch": 0.10849448333135603,
      "grad_norm": 1.948354721069336,
      "learning_rate": 1.98114948589507e-05,
      "loss": 0.0115,
      "step": 1829
    },
    {
      "epoch": 0.1085538023490331,
      "grad_norm": 36.222755432128906,
      "learning_rate": 1.9810176641181125e-05,
      "loss": 0.5019,
      "step": 1830
    },
    {
      "epoch": 0.10861312136671017,
      "grad_norm": 5.695863246917725,
      "learning_rate": 1.980885842341155e-05,
      "loss": 0.0635,
      "step": 1831
    },
    {
      "epoch": 0.10867244038438724,
      "grad_norm": 0.9582344889640808,
      "learning_rate": 1.9807540205641974e-05,
      "loss": 0.0077,
      "step": 1832
    },
    {
      "epoch": 0.1087317594020643,
      "grad_norm": 7.890334129333496,
      "learning_rate": 1.98062219878724e-05,
      "loss": 0.0522,
      "step": 1833
    },
    {
      "epoch": 0.10879107841974137,
      "grad_norm": 7.325939178466797,
      "learning_rate": 1.9804903770102822e-05,
      "loss": 0.0287,
      "step": 1834
    },
    {
      "epoch": 0.10885039743741844,
      "grad_norm": 25.002992630004883,
      "learning_rate": 1.9803585552333248e-05,
      "loss": 0.6705,
      "step": 1835
    },
    {
      "epoch": 0.1089097164550955,
      "grad_norm": 14.414714813232422,
      "learning_rate": 1.980226733456367e-05,
      "loss": 0.4787,
      "step": 1836
    },
    {
      "epoch": 0.10896903547277258,
      "grad_norm": 20.023597717285156,
      "learning_rate": 1.9800949116794096e-05,
      "loss": 0.1696,
      "step": 1837
    },
    {
      "epoch": 0.10902835449044963,
      "grad_norm": 13.11434268951416,
      "learning_rate": 1.979963089902452e-05,
      "loss": 0.3965,
      "step": 1838
    },
    {
      "epoch": 0.1090876735081267,
      "grad_norm": 8.773143768310547,
      "learning_rate": 1.9798312681254945e-05,
      "loss": 0.1301,
      "step": 1839
    },
    {
      "epoch": 0.10914699252580377,
      "grad_norm": 5.5529866218566895,
      "learning_rate": 1.9796994463485367e-05,
      "loss": 0.0431,
      "step": 1840
    },
    {
      "epoch": 0.10920631154348084,
      "grad_norm": 5.31469202041626,
      "learning_rate": 1.9795676245715793e-05,
      "loss": 0.0182,
      "step": 1841
    },
    {
      "epoch": 0.1092656305611579,
      "grad_norm": 35.149864196777344,
      "learning_rate": 1.979435802794622e-05,
      "loss": 0.1391,
      "step": 1842
    },
    {
      "epoch": 0.10932494957883497,
      "grad_norm": 25.296125411987305,
      "learning_rate": 1.979303981017664e-05,
      "loss": 1.4401,
      "step": 1843
    },
    {
      "epoch": 0.10938426859651204,
      "grad_norm": 4.9825119972229,
      "learning_rate": 1.9791721592407067e-05,
      "loss": 0.0276,
      "step": 1844
    },
    {
      "epoch": 0.10944358761418911,
      "grad_norm": 0.12059780210256577,
      "learning_rate": 1.9790403374637493e-05,
      "loss": 0.0025,
      "step": 1845
    },
    {
      "epoch": 0.10950290663186618,
      "grad_norm": 22.039396286010742,
      "learning_rate": 1.9789085156867916e-05,
      "loss": 0.381,
      "step": 1846
    },
    {
      "epoch": 0.10956222564954324,
      "grad_norm": 1.571239709854126,
      "learning_rate": 1.978776693909834e-05,
      "loss": 0.0155,
      "step": 1847
    },
    {
      "epoch": 0.10962154466722031,
      "grad_norm": 21.1003360748291,
      "learning_rate": 1.9786448721328767e-05,
      "loss": 0.2132,
      "step": 1848
    },
    {
      "epoch": 0.10968086368489738,
      "grad_norm": 0.9384000897407532,
      "learning_rate": 1.978513050355919e-05,
      "loss": 0.0058,
      "step": 1849
    },
    {
      "epoch": 0.10974018270257445,
      "grad_norm": 0.13931924104690552,
      "learning_rate": 1.9783812285789616e-05,
      "loss": 0.0016,
      "step": 1850
    },
    {
      "epoch": 0.10979950172025152,
      "grad_norm": 12.387277603149414,
      "learning_rate": 1.9782494068020038e-05,
      "loss": 0.3706,
      "step": 1851
    },
    {
      "epoch": 0.10985882073792858,
      "grad_norm": 0.8144074082374573,
      "learning_rate": 1.9781175850250464e-05,
      "loss": 0.0088,
      "step": 1852
    },
    {
      "epoch": 0.10991813975560565,
      "grad_norm": 0.030882127583026886,
      "learning_rate": 1.9779857632480887e-05,
      "loss": 0.0008,
      "step": 1853
    },
    {
      "epoch": 0.10997745877328272,
      "grad_norm": 0.09024783223867416,
      "learning_rate": 1.9778539414711313e-05,
      "loss": 0.0011,
      "step": 1854
    },
    {
      "epoch": 0.11003677779095979,
      "grad_norm": 37.99784469604492,
      "learning_rate": 1.9777221196941735e-05,
      "loss": 0.245,
      "step": 1855
    },
    {
      "epoch": 0.11009609680863684,
      "grad_norm": 29.462106704711914,
      "learning_rate": 1.977590297917216e-05,
      "loss": 1.7718,
      "step": 1856
    },
    {
      "epoch": 0.11015541582631391,
      "grad_norm": 3.517159938812256,
      "learning_rate": 1.9774584761402583e-05,
      "loss": 0.0223,
      "step": 1857
    },
    {
      "epoch": 0.11021473484399098,
      "grad_norm": 2.6893837451934814,
      "learning_rate": 1.977326654363301e-05,
      "loss": 0.0159,
      "step": 1858
    },
    {
      "epoch": 0.11027405386166805,
      "grad_norm": 0.16398406028747559,
      "learning_rate": 1.9771948325863435e-05,
      "loss": 0.0031,
      "step": 1859
    },
    {
      "epoch": 0.11033337287934512,
      "grad_norm": 18.237529754638672,
      "learning_rate": 1.9770630108093858e-05,
      "loss": 0.0568,
      "step": 1860
    },
    {
      "epoch": 0.11039269189702218,
      "grad_norm": 76.96703338623047,
      "learning_rate": 1.9769311890324284e-05,
      "loss": 1.1875,
      "step": 1861
    },
    {
      "epoch": 0.11045201091469925,
      "grad_norm": 11.23691463470459,
      "learning_rate": 1.976799367255471e-05,
      "loss": 0.0576,
      "step": 1862
    },
    {
      "epoch": 0.11051132993237632,
      "grad_norm": 23.283445358276367,
      "learning_rate": 1.9766675454785132e-05,
      "loss": 1.1597,
      "step": 1863
    },
    {
      "epoch": 0.11057064895005339,
      "grad_norm": 31.48837661743164,
      "learning_rate": 1.9765357237015558e-05,
      "loss": 0.4346,
      "step": 1864
    },
    {
      "epoch": 0.11062996796773046,
      "grad_norm": 0.03238557279109955,
      "learning_rate": 1.976403901924598e-05,
      "loss": 0.0006,
      "step": 1865
    },
    {
      "epoch": 0.11068928698540752,
      "grad_norm": 7.342925071716309,
      "learning_rate": 1.9762720801476406e-05,
      "loss": 0.3589,
      "step": 1866
    },
    {
      "epoch": 0.11074860600308459,
      "grad_norm": 22.72536849975586,
      "learning_rate": 1.976140258370683e-05,
      "loss": 0.6785,
      "step": 1867
    },
    {
      "epoch": 0.11080792502076166,
      "grad_norm": 0.6485218405723572,
      "learning_rate": 1.9760084365937255e-05,
      "loss": 0.0043,
      "step": 1868
    },
    {
      "epoch": 0.11086724403843873,
      "grad_norm": 0.13831542432308197,
      "learning_rate": 1.9758766148167677e-05,
      "loss": 0.0023,
      "step": 1869
    },
    {
      "epoch": 0.11092656305611578,
      "grad_norm": 0.522592306137085,
      "learning_rate": 1.9757447930398103e-05,
      "loss": 0.0012,
      "step": 1870
    },
    {
      "epoch": 0.11098588207379285,
      "grad_norm": 21.662397384643555,
      "learning_rate": 1.9756129712628525e-05,
      "loss": 1.1034,
      "step": 1871
    },
    {
      "epoch": 0.11104520109146993,
      "grad_norm": 2.7284841537475586,
      "learning_rate": 1.975481149485895e-05,
      "loss": 0.0208,
      "step": 1872
    },
    {
      "epoch": 0.111104520109147,
      "grad_norm": 11.913434028625488,
      "learning_rate": 1.9753493277089377e-05,
      "loss": 0.3357,
      "step": 1873
    },
    {
      "epoch": 0.11116383912682407,
      "grad_norm": 13.444726943969727,
      "learning_rate": 1.97521750593198e-05,
      "loss": 0.1352,
      "step": 1874
    },
    {
      "epoch": 0.11122315814450112,
      "grad_norm": 12.485007286071777,
      "learning_rate": 1.9750856841550226e-05,
      "loss": 0.4063,
      "step": 1875
    },
    {
      "epoch": 0.11128247716217819,
      "grad_norm": 4.7693023681640625,
      "learning_rate": 1.974953862378065e-05,
      "loss": 0.0613,
      "step": 1876
    },
    {
      "epoch": 0.11134179617985526,
      "grad_norm": 1.5877152681350708,
      "learning_rate": 1.9748220406011074e-05,
      "loss": 0.0123,
      "step": 1877
    },
    {
      "epoch": 0.11140111519753233,
      "grad_norm": 0.32147708535194397,
      "learning_rate": 1.97469021882415e-05,
      "loss": 0.0033,
      "step": 1878
    },
    {
      "epoch": 0.1114604342152094,
      "grad_norm": 10.938427925109863,
      "learning_rate": 1.9745583970471926e-05,
      "loss": 0.3647,
      "step": 1879
    },
    {
      "epoch": 0.11151975323288646,
      "grad_norm": 0.7734147906303406,
      "learning_rate": 1.9744265752702348e-05,
      "loss": 0.0083,
      "step": 1880
    },
    {
      "epoch": 0.11157907225056353,
      "grad_norm": 43.86785125732422,
      "learning_rate": 1.9742947534932774e-05,
      "loss": 1.6174,
      "step": 1881
    },
    {
      "epoch": 0.1116383912682406,
      "grad_norm": 12.622840881347656,
      "learning_rate": 1.9741629317163197e-05,
      "loss": 0.1778,
      "step": 1882
    },
    {
      "epoch": 0.11169771028591767,
      "grad_norm": 8.700052261352539,
      "learning_rate": 1.9740311099393622e-05,
      "loss": 0.2386,
      "step": 1883
    },
    {
      "epoch": 0.11175702930359473,
      "grad_norm": 3.9171411991119385,
      "learning_rate": 1.9738992881624045e-05,
      "loss": 0.0396,
      "step": 1884
    },
    {
      "epoch": 0.1118163483212718,
      "grad_norm": 0.5374526977539062,
      "learning_rate": 1.973767466385447e-05,
      "loss": 0.0057,
      "step": 1885
    },
    {
      "epoch": 0.11187566733894887,
      "grad_norm": 11.79708480834961,
      "learning_rate": 1.9736356446084893e-05,
      "loss": 0.137,
      "step": 1886
    },
    {
      "epoch": 0.11193498635662594,
      "grad_norm": 0.4277007281780243,
      "learning_rate": 1.973503822831532e-05,
      "loss": 0.0072,
      "step": 1887
    },
    {
      "epoch": 0.11199430537430301,
      "grad_norm": 9.800515174865723,
      "learning_rate": 1.973372001054574e-05,
      "loss": 0.4068,
      "step": 1888
    },
    {
      "epoch": 0.11205362439198006,
      "grad_norm": 39.50391387939453,
      "learning_rate": 1.9732401792776168e-05,
      "loss": 0.2765,
      "step": 1889
    },
    {
      "epoch": 0.11211294340965713,
      "grad_norm": 17.231861114501953,
      "learning_rate": 1.9731083575006593e-05,
      "loss": 0.0541,
      "step": 1890
    },
    {
      "epoch": 0.1121722624273342,
      "grad_norm": 0.25846412777900696,
      "learning_rate": 1.9729765357237016e-05,
      "loss": 0.0017,
      "step": 1891
    },
    {
      "epoch": 0.11223158144501127,
      "grad_norm": 0.032536573708057404,
      "learning_rate": 1.9728447139467442e-05,
      "loss": 0.0005,
      "step": 1892
    },
    {
      "epoch": 0.11229090046268833,
      "grad_norm": 7.073793411254883,
      "learning_rate": 1.9727128921697868e-05,
      "loss": 0.083,
      "step": 1893
    },
    {
      "epoch": 0.1123502194803654,
      "grad_norm": 16.308469772338867,
      "learning_rate": 1.972581070392829e-05,
      "loss": 0.3567,
      "step": 1894
    },
    {
      "epoch": 0.11240953849804247,
      "grad_norm": 5.303832054138184,
      "learning_rate": 1.9724492486158716e-05,
      "loss": 0.1943,
      "step": 1895
    },
    {
      "epoch": 0.11246885751571954,
      "grad_norm": 1.5656710863113403,
      "learning_rate": 1.9723174268389142e-05,
      "loss": 0.0135,
      "step": 1896
    },
    {
      "epoch": 0.11252817653339661,
      "grad_norm": 9.335254669189453,
      "learning_rate": 1.9721856050619564e-05,
      "loss": 0.127,
      "step": 1897
    },
    {
      "epoch": 0.11258749555107367,
      "grad_norm": 0.30872610211372375,
      "learning_rate": 1.9720537832849987e-05,
      "loss": 0.003,
      "step": 1898
    },
    {
      "epoch": 0.11264681456875074,
      "grad_norm": 0.01133439876139164,
      "learning_rate": 1.9719219615080413e-05,
      "loss": 0.0003,
      "step": 1899
    },
    {
      "epoch": 0.11270613358642781,
      "grad_norm": 12.156777381896973,
      "learning_rate": 1.9717901397310835e-05,
      "loss": 0.1654,
      "step": 1900
    },
    {
      "epoch": 0.11276545260410488,
      "grad_norm": 30.225528717041016,
      "learning_rate": 1.971658317954126e-05,
      "loss": 0.5596,
      "step": 1901
    },
    {
      "epoch": 0.11282477162178195,
      "grad_norm": 32.61360168457031,
      "learning_rate": 1.9715264961771687e-05,
      "loss": 0.7267,
      "step": 1902
    },
    {
      "epoch": 0.112884090639459,
      "grad_norm": 3.316751480102539,
      "learning_rate": 1.971394674400211e-05,
      "loss": 0.0459,
      "step": 1903
    },
    {
      "epoch": 0.11294340965713608,
      "grad_norm": 10.455100059509277,
      "learning_rate": 1.9712628526232535e-05,
      "loss": 0.3748,
      "step": 1904
    },
    {
      "epoch": 0.11300272867481315,
      "grad_norm": 7.621172904968262,
      "learning_rate": 1.9711310308462958e-05,
      "loss": 0.6545,
      "step": 1905
    },
    {
      "epoch": 0.11306204769249022,
      "grad_norm": 15.220565795898438,
      "learning_rate": 1.9709992090693384e-05,
      "loss": 0.1966,
      "step": 1906
    },
    {
      "epoch": 0.11312136671016727,
      "grad_norm": 0.16552533209323883,
      "learning_rate": 1.970867387292381e-05,
      "loss": 0.0013,
      "step": 1907
    },
    {
      "epoch": 0.11318068572784434,
      "grad_norm": 11.063081741333008,
      "learning_rate": 1.9707355655154232e-05,
      "loss": 0.4796,
      "step": 1908
    },
    {
      "epoch": 0.11324000474552141,
      "grad_norm": 7.707054615020752,
      "learning_rate": 1.9706037437384658e-05,
      "loss": 0.0399,
      "step": 1909
    },
    {
      "epoch": 0.11329932376319848,
      "grad_norm": 16.307113647460938,
      "learning_rate": 1.9704719219615084e-05,
      "loss": 0.585,
      "step": 1910
    },
    {
      "epoch": 0.11335864278087555,
      "grad_norm": 8.912778854370117,
      "learning_rate": 1.9703401001845506e-05,
      "loss": 0.3022,
      "step": 1911
    },
    {
      "epoch": 0.11341796179855261,
      "grad_norm": 1.354289174079895,
      "learning_rate": 1.9702082784075932e-05,
      "loss": 0.006,
      "step": 1912
    },
    {
      "epoch": 0.11347728081622968,
      "grad_norm": 33.81754684448242,
      "learning_rate": 1.9700764566306355e-05,
      "loss": 0.3198,
      "step": 1913
    },
    {
      "epoch": 0.11353659983390675,
      "grad_norm": 36.22502136230469,
      "learning_rate": 1.969944634853678e-05,
      "loss": 0.5299,
      "step": 1914
    },
    {
      "epoch": 0.11359591885158382,
      "grad_norm": 0.09147089719772339,
      "learning_rate": 1.9698128130767203e-05,
      "loss": 0.0007,
      "step": 1915
    },
    {
      "epoch": 0.11365523786926089,
      "grad_norm": 0.03974379599094391,
      "learning_rate": 1.969680991299763e-05,
      "loss": 0.0008,
      "step": 1916
    },
    {
      "epoch": 0.11371455688693795,
      "grad_norm": 28.739835739135742,
      "learning_rate": 1.969549169522805e-05,
      "loss": 1.0031,
      "step": 1917
    },
    {
      "epoch": 0.11377387590461502,
      "grad_norm": 0.050417460501194,
      "learning_rate": 1.9694173477458477e-05,
      "loss": 0.0013,
      "step": 1918
    },
    {
      "epoch": 0.11383319492229209,
      "grad_norm": 0.48366087675094604,
      "learning_rate": 1.9692855259688903e-05,
      "loss": 0.0052,
      "step": 1919
    },
    {
      "epoch": 0.11389251393996916,
      "grad_norm": 0.038164831697940826,
      "learning_rate": 1.9691537041919326e-05,
      "loss": 0.0006,
      "step": 1920
    },
    {
      "epoch": 0.11395183295764622,
      "grad_norm": 11.34743881225586,
      "learning_rate": 1.969021882414975e-05,
      "loss": 0.9882,
      "step": 1921
    },
    {
      "epoch": 0.11401115197532329,
      "grad_norm": 25.920440673828125,
      "learning_rate": 1.9688900606380174e-05,
      "loss": 0.7035,
      "step": 1922
    },
    {
      "epoch": 0.11407047099300036,
      "grad_norm": 2.639766216278076,
      "learning_rate": 1.96875823886106e-05,
      "loss": 0.043,
      "step": 1923
    },
    {
      "epoch": 0.11412979001067743,
      "grad_norm": 19.71025276184082,
      "learning_rate": 1.9686264170841026e-05,
      "loss": 0.2896,
      "step": 1924
    },
    {
      "epoch": 0.1141891090283545,
      "grad_norm": 23.29302215576172,
      "learning_rate": 1.968494595307145e-05,
      "loss": 0.7783,
      "step": 1925
    },
    {
      "epoch": 0.11424842804603155,
      "grad_norm": 13.164106369018555,
      "learning_rate": 1.9683627735301874e-05,
      "loss": 0.0729,
      "step": 1926
    },
    {
      "epoch": 0.11430774706370862,
      "grad_norm": 12.687636375427246,
      "learning_rate": 1.96823095175323e-05,
      "loss": 0.7643,
      "step": 1927
    },
    {
      "epoch": 0.1143670660813857,
      "grad_norm": 0.18089258670806885,
      "learning_rate": 1.9680991299762723e-05,
      "loss": 0.0015,
      "step": 1928
    },
    {
      "epoch": 0.11442638509906276,
      "grad_norm": 27.404972076416016,
      "learning_rate": 1.967967308199315e-05,
      "loss": 0.8286,
      "step": 1929
    },
    {
      "epoch": 0.11448570411673983,
      "grad_norm": 25.11716651916504,
      "learning_rate": 1.967835486422357e-05,
      "loss": 0.4729,
      "step": 1930
    },
    {
      "epoch": 0.11454502313441689,
      "grad_norm": 0.712995707988739,
      "learning_rate": 1.9677036646453997e-05,
      "loss": 0.0085,
      "step": 1931
    },
    {
      "epoch": 0.11460434215209396,
      "grad_norm": 7.897322654724121,
      "learning_rate": 1.967571842868442e-05,
      "loss": 0.0937,
      "step": 1932
    },
    {
      "epoch": 0.11466366116977103,
      "grad_norm": 35.83353805541992,
      "learning_rate": 1.9674400210914845e-05,
      "loss": 0.279,
      "step": 1933
    },
    {
      "epoch": 0.1147229801874481,
      "grad_norm": 1.8401942253112793,
      "learning_rate": 1.9673081993145268e-05,
      "loss": 0.0131,
      "step": 1934
    },
    {
      "epoch": 0.11478229920512516,
      "grad_norm": 9.189449310302734,
      "learning_rate": 1.9671763775375694e-05,
      "loss": 0.2136,
      "step": 1935
    },
    {
      "epoch": 0.11484161822280223,
      "grad_norm": 25.550628662109375,
      "learning_rate": 1.9670445557606116e-05,
      "loss": 0.4844,
      "step": 1936
    },
    {
      "epoch": 0.1149009372404793,
      "grad_norm": 0.9992272853851318,
      "learning_rate": 1.9669127339836542e-05,
      "loss": 0.0046,
      "step": 1937
    },
    {
      "epoch": 0.11496025625815637,
      "grad_norm": 0.39095431566238403,
      "learning_rate": 1.9667809122066968e-05,
      "loss": 0.0072,
      "step": 1938
    },
    {
      "epoch": 0.11501957527583344,
      "grad_norm": 10.338289260864258,
      "learning_rate": 1.966649090429739e-05,
      "loss": 0.1018,
      "step": 1939
    },
    {
      "epoch": 0.1150788942935105,
      "grad_norm": 0.3302549421787262,
      "learning_rate": 1.9665172686527816e-05,
      "loss": 0.0029,
      "step": 1940
    },
    {
      "epoch": 0.11513821331118756,
      "grad_norm": 7.665195941925049,
      "learning_rate": 1.9663854468758242e-05,
      "loss": 0.1422,
      "step": 1941
    },
    {
      "epoch": 0.11519753232886464,
      "grad_norm": 4.6472907066345215,
      "learning_rate": 1.9662536250988665e-05,
      "loss": 0.0414,
      "step": 1942
    },
    {
      "epoch": 0.1152568513465417,
      "grad_norm": 8.749403953552246,
      "learning_rate": 1.966121803321909e-05,
      "loss": 0.0999,
      "step": 1943
    },
    {
      "epoch": 0.11531617036421876,
      "grad_norm": 50.45165252685547,
      "learning_rate": 1.9659899815449513e-05,
      "loss": 0.2685,
      "step": 1944
    },
    {
      "epoch": 0.11537548938189583,
      "grad_norm": 22.837623596191406,
      "learning_rate": 1.965858159767994e-05,
      "loss": 0.5665,
      "step": 1945
    },
    {
      "epoch": 0.1154348083995729,
      "grad_norm": 5.288039684295654,
      "learning_rate": 1.965726337991036e-05,
      "loss": 0.0434,
      "step": 1946
    },
    {
      "epoch": 0.11549412741724997,
      "grad_norm": 16.295673370361328,
      "learning_rate": 1.9655945162140787e-05,
      "loss": 0.1352,
      "step": 1947
    },
    {
      "epoch": 0.11555344643492704,
      "grad_norm": 7.43202543258667,
      "learning_rate": 1.965462694437121e-05,
      "loss": 0.1049,
      "step": 1948
    },
    {
      "epoch": 0.1156127654526041,
      "grad_norm": 0.19842511415481567,
      "learning_rate": 1.9653308726601636e-05,
      "loss": 0.0027,
      "step": 1949
    },
    {
      "epoch": 0.11567208447028117,
      "grad_norm": 18.702621459960938,
      "learning_rate": 1.965199050883206e-05,
      "loss": 0.0671,
      "step": 1950
    },
    {
      "epoch": 0.11573140348795824,
      "grad_norm": 0.09706771373748779,
      "learning_rate": 1.9650672291062484e-05,
      "loss": 0.0014,
      "step": 1951
    },
    {
      "epoch": 0.11579072250563531,
      "grad_norm": 12.466496467590332,
      "learning_rate": 1.964935407329291e-05,
      "loss": 0.1722,
      "step": 1952
    },
    {
      "epoch": 0.11585004152331238,
      "grad_norm": 10.250978469848633,
      "learning_rate": 1.9648035855523332e-05,
      "loss": 0.0341,
      "step": 1953
    },
    {
      "epoch": 0.11590936054098944,
      "grad_norm": 17.295238494873047,
      "learning_rate": 1.964671763775376e-05,
      "loss": 0.3343,
      "step": 1954
    },
    {
      "epoch": 0.11596867955866651,
      "grad_norm": 94.49056243896484,
      "learning_rate": 1.9645399419984184e-05,
      "loss": 0.3037,
      "step": 1955
    },
    {
      "epoch": 0.11602799857634358,
      "grad_norm": 0.6339442729949951,
      "learning_rate": 1.9644081202214607e-05,
      "loss": 0.0052,
      "step": 1956
    },
    {
      "epoch": 0.11608731759402065,
      "grad_norm": 0.12446480244398117,
      "learning_rate": 1.9642762984445033e-05,
      "loss": 0.0018,
      "step": 1957
    },
    {
      "epoch": 0.1161466366116977,
      "grad_norm": 52.70362854003906,
      "learning_rate": 1.964144476667546e-05,
      "loss": 0.8719,
      "step": 1958
    },
    {
      "epoch": 0.11620595562937477,
      "grad_norm": 0.48156851530075073,
      "learning_rate": 1.964012654890588e-05,
      "loss": 0.0053,
      "step": 1959
    },
    {
      "epoch": 0.11626527464705184,
      "grad_norm": 34.35957336425781,
      "learning_rate": 1.9638808331136307e-05,
      "loss": 0.4146,
      "step": 1960
    },
    {
      "epoch": 0.11632459366472891,
      "grad_norm": 0.29466646909713745,
      "learning_rate": 1.963749011336673e-05,
      "loss": 0.0049,
      "step": 1961
    },
    {
      "epoch": 0.11638391268240598,
      "grad_norm": 31.79156494140625,
      "learning_rate": 1.9636171895597155e-05,
      "loss": 0.3692,
      "step": 1962
    },
    {
      "epoch": 0.11644323170008304,
      "grad_norm": 14.09549331665039,
      "learning_rate": 1.9634853677827578e-05,
      "loss": 0.042,
      "step": 1963
    },
    {
      "epoch": 0.11650255071776011,
      "grad_norm": 9.336103439331055,
      "learning_rate": 1.9633535460058004e-05,
      "loss": 0.5268,
      "step": 1964
    },
    {
      "epoch": 0.11656186973543718,
      "grad_norm": 0.8681945204734802,
      "learning_rate": 1.9632217242288426e-05,
      "loss": 0.0094,
      "step": 1965
    },
    {
      "epoch": 0.11662118875311425,
      "grad_norm": 5.833942413330078,
      "learning_rate": 1.9630899024518852e-05,
      "loss": 0.0486,
      "step": 1966
    },
    {
      "epoch": 0.11668050777079132,
      "grad_norm": 1.632191777229309,
      "learning_rate": 1.9629580806749278e-05,
      "loss": 0.0186,
      "step": 1967
    },
    {
      "epoch": 0.11673982678846838,
      "grad_norm": 6.464236736297607,
      "learning_rate": 1.96282625889797e-05,
      "loss": 0.085,
      "step": 1968
    },
    {
      "epoch": 0.11679914580614545,
      "grad_norm": 2.413914442062378,
      "learning_rate": 1.9626944371210126e-05,
      "loss": 0.0162,
      "step": 1969
    },
    {
      "epoch": 0.11685846482382252,
      "grad_norm": 23.787757873535156,
      "learning_rate": 1.962562615344055e-05,
      "loss": 1.423,
      "step": 1970
    },
    {
      "epoch": 0.11691778384149959,
      "grad_norm": 0.07586190849542618,
      "learning_rate": 1.9624307935670975e-05,
      "loss": 0.0017,
      "step": 1971
    },
    {
      "epoch": 0.11697710285917665,
      "grad_norm": 11.580215454101562,
      "learning_rate": 1.96229897179014e-05,
      "loss": 0.387,
      "step": 1972
    },
    {
      "epoch": 0.11703642187685372,
      "grad_norm": 3.2677230834960938,
      "learning_rate": 1.9621671500131823e-05,
      "loss": 0.0344,
      "step": 1973
    },
    {
      "epoch": 0.11709574089453079,
      "grad_norm": 13.316149711608887,
      "learning_rate": 1.962035328236225e-05,
      "loss": 0.0737,
      "step": 1974
    },
    {
      "epoch": 0.11715505991220786,
      "grad_norm": 27.447860717773438,
      "learning_rate": 1.9619035064592675e-05,
      "loss": 1.1275,
      "step": 1975
    },
    {
      "epoch": 0.11721437892988493,
      "grad_norm": 10.407122611999512,
      "learning_rate": 1.9617716846823097e-05,
      "loss": 0.3222,
      "step": 1976
    },
    {
      "epoch": 0.11727369794756198,
      "grad_norm": 44.94478225708008,
      "learning_rate": 1.961639862905352e-05,
      "loss": 0.3064,
      "step": 1977
    },
    {
      "epoch": 0.11733301696523905,
      "grad_norm": 5.912297248840332,
      "learning_rate": 1.9615080411283946e-05,
      "loss": 0.0419,
      "step": 1978
    },
    {
      "epoch": 0.11739233598291612,
      "grad_norm": 3.049668550491333,
      "learning_rate": 1.9613762193514368e-05,
      "loss": 0.0071,
      "step": 1979
    },
    {
      "epoch": 0.1174516550005932,
      "grad_norm": 4.94427490234375,
      "learning_rate": 1.9612443975744794e-05,
      "loss": 0.0301,
      "step": 1980
    },
    {
      "epoch": 0.11751097401827025,
      "grad_norm": 0.5343838930130005,
      "learning_rate": 1.961112575797522e-05,
      "loss": 0.0085,
      "step": 1981
    },
    {
      "epoch": 0.11757029303594732,
      "grad_norm": 3.4212138652801514,
      "learning_rate": 1.9609807540205642e-05,
      "loss": 0.0433,
      "step": 1982
    },
    {
      "epoch": 0.11762961205362439,
      "grad_norm": 20.160667419433594,
      "learning_rate": 1.9608489322436068e-05,
      "loss": 0.7225,
      "step": 1983
    },
    {
      "epoch": 0.11768893107130146,
      "grad_norm": 27.14240074157715,
      "learning_rate": 1.960717110466649e-05,
      "loss": 1.0877,
      "step": 1984
    },
    {
      "epoch": 0.11774825008897853,
      "grad_norm": 12.040621757507324,
      "learning_rate": 1.9605852886896917e-05,
      "loss": 0.167,
      "step": 1985
    },
    {
      "epoch": 0.11780756910665559,
      "grad_norm": 0.03045761026442051,
      "learning_rate": 1.9604534669127342e-05,
      "loss": 0.0005,
      "step": 1986
    },
    {
      "epoch": 0.11786688812433266,
      "grad_norm": 27.28318214416504,
      "learning_rate": 1.9603216451357765e-05,
      "loss": 0.8516,
      "step": 1987
    },
    {
      "epoch": 0.11792620714200973,
      "grad_norm": 0.12476493418216705,
      "learning_rate": 1.960189823358819e-05,
      "loss": 0.0017,
      "step": 1988
    },
    {
      "epoch": 0.1179855261596868,
      "grad_norm": 15.82370662689209,
      "learning_rate": 1.9600580015818617e-05,
      "loss": 1.1389,
      "step": 1989
    },
    {
      "epoch": 0.11804484517736387,
      "grad_norm": 2.3424153327941895,
      "learning_rate": 1.959926179804904e-05,
      "loss": 0.0147,
      "step": 1990
    },
    {
      "epoch": 0.11810416419504093,
      "grad_norm": 9.286168098449707,
      "learning_rate": 1.9597943580279465e-05,
      "loss": 0.1489,
      "step": 1991
    },
    {
      "epoch": 0.118163483212718,
      "grad_norm": 18.714187622070312,
      "learning_rate": 1.9596625362509888e-05,
      "loss": 0.8681,
      "step": 1992
    },
    {
      "epoch": 0.11822280223039507,
      "grad_norm": 7.7069315910339355,
      "learning_rate": 1.9595307144740313e-05,
      "loss": 0.1765,
      "step": 1993
    },
    {
      "epoch": 0.11828212124807214,
      "grad_norm": 10.289827346801758,
      "learning_rate": 1.9593988926970736e-05,
      "loss": 0.1406,
      "step": 1994
    },
    {
      "epoch": 0.11834144026574919,
      "grad_norm": 12.895035743713379,
      "learning_rate": 1.9592670709201162e-05,
      "loss": 0.8583,
      "step": 1995
    },
    {
      "epoch": 0.11840075928342626,
      "grad_norm": 11.169584274291992,
      "learning_rate": 1.9591352491431584e-05,
      "loss": 0.501,
      "step": 1996
    },
    {
      "epoch": 0.11846007830110333,
      "grad_norm": 1.055335521697998,
      "learning_rate": 1.959003427366201e-05,
      "loss": 0.0046,
      "step": 1997
    },
    {
      "epoch": 0.1185193973187804,
      "grad_norm": 14.395147323608398,
      "learning_rate": 1.9588716055892436e-05,
      "loss": 0.5786,
      "step": 1998
    },
    {
      "epoch": 0.11857871633645747,
      "grad_norm": 0.04430964216589928,
      "learning_rate": 1.958739783812286e-05,
      "loss": 0.0007,
      "step": 1999
    },
    {
      "epoch": 0.11863803535413453,
      "grad_norm": 5.966296672821045,
      "learning_rate": 1.9586079620353284e-05,
      "loss": 0.1503,
      "step": 2000
    },
    {
      "epoch": 0.1186973543718116,
      "grad_norm": 34.83024215698242,
      "learning_rate": 1.9584761402583707e-05,
      "loss": 1.454,
      "step": 2001
    },
    {
      "epoch": 0.11875667338948867,
      "grad_norm": 6.579766273498535,
      "learning_rate": 1.9583443184814133e-05,
      "loss": 0.1108,
      "step": 2002
    },
    {
      "epoch": 0.11881599240716574,
      "grad_norm": 12.038656234741211,
      "learning_rate": 1.958212496704456e-05,
      "loss": 0.2725,
      "step": 2003
    },
    {
      "epoch": 0.11887531142484281,
      "grad_norm": 22.36051368713379,
      "learning_rate": 1.958080674927498e-05,
      "loss": 1.1089,
      "step": 2004
    },
    {
      "epoch": 0.11893463044251987,
      "grad_norm": 7.42245626449585,
      "learning_rate": 1.9579488531505407e-05,
      "loss": 0.124,
      "step": 2005
    },
    {
      "epoch": 0.11899394946019694,
      "grad_norm": 23.626846313476562,
      "learning_rate": 1.9578170313735833e-05,
      "loss": 0.701,
      "step": 2006
    },
    {
      "epoch": 0.11905326847787401,
      "grad_norm": 1.8346400260925293,
      "learning_rate": 1.9576852095966255e-05,
      "loss": 0.0141,
      "step": 2007
    },
    {
      "epoch": 0.11911258749555108,
      "grad_norm": 0.05125083029270172,
      "learning_rate": 1.957553387819668e-05,
      "loss": 0.0011,
      "step": 2008
    },
    {
      "epoch": 0.11917190651322813,
      "grad_norm": 17.945446014404297,
      "learning_rate": 1.9574215660427104e-05,
      "loss": 0.1367,
      "step": 2009
    },
    {
      "epoch": 0.1192312255309052,
      "grad_norm": 5.670905113220215,
      "learning_rate": 1.9572897442657526e-05,
      "loss": 0.0782,
      "step": 2010
    },
    {
      "epoch": 0.11929054454858228,
      "grad_norm": 9.064034461975098,
      "learning_rate": 1.9571579224887952e-05,
      "loss": 0.26,
      "step": 2011
    },
    {
      "epoch": 0.11934986356625935,
      "grad_norm": 16.80436134338379,
      "learning_rate": 1.9570261007118378e-05,
      "loss": 0.3088,
      "step": 2012
    },
    {
      "epoch": 0.11940918258393642,
      "grad_norm": 76.43623352050781,
      "learning_rate": 1.95689427893488e-05,
      "loss": 2.8578,
      "step": 2013
    },
    {
      "epoch": 0.11946850160161347,
      "grad_norm": 0.23435276746749878,
      "learning_rate": 1.9567624571579226e-05,
      "loss": 0.0023,
      "step": 2014
    },
    {
      "epoch": 0.11952782061929054,
      "grad_norm": 5.118841648101807,
      "learning_rate": 1.9566306353809652e-05,
      "loss": 0.2112,
      "step": 2015
    },
    {
      "epoch": 0.11958713963696761,
      "grad_norm": 32.900970458984375,
      "learning_rate": 1.9564988136040075e-05,
      "loss": 0.5533,
      "step": 2016
    },
    {
      "epoch": 0.11964645865464468,
      "grad_norm": 7.22422981262207,
      "learning_rate": 1.95636699182705e-05,
      "loss": 0.0594,
      "step": 2017
    },
    {
      "epoch": 0.11970577767232175,
      "grad_norm": 4.6348090171813965,
      "learning_rate": 1.9562351700500923e-05,
      "loss": 0.0255,
      "step": 2018
    },
    {
      "epoch": 0.11976509668999881,
      "grad_norm": 10.90060806274414,
      "learning_rate": 1.956103348273135e-05,
      "loss": 0.4003,
      "step": 2019
    },
    {
      "epoch": 0.11982441570767588,
      "grad_norm": 1.5676908493041992,
      "learning_rate": 1.9559715264961775e-05,
      "loss": 0.0059,
      "step": 2020
    },
    {
      "epoch": 0.11988373472535295,
      "grad_norm": 20.704004287719727,
      "learning_rate": 1.9558397047192197e-05,
      "loss": 0.535,
      "step": 2021
    },
    {
      "epoch": 0.11994305374303002,
      "grad_norm": 19.124849319458008,
      "learning_rate": 1.9557078829422623e-05,
      "loss": 0.637,
      "step": 2022
    },
    {
      "epoch": 0.12000237276070708,
      "grad_norm": 3.9126596450805664,
      "learning_rate": 1.9555760611653046e-05,
      "loss": 0.0182,
      "step": 2023
    },
    {
      "epoch": 0.12006169177838415,
      "grad_norm": 0.6090683341026306,
      "learning_rate": 1.9554442393883472e-05,
      "loss": 0.0064,
      "step": 2024
    },
    {
      "epoch": 0.12012101079606122,
      "grad_norm": 0.8446335792541504,
      "learning_rate": 1.9553124176113894e-05,
      "loss": 0.0117,
      "step": 2025
    },
    {
      "epoch": 0.12018032981373829,
      "grad_norm": 0.12940888106822968,
      "learning_rate": 1.955180595834432e-05,
      "loss": 0.002,
      "step": 2026
    },
    {
      "epoch": 0.12023964883141536,
      "grad_norm": 16.438196182250977,
      "learning_rate": 1.9550487740574743e-05,
      "loss": 0.5014,
      "step": 2027
    },
    {
      "epoch": 0.12029896784909241,
      "grad_norm": 18.789194107055664,
      "learning_rate": 1.954916952280517e-05,
      "loss": 1.1592,
      "step": 2028
    },
    {
      "epoch": 0.12035828686676948,
      "grad_norm": 6.939211368560791,
      "learning_rate": 1.9547851305035594e-05,
      "loss": 0.097,
      "step": 2029
    },
    {
      "epoch": 0.12041760588444655,
      "grad_norm": 4.277002334594727,
      "learning_rate": 1.9546533087266017e-05,
      "loss": 0.0288,
      "step": 2030
    },
    {
      "epoch": 0.12047692490212362,
      "grad_norm": 10.090204238891602,
      "learning_rate": 1.9545214869496443e-05,
      "loss": 0.8032,
      "step": 2031
    },
    {
      "epoch": 0.12053624391980068,
      "grad_norm": 11.499403953552246,
      "learning_rate": 1.9543896651726865e-05,
      "loss": 0.2628,
      "step": 2032
    },
    {
      "epoch": 0.12059556293747775,
      "grad_norm": 3.5645670890808105,
      "learning_rate": 1.954257843395729e-05,
      "loss": 0.0712,
      "step": 2033
    },
    {
      "epoch": 0.12065488195515482,
      "grad_norm": 4.997496604919434,
      "learning_rate": 1.9541260216187717e-05,
      "loss": 0.1072,
      "step": 2034
    },
    {
      "epoch": 0.12071420097283189,
      "grad_norm": 0.7259389758110046,
      "learning_rate": 1.953994199841814e-05,
      "loss": 0.0093,
      "step": 2035
    },
    {
      "epoch": 0.12077351999050896,
      "grad_norm": 6.671255588531494,
      "learning_rate": 1.9538623780648565e-05,
      "loss": 0.0941,
      "step": 2036
    },
    {
      "epoch": 0.12083283900818602,
      "grad_norm": 16.618696212768555,
      "learning_rate": 1.953730556287899e-05,
      "loss": 0.3931,
      "step": 2037
    },
    {
      "epoch": 0.12089215802586309,
      "grad_norm": 2.212608575820923,
      "learning_rate": 1.9535987345109414e-05,
      "loss": 0.0215,
      "step": 2038
    },
    {
      "epoch": 0.12095147704354016,
      "grad_norm": 2.7774529457092285,
      "learning_rate": 1.953466912733984e-05,
      "loss": 0.0559,
      "step": 2039
    },
    {
      "epoch": 0.12101079606121723,
      "grad_norm": 4.841415882110596,
      "learning_rate": 1.9533350909570262e-05,
      "loss": 0.0671,
      "step": 2040
    },
    {
      "epoch": 0.1210701150788943,
      "grad_norm": 2.3370203971862793,
      "learning_rate": 1.9532032691800688e-05,
      "loss": 0.0351,
      "step": 2041
    },
    {
      "epoch": 0.12112943409657136,
      "grad_norm": 17.930158615112305,
      "learning_rate": 1.953071447403111e-05,
      "loss": 1.1333,
      "step": 2042
    },
    {
      "epoch": 0.12118875311424843,
      "grad_norm": 0.739870011806488,
      "learning_rate": 1.9529396256261536e-05,
      "loss": 0.0102,
      "step": 2043
    },
    {
      "epoch": 0.1212480721319255,
      "grad_norm": 0.7857604026794434,
      "learning_rate": 1.952807803849196e-05,
      "loss": 0.0038,
      "step": 2044
    },
    {
      "epoch": 0.12130739114960257,
      "grad_norm": 2.754485845565796,
      "learning_rate": 1.9526759820722385e-05,
      "loss": 0.0203,
      "step": 2045
    },
    {
      "epoch": 0.12136671016727962,
      "grad_norm": 8.855405807495117,
      "learning_rate": 1.952544160295281e-05,
      "loss": 0.1426,
      "step": 2046
    },
    {
      "epoch": 0.1214260291849567,
      "grad_norm": 0.6677206754684448,
      "learning_rate": 1.9524123385183233e-05,
      "loss": 0.0074,
      "step": 2047
    },
    {
      "epoch": 0.12148534820263376,
      "grad_norm": 1.1546399593353271,
      "learning_rate": 1.952280516741366e-05,
      "loss": 0.0124,
      "step": 2048
    },
    {
      "epoch": 0.12154466722031083,
      "grad_norm": 14.271228790283203,
      "learning_rate": 1.952148694964408e-05,
      "loss": 0.2836,
      "step": 2049
    },
    {
      "epoch": 0.1216039862379879,
      "grad_norm": 25.940956115722656,
      "learning_rate": 1.9520168731874507e-05,
      "loss": 0.2207,
      "step": 2050
    },
    {
      "epoch": 0.12166330525566496,
      "grad_norm": 0.28576940298080444,
      "learning_rate": 1.9518850514104933e-05,
      "loss": 0.0028,
      "step": 2051
    },
    {
      "epoch": 0.12172262427334203,
      "grad_norm": 16.73940658569336,
      "learning_rate": 1.9517532296335356e-05,
      "loss": 0.3313,
      "step": 2052
    },
    {
      "epoch": 0.1217819432910191,
      "grad_norm": 0.1735716015100479,
      "learning_rate": 1.951621407856578e-05,
      "loss": 0.0016,
      "step": 2053
    },
    {
      "epoch": 0.12184126230869617,
      "grad_norm": 0.1975754350423813,
      "learning_rate": 1.9514895860796204e-05,
      "loss": 0.003,
      "step": 2054
    },
    {
      "epoch": 0.12190058132637324,
      "grad_norm": 0.2153645008802414,
      "learning_rate": 1.951357764302663e-05,
      "loss": 0.0023,
      "step": 2055
    },
    {
      "epoch": 0.1219599003440503,
      "grad_norm": 7.070010662078857,
      "learning_rate": 1.9512259425257053e-05,
      "loss": 0.0843,
      "step": 2056
    },
    {
      "epoch": 0.12201921936172737,
      "grad_norm": 0.4929220378398895,
      "learning_rate": 1.951094120748748e-05,
      "loss": 0.005,
      "step": 2057
    },
    {
      "epoch": 0.12207853837940444,
      "grad_norm": 3.8399336338043213,
      "learning_rate": 1.95096229897179e-05,
      "loss": 0.0271,
      "step": 2058
    },
    {
      "epoch": 0.12213785739708151,
      "grad_norm": 41.329429626464844,
      "learning_rate": 1.9508304771948327e-05,
      "loss": 0.2956,
      "step": 2059
    },
    {
      "epoch": 0.12219717641475857,
      "grad_norm": 1.8904536962509155,
      "learning_rate": 1.9506986554178753e-05,
      "loss": 0.014,
      "step": 2060
    },
    {
      "epoch": 0.12225649543243564,
      "grad_norm": 0.39223596453666687,
      "learning_rate": 1.9505668336409175e-05,
      "loss": 0.0029,
      "step": 2061
    },
    {
      "epoch": 0.1223158144501127,
      "grad_norm": 26.00716781616211,
      "learning_rate": 1.95043501186396e-05,
      "loss": 0.4275,
      "step": 2062
    },
    {
      "epoch": 0.12237513346778978,
      "grad_norm": 12.494170188903809,
      "learning_rate": 1.9503031900870027e-05,
      "loss": 0.1295,
      "step": 2063
    },
    {
      "epoch": 0.12243445248546685,
      "grad_norm": 15.2223482131958,
      "learning_rate": 1.950171368310045e-05,
      "loss": 1.1252,
      "step": 2064
    },
    {
      "epoch": 0.1224937715031439,
      "grad_norm": 0.0942913144826889,
      "learning_rate": 1.9500395465330875e-05,
      "loss": 0.0025,
      "step": 2065
    },
    {
      "epoch": 0.12255309052082097,
      "grad_norm": 4.496825218200684,
      "learning_rate": 1.9499077247561298e-05,
      "loss": 0.0389,
      "step": 2066
    },
    {
      "epoch": 0.12261240953849804,
      "grad_norm": 12.087788581848145,
      "learning_rate": 1.9497759029791724e-05,
      "loss": 0.6007,
      "step": 2067
    },
    {
      "epoch": 0.12267172855617511,
      "grad_norm": 2.3284358978271484,
      "learning_rate": 1.949644081202215e-05,
      "loss": 0.0323,
      "step": 2068
    },
    {
      "epoch": 0.12273104757385218,
      "grad_norm": 1.2347863912582397,
      "learning_rate": 1.9495122594252572e-05,
      "loss": 0.0155,
      "step": 2069
    },
    {
      "epoch": 0.12279036659152924,
      "grad_norm": 6.415971755981445,
      "learning_rate": 1.9493804376482998e-05,
      "loss": 0.1865,
      "step": 2070
    },
    {
      "epoch": 0.12284968560920631,
      "grad_norm": 13.101306915283203,
      "learning_rate": 1.949248615871342e-05,
      "loss": 0.3185,
      "step": 2071
    },
    {
      "epoch": 0.12290900462688338,
      "grad_norm": 3.878769874572754,
      "learning_rate": 1.9491167940943846e-05,
      "loss": 0.0421,
      "step": 2072
    },
    {
      "epoch": 0.12296832364456045,
      "grad_norm": 17.749109268188477,
      "learning_rate": 1.948984972317427e-05,
      "loss": 0.4299,
      "step": 2073
    },
    {
      "epoch": 0.12302764266223751,
      "grad_norm": 13.38380241394043,
      "learning_rate": 1.9488531505404695e-05,
      "loss": 0.1296,
      "step": 2074
    },
    {
      "epoch": 0.12308696167991458,
      "grad_norm": 63.77975845336914,
      "learning_rate": 1.9487213287635117e-05,
      "loss": 0.5584,
      "step": 2075
    },
    {
      "epoch": 0.12314628069759165,
      "grad_norm": 0.055673062801361084,
      "learning_rate": 1.9485895069865543e-05,
      "loss": 0.001,
      "step": 2076
    },
    {
      "epoch": 0.12320559971526872,
      "grad_norm": 0.09388185292482376,
      "learning_rate": 1.948457685209597e-05,
      "loss": 0.001,
      "step": 2077
    },
    {
      "epoch": 0.12326491873294579,
      "grad_norm": 15.141138076782227,
      "learning_rate": 1.948325863432639e-05,
      "loss": 0.2837,
      "step": 2078
    },
    {
      "epoch": 0.12332423775062284,
      "grad_norm": 0.13058628141880035,
      "learning_rate": 1.9481940416556817e-05,
      "loss": 0.0023,
      "step": 2079
    },
    {
      "epoch": 0.12338355676829992,
      "grad_norm": 41.45913314819336,
      "learning_rate": 1.948062219878724e-05,
      "loss": 0.2687,
      "step": 2080
    },
    {
      "epoch": 0.12344287578597699,
      "grad_norm": 2.0514414310455322,
      "learning_rate": 1.9479303981017666e-05,
      "loss": 0.0217,
      "step": 2081
    },
    {
      "epoch": 0.12350219480365406,
      "grad_norm": 2.881429433822632,
      "learning_rate": 1.947798576324809e-05,
      "loss": 0.0279,
      "step": 2082
    },
    {
      "epoch": 0.12356151382133111,
      "grad_norm": 0.06684301048517227,
      "learning_rate": 1.9476667545478514e-05,
      "loss": 0.0011,
      "step": 2083
    },
    {
      "epoch": 0.12362083283900818,
      "grad_norm": 12.983064651489258,
      "learning_rate": 1.947534932770894e-05,
      "loss": 0.271,
      "step": 2084
    },
    {
      "epoch": 0.12368015185668525,
      "grad_norm": 7.096198558807373,
      "learning_rate": 1.9474031109939366e-05,
      "loss": 0.0346,
      "step": 2085
    },
    {
      "epoch": 0.12373947087436232,
      "grad_norm": 27.950260162353516,
      "learning_rate": 1.9472712892169788e-05,
      "loss": 0.3408,
      "step": 2086
    },
    {
      "epoch": 0.1237987898920394,
      "grad_norm": 0.4545310437679291,
      "learning_rate": 1.9471394674400214e-05,
      "loss": 0.0024,
      "step": 2087
    },
    {
      "epoch": 0.12385810890971645,
      "grad_norm": 29.82648277282715,
      "learning_rate": 1.9470076456630637e-05,
      "loss": 0.3978,
      "step": 2088
    },
    {
      "epoch": 0.12391742792739352,
      "grad_norm": 3.201427698135376,
      "learning_rate": 1.946875823886106e-05,
      "loss": 0.0356,
      "step": 2089
    },
    {
      "epoch": 0.12397674694507059,
      "grad_norm": 9.612330436706543,
      "learning_rate": 1.9467440021091485e-05,
      "loss": 0.0824,
      "step": 2090
    },
    {
      "epoch": 0.12403606596274766,
      "grad_norm": 16.196638107299805,
      "learning_rate": 1.946612180332191e-05,
      "loss": 0.7045,
      "step": 2091
    },
    {
      "epoch": 0.12409538498042473,
      "grad_norm": 3.123669385910034,
      "learning_rate": 1.9464803585552333e-05,
      "loss": 0.0264,
      "step": 2092
    },
    {
      "epoch": 0.12415470399810179,
      "grad_norm": 2.8576765060424805,
      "learning_rate": 1.946348536778276e-05,
      "loss": 0.0241,
      "step": 2093
    },
    {
      "epoch": 0.12421402301577886,
      "grad_norm": 0.09072139114141464,
      "learning_rate": 1.9462167150013185e-05,
      "loss": 0.0014,
      "step": 2094
    },
    {
      "epoch": 0.12427334203345593,
      "grad_norm": 6.4885077476501465,
      "learning_rate": 1.9460848932243608e-05,
      "loss": 0.1952,
      "step": 2095
    },
    {
      "epoch": 0.124332661051133,
      "grad_norm": 0.23482781648635864,
      "learning_rate": 1.9459530714474034e-05,
      "loss": 0.0033,
      "step": 2096
    },
    {
      "epoch": 0.12439198006881005,
      "grad_norm": 5.234538555145264,
      "learning_rate": 1.9458212496704456e-05,
      "loss": 0.0736,
      "step": 2097
    },
    {
      "epoch": 0.12445129908648712,
      "grad_norm": 5.500602722167969,
      "learning_rate": 1.9456894278934882e-05,
      "loss": 0.1137,
      "step": 2098
    },
    {
      "epoch": 0.1245106181041642,
      "grad_norm": 10.276457786560059,
      "learning_rate": 1.9455576061165308e-05,
      "loss": 0.5141,
      "step": 2099
    },
    {
      "epoch": 0.12456993712184126,
      "grad_norm": 2.184622049331665,
      "learning_rate": 1.945425784339573e-05,
      "loss": 0.0162,
      "step": 2100
    },
    {
      "epoch": 0.12462925613951834,
      "grad_norm": 0.23935182392597198,
      "learning_rate": 1.9452939625626156e-05,
      "loss": 0.0023,
      "step": 2101
    },
    {
      "epoch": 0.12468857515719539,
      "grad_norm": 32.76197052001953,
      "learning_rate": 1.945162140785658e-05,
      "loss": 1.325,
      "step": 2102
    },
    {
      "epoch": 0.12474789417487246,
      "grad_norm": 5.599207401275635,
      "learning_rate": 1.9450303190087005e-05,
      "loss": 0.2643,
      "step": 2103
    },
    {
      "epoch": 0.12480721319254953,
      "grad_norm": 19.441444396972656,
      "learning_rate": 1.9448984972317427e-05,
      "loss": 0.5247,
      "step": 2104
    },
    {
      "epoch": 0.1248665322102266,
      "grad_norm": 8.77446460723877,
      "learning_rate": 1.9447666754547853e-05,
      "loss": 0.108,
      "step": 2105
    },
    {
      "epoch": 0.12492585122790367,
      "grad_norm": 2.0323874950408936,
      "learning_rate": 1.9446348536778275e-05,
      "loss": 0.0163,
      "step": 2106
    },
    {
      "epoch": 0.12498517024558073,
      "grad_norm": 2.2242190837860107,
      "learning_rate": 1.94450303190087e-05,
      "loss": 0.0102,
      "step": 2107
    },
    {
      "epoch": 0.1250444892632578,
      "grad_norm": 8.624391555786133,
      "learning_rate": 1.9443712101239127e-05,
      "loss": 0.3346,
      "step": 2108
    },
    {
      "epoch": 0.12510380828093487,
      "grad_norm": 23.651256561279297,
      "learning_rate": 1.944239388346955e-05,
      "loss": 0.3791,
      "step": 2109
    },
    {
      "epoch": 0.12516312729861193,
      "grad_norm": 3.63736629486084,
      "learning_rate": 1.9441075665699976e-05,
      "loss": 0.04,
      "step": 2110
    },
    {
      "epoch": 0.125222446316289,
      "grad_norm": 15.098515510559082,
      "learning_rate": 1.94397574479304e-05,
      "loss": 0.5382,
      "step": 2111
    },
    {
      "epoch": 0.12528176533396607,
      "grad_norm": 0.25758281350135803,
      "learning_rate": 1.9438439230160824e-05,
      "loss": 0.0017,
      "step": 2112
    },
    {
      "epoch": 0.12534108435164315,
      "grad_norm": 0.03917287662625313,
      "learning_rate": 1.943712101239125e-05,
      "loss": 0.0007,
      "step": 2113
    },
    {
      "epoch": 0.1254004033693202,
      "grad_norm": 0.09576132148504257,
      "learning_rate": 1.9435802794621672e-05,
      "loss": 0.0017,
      "step": 2114
    },
    {
      "epoch": 0.12545972238699726,
      "grad_norm": 11.46845817565918,
      "learning_rate": 1.9434484576852098e-05,
      "loss": 0.1645,
      "step": 2115
    },
    {
      "epoch": 0.12551904140467435,
      "grad_norm": 4.835083484649658,
      "learning_rate": 1.9433166359082524e-05,
      "loss": 0.08,
      "step": 2116
    },
    {
      "epoch": 0.1255783604223514,
      "grad_norm": 3.6876673698425293,
      "learning_rate": 1.9431848141312947e-05,
      "loss": 0.0637,
      "step": 2117
    },
    {
      "epoch": 0.12563767944002846,
      "grad_norm": 0.616777777671814,
      "learning_rate": 1.9430529923543372e-05,
      "loss": 0.0026,
      "step": 2118
    },
    {
      "epoch": 0.12569699845770554,
      "grad_norm": 0.058802396059036255,
      "learning_rate": 1.9429211705773795e-05,
      "loss": 0.0011,
      "step": 2119
    },
    {
      "epoch": 0.1257563174753826,
      "grad_norm": 0.1578138917684555,
      "learning_rate": 1.942789348800422e-05,
      "loss": 0.0011,
      "step": 2120
    },
    {
      "epoch": 0.12581563649305968,
      "grad_norm": 24.72342872619629,
      "learning_rate": 1.9426575270234643e-05,
      "loss": 1.0116,
      "step": 2121
    },
    {
      "epoch": 0.12587495551073674,
      "grad_norm": 3.9252047538757324,
      "learning_rate": 1.942525705246507e-05,
      "loss": 0.0424,
      "step": 2122
    },
    {
      "epoch": 0.1259342745284138,
      "grad_norm": 18.146926879882812,
      "learning_rate": 1.942393883469549e-05,
      "loss": 0.0962,
      "step": 2123
    },
    {
      "epoch": 0.12599359354609088,
      "grad_norm": 1.8053840398788452,
      "learning_rate": 1.9422620616925918e-05,
      "loss": 0.0088,
      "step": 2124
    },
    {
      "epoch": 0.12605291256376794,
      "grad_norm": 0.04436132684350014,
      "learning_rate": 1.9421302399156343e-05,
      "loss": 0.0009,
      "step": 2125
    },
    {
      "epoch": 0.12611223158144502,
      "grad_norm": 2.1969165802001953,
      "learning_rate": 1.9419984181386766e-05,
      "loss": 0.0271,
      "step": 2126
    },
    {
      "epoch": 0.12617155059912208,
      "grad_norm": 16.55000114440918,
      "learning_rate": 1.9418665963617192e-05,
      "loss": 0.067,
      "step": 2127
    },
    {
      "epoch": 0.12623086961679914,
      "grad_norm": 0.05004937946796417,
      "learning_rate": 1.9417347745847614e-05,
      "loss": 0.0006,
      "step": 2128
    },
    {
      "epoch": 0.12629018863447622,
      "grad_norm": 5.256044387817383,
      "learning_rate": 1.941602952807804e-05,
      "loss": 0.4787,
      "step": 2129
    },
    {
      "epoch": 0.12634950765215328,
      "grad_norm": 32.4901237487793,
      "learning_rate": 1.9414711310308466e-05,
      "loss": 1.7888,
      "step": 2130
    },
    {
      "epoch": 0.12640882666983036,
      "grad_norm": 9.42674446105957,
      "learning_rate": 1.941339309253889e-05,
      "loss": 0.2718,
      "step": 2131
    },
    {
      "epoch": 0.12646814568750742,
      "grad_norm": 0.5628858804702759,
      "learning_rate": 1.9412074874769314e-05,
      "loss": 0.0056,
      "step": 2132
    },
    {
      "epoch": 0.12652746470518447,
      "grad_norm": 43.1642951965332,
      "learning_rate": 1.9410756656999737e-05,
      "loss": 0.5398,
      "step": 2133
    },
    {
      "epoch": 0.12658678372286156,
      "grad_norm": 54.50801086425781,
      "learning_rate": 1.9409438439230163e-05,
      "loss": 0.9617,
      "step": 2134
    },
    {
      "epoch": 0.1266461027405386,
      "grad_norm": 26.164222717285156,
      "learning_rate": 1.9408120221460585e-05,
      "loss": 0.1166,
      "step": 2135
    },
    {
      "epoch": 0.1267054217582157,
      "grad_norm": 0.04262497276067734,
      "learning_rate": 1.940680200369101e-05,
      "loss": 0.0006,
      "step": 2136
    },
    {
      "epoch": 0.12676474077589275,
      "grad_norm": 2.453559637069702,
      "learning_rate": 1.9405483785921434e-05,
      "loss": 0.0125,
      "step": 2137
    },
    {
      "epoch": 0.1268240597935698,
      "grad_norm": 13.491965293884277,
      "learning_rate": 1.940416556815186e-05,
      "loss": 0.2905,
      "step": 2138
    },
    {
      "epoch": 0.1268833788112469,
      "grad_norm": 8.745945930480957,
      "learning_rate": 1.9402847350382285e-05,
      "loss": 0.0533,
      "step": 2139
    },
    {
      "epoch": 0.12694269782892395,
      "grad_norm": 11.967945098876953,
      "learning_rate": 1.9401529132612708e-05,
      "loss": 0.1767,
      "step": 2140
    },
    {
      "epoch": 0.127002016846601,
      "grad_norm": 2.059772253036499,
      "learning_rate": 1.9400210914843134e-05,
      "loss": 0.0337,
      "step": 2141
    },
    {
      "epoch": 0.1270613358642781,
      "grad_norm": 40.0911865234375,
      "learning_rate": 1.939889269707356e-05,
      "loss": 1.8134,
      "step": 2142
    },
    {
      "epoch": 0.12712065488195515,
      "grad_norm": 32.55815505981445,
      "learning_rate": 1.9397574479303982e-05,
      "loss": 0.3566,
      "step": 2143
    },
    {
      "epoch": 0.12717997389963223,
      "grad_norm": 2.5483744144439697,
      "learning_rate": 1.9396256261534408e-05,
      "loss": 0.0534,
      "step": 2144
    },
    {
      "epoch": 0.1272392929173093,
      "grad_norm": 0.7000970840454102,
      "learning_rate": 1.939493804376483e-05,
      "loss": 0.0032,
      "step": 2145
    },
    {
      "epoch": 0.12729861193498634,
      "grad_norm": 2.6973209381103516,
      "learning_rate": 1.9393619825995256e-05,
      "loss": 0.0197,
      "step": 2146
    },
    {
      "epoch": 0.12735793095266343,
      "grad_norm": 0.37604159116744995,
      "learning_rate": 1.9392301608225682e-05,
      "loss": 0.0052,
      "step": 2147
    },
    {
      "epoch": 0.12741724997034048,
      "grad_norm": 32.11961364746094,
      "learning_rate": 1.9390983390456105e-05,
      "loss": 0.93,
      "step": 2148
    },
    {
      "epoch": 0.12747656898801757,
      "grad_norm": 0.15128955245018005,
      "learning_rate": 1.938966517268653e-05,
      "loss": 0.0023,
      "step": 2149
    },
    {
      "epoch": 0.12753588800569463,
      "grad_norm": 0.6347789764404297,
      "learning_rate": 1.9388346954916953e-05,
      "loss": 0.011,
      "step": 2150
    },
    {
      "epoch": 0.12759520702337168,
      "grad_norm": 18.683115005493164,
      "learning_rate": 1.938702873714738e-05,
      "loss": 0.3665,
      "step": 2151
    },
    {
      "epoch": 0.12765452604104877,
      "grad_norm": 15.76718521118164,
      "learning_rate": 1.93857105193778e-05,
      "loss": 0.3171,
      "step": 2152
    },
    {
      "epoch": 0.12771384505872582,
      "grad_norm": 8.115066528320312,
      "learning_rate": 1.9384392301608227e-05,
      "loss": 0.1005,
      "step": 2153
    },
    {
      "epoch": 0.1277731640764029,
      "grad_norm": 2.383885383605957,
      "learning_rate": 1.938307408383865e-05,
      "loss": 0.0364,
      "step": 2154
    },
    {
      "epoch": 0.12783248309407996,
      "grad_norm": 8.245038986206055,
      "learning_rate": 1.9381755866069076e-05,
      "loss": 0.0515,
      "step": 2155
    },
    {
      "epoch": 0.12789180211175702,
      "grad_norm": 13.583910942077637,
      "learning_rate": 1.9380437648299502e-05,
      "loss": 0.1947,
      "step": 2156
    },
    {
      "epoch": 0.1279511211294341,
      "grad_norm": 37.053653717041016,
      "learning_rate": 1.9379119430529924e-05,
      "loss": 0.208,
      "step": 2157
    },
    {
      "epoch": 0.12801044014711116,
      "grad_norm": 6.270635604858398,
      "learning_rate": 1.937780121276035e-05,
      "loss": 0.0983,
      "step": 2158
    },
    {
      "epoch": 0.12806975916478824,
      "grad_norm": 12.673685073852539,
      "learning_rate": 1.9376482994990776e-05,
      "loss": 0.1386,
      "step": 2159
    },
    {
      "epoch": 0.1281290781824653,
      "grad_norm": 0.18634696304798126,
      "learning_rate": 1.93751647772212e-05,
      "loss": 0.004,
      "step": 2160
    },
    {
      "epoch": 0.12818839720014236,
      "grad_norm": 0.0316181518137455,
      "learning_rate": 1.9373846559451624e-05,
      "loss": 0.0007,
      "step": 2161
    },
    {
      "epoch": 0.12824771621781944,
      "grad_norm": 29.25663948059082,
      "learning_rate": 1.9372528341682047e-05,
      "loss": 0.0442,
      "step": 2162
    },
    {
      "epoch": 0.1283070352354965,
      "grad_norm": 0.10448897629976273,
      "learning_rate": 1.9371210123912473e-05,
      "loss": 0.0013,
      "step": 2163
    },
    {
      "epoch": 0.12836635425317358,
      "grad_norm": 0.6694258451461792,
      "learning_rate": 1.93698919061429e-05,
      "loss": 0.0036,
      "step": 2164
    },
    {
      "epoch": 0.12842567327085064,
      "grad_norm": 8.453218460083008,
      "learning_rate": 1.936857368837332e-05,
      "loss": 0.5986,
      "step": 2165
    },
    {
      "epoch": 0.1284849922885277,
      "grad_norm": 0.08672237396240234,
      "learning_rate": 1.9367255470603744e-05,
      "loss": 0.0015,
      "step": 2166
    },
    {
      "epoch": 0.12854431130620478,
      "grad_norm": 13.0928316116333,
      "learning_rate": 1.936593725283417e-05,
      "loss": 0.4556,
      "step": 2167
    },
    {
      "epoch": 0.12860363032388183,
      "grad_norm": 10.296685218811035,
      "learning_rate": 1.9364619035064592e-05,
      "loss": 0.2581,
      "step": 2168
    },
    {
      "epoch": 0.1286629493415589,
      "grad_norm": 0.03220019489526749,
      "learning_rate": 1.9363300817295018e-05,
      "loss": 0.0003,
      "step": 2169
    },
    {
      "epoch": 0.12872226835923598,
      "grad_norm": 1.7821115255355835,
      "learning_rate": 1.9361982599525444e-05,
      "loss": 0.0283,
      "step": 2170
    },
    {
      "epoch": 0.12878158737691303,
      "grad_norm": 2.064622163772583,
      "learning_rate": 1.9360664381755866e-05,
      "loss": 0.0184,
      "step": 2171
    },
    {
      "epoch": 0.12884090639459012,
      "grad_norm": 6.13368034362793,
      "learning_rate": 1.9359346163986292e-05,
      "loss": 0.0458,
      "step": 2172
    },
    {
      "epoch": 0.12890022541226717,
      "grad_norm": 13.704392433166504,
      "learning_rate": 1.9358027946216718e-05,
      "loss": 0.0574,
      "step": 2173
    },
    {
      "epoch": 0.12895954442994423,
      "grad_norm": 0.04040496423840523,
      "learning_rate": 1.935670972844714e-05,
      "loss": 0.0004,
      "step": 2174
    },
    {
      "epoch": 0.1290188634476213,
      "grad_norm": 25.339128494262695,
      "learning_rate": 1.9355391510677566e-05,
      "loss": 0.4591,
      "step": 2175
    },
    {
      "epoch": 0.12907818246529837,
      "grad_norm": 2.6278796195983887,
      "learning_rate": 1.9354073292907992e-05,
      "loss": 0.0174,
      "step": 2176
    },
    {
      "epoch": 0.12913750148297545,
      "grad_norm": 0.7739210724830627,
      "learning_rate": 1.9352755075138415e-05,
      "loss": 0.0109,
      "step": 2177
    },
    {
      "epoch": 0.1291968205006525,
      "grad_norm": 0.5589236617088318,
      "learning_rate": 1.935143685736884e-05,
      "loss": 0.0028,
      "step": 2178
    },
    {
      "epoch": 0.12925613951832957,
      "grad_norm": 0.1658790558576584,
      "learning_rate": 1.9350118639599263e-05,
      "loss": 0.0025,
      "step": 2179
    },
    {
      "epoch": 0.12931545853600665,
      "grad_norm": 26.885759353637695,
      "learning_rate": 1.934880042182969e-05,
      "loss": 0.9455,
      "step": 2180
    },
    {
      "epoch": 0.1293747775536837,
      "grad_norm": 0.058465566486120224,
      "learning_rate": 1.934748220406011e-05,
      "loss": 0.0008,
      "step": 2181
    },
    {
      "epoch": 0.1294340965713608,
      "grad_norm": 15.009960174560547,
      "learning_rate": 1.9346163986290537e-05,
      "loss": 0.836,
      "step": 2182
    },
    {
      "epoch": 0.12949341558903785,
      "grad_norm": 81.20507049560547,
      "learning_rate": 1.934484576852096e-05,
      "loss": 0.0587,
      "step": 2183
    },
    {
      "epoch": 0.1295527346067149,
      "grad_norm": 21.16217803955078,
      "learning_rate": 1.9343527550751386e-05,
      "loss": 0.2062,
      "step": 2184
    },
    {
      "epoch": 0.129612053624392,
      "grad_norm": 10.519867897033691,
      "learning_rate": 1.9342209332981808e-05,
      "loss": 0.3391,
      "step": 2185
    },
    {
      "epoch": 0.12967137264206904,
      "grad_norm": 5.000918388366699,
      "learning_rate": 1.9340891115212234e-05,
      "loss": 0.0811,
      "step": 2186
    },
    {
      "epoch": 0.12973069165974613,
      "grad_norm": 14.22983455657959,
      "learning_rate": 1.933957289744266e-05,
      "loss": 0.2021,
      "step": 2187
    },
    {
      "epoch": 0.12979001067742318,
      "grad_norm": 56.69489288330078,
      "learning_rate": 1.9338254679673082e-05,
      "loss": 1.5626,
      "step": 2188
    },
    {
      "epoch": 0.12984932969510024,
      "grad_norm": 6.909758567810059,
      "learning_rate": 1.933693646190351e-05,
      "loss": 0.0187,
      "step": 2189
    },
    {
      "epoch": 0.12990864871277732,
      "grad_norm": 25.83112144470215,
      "learning_rate": 1.9335618244133934e-05,
      "loss": 0.5723,
      "step": 2190
    },
    {
      "epoch": 0.12996796773045438,
      "grad_norm": 23.7492733001709,
      "learning_rate": 1.9334300026364357e-05,
      "loss": 0.2191,
      "step": 2191
    },
    {
      "epoch": 0.13002728674813144,
      "grad_norm": 8.920785903930664,
      "learning_rate": 1.9332981808594783e-05,
      "loss": 0.4857,
      "step": 2192
    },
    {
      "epoch": 0.13008660576580852,
      "grad_norm": 3.77386212348938,
      "learning_rate": 1.9331663590825205e-05,
      "loss": 0.0336,
      "step": 2193
    },
    {
      "epoch": 0.13014592478348558,
      "grad_norm": 7.902388095855713,
      "learning_rate": 1.933034537305563e-05,
      "loss": 0.4147,
      "step": 2194
    },
    {
      "epoch": 0.13020524380116266,
      "grad_norm": 17.032052993774414,
      "learning_rate": 1.9329027155286057e-05,
      "loss": 0.15,
      "step": 2195
    },
    {
      "epoch": 0.13026456281883972,
      "grad_norm": 4.702345371246338,
      "learning_rate": 1.932770893751648e-05,
      "loss": 0.0938,
      "step": 2196
    },
    {
      "epoch": 0.13032388183651678,
      "grad_norm": 9.747482299804688,
      "learning_rate": 1.9326390719746905e-05,
      "loss": 0.0151,
      "step": 2197
    },
    {
      "epoch": 0.13038320085419386,
      "grad_norm": 12.237688064575195,
      "learning_rate": 1.9325072501977328e-05,
      "loss": 0.4357,
      "step": 2198
    },
    {
      "epoch": 0.13044251987187092,
      "grad_norm": 1.25326406955719,
      "learning_rate": 1.932375428420775e-05,
      "loss": 0.0163,
      "step": 2199
    },
    {
      "epoch": 0.130501838889548,
      "grad_norm": 0.015627318993210793,
      "learning_rate": 1.9322436066438176e-05,
      "loss": 0.0004,
      "step": 2200
    },
    {
      "epoch": 0.13056115790722506,
      "grad_norm": 7.25621223449707,
      "learning_rate": 1.9321117848668602e-05,
      "loss": 0.1242,
      "step": 2201
    },
    {
      "epoch": 0.1306204769249021,
      "grad_norm": 0.024979745969176292,
      "learning_rate": 1.9319799630899024e-05,
      "loss": 0.0004,
      "step": 2202
    },
    {
      "epoch": 0.1306797959425792,
      "grad_norm": 4.032281875610352,
      "learning_rate": 1.931848141312945e-05,
      "loss": 0.2009,
      "step": 2203
    },
    {
      "epoch": 0.13073911496025625,
      "grad_norm": 22.402587890625,
      "learning_rate": 1.9317163195359876e-05,
      "loss": 0.4226,
      "step": 2204
    },
    {
      "epoch": 0.13079843397793334,
      "grad_norm": 14.23377513885498,
      "learning_rate": 1.93158449775903e-05,
      "loss": 0.5939,
      "step": 2205
    },
    {
      "epoch": 0.1308577529956104,
      "grad_norm": 3.1040055751800537,
      "learning_rate": 1.9314526759820725e-05,
      "loss": 0.0129,
      "step": 2206
    },
    {
      "epoch": 0.13091707201328745,
      "grad_norm": 15.83682918548584,
      "learning_rate": 1.931320854205115e-05,
      "loss": 0.3385,
      "step": 2207
    },
    {
      "epoch": 0.13097639103096453,
      "grad_norm": 39.25847244262695,
      "learning_rate": 1.9311890324281573e-05,
      "loss": 0.4063,
      "step": 2208
    },
    {
      "epoch": 0.1310357100486416,
      "grad_norm": 7.118466377258301,
      "learning_rate": 1.9310572106512e-05,
      "loss": 0.0869,
      "step": 2209
    },
    {
      "epoch": 0.13109502906631867,
      "grad_norm": 5.256161689758301,
      "learning_rate": 1.930925388874242e-05,
      "loss": 0.0642,
      "step": 2210
    },
    {
      "epoch": 0.13115434808399573,
      "grad_norm": 0.14404182136058807,
      "learning_rate": 1.9307935670972847e-05,
      "loss": 0.0026,
      "step": 2211
    },
    {
      "epoch": 0.1312136671016728,
      "grad_norm": 2.7475674152374268,
      "learning_rate": 1.930661745320327e-05,
      "loss": 0.0229,
      "step": 2212
    },
    {
      "epoch": 0.13127298611934987,
      "grad_norm": 0.033690519630908966,
      "learning_rate": 1.9305299235433696e-05,
      "loss": 0.0005,
      "step": 2213
    },
    {
      "epoch": 0.13133230513702693,
      "grad_norm": 0.06591738015413284,
      "learning_rate": 1.9303981017664118e-05,
      "loss": 0.001,
      "step": 2214
    },
    {
      "epoch": 0.13139162415470398,
      "grad_norm": 15.434233665466309,
      "learning_rate": 1.9302662799894544e-05,
      "loss": 0.134,
      "step": 2215
    },
    {
      "epoch": 0.13145094317238107,
      "grad_norm": 3.0973527431488037,
      "learning_rate": 1.9301344582124966e-05,
      "loss": 0.0411,
      "step": 2216
    },
    {
      "epoch": 0.13151026219005812,
      "grad_norm": 15.563167572021484,
      "learning_rate": 1.9300026364355392e-05,
      "loss": 0.4032,
      "step": 2217
    },
    {
      "epoch": 0.1315695812077352,
      "grad_norm": 5.8891496658325195,
      "learning_rate": 1.9298708146585818e-05,
      "loss": 0.1127,
      "step": 2218
    },
    {
      "epoch": 0.13162890022541227,
      "grad_norm": 10.042019844055176,
      "learning_rate": 1.929738992881624e-05,
      "loss": 0.2859,
      "step": 2219
    },
    {
      "epoch": 0.13168821924308932,
      "grad_norm": 2.2175331115722656,
      "learning_rate": 1.9296071711046667e-05,
      "loss": 0.0195,
      "step": 2220
    },
    {
      "epoch": 0.1317475382607664,
      "grad_norm": 0.19136595726013184,
      "learning_rate": 1.9294753493277093e-05,
      "loss": 0.0009,
      "step": 2221
    },
    {
      "epoch": 0.13180685727844346,
      "grad_norm": 0.037546608597040176,
      "learning_rate": 1.9293435275507515e-05,
      "loss": 0.0008,
      "step": 2222
    },
    {
      "epoch": 0.13186617629612055,
      "grad_norm": 28.269350051879883,
      "learning_rate": 1.929211705773794e-05,
      "loss": 0.2097,
      "step": 2223
    },
    {
      "epoch": 0.1319254953137976,
      "grad_norm": 18.837636947631836,
      "learning_rate": 1.9290798839968367e-05,
      "loss": 0.2616,
      "step": 2224
    },
    {
      "epoch": 0.13198481433147466,
      "grad_norm": 5.658854007720947,
      "learning_rate": 1.928948062219879e-05,
      "loss": 0.0442,
      "step": 2225
    },
    {
      "epoch": 0.13204413334915174,
      "grad_norm": 13.210821151733398,
      "learning_rate": 1.9288162404429215e-05,
      "loss": 0.7794,
      "step": 2226
    },
    {
      "epoch": 0.1321034523668288,
      "grad_norm": 0.8359687924385071,
      "learning_rate": 1.9286844186659638e-05,
      "loss": 0.0154,
      "step": 2227
    },
    {
      "epoch": 0.13216277138450588,
      "grad_norm": 11.941085815429688,
      "learning_rate": 1.9285525968890064e-05,
      "loss": 0.2888,
      "step": 2228
    },
    {
      "epoch": 0.13222209040218294,
      "grad_norm": 12.660256385803223,
      "learning_rate": 1.9284207751120486e-05,
      "loss": 0.5782,
      "step": 2229
    },
    {
      "epoch": 0.13228140941986,
      "grad_norm": 3.840106725692749,
      "learning_rate": 1.9282889533350912e-05,
      "loss": 0.1916,
      "step": 2230
    },
    {
      "epoch": 0.13234072843753708,
      "grad_norm": 38.659236907958984,
      "learning_rate": 1.9281571315581334e-05,
      "loss": 0.8472,
      "step": 2231
    },
    {
      "epoch": 0.13240004745521414,
      "grad_norm": 20.477069854736328,
      "learning_rate": 1.928025309781176e-05,
      "loss": 0.4984,
      "step": 2232
    },
    {
      "epoch": 0.13245936647289122,
      "grad_norm": 4.611607074737549,
      "learning_rate": 1.9278934880042183e-05,
      "loss": 0.4298,
      "step": 2233
    },
    {
      "epoch": 0.13251868549056828,
      "grad_norm": 8.336904525756836,
      "learning_rate": 1.927761666227261e-05,
      "loss": 0.2552,
      "step": 2234
    },
    {
      "epoch": 0.13257800450824533,
      "grad_norm": 4.342036247253418,
      "learning_rate": 1.9276298444503035e-05,
      "loss": 0.0578,
      "step": 2235
    },
    {
      "epoch": 0.13263732352592242,
      "grad_norm": 25.76555824279785,
      "learning_rate": 1.9274980226733457e-05,
      "loss": 0.5709,
      "step": 2236
    },
    {
      "epoch": 0.13269664254359947,
      "grad_norm": 7.223270416259766,
      "learning_rate": 1.9273662008963883e-05,
      "loss": 0.0805,
      "step": 2237
    },
    {
      "epoch": 0.13275596156127656,
      "grad_norm": 8.138385772705078,
      "learning_rate": 1.927234379119431e-05,
      "loss": 0.3074,
      "step": 2238
    },
    {
      "epoch": 0.13281528057895362,
      "grad_norm": 0.060425128787755966,
      "learning_rate": 1.927102557342473e-05,
      "loss": 0.0006,
      "step": 2239
    },
    {
      "epoch": 0.13287459959663067,
      "grad_norm": 4.229936122894287,
      "learning_rate": 1.9269707355655157e-05,
      "loss": 0.0636,
      "step": 2240
    },
    {
      "epoch": 0.13293391861430776,
      "grad_norm": 0.2008826732635498,
      "learning_rate": 1.926838913788558e-05,
      "loss": 0.0024,
      "step": 2241
    },
    {
      "epoch": 0.1329932376319848,
      "grad_norm": 6.021299362182617,
      "learning_rate": 1.9267070920116006e-05,
      "loss": 0.0641,
      "step": 2242
    },
    {
      "epoch": 0.13305255664966187,
      "grad_norm": 1.6432580947875977,
      "learning_rate": 1.9265752702346428e-05,
      "loss": 0.0076,
      "step": 2243
    },
    {
      "epoch": 0.13311187566733895,
      "grad_norm": 5.497861385345459,
      "learning_rate": 1.9264434484576854e-05,
      "loss": 0.0845,
      "step": 2244
    },
    {
      "epoch": 0.133171194685016,
      "grad_norm": 6.513557434082031,
      "learning_rate": 1.9263116266807276e-05,
      "loss": 0.267,
      "step": 2245
    },
    {
      "epoch": 0.1332305137026931,
      "grad_norm": 10.118882179260254,
      "learning_rate": 1.9261798049037702e-05,
      "loss": 0.1278,
      "step": 2246
    },
    {
      "epoch": 0.13328983272037015,
      "grad_norm": 0.07488319277763367,
      "learning_rate": 1.9260479831268125e-05,
      "loss": 0.0014,
      "step": 2247
    },
    {
      "epoch": 0.1333491517380472,
      "grad_norm": 20.504873275756836,
      "learning_rate": 1.925916161349855e-05,
      "loss": 0.3872,
      "step": 2248
    },
    {
      "epoch": 0.1334084707557243,
      "grad_norm": 30.7465763092041,
      "learning_rate": 1.9257843395728977e-05,
      "loss": 0.5069,
      "step": 2249
    },
    {
      "epoch": 0.13346778977340135,
      "grad_norm": 47.13693618774414,
      "learning_rate": 1.92565251779594e-05,
      "loss": 0.5294,
      "step": 2250
    },
    {
      "epoch": 0.13352710879107843,
      "grad_norm": 47.79640579223633,
      "learning_rate": 1.9255206960189825e-05,
      "loss": 0.4055,
      "step": 2251
    },
    {
      "epoch": 0.1335864278087555,
      "grad_norm": 71.17813110351562,
      "learning_rate": 1.925388874242025e-05,
      "loss": 0.6406,
      "step": 2252
    },
    {
      "epoch": 0.13364574682643254,
      "grad_norm": 0.02438623458147049,
      "learning_rate": 1.9252570524650673e-05,
      "loss": 0.0007,
      "step": 2253
    },
    {
      "epoch": 0.13370506584410963,
      "grad_norm": 4.443113327026367,
      "learning_rate": 1.92512523068811e-05,
      "loss": 0.0526,
      "step": 2254
    },
    {
      "epoch": 0.13376438486178668,
      "grad_norm": 28.69581413269043,
      "learning_rate": 1.9249934089111525e-05,
      "loss": 0.2134,
      "step": 2255
    },
    {
      "epoch": 0.13382370387946377,
      "grad_norm": 4.945471286773682,
      "learning_rate": 1.9248615871341948e-05,
      "loss": 0.2761,
      "step": 2256
    },
    {
      "epoch": 0.13388302289714082,
      "grad_norm": 23.227188110351562,
      "learning_rate": 1.9247297653572373e-05,
      "loss": 0.2939,
      "step": 2257
    },
    {
      "epoch": 0.13394234191481788,
      "grad_norm": 6.07299280166626,
      "learning_rate": 1.9245979435802796e-05,
      "loss": 0.0724,
      "step": 2258
    },
    {
      "epoch": 0.13400166093249496,
      "grad_norm": 8.882505416870117,
      "learning_rate": 1.9244661218033222e-05,
      "loss": 0.3836,
      "step": 2259
    },
    {
      "epoch": 0.13406097995017202,
      "grad_norm": 25.214126586914062,
      "learning_rate": 1.9243343000263644e-05,
      "loss": 0.4521,
      "step": 2260
    },
    {
      "epoch": 0.1341202989678491,
      "grad_norm": 4.213747978210449,
      "learning_rate": 1.924202478249407e-05,
      "loss": 0.027,
      "step": 2261
    },
    {
      "epoch": 0.13417961798552616,
      "grad_norm": 0.7522255778312683,
      "learning_rate": 1.9240706564724493e-05,
      "loss": 0.0134,
      "step": 2262
    },
    {
      "epoch": 0.13423893700320322,
      "grad_norm": 0.12709252536296844,
      "learning_rate": 1.923938834695492e-05,
      "loss": 0.0023,
      "step": 2263
    },
    {
      "epoch": 0.1342982560208803,
      "grad_norm": 9.323092460632324,
      "learning_rate": 1.923807012918534e-05,
      "loss": 0.2883,
      "step": 2264
    },
    {
      "epoch": 0.13435757503855736,
      "grad_norm": 0.5279523134231567,
      "learning_rate": 1.9236751911415767e-05,
      "loss": 0.0058,
      "step": 2265
    },
    {
      "epoch": 0.13441689405623442,
      "grad_norm": 1.766650676727295,
      "learning_rate": 1.9235433693646193e-05,
      "loss": 0.008,
      "step": 2266
    },
    {
      "epoch": 0.1344762130739115,
      "grad_norm": 1.146599531173706,
      "learning_rate": 1.9234115475876615e-05,
      "loss": 0.0151,
      "step": 2267
    },
    {
      "epoch": 0.13453553209158856,
      "grad_norm": 2.1083076000213623,
      "learning_rate": 1.923279725810704e-05,
      "loss": 0.0189,
      "step": 2268
    },
    {
      "epoch": 0.13459485110926564,
      "grad_norm": 0.015559389255940914,
      "learning_rate": 1.9231479040337467e-05,
      "loss": 0.0005,
      "step": 2269
    },
    {
      "epoch": 0.1346541701269427,
      "grad_norm": 4.471551418304443,
      "learning_rate": 1.923016082256789e-05,
      "loss": 0.1108,
      "step": 2270
    },
    {
      "epoch": 0.13471348914461975,
      "grad_norm": 0.05477031692862511,
      "learning_rate": 1.9228842604798315e-05,
      "loss": 0.0009,
      "step": 2271
    },
    {
      "epoch": 0.13477280816229684,
      "grad_norm": 26.527713775634766,
      "learning_rate": 1.922752438702874e-05,
      "loss": 0.6435,
      "step": 2272
    },
    {
      "epoch": 0.1348321271799739,
      "grad_norm": 0.14132475852966309,
      "learning_rate": 1.9226206169259164e-05,
      "loss": 0.0023,
      "step": 2273
    },
    {
      "epoch": 0.13489144619765098,
      "grad_norm": 0.11560885608196259,
      "learning_rate": 1.922488795148959e-05,
      "loss": 0.0017,
      "step": 2274
    },
    {
      "epoch": 0.13495076521532803,
      "grad_norm": 6.141973972320557,
      "learning_rate": 1.9223569733720012e-05,
      "loss": 0.2454,
      "step": 2275
    },
    {
      "epoch": 0.1350100842330051,
      "grad_norm": 22.27594757080078,
      "learning_rate": 1.9222251515950438e-05,
      "loss": 0.2567,
      "step": 2276
    },
    {
      "epoch": 0.13506940325068217,
      "grad_norm": 1.0030096769332886,
      "learning_rate": 1.922093329818086e-05,
      "loss": 0.005,
      "step": 2277
    },
    {
      "epoch": 0.13512872226835923,
      "grad_norm": 6.115011215209961,
      "learning_rate": 1.9219615080411283e-05,
      "loss": 0.0737,
      "step": 2278
    },
    {
      "epoch": 0.13518804128603631,
      "grad_norm": 0.2622203826904297,
      "learning_rate": 1.921829686264171e-05,
      "loss": 0.0015,
      "step": 2279
    },
    {
      "epoch": 0.13524736030371337,
      "grad_norm": 9.890402793884277,
      "learning_rate": 1.9216978644872135e-05,
      "loss": 0.2598,
      "step": 2280
    },
    {
      "epoch": 0.13530667932139043,
      "grad_norm": 8.1327486038208,
      "learning_rate": 1.9215660427102557e-05,
      "loss": 0.0895,
      "step": 2281
    },
    {
      "epoch": 0.1353659983390675,
      "grad_norm": 28.70524024963379,
      "learning_rate": 1.9214342209332983e-05,
      "loss": 0.3171,
      "step": 2282
    },
    {
      "epoch": 0.13542531735674457,
      "grad_norm": 0.07031668722629547,
      "learning_rate": 1.921302399156341e-05,
      "loss": 0.001,
      "step": 2283
    },
    {
      "epoch": 0.13548463637442165,
      "grad_norm": 22.47334098815918,
      "learning_rate": 1.921170577379383e-05,
      "loss": 0.5853,
      "step": 2284
    },
    {
      "epoch": 0.1355439553920987,
      "grad_norm": 30.284215927124023,
      "learning_rate": 1.9210387556024257e-05,
      "loss": 1.046,
      "step": 2285
    },
    {
      "epoch": 0.13560327440977576,
      "grad_norm": 0.46800509095191956,
      "learning_rate": 1.9209069338254683e-05,
      "loss": 0.0072,
      "step": 2286
    },
    {
      "epoch": 0.13566259342745285,
      "grad_norm": 8.839098930358887,
      "learning_rate": 1.9207751120485106e-05,
      "loss": 0.1192,
      "step": 2287
    },
    {
      "epoch": 0.1357219124451299,
      "grad_norm": 0.44214707612991333,
      "learning_rate": 1.920643290271553e-05,
      "loss": 0.0046,
      "step": 2288
    },
    {
      "epoch": 0.135781231462807,
      "grad_norm": 0.01985551416873932,
      "learning_rate": 1.9205114684945954e-05,
      "loss": 0.0004,
      "step": 2289
    },
    {
      "epoch": 0.13584055048048405,
      "grad_norm": 0.17867380380630493,
      "learning_rate": 1.920379646717638e-05,
      "loss": 0.0017,
      "step": 2290
    },
    {
      "epoch": 0.1358998694981611,
      "grad_norm": 15.886948585510254,
      "learning_rate": 1.9202478249406803e-05,
      "loss": 0.3199,
      "step": 2291
    },
    {
      "epoch": 0.1359591885158382,
      "grad_norm": 4.147029399871826,
      "learning_rate": 1.920116003163723e-05,
      "loss": 0.0181,
      "step": 2292
    },
    {
      "epoch": 0.13601850753351524,
      "grad_norm": 5.573911666870117,
      "learning_rate": 1.919984181386765e-05,
      "loss": 0.2812,
      "step": 2293
    },
    {
      "epoch": 0.1360778265511923,
      "grad_norm": 18.51087760925293,
      "learning_rate": 1.9198523596098077e-05,
      "loss": 0.4091,
      "step": 2294
    },
    {
      "epoch": 0.13613714556886938,
      "grad_norm": 24.041440963745117,
      "learning_rate": 1.91972053783285e-05,
      "loss": 1.0857,
      "step": 2295
    },
    {
      "epoch": 0.13619646458654644,
      "grad_norm": 18.348608016967773,
      "learning_rate": 1.9195887160558925e-05,
      "loss": 0.6287,
      "step": 2296
    },
    {
      "epoch": 0.13625578360422352,
      "grad_norm": 1.4281597137451172,
      "learning_rate": 1.919456894278935e-05,
      "loss": 0.0161,
      "step": 2297
    },
    {
      "epoch": 0.13631510262190058,
      "grad_norm": 0.18600517511367798,
      "learning_rate": 1.9193250725019774e-05,
      "loss": 0.0037,
      "step": 2298
    },
    {
      "epoch": 0.13637442163957764,
      "grad_norm": 0.0359157957136631,
      "learning_rate": 1.91919325072502e-05,
      "loss": 0.0007,
      "step": 2299
    },
    {
      "epoch": 0.13643374065725472,
      "grad_norm": 1.5165413618087769,
      "learning_rate": 1.9190614289480625e-05,
      "loss": 0.0192,
      "step": 2300
    },
    {
      "epoch": 0.13649305967493178,
      "grad_norm": 19.9158935546875,
      "learning_rate": 1.9189296071711048e-05,
      "loss": 0.5367,
      "step": 2301
    },
    {
      "epoch": 0.13655237869260886,
      "grad_norm": 16.227144241333008,
      "learning_rate": 1.9187977853941474e-05,
      "loss": 0.6677,
      "step": 2302
    },
    {
      "epoch": 0.13661169771028592,
      "grad_norm": 0.23141694068908691,
      "learning_rate": 1.91866596361719e-05,
      "loss": 0.0027,
      "step": 2303
    },
    {
      "epoch": 0.13667101672796297,
      "grad_norm": 1.6017656326293945,
      "learning_rate": 1.9185341418402322e-05,
      "loss": 0.013,
      "step": 2304
    },
    {
      "epoch": 0.13673033574564006,
      "grad_norm": 1.860013484954834,
      "learning_rate": 1.9184023200632748e-05,
      "loss": 0.0244,
      "step": 2305
    },
    {
      "epoch": 0.13678965476331711,
      "grad_norm": 0.09079980850219727,
      "learning_rate": 1.918270498286317e-05,
      "loss": 0.0017,
      "step": 2306
    },
    {
      "epoch": 0.1368489737809942,
      "grad_norm": 6.519708633422852,
      "learning_rate": 1.9181386765093596e-05,
      "loss": 0.3145,
      "step": 2307
    },
    {
      "epoch": 0.13690829279867126,
      "grad_norm": 9.429365158081055,
      "learning_rate": 1.918006854732402e-05,
      "loss": 0.1625,
      "step": 2308
    },
    {
      "epoch": 0.1369676118163483,
      "grad_norm": 0.7906415462493896,
      "learning_rate": 1.9178750329554445e-05,
      "loss": 0.0127,
      "step": 2309
    },
    {
      "epoch": 0.1370269308340254,
      "grad_norm": 4.883070468902588,
      "learning_rate": 1.9177432111784867e-05,
      "loss": 0.1042,
      "step": 2310
    },
    {
      "epoch": 0.13708624985170245,
      "grad_norm": 30.907054901123047,
      "learning_rate": 1.9176113894015293e-05,
      "loss": 0.3667,
      "step": 2311
    },
    {
      "epoch": 0.13714556886937954,
      "grad_norm": 0.5303599238395691,
      "learning_rate": 1.9174795676245716e-05,
      "loss": 0.0027,
      "step": 2312
    },
    {
      "epoch": 0.1372048878870566,
      "grad_norm": 1.08794105052948,
      "learning_rate": 1.917347745847614e-05,
      "loss": 0.0065,
      "step": 2313
    },
    {
      "epoch": 0.13726420690473365,
      "grad_norm": 0.016713947057724,
      "learning_rate": 1.9172159240706567e-05,
      "loss": 0.0004,
      "step": 2314
    },
    {
      "epoch": 0.13732352592241073,
      "grad_norm": 38.759910583496094,
      "learning_rate": 1.917084102293699e-05,
      "loss": 0.9999,
      "step": 2315
    },
    {
      "epoch": 0.1373828449400878,
      "grad_norm": 0.54566490650177,
      "learning_rate": 1.9169522805167416e-05,
      "loss": 0.0063,
      "step": 2316
    },
    {
      "epoch": 0.13744216395776485,
      "grad_norm": 0.16900764405727386,
      "learning_rate": 1.916820458739784e-05,
      "loss": 0.0025,
      "step": 2317
    },
    {
      "epoch": 0.13750148297544193,
      "grad_norm": 0.008394360542297363,
      "learning_rate": 1.9166886369628264e-05,
      "loss": 0.0002,
      "step": 2318
    },
    {
      "epoch": 0.137560801993119,
      "grad_norm": 0.3882911801338196,
      "learning_rate": 1.916556815185869e-05,
      "loss": 0.004,
      "step": 2319
    },
    {
      "epoch": 0.13762012101079607,
      "grad_norm": 0.03919186443090439,
      "learning_rate": 1.9164249934089116e-05,
      "loss": 0.0009,
      "step": 2320
    },
    {
      "epoch": 0.13767944002847313,
      "grad_norm": 3.7284977436065674,
      "learning_rate": 1.916293171631954e-05,
      "loss": 0.1783,
      "step": 2321
    },
    {
      "epoch": 0.13773875904615018,
      "grad_norm": 0.31954193115234375,
      "learning_rate": 1.916161349854996e-05,
      "loss": 0.0016,
      "step": 2322
    },
    {
      "epoch": 0.13779807806382727,
      "grad_norm": 0.7585822343826294,
      "learning_rate": 1.9160295280780387e-05,
      "loss": 0.0047,
      "step": 2323
    },
    {
      "epoch": 0.13785739708150432,
      "grad_norm": 26.537826538085938,
      "learning_rate": 1.915897706301081e-05,
      "loss": 0.3345,
      "step": 2324
    },
    {
      "epoch": 0.1379167160991814,
      "grad_norm": 34.34189987182617,
      "learning_rate": 1.9157658845241235e-05,
      "loss": 1.0345,
      "step": 2325
    },
    {
      "epoch": 0.13797603511685846,
      "grad_norm": 0.0194797832518816,
      "learning_rate": 1.9156340627471658e-05,
      "loss": 0.0004,
      "step": 2326
    },
    {
      "epoch": 0.13803535413453552,
      "grad_norm": 21.33892822265625,
      "learning_rate": 1.9155022409702083e-05,
      "loss": 1.2605,
      "step": 2327
    },
    {
      "epoch": 0.1380946731522126,
      "grad_norm": 0.8547751903533936,
      "learning_rate": 1.915370419193251e-05,
      "loss": 0.0108,
      "step": 2328
    },
    {
      "epoch": 0.13815399216988966,
      "grad_norm": 23.923837661743164,
      "learning_rate": 1.9152385974162932e-05,
      "loss": 0.3973,
      "step": 2329
    },
    {
      "epoch": 0.13821331118756675,
      "grad_norm": 0.06560587137937546,
      "learning_rate": 1.9151067756393358e-05,
      "loss": 0.0013,
      "step": 2330
    },
    {
      "epoch": 0.1382726302052438,
      "grad_norm": 12.22252368927002,
      "learning_rate": 1.9149749538623784e-05,
      "loss": 0.8293,
      "step": 2331
    },
    {
      "epoch": 0.13833194922292086,
      "grad_norm": 0.7590052485466003,
      "learning_rate": 1.9148431320854206e-05,
      "loss": 0.0058,
      "step": 2332
    },
    {
      "epoch": 0.13839126824059794,
      "grad_norm": 5.120445728302002,
      "learning_rate": 1.9147113103084632e-05,
      "loss": 0.034,
      "step": 2333
    },
    {
      "epoch": 0.138450587258275,
      "grad_norm": 10.30722713470459,
      "learning_rate": 1.9145794885315058e-05,
      "loss": 0.0573,
      "step": 2334
    },
    {
      "epoch": 0.13850990627595208,
      "grad_norm": 15.493309020996094,
      "learning_rate": 1.914447666754548e-05,
      "loss": 1.0436,
      "step": 2335
    },
    {
      "epoch": 0.13856922529362914,
      "grad_norm": 36.32414245605469,
      "learning_rate": 1.9143158449775906e-05,
      "loss": 1.4579,
      "step": 2336
    },
    {
      "epoch": 0.1386285443113062,
      "grad_norm": 3.3246734142303467,
      "learning_rate": 1.914184023200633e-05,
      "loss": 0.0317,
      "step": 2337
    },
    {
      "epoch": 0.13868786332898328,
      "grad_norm": 9.072765350341797,
      "learning_rate": 1.9140522014236755e-05,
      "loss": 0.251,
      "step": 2338
    },
    {
      "epoch": 0.13874718234666034,
      "grad_norm": 10.47187328338623,
      "learning_rate": 1.9139203796467177e-05,
      "loss": 0.3057,
      "step": 2339
    },
    {
      "epoch": 0.13880650136433742,
      "grad_norm": 13.25755786895752,
      "learning_rate": 1.9137885578697603e-05,
      "loss": 0.2169,
      "step": 2340
    },
    {
      "epoch": 0.13886582038201448,
      "grad_norm": 8.463085174560547,
      "learning_rate": 1.9136567360928025e-05,
      "loss": 0.0521,
      "step": 2341
    },
    {
      "epoch": 0.13892513939969153,
      "grad_norm": 0.2713376581668854,
      "learning_rate": 1.913524914315845e-05,
      "loss": 0.0047,
      "step": 2342
    },
    {
      "epoch": 0.13898445841736862,
      "grad_norm": 3.540977954864502,
      "learning_rate": 1.9133930925388874e-05,
      "loss": 0.0478,
      "step": 2343
    },
    {
      "epoch": 0.13904377743504567,
      "grad_norm": 7.937143325805664,
      "learning_rate": 1.91326127076193e-05,
      "loss": 0.1948,
      "step": 2344
    },
    {
      "epoch": 0.13910309645272273,
      "grad_norm": 0.08129313588142395,
      "learning_rate": 1.9131294489849726e-05,
      "loss": 0.0016,
      "step": 2345
    },
    {
      "epoch": 0.13916241547039981,
      "grad_norm": 0.1516016274690628,
      "learning_rate": 1.9129976272080148e-05,
      "loss": 0.0022,
      "step": 2346
    },
    {
      "epoch": 0.13922173448807687,
      "grad_norm": 0.025352809578180313,
      "learning_rate": 1.9128658054310574e-05,
      "loss": 0.0007,
      "step": 2347
    },
    {
      "epoch": 0.13928105350575395,
      "grad_norm": 43.858524322509766,
      "learning_rate": 1.9127339836541e-05,
      "loss": 1.2186,
      "step": 2348
    },
    {
      "epoch": 0.139340372523431,
      "grad_norm": 4.251982688903809,
      "learning_rate": 1.9126021618771422e-05,
      "loss": 0.0977,
      "step": 2349
    },
    {
      "epoch": 0.13939969154110807,
      "grad_norm": 16.086395263671875,
      "learning_rate": 1.9124703401001848e-05,
      "loss": 0.2285,
      "step": 2350
    },
    {
      "epoch": 0.13945901055878515,
      "grad_norm": 13.079578399658203,
      "learning_rate": 1.9123385183232274e-05,
      "loss": 0.3089,
      "step": 2351
    },
    {
      "epoch": 0.1395183295764622,
      "grad_norm": 0.3198179006576538,
      "learning_rate": 1.9122066965462697e-05,
      "loss": 0.0061,
      "step": 2352
    },
    {
      "epoch": 0.1395776485941393,
      "grad_norm": 8.195053100585938,
      "learning_rate": 1.9120748747693122e-05,
      "loss": 0.1262,
      "step": 2353
    },
    {
      "epoch": 0.13963696761181635,
      "grad_norm": 7.066367149353027,
      "learning_rate": 1.9119430529923545e-05,
      "loss": 0.028,
      "step": 2354
    },
    {
      "epoch": 0.1396962866294934,
      "grad_norm": 3.1287002563476562,
      "learning_rate": 1.9118112312153967e-05,
      "loss": 0.0244,
      "step": 2355
    },
    {
      "epoch": 0.1397556056471705,
      "grad_norm": 1.8278356790542603,
      "learning_rate": 1.9116794094384393e-05,
      "loss": 0.0191,
      "step": 2356
    },
    {
      "epoch": 0.13981492466484755,
      "grad_norm": 2.803619384765625,
      "learning_rate": 1.9115475876614816e-05,
      "loss": 0.0704,
      "step": 2357
    },
    {
      "epoch": 0.13987424368252463,
      "grad_norm": 14.54263687133789,
      "learning_rate": 1.9114157658845242e-05,
      "loss": 0.0613,
      "step": 2358
    },
    {
      "epoch": 0.13993356270020169,
      "grad_norm": 3.484988212585449,
      "learning_rate": 1.9112839441075668e-05,
      "loss": 0.0933,
      "step": 2359
    },
    {
      "epoch": 0.13999288171787874,
      "grad_norm": 0.0220864899456501,
      "learning_rate": 1.911152122330609e-05,
      "loss": 0.0008,
      "step": 2360
    },
    {
      "epoch": 0.14005220073555583,
      "grad_norm": 35.00946044921875,
      "learning_rate": 1.9110203005536516e-05,
      "loss": 0.3921,
      "step": 2361
    },
    {
      "epoch": 0.14011151975323288,
      "grad_norm": 15.74392318725586,
      "learning_rate": 1.9108884787766942e-05,
      "loss": 0.2714,
      "step": 2362
    },
    {
      "epoch": 0.14017083877090997,
      "grad_norm": 10.430472373962402,
      "learning_rate": 1.9107566569997364e-05,
      "loss": 0.0954,
      "step": 2363
    },
    {
      "epoch": 0.14023015778858702,
      "grad_norm": 1.2253687381744385,
      "learning_rate": 1.910624835222779e-05,
      "loss": 0.0142,
      "step": 2364
    },
    {
      "epoch": 0.14028947680626408,
      "grad_norm": 0.9906361103057861,
      "learning_rate": 1.9104930134458216e-05,
      "loss": 0.0082,
      "step": 2365
    },
    {
      "epoch": 0.14034879582394116,
      "grad_norm": 0.122122623026371,
      "learning_rate": 1.910361191668864e-05,
      "loss": 0.0032,
      "step": 2366
    },
    {
      "epoch": 0.14040811484161822,
      "grad_norm": 14.240453720092773,
      "learning_rate": 1.9102293698919064e-05,
      "loss": 0.4021,
      "step": 2367
    },
    {
      "epoch": 0.14046743385929528,
      "grad_norm": 0.15950673818588257,
      "learning_rate": 1.9100975481149487e-05,
      "loss": 0.0013,
      "step": 2368
    },
    {
      "epoch": 0.14052675287697236,
      "grad_norm": 6.634774684906006,
      "learning_rate": 1.9099657263379913e-05,
      "loss": 0.1236,
      "step": 2369
    },
    {
      "epoch": 0.14058607189464942,
      "grad_norm": 24.768648147583008,
      "learning_rate": 1.9098339045610335e-05,
      "loss": 0.5112,
      "step": 2370
    },
    {
      "epoch": 0.1406453909123265,
      "grad_norm": 0.474115788936615,
      "learning_rate": 1.909702082784076e-05,
      "loss": 0.0079,
      "step": 2371
    },
    {
      "epoch": 0.14070470993000356,
      "grad_norm": 0.7667646408081055,
      "learning_rate": 1.9095702610071184e-05,
      "loss": 0.0121,
      "step": 2372
    },
    {
      "epoch": 0.14076402894768061,
      "grad_norm": 20.338119506835938,
      "learning_rate": 1.909438439230161e-05,
      "loss": 0.3539,
      "step": 2373
    },
    {
      "epoch": 0.1408233479653577,
      "grad_norm": 0.4447215497493744,
      "learning_rate": 1.9093066174532032e-05,
      "loss": 0.0063,
      "step": 2374
    },
    {
      "epoch": 0.14088266698303475,
      "grad_norm": 23.151348114013672,
      "learning_rate": 1.9091747956762458e-05,
      "loss": 0.7425,
      "step": 2375
    },
    {
      "epoch": 0.14094198600071184,
      "grad_norm": 40.19961929321289,
      "learning_rate": 1.9090429738992884e-05,
      "loss": 0.9979,
      "step": 2376
    },
    {
      "epoch": 0.1410013050183889,
      "grad_norm": 14.199997901916504,
      "learning_rate": 1.9089111521223306e-05,
      "loss": 0.1334,
      "step": 2377
    },
    {
      "epoch": 0.14106062403606595,
      "grad_norm": 2.4738106727600098,
      "learning_rate": 1.9087793303453732e-05,
      "loss": 0.023,
      "step": 2378
    },
    {
      "epoch": 0.14111994305374304,
      "grad_norm": 19.77848243713379,
      "learning_rate": 1.9086475085684158e-05,
      "loss": 0.0543,
      "step": 2379
    },
    {
      "epoch": 0.1411792620714201,
      "grad_norm": 8.223210334777832,
      "learning_rate": 1.908515686791458e-05,
      "loss": 0.1538,
      "step": 2380
    },
    {
      "epoch": 0.14123858108909718,
      "grad_norm": 12.986451148986816,
      "learning_rate": 1.9083838650145006e-05,
      "loss": 0.5453,
      "step": 2381
    },
    {
      "epoch": 0.14129790010677423,
      "grad_norm": 15.913728713989258,
      "learning_rate": 1.9082520432375432e-05,
      "loss": 0.3642,
      "step": 2382
    },
    {
      "epoch": 0.1413572191244513,
      "grad_norm": 0.18268389999866486,
      "learning_rate": 1.9081202214605855e-05,
      "loss": 0.0021,
      "step": 2383
    },
    {
      "epoch": 0.14141653814212837,
      "grad_norm": 6.2362895011901855,
      "learning_rate": 1.907988399683628e-05,
      "loss": 0.2024,
      "step": 2384
    },
    {
      "epoch": 0.14147585715980543,
      "grad_norm": 19.74549674987793,
      "learning_rate": 1.9078565779066703e-05,
      "loss": 0.235,
      "step": 2385
    },
    {
      "epoch": 0.1415351761774825,
      "grad_norm": 16.90825653076172,
      "learning_rate": 1.907724756129713e-05,
      "loss": 1.3964,
      "step": 2386
    },
    {
      "epoch": 0.14159449519515957,
      "grad_norm": 29.21802520751953,
      "learning_rate": 1.907592934352755e-05,
      "loss": 0.0935,
      "step": 2387
    },
    {
      "epoch": 0.14165381421283663,
      "grad_norm": 10.377398490905762,
      "learning_rate": 1.9074611125757977e-05,
      "loss": 0.6395,
      "step": 2388
    },
    {
      "epoch": 0.1417131332305137,
      "grad_norm": 2.376676559448242,
      "learning_rate": 1.90732929079884e-05,
      "loss": 0.0497,
      "step": 2389
    },
    {
      "epoch": 0.14177245224819077,
      "grad_norm": 0.4632115364074707,
      "learning_rate": 1.9071974690218826e-05,
      "loss": 0.0032,
      "step": 2390
    },
    {
      "epoch": 0.14183177126586785,
      "grad_norm": 0.28465405106544495,
      "learning_rate": 1.907065647244925e-05,
      "loss": 0.0035,
      "step": 2391
    },
    {
      "epoch": 0.1418910902835449,
      "grad_norm": 14.022473335266113,
      "learning_rate": 1.9069338254679674e-05,
      "loss": 0.1326,
      "step": 2392
    },
    {
      "epoch": 0.14195040930122196,
      "grad_norm": 8.163444519042969,
      "learning_rate": 1.90680200369101e-05,
      "loss": 0.0598,
      "step": 2393
    },
    {
      "epoch": 0.14200972831889905,
      "grad_norm": 14.149965286254883,
      "learning_rate": 1.9066701819140523e-05,
      "loss": 0.2916,
      "step": 2394
    },
    {
      "epoch": 0.1420690473365761,
      "grad_norm": 3.0675582885742188,
      "learning_rate": 1.906538360137095e-05,
      "loss": 0.0452,
      "step": 2395
    },
    {
      "epoch": 0.14212836635425316,
      "grad_norm": 3.843672275543213,
      "learning_rate": 1.9064065383601374e-05,
      "loss": 0.0249,
      "step": 2396
    },
    {
      "epoch": 0.14218768537193024,
      "grad_norm": 2.158930778503418,
      "learning_rate": 1.9062747165831797e-05,
      "loss": 0.0064,
      "step": 2397
    },
    {
      "epoch": 0.1422470043896073,
      "grad_norm": 18.572145462036133,
      "learning_rate": 1.9061428948062223e-05,
      "loss": 0.533,
      "step": 2398
    },
    {
      "epoch": 0.14230632340728439,
      "grad_norm": 1.2363576889038086,
      "learning_rate": 1.9060110730292645e-05,
      "loss": 0.0143,
      "step": 2399
    },
    {
      "epoch": 0.14236564242496144,
      "grad_norm": 5.6805524826049805,
      "learning_rate": 1.905879251252307e-05,
      "loss": 0.0473,
      "step": 2400
    },
    {
      "epoch": 0.1424249614426385,
      "grad_norm": 17.996055603027344,
      "learning_rate": 1.9057474294753494e-05,
      "loss": 0.6943,
      "step": 2401
    },
    {
      "epoch": 0.14248428046031558,
      "grad_norm": 12.817514419555664,
      "learning_rate": 1.905615607698392e-05,
      "loss": 0.142,
      "step": 2402
    },
    {
      "epoch": 0.14254359947799264,
      "grad_norm": 7.666094779968262,
      "learning_rate": 1.9054837859214342e-05,
      "loss": 0.1336,
      "step": 2403
    },
    {
      "epoch": 0.14260291849566972,
      "grad_norm": 25.47317123413086,
      "learning_rate": 1.9053519641444768e-05,
      "loss": 0.1084,
      "step": 2404
    },
    {
      "epoch": 0.14266223751334678,
      "grad_norm": 36.71865463256836,
      "learning_rate": 1.905220142367519e-05,
      "loss": 0.7115,
      "step": 2405
    },
    {
      "epoch": 0.14272155653102384,
      "grad_norm": 5.443715572357178,
      "learning_rate": 1.9050883205905616e-05,
      "loss": 0.0443,
      "step": 2406
    },
    {
      "epoch": 0.14278087554870092,
      "grad_norm": 3.268362045288086,
      "learning_rate": 1.9049564988136042e-05,
      "loss": 0.0367,
      "step": 2407
    },
    {
      "epoch": 0.14284019456637798,
      "grad_norm": 10.181588172912598,
      "learning_rate": 1.9048246770366465e-05,
      "loss": 0.1105,
      "step": 2408
    },
    {
      "epoch": 0.14289951358405506,
      "grad_norm": 2.798424005508423,
      "learning_rate": 1.904692855259689e-05,
      "loss": 0.0735,
      "step": 2409
    },
    {
      "epoch": 0.14295883260173212,
      "grad_norm": 0.0523262582719326,
      "learning_rate": 1.9045610334827316e-05,
      "loss": 0.0005,
      "step": 2410
    },
    {
      "epoch": 0.14301815161940917,
      "grad_norm": 17.368440628051758,
      "learning_rate": 1.904429211705774e-05,
      "loss": 0.7335,
      "step": 2411
    },
    {
      "epoch": 0.14307747063708626,
      "grad_norm": 6.643744468688965,
      "learning_rate": 1.9042973899288165e-05,
      "loss": 0.1972,
      "step": 2412
    },
    {
      "epoch": 0.1431367896547633,
      "grad_norm": 9.38239574432373,
      "learning_rate": 1.904165568151859e-05,
      "loss": 0.2784,
      "step": 2413
    },
    {
      "epoch": 0.1431961086724404,
      "grad_norm": 2.1241252422332764,
      "learning_rate": 1.9040337463749013e-05,
      "loss": 0.0462,
      "step": 2414
    },
    {
      "epoch": 0.14325542769011745,
      "grad_norm": 145.0312957763672,
      "learning_rate": 1.903901924597944e-05,
      "loss": 1.4107,
      "step": 2415
    },
    {
      "epoch": 0.1433147467077945,
      "grad_norm": 4.179278373718262,
      "learning_rate": 1.903770102820986e-05,
      "loss": 0.1374,
      "step": 2416
    },
    {
      "epoch": 0.1433740657254716,
      "grad_norm": 5.136069297790527,
      "learning_rate": 1.9036382810440287e-05,
      "loss": 0.2274,
      "step": 2417
    },
    {
      "epoch": 0.14343338474314865,
      "grad_norm": 8.285266876220703,
      "learning_rate": 1.903506459267071e-05,
      "loss": 0.1737,
      "step": 2418
    },
    {
      "epoch": 0.1434927037608257,
      "grad_norm": 11.590242385864258,
      "learning_rate": 1.9033746374901136e-05,
      "loss": 0.1307,
      "step": 2419
    },
    {
      "epoch": 0.1435520227785028,
      "grad_norm": 1.5297467708587646,
      "learning_rate": 1.9032428157131558e-05,
      "loss": 0.0233,
      "step": 2420
    },
    {
      "epoch": 0.14361134179617985,
      "grad_norm": 22.019433975219727,
      "learning_rate": 1.9031109939361984e-05,
      "loss": 0.2931,
      "step": 2421
    },
    {
      "epoch": 0.14367066081385693,
      "grad_norm": 10.54574203491211,
      "learning_rate": 1.9029791721592407e-05,
      "loss": 0.1807,
      "step": 2422
    },
    {
      "epoch": 0.143729979831534,
      "grad_norm": 2.0821242332458496,
      "learning_rate": 1.9028473503822833e-05,
      "loss": 0.0215,
      "step": 2423
    },
    {
      "epoch": 0.14378929884921104,
      "grad_norm": 5.66672945022583,
      "learning_rate": 1.902715528605326e-05,
      "loss": 0.0781,
      "step": 2424
    },
    {
      "epoch": 0.14384861786688813,
      "grad_norm": 14.357341766357422,
      "learning_rate": 1.902583706828368e-05,
      "loss": 0.3851,
      "step": 2425
    },
    {
      "epoch": 0.14390793688456519,
      "grad_norm": 0.7168753147125244,
      "learning_rate": 1.9024518850514107e-05,
      "loss": 0.0095,
      "step": 2426
    },
    {
      "epoch": 0.14396725590224227,
      "grad_norm": 0.151190847158432,
      "learning_rate": 1.9023200632744533e-05,
      "loss": 0.0026,
      "step": 2427
    },
    {
      "epoch": 0.14402657491991933,
      "grad_norm": 7.080760955810547,
      "learning_rate": 1.9021882414974955e-05,
      "loss": 0.0322,
      "step": 2428
    },
    {
      "epoch": 0.14408589393759638,
      "grad_norm": 24.7580509185791,
      "learning_rate": 1.902056419720538e-05,
      "loss": 0.3364,
      "step": 2429
    },
    {
      "epoch": 0.14414521295527347,
      "grad_norm": 12.278056144714355,
      "learning_rate": 1.9019245979435807e-05,
      "loss": 0.2104,
      "step": 2430
    },
    {
      "epoch": 0.14420453197295052,
      "grad_norm": 0.6585758924484253,
      "learning_rate": 1.901792776166623e-05,
      "loss": 0.009,
      "step": 2431
    },
    {
      "epoch": 0.1442638509906276,
      "grad_norm": 4.213527679443359,
      "learning_rate": 1.9016609543896655e-05,
      "loss": 0.1086,
      "step": 2432
    },
    {
      "epoch": 0.14432317000830466,
      "grad_norm": 4.30472993850708,
      "learning_rate": 1.9015291326127078e-05,
      "loss": 0.0418,
      "step": 2433
    },
    {
      "epoch": 0.14438248902598172,
      "grad_norm": 3.886673927307129,
      "learning_rate": 1.90139731083575e-05,
      "loss": 0.0444,
      "step": 2434
    },
    {
      "epoch": 0.1444418080436588,
      "grad_norm": 3.7741191387176514,
      "learning_rate": 1.9012654890587926e-05,
      "loss": 0.1297,
      "step": 2435
    },
    {
      "epoch": 0.14450112706133586,
      "grad_norm": 14.021056175231934,
      "learning_rate": 1.901133667281835e-05,
      "loss": 1.2615,
      "step": 2436
    },
    {
      "epoch": 0.14456044607901294,
      "grad_norm": 9.030089378356934,
      "learning_rate": 1.9010018455048775e-05,
      "loss": 0.1549,
      "step": 2437
    },
    {
      "epoch": 0.14461976509669,
      "grad_norm": 0.018983833491802216,
      "learning_rate": 1.90087002372792e-05,
      "loss": 0.0005,
      "step": 2438
    },
    {
      "epoch": 0.14467908411436706,
      "grad_norm": 4.8393473625183105,
      "learning_rate": 1.9007382019509623e-05,
      "loss": 0.0315,
      "step": 2439
    },
    {
      "epoch": 0.14473840313204414,
      "grad_norm": 3.5899744033813477,
      "learning_rate": 1.900606380174005e-05,
      "loss": 0.1001,
      "step": 2440
    },
    {
      "epoch": 0.1447977221497212,
      "grad_norm": 15.201573371887207,
      "learning_rate": 1.9004745583970475e-05,
      "loss": 0.1773,
      "step": 2441
    },
    {
      "epoch": 0.14485704116739828,
      "grad_norm": 20.21936798095703,
      "learning_rate": 1.9003427366200897e-05,
      "loss": 0.2425,
      "step": 2442
    },
    {
      "epoch": 0.14491636018507534,
      "grad_norm": 2.315592050552368,
      "learning_rate": 1.9002109148431323e-05,
      "loss": 0.0261,
      "step": 2443
    },
    {
      "epoch": 0.1449756792027524,
      "grad_norm": 0.1756664663553238,
      "learning_rate": 1.900079093066175e-05,
      "loss": 0.0024,
      "step": 2444
    },
    {
      "epoch": 0.14503499822042948,
      "grad_norm": 6.5530877113342285,
      "learning_rate": 1.899947271289217e-05,
      "loss": 0.0893,
      "step": 2445
    },
    {
      "epoch": 0.14509431723810653,
      "grad_norm": 5.760462284088135,
      "learning_rate": 1.8998154495122597e-05,
      "loss": 0.1899,
      "step": 2446
    },
    {
      "epoch": 0.1451536362557836,
      "grad_norm": 6.971032619476318,
      "learning_rate": 1.899683627735302e-05,
      "loss": 0.0374,
      "step": 2447
    },
    {
      "epoch": 0.14521295527346068,
      "grad_norm": 43.32951354980469,
      "learning_rate": 1.8995518059583446e-05,
      "loss": 0.6059,
      "step": 2448
    },
    {
      "epoch": 0.14527227429113773,
      "grad_norm": 0.020725930109620094,
      "learning_rate": 1.8994199841813868e-05,
      "loss": 0.0007,
      "step": 2449
    },
    {
      "epoch": 0.14533159330881482,
      "grad_norm": 24.47785186767578,
      "learning_rate": 1.8992881624044294e-05,
      "loss": 1.1945,
      "step": 2450
    },
    {
      "epoch": 0.14539091232649187,
      "grad_norm": 10.356670379638672,
      "learning_rate": 1.8991563406274717e-05,
      "loss": 0.1071,
      "step": 2451
    },
    {
      "epoch": 0.14545023134416893,
      "grad_norm": 15.406338691711426,
      "learning_rate": 1.8990245188505142e-05,
      "loss": 0.8201,
      "step": 2452
    },
    {
      "epoch": 0.145509550361846,
      "grad_norm": 21.747652053833008,
      "learning_rate": 1.8988926970735565e-05,
      "loss": 0.5031,
      "step": 2453
    },
    {
      "epoch": 0.14556886937952307,
      "grad_norm": 0.4663105905056,
      "learning_rate": 1.898760875296599e-05,
      "loss": 0.0049,
      "step": 2454
    },
    {
      "epoch": 0.14562818839720015,
      "grad_norm": 0.3579923212528229,
      "learning_rate": 1.8986290535196417e-05,
      "loss": 0.0056,
      "step": 2455
    },
    {
      "epoch": 0.1456875074148772,
      "grad_norm": 0.3960130214691162,
      "learning_rate": 1.898497231742684e-05,
      "loss": 0.0037,
      "step": 2456
    },
    {
      "epoch": 0.14574682643255427,
      "grad_norm": 0.3001980483531952,
      "learning_rate": 1.8983654099657265e-05,
      "loss": 0.0022,
      "step": 2457
    },
    {
      "epoch": 0.14580614545023135,
      "grad_norm": 4.673454761505127,
      "learning_rate": 1.898233588188769e-05,
      "loss": 0.1217,
      "step": 2458
    },
    {
      "epoch": 0.1458654644679084,
      "grad_norm": 3.213456630706787,
      "learning_rate": 1.8981017664118113e-05,
      "loss": 0.035,
      "step": 2459
    },
    {
      "epoch": 0.1459247834855855,
      "grad_norm": 7.502070903778076,
      "learning_rate": 1.897969944634854e-05,
      "loss": 0.0592,
      "step": 2460
    },
    {
      "epoch": 0.14598410250326255,
      "grad_norm": 15.401369094848633,
      "learning_rate": 1.8978381228578965e-05,
      "loss": 1.4289,
      "step": 2461
    },
    {
      "epoch": 0.1460434215209396,
      "grad_norm": 0.31625646352767944,
      "learning_rate": 1.8977063010809388e-05,
      "loss": 0.0027,
      "step": 2462
    },
    {
      "epoch": 0.1461027405386167,
      "grad_norm": 11.913061141967773,
      "learning_rate": 1.8975744793039814e-05,
      "loss": 1.1724,
      "step": 2463
    },
    {
      "epoch": 0.14616205955629374,
      "grad_norm": 6.449497699737549,
      "learning_rate": 1.8974426575270236e-05,
      "loss": 0.1555,
      "step": 2464
    },
    {
      "epoch": 0.14622137857397083,
      "grad_norm": 14.774618148803711,
      "learning_rate": 1.8973108357500662e-05,
      "loss": 0.8308,
      "step": 2465
    },
    {
      "epoch": 0.14628069759164788,
      "grad_norm": 3.60650372505188,
      "learning_rate": 1.8971790139731084e-05,
      "loss": 0.0899,
      "step": 2466
    },
    {
      "epoch": 0.14634001660932494,
      "grad_norm": 10.808064460754395,
      "learning_rate": 1.8970471921961507e-05,
      "loss": 0.8418,
      "step": 2467
    },
    {
      "epoch": 0.14639933562700203,
      "grad_norm": 0.41014301776885986,
      "learning_rate": 1.8969153704191933e-05,
      "loss": 0.0049,
      "step": 2468
    },
    {
      "epoch": 0.14645865464467908,
      "grad_norm": 0.09419611096382141,
      "learning_rate": 1.896783548642236e-05,
      "loss": 0.0027,
      "step": 2469
    },
    {
      "epoch": 0.14651797366235614,
      "grad_norm": 1.2484033107757568,
      "learning_rate": 1.896651726865278e-05,
      "loss": 0.0067,
      "step": 2470
    },
    {
      "epoch": 0.14657729268003322,
      "grad_norm": 0.01634180173277855,
      "learning_rate": 1.8965199050883207e-05,
      "loss": 0.0006,
      "step": 2471
    },
    {
      "epoch": 0.14663661169771028,
      "grad_norm": 0.14751626551151276,
      "learning_rate": 1.8963880833113633e-05,
      "loss": 0.0024,
      "step": 2472
    },
    {
      "epoch": 0.14669593071538736,
      "grad_norm": 0.12164326757192612,
      "learning_rate": 1.8962562615344055e-05,
      "loss": 0.0021,
      "step": 2473
    },
    {
      "epoch": 0.14675524973306442,
      "grad_norm": 10.40164852142334,
      "learning_rate": 1.896124439757448e-05,
      "loss": 0.3814,
      "step": 2474
    },
    {
      "epoch": 0.14681456875074148,
      "grad_norm": 0.4700014591217041,
      "learning_rate": 1.8959926179804907e-05,
      "loss": 0.0038,
      "step": 2475
    },
    {
      "epoch": 0.14687388776841856,
      "grad_norm": 0.10434339195489883,
      "learning_rate": 1.895860796203533e-05,
      "loss": 0.0018,
      "step": 2476
    },
    {
      "epoch": 0.14693320678609562,
      "grad_norm": 8.892037391662598,
      "learning_rate": 1.8957289744265756e-05,
      "loss": 0.1869,
      "step": 2477
    },
    {
      "epoch": 0.1469925258037727,
      "grad_norm": 1.2325623035430908,
      "learning_rate": 1.8955971526496178e-05,
      "loss": 0.0292,
      "step": 2478
    },
    {
      "epoch": 0.14705184482144976,
      "grad_norm": 3.4302611351013184,
      "learning_rate": 1.8954653308726604e-05,
      "loss": 0.0415,
      "step": 2479
    },
    {
      "epoch": 0.1471111638391268,
      "grad_norm": 0.9198535680770874,
      "learning_rate": 1.8953335090957026e-05,
      "loss": 0.0066,
      "step": 2480
    },
    {
      "epoch": 0.1471704828568039,
      "grad_norm": 2.0582008361816406,
      "learning_rate": 1.8952016873187452e-05,
      "loss": 0.0426,
      "step": 2481
    },
    {
      "epoch": 0.14722980187448095,
      "grad_norm": 0.01046689972281456,
      "learning_rate": 1.8950698655417875e-05,
      "loss": 0.0003,
      "step": 2482
    },
    {
      "epoch": 0.14728912089215804,
      "grad_norm": 5.09159517288208,
      "learning_rate": 1.89493804376483e-05,
      "loss": 0.0479,
      "step": 2483
    },
    {
      "epoch": 0.1473484399098351,
      "grad_norm": 5.252228736877441,
      "learning_rate": 1.8948062219878723e-05,
      "loss": 0.0469,
      "step": 2484
    },
    {
      "epoch": 0.14740775892751215,
      "grad_norm": 0.7493062019348145,
      "learning_rate": 1.894674400210915e-05,
      "loss": 0.0084,
      "step": 2485
    },
    {
      "epoch": 0.14746707794518923,
      "grad_norm": 58.5173454284668,
      "learning_rate": 1.8945425784339575e-05,
      "loss": 1.6664,
      "step": 2486
    },
    {
      "epoch": 0.1475263969628663,
      "grad_norm": 4.521894931793213,
      "learning_rate": 1.8944107566569997e-05,
      "loss": 0.0267,
      "step": 2487
    },
    {
      "epoch": 0.14758571598054337,
      "grad_norm": 0.02985217794775963,
      "learning_rate": 1.8942789348800423e-05,
      "loss": 0.0007,
      "step": 2488
    },
    {
      "epoch": 0.14764503499822043,
      "grad_norm": 24.45555877685547,
      "learning_rate": 1.894147113103085e-05,
      "loss": 1.655,
      "step": 2489
    },
    {
      "epoch": 0.1477043540158975,
      "grad_norm": 1.080723762512207,
      "learning_rate": 1.894015291326127e-05,
      "loss": 0.0144,
      "step": 2490
    },
    {
      "epoch": 0.14776367303357457,
      "grad_norm": 10.074987411499023,
      "learning_rate": 1.8938834695491698e-05,
      "loss": 0.1427,
      "step": 2491
    },
    {
      "epoch": 0.14782299205125163,
      "grad_norm": 41.658935546875,
      "learning_rate": 1.8937516477722123e-05,
      "loss": 1.1017,
      "step": 2492
    },
    {
      "epoch": 0.1478823110689287,
      "grad_norm": 10.46479606628418,
      "learning_rate": 1.8936198259952546e-05,
      "loss": 0.0864,
      "step": 2493
    },
    {
      "epoch": 0.14794163008660577,
      "grad_norm": 19.45539093017578,
      "learning_rate": 1.8934880042182972e-05,
      "loss": 0.1001,
      "step": 2494
    },
    {
      "epoch": 0.14800094910428283,
      "grad_norm": 0.08376803249120712,
      "learning_rate": 1.8933561824413394e-05,
      "loss": 0.0016,
      "step": 2495
    },
    {
      "epoch": 0.1480602681219599,
      "grad_norm": 0.8745924234390259,
      "learning_rate": 1.893224360664382e-05,
      "loss": 0.0092,
      "step": 2496
    },
    {
      "epoch": 0.14811958713963697,
      "grad_norm": 9.347638130187988,
      "learning_rate": 1.8930925388874243e-05,
      "loss": 0.7342,
      "step": 2497
    },
    {
      "epoch": 0.14817890615731402,
      "grad_norm": 0.12619124352931976,
      "learning_rate": 1.892960717110467e-05,
      "loss": 0.0013,
      "step": 2498
    },
    {
      "epoch": 0.1482382251749911,
      "grad_norm": 0.07421071082353592,
      "learning_rate": 1.892828895333509e-05,
      "loss": 0.0009,
      "step": 2499
    },
    {
      "epoch": 0.14829754419266816,
      "grad_norm": 0.30739572644233704,
      "learning_rate": 1.8926970735565517e-05,
      "loss": 0.0038,
      "step": 2500
    },
    {
      "epoch": 0.14835686321034525,
      "grad_norm": 3.871317148208618,
      "learning_rate": 1.892565251779594e-05,
      "loss": 0.1269,
      "step": 2501
    },
    {
      "epoch": 0.1484161822280223,
      "grad_norm": 0.05593837797641754,
      "learning_rate": 1.8924334300026365e-05,
      "loss": 0.0014,
      "step": 2502
    },
    {
      "epoch": 0.14847550124569936,
      "grad_norm": 1.3726507425308228,
      "learning_rate": 1.892301608225679e-05,
      "loss": 0.0092,
      "step": 2503
    },
    {
      "epoch": 0.14853482026337644,
      "grad_norm": 13.326011657714844,
      "learning_rate": 1.8921697864487214e-05,
      "loss": 0.1397,
      "step": 2504
    },
    {
      "epoch": 0.1485941392810535,
      "grad_norm": 0.47241440415382385,
      "learning_rate": 1.892037964671764e-05,
      "loss": 0.005,
      "step": 2505
    },
    {
      "epoch": 0.14865345829873058,
      "grad_norm": 0.09413965791463852,
      "learning_rate": 1.8919061428948065e-05,
      "loss": 0.001,
      "step": 2506
    },
    {
      "epoch": 0.14871277731640764,
      "grad_norm": 10.100933074951172,
      "learning_rate": 1.8917743211178488e-05,
      "loss": 0.1994,
      "step": 2507
    },
    {
      "epoch": 0.1487720963340847,
      "grad_norm": 4.991982936859131,
      "learning_rate": 1.8916424993408914e-05,
      "loss": 0.0696,
      "step": 2508
    },
    {
      "epoch": 0.14883141535176178,
      "grad_norm": 0.17710450291633606,
      "learning_rate": 1.891510677563934e-05,
      "loss": 0.0027,
      "step": 2509
    },
    {
      "epoch": 0.14889073436943884,
      "grad_norm": 0.15255703032016754,
      "learning_rate": 1.8913788557869762e-05,
      "loss": 0.0024,
      "step": 2510
    },
    {
      "epoch": 0.14895005338711592,
      "grad_norm": 0.29190805554389954,
      "learning_rate": 1.8912470340100185e-05,
      "loss": 0.0042,
      "step": 2511
    },
    {
      "epoch": 0.14900937240479298,
      "grad_norm": 0.0892997607588768,
      "learning_rate": 1.891115212233061e-05,
      "loss": 0.0014,
      "step": 2512
    },
    {
      "epoch": 0.14906869142247003,
      "grad_norm": 40.589447021484375,
      "learning_rate": 1.8909833904561033e-05,
      "loss": 0.4797,
      "step": 2513
    },
    {
      "epoch": 0.14912801044014712,
      "grad_norm": 9.445919036865234,
      "learning_rate": 1.890851568679146e-05,
      "loss": 0.08,
      "step": 2514
    },
    {
      "epoch": 0.14918732945782417,
      "grad_norm": 10.433952331542969,
      "learning_rate": 1.890719746902188e-05,
      "loss": 0.0824,
      "step": 2515
    },
    {
      "epoch": 0.14924664847550126,
      "grad_norm": 0.6814785599708557,
      "learning_rate": 1.8905879251252307e-05,
      "loss": 0.0075,
      "step": 2516
    },
    {
      "epoch": 0.14930596749317832,
      "grad_norm": 31.79827117919922,
      "learning_rate": 1.8904561033482733e-05,
      "loss": 0.6517,
      "step": 2517
    },
    {
      "epoch": 0.14936528651085537,
      "grad_norm": 5.663547039031982,
      "learning_rate": 1.8903242815713156e-05,
      "loss": 0.0923,
      "step": 2518
    },
    {
      "epoch": 0.14942460552853246,
      "grad_norm": 0.009515715762972832,
      "learning_rate": 1.890192459794358e-05,
      "loss": 0.0002,
      "step": 2519
    },
    {
      "epoch": 0.1494839245462095,
      "grad_norm": 0.14280860126018524,
      "learning_rate": 1.8900606380174007e-05,
      "loss": 0.0018,
      "step": 2520
    },
    {
      "epoch": 0.14954324356388657,
      "grad_norm": 0.0838746726512909,
      "learning_rate": 1.889928816240443e-05,
      "loss": 0.0011,
      "step": 2521
    },
    {
      "epoch": 0.14960256258156365,
      "grad_norm": 2.5468318462371826,
      "learning_rate": 1.8897969944634856e-05,
      "loss": 0.0481,
      "step": 2522
    },
    {
      "epoch": 0.1496618815992407,
      "grad_norm": 6.4617838859558105,
      "learning_rate": 1.8896651726865282e-05,
      "loss": 0.2789,
      "step": 2523
    },
    {
      "epoch": 0.1497212006169178,
      "grad_norm": 3.742459297180176,
      "learning_rate": 1.8895333509095704e-05,
      "loss": 0.038,
      "step": 2524
    },
    {
      "epoch": 0.14978051963459485,
      "grad_norm": 20.499366760253906,
      "learning_rate": 1.889401529132613e-05,
      "loss": 0.306,
      "step": 2525
    },
    {
      "epoch": 0.1498398386522719,
      "grad_norm": 0.38384926319122314,
      "learning_rate": 1.8892697073556553e-05,
      "loss": 0.0046,
      "step": 2526
    },
    {
      "epoch": 0.149899157669949,
      "grad_norm": 9.378449440002441,
      "learning_rate": 1.889137885578698e-05,
      "loss": 0.3419,
      "step": 2527
    },
    {
      "epoch": 0.14995847668762605,
      "grad_norm": 0.2847352921962738,
      "learning_rate": 1.88900606380174e-05,
      "loss": 0.0064,
      "step": 2528
    },
    {
      "epoch": 0.15001779570530313,
      "grad_norm": 0.40393149852752686,
      "learning_rate": 1.8888742420247827e-05,
      "loss": 0.0032,
      "step": 2529
    },
    {
      "epoch": 0.1500771147229802,
      "grad_norm": 5.449664115905762,
      "learning_rate": 1.888742420247825e-05,
      "loss": 0.1393,
      "step": 2530
    },
    {
      "epoch": 0.15013643374065724,
      "grad_norm": 0.027748282998800278,
      "learning_rate": 1.8886105984708675e-05,
      "loss": 0.0007,
      "step": 2531
    },
    {
      "epoch": 0.15019575275833433,
      "grad_norm": 4.477126598358154,
      "learning_rate": 1.8884787766939098e-05,
      "loss": 0.0279,
      "step": 2532
    },
    {
      "epoch": 0.15025507177601138,
      "grad_norm": 2.2162046432495117,
      "learning_rate": 1.8883469549169524e-05,
      "loss": 0.0236,
      "step": 2533
    },
    {
      "epoch": 0.15031439079368847,
      "grad_norm": 0.10225415974855423,
      "learning_rate": 1.888215133139995e-05,
      "loss": 0.0018,
      "step": 2534
    },
    {
      "epoch": 0.15037370981136552,
      "grad_norm": 9.683077812194824,
      "learning_rate": 1.8880833113630372e-05,
      "loss": 0.0591,
      "step": 2535
    },
    {
      "epoch": 0.15043302882904258,
      "grad_norm": 3.315168619155884,
      "learning_rate": 1.8879514895860798e-05,
      "loss": 0.108,
      "step": 2536
    },
    {
      "epoch": 0.15049234784671967,
      "grad_norm": 3.238068103790283,
      "learning_rate": 1.8878196678091224e-05,
      "loss": 0.0275,
      "step": 2537
    },
    {
      "epoch": 0.15055166686439672,
      "grad_norm": 5.5352582931518555,
      "learning_rate": 1.8876878460321646e-05,
      "loss": 0.1632,
      "step": 2538
    },
    {
      "epoch": 0.1506109858820738,
      "grad_norm": 0.026013154536485672,
      "learning_rate": 1.8875560242552072e-05,
      "loss": 0.0007,
      "step": 2539
    },
    {
      "epoch": 0.15067030489975086,
      "grad_norm": 0.023814629763364792,
      "learning_rate": 1.8874242024782498e-05,
      "loss": 0.0008,
      "step": 2540
    },
    {
      "epoch": 0.15072962391742792,
      "grad_norm": 0.5356113910675049,
      "learning_rate": 1.887292380701292e-05,
      "loss": 0.0027,
      "step": 2541
    },
    {
      "epoch": 0.150788942935105,
      "grad_norm": 0.0889701321721077,
      "learning_rate": 1.8871605589243346e-05,
      "loss": 0.0019,
      "step": 2542
    },
    {
      "epoch": 0.15084826195278206,
      "grad_norm": 0.33440065383911133,
      "learning_rate": 1.887028737147377e-05,
      "loss": 0.0019,
      "step": 2543
    },
    {
      "epoch": 0.15090758097045912,
      "grad_norm": 0.04787455499172211,
      "learning_rate": 1.886896915370419e-05,
      "loss": 0.0008,
      "step": 2544
    },
    {
      "epoch": 0.1509668999881362,
      "grad_norm": 1.7542650699615479,
      "learning_rate": 1.8867650935934617e-05,
      "loss": 0.1039,
      "step": 2545
    },
    {
      "epoch": 0.15102621900581326,
      "grad_norm": 0.14875294268131256,
      "learning_rate": 1.886633271816504e-05,
      "loss": 0.0036,
      "step": 2546
    },
    {
      "epoch": 0.15108553802349034,
      "grad_norm": 3.903193950653076,
      "learning_rate": 1.8865014500395466e-05,
      "loss": 0.0516,
      "step": 2547
    },
    {
      "epoch": 0.1511448570411674,
      "grad_norm": 2.22652006149292,
      "learning_rate": 1.886369628262589e-05,
      "loss": 0.0143,
      "step": 2548
    },
    {
      "epoch": 0.15120417605884445,
      "grad_norm": 10.037541389465332,
      "learning_rate": 1.8862378064856314e-05,
      "loss": 0.1153,
      "step": 2549
    },
    {
      "epoch": 0.15126349507652154,
      "grad_norm": 0.04943614825606346,
      "learning_rate": 1.886105984708674e-05,
      "loss": 0.0007,
      "step": 2550
    },
    {
      "epoch": 0.1513228140941986,
      "grad_norm": 5.831139087677002,
      "learning_rate": 1.8859741629317166e-05,
      "loss": 0.0274,
      "step": 2551
    },
    {
      "epoch": 0.15138213311187568,
      "grad_norm": 13.030226707458496,
      "learning_rate": 1.8858423411547588e-05,
      "loss": 0.4004,
      "step": 2552
    },
    {
      "epoch": 0.15144145212955273,
      "grad_norm": 8.896125793457031,
      "learning_rate": 1.8857105193778014e-05,
      "loss": 0.041,
      "step": 2553
    },
    {
      "epoch": 0.1515007711472298,
      "grad_norm": 17.30299186706543,
      "learning_rate": 1.885578697600844e-05,
      "loss": 0.3798,
      "step": 2554
    },
    {
      "epoch": 0.15156009016490687,
      "grad_norm": 15.282581329345703,
      "learning_rate": 1.8854468758238862e-05,
      "loss": 0.9408,
      "step": 2555
    },
    {
      "epoch": 0.15161940918258393,
      "grad_norm": 0.0794733315706253,
      "learning_rate": 1.885315054046929e-05,
      "loss": 0.0014,
      "step": 2556
    },
    {
      "epoch": 0.15167872820026101,
      "grad_norm": 27.092153549194336,
      "learning_rate": 1.885183232269971e-05,
      "loss": 0.2271,
      "step": 2557
    },
    {
      "epoch": 0.15173804721793807,
      "grad_norm": 9.290254592895508,
      "learning_rate": 1.8850514104930137e-05,
      "loss": 0.1879,
      "step": 2558
    },
    {
      "epoch": 0.15179736623561513,
      "grad_norm": 0.00749787176027894,
      "learning_rate": 1.884919588716056e-05,
      "loss": 0.0002,
      "step": 2559
    },
    {
      "epoch": 0.1518566852532922,
      "grad_norm": 1.8765313625335693,
      "learning_rate": 1.8847877669390985e-05,
      "loss": 0.0182,
      "step": 2560
    },
    {
      "epoch": 0.15191600427096927,
      "grad_norm": 0.07537737488746643,
      "learning_rate": 1.8846559451621408e-05,
      "loss": 0.0015,
      "step": 2561
    },
    {
      "epoch": 0.15197532328864635,
      "grad_norm": 9.450915336608887,
      "learning_rate": 1.8845241233851833e-05,
      "loss": 0.0304,
      "step": 2562
    },
    {
      "epoch": 0.1520346423063234,
      "grad_norm": 4.153063774108887,
      "learning_rate": 1.8843923016082256e-05,
      "loss": 0.0433,
      "step": 2563
    },
    {
      "epoch": 0.15209396132400047,
      "grad_norm": 4.17598295211792,
      "learning_rate": 1.8842604798312682e-05,
      "loss": 0.0818,
      "step": 2564
    },
    {
      "epoch": 0.15215328034167755,
      "grad_norm": 56.15107345581055,
      "learning_rate": 1.8841286580543108e-05,
      "loss": 0.7132,
      "step": 2565
    },
    {
      "epoch": 0.1522125993593546,
      "grad_norm": 17.166048049926758,
      "learning_rate": 1.883996836277353e-05,
      "loss": 0.421,
      "step": 2566
    },
    {
      "epoch": 0.1522719183770317,
      "grad_norm": 0.44085395336151123,
      "learning_rate": 1.8838650145003956e-05,
      "loss": 0.0046,
      "step": 2567
    },
    {
      "epoch": 0.15233123739470875,
      "grad_norm": 1.4743692874908447,
      "learning_rate": 1.8837331927234382e-05,
      "loss": 0.0179,
      "step": 2568
    },
    {
      "epoch": 0.1523905564123858,
      "grad_norm": 24.243206024169922,
      "learning_rate": 1.8836013709464804e-05,
      "loss": 0.4852,
      "step": 2569
    },
    {
      "epoch": 0.1524498754300629,
      "grad_norm": 2.0794732570648193,
      "learning_rate": 1.883469549169523e-05,
      "loss": 0.0117,
      "step": 2570
    },
    {
      "epoch": 0.15250919444773994,
      "grad_norm": 16.677040100097656,
      "learning_rate": 1.8833377273925656e-05,
      "loss": 0.2223,
      "step": 2571
    },
    {
      "epoch": 0.152568513465417,
      "grad_norm": 0.09938021749258041,
      "learning_rate": 1.883205905615608e-05,
      "loss": 0.0016,
      "step": 2572
    },
    {
      "epoch": 0.15262783248309408,
      "grad_norm": 0.44635018706321716,
      "learning_rate": 1.8830740838386505e-05,
      "loss": 0.0036,
      "step": 2573
    },
    {
      "epoch": 0.15268715150077114,
      "grad_norm": 21.92803192138672,
      "learning_rate": 1.8829422620616927e-05,
      "loss": 0.409,
      "step": 2574
    },
    {
      "epoch": 0.15274647051844822,
      "grad_norm": 9.58051872253418,
      "learning_rate": 1.8828104402847353e-05,
      "loss": 0.2863,
      "step": 2575
    },
    {
      "epoch": 0.15280578953612528,
      "grad_norm": 3.5760791301727295,
      "learning_rate": 1.8826786185077775e-05,
      "loss": 0.1922,
      "step": 2576
    },
    {
      "epoch": 0.15286510855380234,
      "grad_norm": 6.876234531402588,
      "learning_rate": 1.88254679673082e-05,
      "loss": 0.0463,
      "step": 2577
    },
    {
      "epoch": 0.15292442757147942,
      "grad_norm": 1.2546682357788086,
      "learning_rate": 1.8824149749538624e-05,
      "loss": 0.0153,
      "step": 2578
    },
    {
      "epoch": 0.15298374658915648,
      "grad_norm": 6.675426483154297,
      "learning_rate": 1.882283153176905e-05,
      "loss": 0.2652,
      "step": 2579
    },
    {
      "epoch": 0.15304306560683356,
      "grad_norm": 8.582898139953613,
      "learning_rate": 1.8821513313999472e-05,
      "loss": 0.1263,
      "step": 2580
    },
    {
      "epoch": 0.15310238462451062,
      "grad_norm": 18.605581283569336,
      "learning_rate": 1.8820195096229898e-05,
      "loss": 0.1551,
      "step": 2581
    },
    {
      "epoch": 0.15316170364218767,
      "grad_norm": 13.484723091125488,
      "learning_rate": 1.8818876878460324e-05,
      "loss": 0.3727,
      "step": 2582
    },
    {
      "epoch": 0.15322102265986476,
      "grad_norm": 0.5873247385025024,
      "learning_rate": 1.8817558660690746e-05,
      "loss": 0.0071,
      "step": 2583
    },
    {
      "epoch": 0.15328034167754181,
      "grad_norm": 20.891267776489258,
      "learning_rate": 1.8816240442921172e-05,
      "loss": 0.2627,
      "step": 2584
    },
    {
      "epoch": 0.1533396606952189,
      "grad_norm": 0.6290441155433655,
      "learning_rate": 1.8814922225151598e-05,
      "loss": 0.0045,
      "step": 2585
    },
    {
      "epoch": 0.15339897971289596,
      "grad_norm": 0.11907438188791275,
      "learning_rate": 1.881360400738202e-05,
      "loss": 0.0024,
      "step": 2586
    },
    {
      "epoch": 0.153458298730573,
      "grad_norm": 0.04495972767472267,
      "learning_rate": 1.8812285789612447e-05,
      "loss": 0.001,
      "step": 2587
    },
    {
      "epoch": 0.1535176177482501,
      "grad_norm": 0.6020511388778687,
      "learning_rate": 1.8810967571842873e-05,
      "loss": 0.0091,
      "step": 2588
    },
    {
      "epoch": 0.15357693676592715,
      "grad_norm": 17.626792907714844,
      "learning_rate": 1.8809649354073295e-05,
      "loss": 0.2982,
      "step": 2589
    },
    {
      "epoch": 0.15363625578360424,
      "grad_norm": 29.235530853271484,
      "learning_rate": 1.8808331136303717e-05,
      "loss": 0.3157,
      "step": 2590
    },
    {
      "epoch": 0.1536955748012813,
      "grad_norm": 1.3984487056732178,
      "learning_rate": 1.8807012918534143e-05,
      "loss": 0.0276,
      "step": 2591
    },
    {
      "epoch": 0.15375489381895835,
      "grad_norm": 14.128662109375,
      "learning_rate": 1.8805694700764566e-05,
      "loss": 0.1515,
      "step": 2592
    },
    {
      "epoch": 0.15381421283663543,
      "grad_norm": 0.3045355975627899,
      "learning_rate": 1.8804376482994992e-05,
      "loss": 0.005,
      "step": 2593
    },
    {
      "epoch": 0.1538735318543125,
      "grad_norm": 23.77847671508789,
      "learning_rate": 1.8803058265225418e-05,
      "loss": 0.584,
      "step": 2594
    },
    {
      "epoch": 0.15393285087198955,
      "grad_norm": 0.5510661005973816,
      "learning_rate": 1.880174004745584e-05,
      "loss": 0.0066,
      "step": 2595
    },
    {
      "epoch": 0.15399216988966663,
      "grad_norm": 9.616832733154297,
      "learning_rate": 1.8800421829686266e-05,
      "loss": 0.0926,
      "step": 2596
    },
    {
      "epoch": 0.1540514889073437,
      "grad_norm": 7.267475605010986,
      "learning_rate": 1.879910361191669e-05,
      "loss": 0.1198,
      "step": 2597
    },
    {
      "epoch": 0.15411080792502077,
      "grad_norm": 8.947003364562988,
      "learning_rate": 1.8797785394147114e-05,
      "loss": 0.3027,
      "step": 2598
    },
    {
      "epoch": 0.15417012694269783,
      "grad_norm": 15.829460144042969,
      "learning_rate": 1.879646717637754e-05,
      "loss": 0.2735,
      "step": 2599
    },
    {
      "epoch": 0.15422944596037488,
      "grad_norm": 0.005865918006747961,
      "learning_rate": 1.8795148958607963e-05,
      "loss": 0.0002,
      "step": 2600
    },
    {
      "epoch": 0.15428876497805197,
      "grad_norm": 0.061316199600696564,
      "learning_rate": 1.879383074083839e-05,
      "loss": 0.001,
      "step": 2601
    },
    {
      "epoch": 0.15434808399572902,
      "grad_norm": 8.71391773223877,
      "learning_rate": 1.8792512523068815e-05,
      "loss": 0.0872,
      "step": 2602
    },
    {
      "epoch": 0.1544074030134061,
      "grad_norm": 0.5003318190574646,
      "learning_rate": 1.8791194305299237e-05,
      "loss": 0.0067,
      "step": 2603
    },
    {
      "epoch": 0.15446672203108316,
      "grad_norm": 1.5647205114364624,
      "learning_rate": 1.8789876087529663e-05,
      "loss": 0.0199,
      "step": 2604
    },
    {
      "epoch": 0.15452604104876022,
      "grad_norm": 28.772911071777344,
      "learning_rate": 1.8788557869760085e-05,
      "loss": 0.6774,
      "step": 2605
    },
    {
      "epoch": 0.1545853600664373,
      "grad_norm": 3.3953912258148193,
      "learning_rate": 1.878723965199051e-05,
      "loss": 0.06,
      "step": 2606
    },
    {
      "epoch": 0.15464467908411436,
      "grad_norm": 6.757011890411377,
      "learning_rate": 1.8785921434220934e-05,
      "loss": 0.0501,
      "step": 2607
    },
    {
      "epoch": 0.15470399810179145,
      "grad_norm": 15.450982093811035,
      "learning_rate": 1.878460321645136e-05,
      "loss": 0.5109,
      "step": 2608
    },
    {
      "epoch": 0.1547633171194685,
      "grad_norm": 21.816757202148438,
      "learning_rate": 1.8783284998681782e-05,
      "loss": 1.3492,
      "step": 2609
    },
    {
      "epoch": 0.15482263613714556,
      "grad_norm": 14.492304801940918,
      "learning_rate": 1.8781966780912208e-05,
      "loss": 0.6704,
      "step": 2610
    },
    {
      "epoch": 0.15488195515482264,
      "grad_norm": 38.68092727661133,
      "learning_rate": 1.878064856314263e-05,
      "loss": 0.1648,
      "step": 2611
    },
    {
      "epoch": 0.1549412741724997,
      "grad_norm": 3.6716506481170654,
      "learning_rate": 1.8779330345373056e-05,
      "loss": 0.0295,
      "step": 2612
    },
    {
      "epoch": 0.15500059319017678,
      "grad_norm": 0.04392576217651367,
      "learning_rate": 1.8778012127603482e-05,
      "loss": 0.0006,
      "step": 2613
    },
    {
      "epoch": 0.15505991220785384,
      "grad_norm": 0.007020953577011824,
      "learning_rate": 1.8776693909833905e-05,
      "loss": 0.0002,
      "step": 2614
    },
    {
      "epoch": 0.1551192312255309,
      "grad_norm": 22.61075210571289,
      "learning_rate": 1.877537569206433e-05,
      "loss": 1.154,
      "step": 2615
    },
    {
      "epoch": 0.15517855024320798,
      "grad_norm": 3.7548725605010986,
      "learning_rate": 1.8774057474294757e-05,
      "loss": 0.0383,
      "step": 2616
    },
    {
      "epoch": 0.15523786926088504,
      "grad_norm": 3.5182759761810303,
      "learning_rate": 1.877273925652518e-05,
      "loss": 0.0582,
      "step": 2617
    },
    {
      "epoch": 0.15529718827856212,
      "grad_norm": 12.180319786071777,
      "learning_rate": 1.8771421038755605e-05,
      "loss": 0.6411,
      "step": 2618
    },
    {
      "epoch": 0.15535650729623918,
      "grad_norm": 10.895018577575684,
      "learning_rate": 1.877010282098603e-05,
      "loss": 0.1025,
      "step": 2619
    },
    {
      "epoch": 0.15541582631391623,
      "grad_norm": 0.9156516790390015,
      "learning_rate": 1.8768784603216453e-05,
      "loss": 0.0054,
      "step": 2620
    },
    {
      "epoch": 0.15547514533159332,
      "grad_norm": 19.929210662841797,
      "learning_rate": 1.876746638544688e-05,
      "loss": 0.5937,
      "step": 2621
    },
    {
      "epoch": 0.15553446434927037,
      "grad_norm": 14.343207359313965,
      "learning_rate": 1.87661481676773e-05,
      "loss": 0.2339,
      "step": 2622
    },
    {
      "epoch": 0.15559378336694743,
      "grad_norm": 0.04142604395747185,
      "learning_rate": 1.8764829949907724e-05,
      "loss": 0.0008,
      "step": 2623
    },
    {
      "epoch": 0.15565310238462451,
      "grad_norm": 10.927425384521484,
      "learning_rate": 1.876351173213815e-05,
      "loss": 0.1449,
      "step": 2624
    },
    {
      "epoch": 0.15571242140230157,
      "grad_norm": 7.736593723297119,
      "learning_rate": 1.8762193514368576e-05,
      "loss": 0.4203,
      "step": 2625
    },
    {
      "epoch": 0.15577174041997865,
      "grad_norm": 7.139978885650635,
      "learning_rate": 1.8760875296599e-05,
      "loss": 0.1016,
      "step": 2626
    },
    {
      "epoch": 0.1558310594376557,
      "grad_norm": 8.69079303741455,
      "learning_rate": 1.8759557078829424e-05,
      "loss": 0.0582,
      "step": 2627
    },
    {
      "epoch": 0.15589037845533277,
      "grad_norm": 12.246451377868652,
      "learning_rate": 1.8758238861059847e-05,
      "loss": 0.0973,
      "step": 2628
    },
    {
      "epoch": 0.15594969747300985,
      "grad_norm": 1.2898035049438477,
      "learning_rate": 1.8756920643290273e-05,
      "loss": 0.0102,
      "step": 2629
    },
    {
      "epoch": 0.1560090164906869,
      "grad_norm": 13.282346725463867,
      "learning_rate": 1.87556024255207e-05,
      "loss": 0.1855,
      "step": 2630
    },
    {
      "epoch": 0.156068335508364,
      "grad_norm": 0.296505331993103,
      "learning_rate": 1.875428420775112e-05,
      "loss": 0.0031,
      "step": 2631
    },
    {
      "epoch": 0.15612765452604105,
      "grad_norm": 9.78968620300293,
      "learning_rate": 1.8752965989981547e-05,
      "loss": 0.1091,
      "step": 2632
    },
    {
      "epoch": 0.1561869735437181,
      "grad_norm": 0.021326756104826927,
      "learning_rate": 1.8751647772211973e-05,
      "loss": 0.0005,
      "step": 2633
    },
    {
      "epoch": 0.1562462925613952,
      "grad_norm": 3.893747091293335,
      "learning_rate": 1.8750329554442395e-05,
      "loss": 0.0385,
      "step": 2634
    },
    {
      "epoch": 0.15630561157907225,
      "grad_norm": 126.1454849243164,
      "learning_rate": 1.874901133667282e-05,
      "loss": 0.5474,
      "step": 2635
    },
    {
      "epoch": 0.15636493059674933,
      "grad_norm": 20.052974700927734,
      "learning_rate": 1.8747693118903244e-05,
      "loss": 1.1914,
      "step": 2636
    },
    {
      "epoch": 0.15642424961442639,
      "grad_norm": 13.815947532653809,
      "learning_rate": 1.874637490113367e-05,
      "loss": 0.1266,
      "step": 2637
    },
    {
      "epoch": 0.15648356863210344,
      "grad_norm": 29.61931610107422,
      "learning_rate": 1.8745056683364092e-05,
      "loss": 0.596,
      "step": 2638
    },
    {
      "epoch": 0.15654288764978053,
      "grad_norm": 8.440863609313965,
      "learning_rate": 1.8743738465594518e-05,
      "loss": 0.3844,
      "step": 2639
    },
    {
      "epoch": 0.15660220666745758,
      "grad_norm": 6.157824993133545,
      "learning_rate": 1.874242024782494e-05,
      "loss": 0.1412,
      "step": 2640
    },
    {
      "epoch": 0.15666152568513467,
      "grad_norm": 3.731924295425415,
      "learning_rate": 1.8741102030055366e-05,
      "loss": 0.1934,
      "step": 2641
    },
    {
      "epoch": 0.15672084470281172,
      "grad_norm": 24.434499740600586,
      "learning_rate": 1.8739783812285792e-05,
      "loss": 0.3204,
      "step": 2642
    },
    {
      "epoch": 0.15678016372048878,
      "grad_norm": 3.1319949626922607,
      "learning_rate": 1.8738465594516215e-05,
      "loss": 0.037,
      "step": 2643
    },
    {
      "epoch": 0.15683948273816586,
      "grad_norm": 6.439969539642334,
      "learning_rate": 1.873714737674664e-05,
      "loss": 0.0499,
      "step": 2644
    },
    {
      "epoch": 0.15689880175584292,
      "grad_norm": 1.2286542654037476,
      "learning_rate": 1.8735829158977063e-05,
      "loss": 0.0135,
      "step": 2645
    },
    {
      "epoch": 0.15695812077351998,
      "grad_norm": 0.4043717086315155,
      "learning_rate": 1.873451094120749e-05,
      "loss": 0.0025,
      "step": 2646
    },
    {
      "epoch": 0.15701743979119706,
      "grad_norm": 0.4018852710723877,
      "learning_rate": 1.8733192723437915e-05,
      "loss": 0.0069,
      "step": 2647
    },
    {
      "epoch": 0.15707675880887412,
      "grad_norm": 0.06399551033973694,
      "learning_rate": 1.8731874505668337e-05,
      "loss": 0.0007,
      "step": 2648
    },
    {
      "epoch": 0.1571360778265512,
      "grad_norm": 5.097866535186768,
      "learning_rate": 1.8730556287898763e-05,
      "loss": 0.0523,
      "step": 2649
    },
    {
      "epoch": 0.15719539684422826,
      "grad_norm": 65.37672424316406,
      "learning_rate": 1.872923807012919e-05,
      "loss": 1.6828,
      "step": 2650
    },
    {
      "epoch": 0.15725471586190531,
      "grad_norm": 0.09702915698289871,
      "learning_rate": 1.872791985235961e-05,
      "loss": 0.002,
      "step": 2651
    },
    {
      "epoch": 0.1573140348795824,
      "grad_norm": 7.457293510437012,
      "learning_rate": 1.8726601634590037e-05,
      "loss": 0.0299,
      "step": 2652
    },
    {
      "epoch": 0.15737335389725945,
      "grad_norm": 0.013199795968830585,
      "learning_rate": 1.872528341682046e-05,
      "loss": 0.0004,
      "step": 2653
    },
    {
      "epoch": 0.15743267291493654,
      "grad_norm": 9.369741439819336,
      "learning_rate": 1.8723965199050886e-05,
      "loss": 0.4944,
      "step": 2654
    },
    {
      "epoch": 0.1574919919326136,
      "grad_norm": 0.11433505266904831,
      "learning_rate": 1.8722646981281308e-05,
      "loss": 0.0012,
      "step": 2655
    },
    {
      "epoch": 0.15755131095029065,
      "grad_norm": 0.13747304677963257,
      "learning_rate": 1.8721328763511734e-05,
      "loss": 0.002,
      "step": 2656
    },
    {
      "epoch": 0.15761062996796774,
      "grad_norm": 0.2941323518753052,
      "learning_rate": 1.8720010545742157e-05,
      "loss": 0.0034,
      "step": 2657
    },
    {
      "epoch": 0.1576699489856448,
      "grad_norm": 29.42167091369629,
      "learning_rate": 1.8718692327972583e-05,
      "loss": 0.0578,
      "step": 2658
    },
    {
      "epoch": 0.15772926800332188,
      "grad_norm": 13.46019172668457,
      "learning_rate": 1.8717374110203005e-05,
      "loss": 0.9446,
      "step": 2659
    },
    {
      "epoch": 0.15778858702099893,
      "grad_norm": 2.3339250087738037,
      "learning_rate": 1.871605589243343e-05,
      "loss": 0.0264,
      "step": 2660
    },
    {
      "epoch": 0.157847906038676,
      "grad_norm": 101.85581970214844,
      "learning_rate": 1.8714737674663857e-05,
      "loss": 0.6804,
      "step": 2661
    },
    {
      "epoch": 0.15790722505635307,
      "grad_norm": 6.055649280548096,
      "learning_rate": 1.871341945689428e-05,
      "loss": 0.0295,
      "step": 2662
    },
    {
      "epoch": 0.15796654407403013,
      "grad_norm": 0.15403509140014648,
      "learning_rate": 1.8712101239124705e-05,
      "loss": 0.0028,
      "step": 2663
    },
    {
      "epoch": 0.1580258630917072,
      "grad_norm": 1.9699501991271973,
      "learning_rate": 1.871078302135513e-05,
      "loss": 0.026,
      "step": 2664
    },
    {
      "epoch": 0.15808518210938427,
      "grad_norm": 8.678177833557129,
      "learning_rate": 1.8709464803585554e-05,
      "loss": 0.0468,
      "step": 2665
    },
    {
      "epoch": 0.15814450112706133,
      "grad_norm": 3.0304532051086426,
      "learning_rate": 1.870814658581598e-05,
      "loss": 0.0233,
      "step": 2666
    },
    {
      "epoch": 0.1582038201447384,
      "grad_norm": 3.6357245445251465,
      "learning_rate": 1.8706828368046402e-05,
      "loss": 0.0302,
      "step": 2667
    },
    {
      "epoch": 0.15826313916241547,
      "grad_norm": 0.18178129196166992,
      "learning_rate": 1.8705510150276828e-05,
      "loss": 0.0028,
      "step": 2668
    },
    {
      "epoch": 0.15832245818009255,
      "grad_norm": 0.12430000305175781,
      "learning_rate": 1.870419193250725e-05,
      "loss": 0.0019,
      "step": 2669
    },
    {
      "epoch": 0.1583817771977696,
      "grad_norm": 83.57182312011719,
      "learning_rate": 1.8702873714737676e-05,
      "loss": 1.2473,
      "step": 2670
    },
    {
      "epoch": 0.15844109621544666,
      "grad_norm": 11.446667671203613,
      "learning_rate": 1.87015554969681e-05,
      "loss": 0.1058,
      "step": 2671
    },
    {
      "epoch": 0.15850041523312375,
      "grad_norm": 11.066258430480957,
      "learning_rate": 1.8700237279198525e-05,
      "loss": 0.3325,
      "step": 2672
    },
    {
      "epoch": 0.1585597342508008,
      "grad_norm": 4.66424036026001,
      "learning_rate": 1.869891906142895e-05,
      "loss": 0.0569,
      "step": 2673
    },
    {
      "epoch": 0.15861905326847786,
      "grad_norm": 0.017733467742800713,
      "learning_rate": 1.8697600843659373e-05,
      "loss": 0.0004,
      "step": 2674
    },
    {
      "epoch": 0.15867837228615495,
      "grad_norm": 0.03089788928627968,
      "learning_rate": 1.86962826258898e-05,
      "loss": 0.0004,
      "step": 2675
    },
    {
      "epoch": 0.158737691303832,
      "grad_norm": 2.999112844467163,
      "learning_rate": 1.869496440812022e-05,
      "loss": 0.0252,
      "step": 2676
    },
    {
      "epoch": 0.15879701032150909,
      "grad_norm": 54.15508270263672,
      "learning_rate": 1.8693646190350647e-05,
      "loss": 0.3275,
      "step": 2677
    },
    {
      "epoch": 0.15885632933918614,
      "grad_norm": 9.995081901550293,
      "learning_rate": 1.8692327972581073e-05,
      "loss": 0.2827,
      "step": 2678
    },
    {
      "epoch": 0.1589156483568632,
      "grad_norm": 5.5772881507873535,
      "learning_rate": 1.8691009754811496e-05,
      "loss": 0.0977,
      "step": 2679
    },
    {
      "epoch": 0.15897496737454028,
      "grad_norm": 8.219141006469727,
      "learning_rate": 1.868969153704192e-05,
      "loss": 0.0761,
      "step": 2680
    },
    {
      "epoch": 0.15903428639221734,
      "grad_norm": 12.07861328125,
      "learning_rate": 1.8688373319272347e-05,
      "loss": 1.1746,
      "step": 2681
    },
    {
      "epoch": 0.15909360540989442,
      "grad_norm": 3.889251947402954,
      "learning_rate": 1.868705510150277e-05,
      "loss": 0.1866,
      "step": 2682
    },
    {
      "epoch": 0.15915292442757148,
      "grad_norm": 8.887144088745117,
      "learning_rate": 1.8685736883733196e-05,
      "loss": 0.6698,
      "step": 2683
    },
    {
      "epoch": 0.15921224344524854,
      "grad_norm": 2.515194892883301,
      "learning_rate": 1.8684418665963618e-05,
      "loss": 0.0315,
      "step": 2684
    },
    {
      "epoch": 0.15927156246292562,
      "grad_norm": 0.11966541409492493,
      "learning_rate": 1.8683100448194044e-05,
      "loss": 0.0014,
      "step": 2685
    },
    {
      "epoch": 0.15933088148060268,
      "grad_norm": 7.1017022132873535,
      "learning_rate": 1.8681782230424467e-05,
      "loss": 0.1822,
      "step": 2686
    },
    {
      "epoch": 0.15939020049827976,
      "grad_norm": 7.805623531341553,
      "learning_rate": 1.8680464012654892e-05,
      "loss": 0.1383,
      "step": 2687
    },
    {
      "epoch": 0.15944951951595682,
      "grad_norm": 0.17679601907730103,
      "learning_rate": 1.8679145794885315e-05,
      "loss": 0.0017,
      "step": 2688
    },
    {
      "epoch": 0.15950883853363387,
      "grad_norm": 4.532641410827637,
      "learning_rate": 1.867782757711574e-05,
      "loss": 0.0382,
      "step": 2689
    },
    {
      "epoch": 0.15956815755131096,
      "grad_norm": 10.547765731811523,
      "learning_rate": 1.8676509359346167e-05,
      "loss": 0.5157,
      "step": 2690
    },
    {
      "epoch": 0.159627476568988,
      "grad_norm": 0.8140497207641602,
      "learning_rate": 1.867519114157659e-05,
      "loss": 0.0055,
      "step": 2691
    },
    {
      "epoch": 0.1596867955866651,
      "grad_norm": 15.585112571716309,
      "learning_rate": 1.8673872923807015e-05,
      "loss": 0.0939,
      "step": 2692
    },
    {
      "epoch": 0.15974611460434215,
      "grad_norm": 12.035078048706055,
      "learning_rate": 1.8672554706037438e-05,
      "loss": 0.1519,
      "step": 2693
    },
    {
      "epoch": 0.1598054336220192,
      "grad_norm": 37.82016372680664,
      "learning_rate": 1.8671236488267863e-05,
      "loss": 0.054,
      "step": 2694
    },
    {
      "epoch": 0.1598647526396963,
      "grad_norm": 0.140389546751976,
      "learning_rate": 1.866991827049829e-05,
      "loss": 0.0015,
      "step": 2695
    },
    {
      "epoch": 0.15992407165737335,
      "grad_norm": 9.208401679992676,
      "learning_rate": 1.8668600052728712e-05,
      "loss": 0.0813,
      "step": 2696
    },
    {
      "epoch": 0.1599833906750504,
      "grad_norm": 0.06341336667537689,
      "learning_rate": 1.8667281834959138e-05,
      "loss": 0.0014,
      "step": 2697
    },
    {
      "epoch": 0.1600427096927275,
      "grad_norm": 40.111839294433594,
      "learning_rate": 1.8665963617189564e-05,
      "loss": 0.1301,
      "step": 2698
    },
    {
      "epoch": 0.16010202871040455,
      "grad_norm": 1.3977614641189575,
      "learning_rate": 1.8664645399419986e-05,
      "loss": 0.0101,
      "step": 2699
    },
    {
      "epoch": 0.16016134772808163,
      "grad_norm": 4.142885208129883,
      "learning_rate": 1.866332718165041e-05,
      "loss": 0.0546,
      "step": 2700
    },
    {
      "epoch": 0.1602206667457587,
      "grad_norm": 48.48392105102539,
      "learning_rate": 1.8662008963880834e-05,
      "loss": 0.7084,
      "step": 2701
    },
    {
      "epoch": 0.16027998576343574,
      "grad_norm": 12.172256469726562,
      "learning_rate": 1.8660690746111257e-05,
      "loss": 0.2961,
      "step": 2702
    },
    {
      "epoch": 0.16033930478111283,
      "grad_norm": 11.986839294433594,
      "learning_rate": 1.8659372528341683e-05,
      "loss": 0.1639,
      "step": 2703
    },
    {
      "epoch": 0.16039862379878989,
      "grad_norm": 0.04592769593000412,
      "learning_rate": 1.865805431057211e-05,
      "loss": 0.0009,
      "step": 2704
    },
    {
      "epoch": 0.16045794281646697,
      "grad_norm": 3.375819444656372,
      "learning_rate": 1.865673609280253e-05,
      "loss": 0.036,
      "step": 2705
    },
    {
      "epoch": 0.16051726183414403,
      "grad_norm": 0.22476564347743988,
      "learning_rate": 1.8655417875032957e-05,
      "loss": 0.0023,
      "step": 2706
    },
    {
      "epoch": 0.16057658085182108,
      "grad_norm": 4.973838806152344,
      "learning_rate": 1.865409965726338e-05,
      "loss": 0.0454,
      "step": 2707
    },
    {
      "epoch": 0.16063589986949817,
      "grad_norm": 11.738927841186523,
      "learning_rate": 1.8652781439493805e-05,
      "loss": 0.0399,
      "step": 2708
    },
    {
      "epoch": 0.16069521888717522,
      "grad_norm": 14.39861011505127,
      "learning_rate": 1.865146322172423e-05,
      "loss": 0.5587,
      "step": 2709
    },
    {
      "epoch": 0.1607545379048523,
      "grad_norm": 20.676149368286133,
      "learning_rate": 1.8650145003954654e-05,
      "loss": 0.6519,
      "step": 2710
    },
    {
      "epoch": 0.16081385692252936,
      "grad_norm": 0.4055356979370117,
      "learning_rate": 1.864882678618508e-05,
      "loss": 0.0059,
      "step": 2711
    },
    {
      "epoch": 0.16087317594020642,
      "grad_norm": 12.436216354370117,
      "learning_rate": 1.8647508568415506e-05,
      "loss": 0.2228,
      "step": 2712
    },
    {
      "epoch": 0.1609324949578835,
      "grad_norm": 6.334934234619141,
      "learning_rate": 1.8646190350645928e-05,
      "loss": 0.1691,
      "step": 2713
    },
    {
      "epoch": 0.16099181397556056,
      "grad_norm": 11.616257667541504,
      "learning_rate": 1.8644872132876354e-05,
      "loss": 0.1925,
      "step": 2714
    },
    {
      "epoch": 0.16105113299323764,
      "grad_norm": 0.07604622840881348,
      "learning_rate": 1.8643553915106776e-05,
      "loss": 0.0015,
      "step": 2715
    },
    {
      "epoch": 0.1611104520109147,
      "grad_norm": 64.3044662475586,
      "learning_rate": 1.8642235697337202e-05,
      "loss": 1.4003,
      "step": 2716
    },
    {
      "epoch": 0.16116977102859176,
      "grad_norm": 6.85292387008667,
      "learning_rate": 1.8640917479567625e-05,
      "loss": 0.2432,
      "step": 2717
    },
    {
      "epoch": 0.16122909004626884,
      "grad_norm": 8.793305397033691,
      "learning_rate": 1.863959926179805e-05,
      "loss": 0.18,
      "step": 2718
    },
    {
      "epoch": 0.1612884090639459,
      "grad_norm": 6.571407794952393,
      "learning_rate": 1.8638281044028473e-05,
      "loss": 0.2317,
      "step": 2719
    },
    {
      "epoch": 0.16134772808162298,
      "grad_norm": 6.026641845703125,
      "learning_rate": 1.86369628262589e-05,
      "loss": 0.0904,
      "step": 2720
    },
    {
      "epoch": 0.16140704709930004,
      "grad_norm": 7.607062816619873,
      "learning_rate": 1.8635644608489325e-05,
      "loss": 0.0939,
      "step": 2721
    },
    {
      "epoch": 0.1614663661169771,
      "grad_norm": 0.5517156720161438,
      "learning_rate": 1.8634326390719747e-05,
      "loss": 0.0049,
      "step": 2722
    },
    {
      "epoch": 0.16152568513465418,
      "grad_norm": 15.703920364379883,
      "learning_rate": 1.8633008172950173e-05,
      "loss": 0.2378,
      "step": 2723
    },
    {
      "epoch": 0.16158500415233124,
      "grad_norm": 28.51719093322754,
      "learning_rate": 1.8631689955180596e-05,
      "loss": 2.6519,
      "step": 2724
    },
    {
      "epoch": 0.1616443231700083,
      "grad_norm": 17.777061462402344,
      "learning_rate": 1.8630371737411022e-05,
      "loss": 0.1411,
      "step": 2725
    },
    {
      "epoch": 0.16170364218768538,
      "grad_norm": 1.42033052444458,
      "learning_rate": 1.8629053519641448e-05,
      "loss": 0.0125,
      "step": 2726
    },
    {
      "epoch": 0.16176296120536243,
      "grad_norm": 29.958524703979492,
      "learning_rate": 1.862773530187187e-05,
      "loss": 0.1373,
      "step": 2727
    },
    {
      "epoch": 0.16182228022303952,
      "grad_norm": 1.8177144527435303,
      "learning_rate": 1.8626417084102296e-05,
      "loss": 0.0097,
      "step": 2728
    },
    {
      "epoch": 0.16188159924071657,
      "grad_norm": 2.309152364730835,
      "learning_rate": 1.8625098866332722e-05,
      "loss": 0.0129,
      "step": 2729
    },
    {
      "epoch": 0.16194091825839363,
      "grad_norm": 0.02484491840004921,
      "learning_rate": 1.8623780648563144e-05,
      "loss": 0.0005,
      "step": 2730
    },
    {
      "epoch": 0.1620002372760707,
      "grad_norm": 3.442666530609131,
      "learning_rate": 1.862246243079357e-05,
      "loss": 0.0407,
      "step": 2731
    },
    {
      "epoch": 0.16205955629374777,
      "grad_norm": 0.16869181394577026,
      "learning_rate": 1.8621144213023993e-05,
      "loss": 0.0028,
      "step": 2732
    },
    {
      "epoch": 0.16211887531142485,
      "grad_norm": 29.38944435119629,
      "learning_rate": 1.861982599525442e-05,
      "loss": 0.253,
      "step": 2733
    },
    {
      "epoch": 0.1621781943291019,
      "grad_norm": 25.40329933166504,
      "learning_rate": 1.861850777748484e-05,
      "loss": 0.5162,
      "step": 2734
    },
    {
      "epoch": 0.16223751334677897,
      "grad_norm": 38.70746612548828,
      "learning_rate": 1.8617189559715267e-05,
      "loss": 1.7037,
      "step": 2735
    },
    {
      "epoch": 0.16229683236445605,
      "grad_norm": 1.1242386102676392,
      "learning_rate": 1.861587134194569e-05,
      "loss": 0.0075,
      "step": 2736
    },
    {
      "epoch": 0.1623561513821331,
      "grad_norm": 27.114543914794922,
      "learning_rate": 1.8614553124176115e-05,
      "loss": 0.1306,
      "step": 2737
    },
    {
      "epoch": 0.1624154703998102,
      "grad_norm": 8.016250610351562,
      "learning_rate": 1.861323490640654e-05,
      "loss": 0.1166,
      "step": 2738
    },
    {
      "epoch": 0.16247478941748725,
      "grad_norm": 11.072344779968262,
      "learning_rate": 1.8611916688636964e-05,
      "loss": 0.6116,
      "step": 2739
    },
    {
      "epoch": 0.1625341084351643,
      "grad_norm": 22.58310317993164,
      "learning_rate": 1.861059847086739e-05,
      "loss": 0.5231,
      "step": 2740
    },
    {
      "epoch": 0.1625934274528414,
      "grad_norm": 21.816837310791016,
      "learning_rate": 1.8609280253097812e-05,
      "loss": 0.2635,
      "step": 2741
    },
    {
      "epoch": 0.16265274647051844,
      "grad_norm": 16.63417625427246,
      "learning_rate": 1.8607962035328238e-05,
      "loss": 0.2618,
      "step": 2742
    },
    {
      "epoch": 0.16271206548819553,
      "grad_norm": 16.902175903320312,
      "learning_rate": 1.8606643817558664e-05,
      "loss": 0.2942,
      "step": 2743
    },
    {
      "epoch": 0.16277138450587259,
      "grad_norm": 0.8541404604911804,
      "learning_rate": 1.8605325599789086e-05,
      "loss": 0.0118,
      "step": 2744
    },
    {
      "epoch": 0.16283070352354964,
      "grad_norm": 0.1263098120689392,
      "learning_rate": 1.8604007382019512e-05,
      "loss": 0.0021,
      "step": 2745
    },
    {
      "epoch": 0.16289002254122673,
      "grad_norm": 4.802679538726807,
      "learning_rate": 1.8602689164249935e-05,
      "loss": 0.0886,
      "step": 2746
    },
    {
      "epoch": 0.16294934155890378,
      "grad_norm": 9.951200485229492,
      "learning_rate": 1.860137094648036e-05,
      "loss": 0.4876,
      "step": 2747
    },
    {
      "epoch": 0.16300866057658084,
      "grad_norm": 3.823683261871338,
      "learning_rate": 1.8600052728710783e-05,
      "loss": 0.12,
      "step": 2748
    },
    {
      "epoch": 0.16306797959425792,
      "grad_norm": 0.8134573101997375,
      "learning_rate": 1.859873451094121e-05,
      "loss": 0.0085,
      "step": 2749
    },
    {
      "epoch": 0.16312729861193498,
      "grad_norm": 0.1823648065328598,
      "learning_rate": 1.859741629317163e-05,
      "loss": 0.0027,
      "step": 2750
    },
    {
      "epoch": 0.16318661762961206,
      "grad_norm": 1.3667519092559814,
      "learning_rate": 1.8596098075402057e-05,
      "loss": 0.0177,
      "step": 2751
    },
    {
      "epoch": 0.16324593664728912,
      "grad_norm": 6.250197410583496,
      "learning_rate": 1.8594779857632483e-05,
      "loss": 0.1081,
      "step": 2752
    },
    {
      "epoch": 0.16330525566496618,
      "grad_norm": 34.994285583496094,
      "learning_rate": 1.8593461639862906e-05,
      "loss": 1.0494,
      "step": 2753
    },
    {
      "epoch": 0.16336457468264326,
      "grad_norm": 0.07767829298973083,
      "learning_rate": 1.859214342209333e-05,
      "loss": 0.0024,
      "step": 2754
    },
    {
      "epoch": 0.16342389370032032,
      "grad_norm": 14.278491973876953,
      "learning_rate": 1.8590825204323754e-05,
      "loss": 0.7119,
      "step": 2755
    },
    {
      "epoch": 0.1634832127179974,
      "grad_norm": 11.035259246826172,
      "learning_rate": 1.858950698655418e-05,
      "loss": 0.3797,
      "step": 2756
    },
    {
      "epoch": 0.16354253173567446,
      "grad_norm": 0.8516969084739685,
      "learning_rate": 1.8588188768784606e-05,
      "loss": 0.0108,
      "step": 2757
    },
    {
      "epoch": 0.1636018507533515,
      "grad_norm": 0.49021628499031067,
      "learning_rate": 1.858687055101503e-05,
      "loss": 0.0047,
      "step": 2758
    },
    {
      "epoch": 0.1636611697710286,
      "grad_norm": 7.170541286468506,
      "learning_rate": 1.8585552333245454e-05,
      "loss": 0.6261,
      "step": 2759
    },
    {
      "epoch": 0.16372048878870565,
      "grad_norm": 0.09128531813621521,
      "learning_rate": 1.858423411547588e-05,
      "loss": 0.0022,
      "step": 2760
    },
    {
      "epoch": 0.16377980780638274,
      "grad_norm": 20.11260223388672,
      "learning_rate": 1.8582915897706303e-05,
      "loss": 0.3234,
      "step": 2761
    },
    {
      "epoch": 0.1638391268240598,
      "grad_norm": 0.5270931720733643,
      "learning_rate": 1.858159767993673e-05,
      "loss": 0.0044,
      "step": 2762
    },
    {
      "epoch": 0.16389844584173685,
      "grad_norm": 0.2350032478570938,
      "learning_rate": 1.858027946216715e-05,
      "loss": 0.0027,
      "step": 2763
    },
    {
      "epoch": 0.16395776485941393,
      "grad_norm": 71.22297668457031,
      "learning_rate": 1.8578961244397577e-05,
      "loss": 1.165,
      "step": 2764
    },
    {
      "epoch": 0.164017083877091,
      "grad_norm": 1.2276030778884888,
      "learning_rate": 1.8577643026628e-05,
      "loss": 0.0097,
      "step": 2765
    },
    {
      "epoch": 0.16407640289476808,
      "grad_norm": 20.189420700073242,
      "learning_rate": 1.8576324808858425e-05,
      "loss": 0.2854,
      "step": 2766
    },
    {
      "epoch": 0.16413572191244513,
      "grad_norm": 0.7301182746887207,
      "learning_rate": 1.8575006591088848e-05,
      "loss": 0.0077,
      "step": 2767
    },
    {
      "epoch": 0.1641950409301222,
      "grad_norm": 5.143401622772217,
      "learning_rate": 1.8573688373319274e-05,
      "loss": 0.0826,
      "step": 2768
    },
    {
      "epoch": 0.16425435994779927,
      "grad_norm": 0.33739086985588074,
      "learning_rate": 1.85723701555497e-05,
      "loss": 0.006,
      "step": 2769
    },
    {
      "epoch": 0.16431367896547633,
      "grad_norm": 6.474942684173584,
      "learning_rate": 1.8571051937780122e-05,
      "loss": 0.1092,
      "step": 2770
    },
    {
      "epoch": 0.1643729979831534,
      "grad_norm": 3.6755502223968506,
      "learning_rate": 1.8569733720010548e-05,
      "loss": 0.1322,
      "step": 2771
    },
    {
      "epoch": 0.16443231700083047,
      "grad_norm": 22.34489631652832,
      "learning_rate": 1.856841550224097e-05,
      "loss": 0.3254,
      "step": 2772
    },
    {
      "epoch": 0.16449163601850753,
      "grad_norm": 1.0033396482467651,
      "learning_rate": 1.8567097284471396e-05,
      "loss": 0.0157,
      "step": 2773
    },
    {
      "epoch": 0.1645509550361846,
      "grad_norm": 0.04168764129281044,
      "learning_rate": 1.8565779066701822e-05,
      "loss": 0.001,
      "step": 2774
    },
    {
      "epoch": 0.16461027405386167,
      "grad_norm": 13.68709945678711,
      "learning_rate": 1.8564460848932245e-05,
      "loss": 1.5087,
      "step": 2775
    },
    {
      "epoch": 0.16466959307153872,
      "grad_norm": 3.0099244117736816,
      "learning_rate": 1.856314263116267e-05,
      "loss": 0.0935,
      "step": 2776
    },
    {
      "epoch": 0.1647289120892158,
      "grad_norm": 0.09358029812574387,
      "learning_rate": 1.8561824413393096e-05,
      "loss": 0.0021,
      "step": 2777
    },
    {
      "epoch": 0.16478823110689286,
      "grad_norm": 2.8098671436309814,
      "learning_rate": 1.856050619562352e-05,
      "loss": 0.0288,
      "step": 2778
    },
    {
      "epoch": 0.16484755012456995,
      "grad_norm": 7.919418811798096,
      "learning_rate": 1.855918797785394e-05,
      "loss": 0.3661,
      "step": 2779
    },
    {
      "epoch": 0.164906869142247,
      "grad_norm": 3.7953457832336426,
      "learning_rate": 1.8557869760084367e-05,
      "loss": 0.1747,
      "step": 2780
    },
    {
      "epoch": 0.16496618815992406,
      "grad_norm": 11.459207534790039,
      "learning_rate": 1.855655154231479e-05,
      "loss": 0.157,
      "step": 2781
    },
    {
      "epoch": 0.16502550717760114,
      "grad_norm": 13.846277236938477,
      "learning_rate": 1.8555233324545216e-05,
      "loss": 0.4261,
      "step": 2782
    },
    {
      "epoch": 0.1650848261952782,
      "grad_norm": 11.946967124938965,
      "learning_rate": 1.855391510677564e-05,
      "loss": 0.0991,
      "step": 2783
    },
    {
      "epoch": 0.16514414521295528,
      "grad_norm": 1.0711725950241089,
      "learning_rate": 1.8552596889006064e-05,
      "loss": 0.0089,
      "step": 2784
    },
    {
      "epoch": 0.16520346423063234,
      "grad_norm": 3.9818942546844482,
      "learning_rate": 1.855127867123649e-05,
      "loss": 0.0277,
      "step": 2785
    },
    {
      "epoch": 0.1652627832483094,
      "grad_norm": 1.1177111864089966,
      "learning_rate": 1.8549960453466916e-05,
      "loss": 0.0266,
      "step": 2786
    },
    {
      "epoch": 0.16532210226598648,
      "grad_norm": 2.3056626319885254,
      "learning_rate": 1.8548642235697338e-05,
      "loss": 0.0233,
      "step": 2787
    },
    {
      "epoch": 0.16538142128366354,
      "grad_norm": 3.89555287361145,
      "learning_rate": 1.8547324017927764e-05,
      "loss": 0.028,
      "step": 2788
    },
    {
      "epoch": 0.16544074030134062,
      "grad_norm": 0.3237697184085846,
      "learning_rate": 1.8546005800158187e-05,
      "loss": 0.0044,
      "step": 2789
    },
    {
      "epoch": 0.16550005931901768,
      "grad_norm": 1.7303520441055298,
      "learning_rate": 1.8544687582388613e-05,
      "loss": 0.0122,
      "step": 2790
    },
    {
      "epoch": 0.16555937833669473,
      "grad_norm": 10.71975040435791,
      "learning_rate": 1.854336936461904e-05,
      "loss": 0.1255,
      "step": 2791
    },
    {
      "epoch": 0.16561869735437182,
      "grad_norm": 3.8293466567993164,
      "learning_rate": 1.854205114684946e-05,
      "loss": 0.0463,
      "step": 2792
    },
    {
      "epoch": 0.16567801637204888,
      "grad_norm": 19.151290893554688,
      "learning_rate": 1.8540732929079887e-05,
      "loss": 1.7158,
      "step": 2793
    },
    {
      "epoch": 0.16573733538972596,
      "grad_norm": 7.631503582000732,
      "learning_rate": 1.853941471131031e-05,
      "loss": 0.3889,
      "step": 2794
    },
    {
      "epoch": 0.16579665440740302,
      "grad_norm": 18.521772384643555,
      "learning_rate": 1.8538096493540735e-05,
      "loss": 0.2394,
      "step": 2795
    },
    {
      "epoch": 0.16585597342508007,
      "grad_norm": 9.193873405456543,
      "learning_rate": 1.8536778275771158e-05,
      "loss": 0.1126,
      "step": 2796
    },
    {
      "epoch": 0.16591529244275716,
      "grad_norm": 0.22333234548568726,
      "learning_rate": 1.8535460058001584e-05,
      "loss": 0.003,
      "step": 2797
    },
    {
      "epoch": 0.1659746114604342,
      "grad_norm": 0.026328900828957558,
      "learning_rate": 1.8534141840232006e-05,
      "loss": 0.0006,
      "step": 2798
    },
    {
      "epoch": 0.16603393047811127,
      "grad_norm": 3.6363942623138428,
      "learning_rate": 1.8532823622462432e-05,
      "loss": 0.0493,
      "step": 2799
    },
    {
      "epoch": 0.16609324949578835,
      "grad_norm": 9.420591354370117,
      "learning_rate": 1.8531505404692858e-05,
      "loss": 0.5898,
      "step": 2800
    },
    {
      "epoch": 0.1661525685134654,
      "grad_norm": 0.060819778591394424,
      "learning_rate": 1.853018718692328e-05,
      "loss": 0.0012,
      "step": 2801
    },
    {
      "epoch": 0.1662118875311425,
      "grad_norm": 5.279304504394531,
      "learning_rate": 1.8528868969153706e-05,
      "loss": 0.0358,
      "step": 2802
    },
    {
      "epoch": 0.16627120654881955,
      "grad_norm": 0.058629296720027924,
      "learning_rate": 1.8527550751384132e-05,
      "loss": 0.0014,
      "step": 2803
    },
    {
      "epoch": 0.1663305255664966,
      "grad_norm": 7.089720726013184,
      "learning_rate": 1.8526232533614555e-05,
      "loss": 0.0511,
      "step": 2804
    },
    {
      "epoch": 0.1663898445841737,
      "grad_norm": 10.282233238220215,
      "learning_rate": 1.852491431584498e-05,
      "loss": 0.3139,
      "step": 2805
    },
    {
      "epoch": 0.16644916360185075,
      "grad_norm": 9.216889381408691,
      "learning_rate": 1.8523596098075403e-05,
      "loss": 0.143,
      "step": 2806
    },
    {
      "epoch": 0.16650848261952783,
      "grad_norm": 0.5592378377914429,
      "learning_rate": 1.852227788030583e-05,
      "loss": 0.0082,
      "step": 2807
    },
    {
      "epoch": 0.1665678016372049,
      "grad_norm": 13.84334945678711,
      "learning_rate": 1.8520959662536255e-05,
      "loss": 0.4925,
      "step": 2808
    },
    {
      "epoch": 0.16662712065488194,
      "grad_norm": 0.04997619241476059,
      "learning_rate": 1.8519641444766677e-05,
      "loss": 0.0011,
      "step": 2809
    },
    {
      "epoch": 0.16668643967255903,
      "grad_norm": 6.1915788650512695,
      "learning_rate": 1.8518323226997103e-05,
      "loss": 0.0499,
      "step": 2810
    },
    {
      "epoch": 0.16674575869023608,
      "grad_norm": 0.6198403835296631,
      "learning_rate": 1.8517005009227526e-05,
      "loss": 0.0056,
      "step": 2811
    },
    {
      "epoch": 0.16680507770791317,
      "grad_norm": 0.28880396485328674,
      "learning_rate": 1.8515686791457948e-05,
      "loss": 0.0042,
      "step": 2812
    },
    {
      "epoch": 0.16686439672559022,
      "grad_norm": 1.2048100233078003,
      "learning_rate": 1.8514368573688374e-05,
      "loss": 0.0167,
      "step": 2813
    },
    {
      "epoch": 0.16692371574326728,
      "grad_norm": 11.52757453918457,
      "learning_rate": 1.85130503559188e-05,
      "loss": 0.3558,
      "step": 2814
    },
    {
      "epoch": 0.16698303476094437,
      "grad_norm": 0.19258782267570496,
      "learning_rate": 1.8511732138149222e-05,
      "loss": 0.0017,
      "step": 2815
    },
    {
      "epoch": 0.16704235377862142,
      "grad_norm": 5.5498480796813965,
      "learning_rate": 1.8510413920379648e-05,
      "loss": 0.0552,
      "step": 2816
    },
    {
      "epoch": 0.1671016727962985,
      "grad_norm": 0.025121185928583145,
      "learning_rate": 1.8509095702610074e-05,
      "loss": 0.0006,
      "step": 2817
    },
    {
      "epoch": 0.16716099181397556,
      "grad_norm": 0.2488357126712799,
      "learning_rate": 1.8507777484840497e-05,
      "loss": 0.004,
      "step": 2818
    },
    {
      "epoch": 0.16722031083165262,
      "grad_norm": 0.1557730883359909,
      "learning_rate": 1.8506459267070922e-05,
      "loss": 0.0033,
      "step": 2819
    },
    {
      "epoch": 0.1672796298493297,
      "grad_norm": 0.03761296719312668,
      "learning_rate": 1.8505141049301345e-05,
      "loss": 0.0009,
      "step": 2820
    },
    {
      "epoch": 0.16733894886700676,
      "grad_norm": 4.759670257568359,
      "learning_rate": 1.850382283153177e-05,
      "loss": 0.0691,
      "step": 2821
    },
    {
      "epoch": 0.16739826788468384,
      "grad_norm": 17.84522247314453,
      "learning_rate": 1.8502504613762197e-05,
      "loss": 1.0973,
      "step": 2822
    },
    {
      "epoch": 0.1674575869023609,
      "grad_norm": 23.889806747436523,
      "learning_rate": 1.850118639599262e-05,
      "loss": 1.0245,
      "step": 2823
    },
    {
      "epoch": 0.16751690592003796,
      "grad_norm": 4.440423011779785,
      "learning_rate": 1.8499868178223045e-05,
      "loss": 0.0179,
      "step": 2824
    },
    {
      "epoch": 0.16757622493771504,
      "grad_norm": 0.47784748673439026,
      "learning_rate": 1.8498549960453468e-05,
      "loss": 0.0055,
      "step": 2825
    },
    {
      "epoch": 0.1676355439553921,
      "grad_norm": 8.075528144836426,
      "learning_rate": 1.8497231742683893e-05,
      "loss": 0.1032,
      "step": 2826
    },
    {
      "epoch": 0.16769486297306915,
      "grad_norm": 11.564414024353027,
      "learning_rate": 1.8495913524914316e-05,
      "loss": 0.1372,
      "step": 2827
    },
    {
      "epoch": 0.16775418199074624,
      "grad_norm": 1.2793947458267212,
      "learning_rate": 1.8494595307144742e-05,
      "loss": 0.0066,
      "step": 2828
    },
    {
      "epoch": 0.1678135010084233,
      "grad_norm": 0.9567810893058777,
      "learning_rate": 1.8493277089375164e-05,
      "loss": 0.006,
      "step": 2829
    },
    {
      "epoch": 0.16787282002610038,
      "grad_norm": 35.42384338378906,
      "learning_rate": 1.849195887160559e-05,
      "loss": 0.4504,
      "step": 2830
    },
    {
      "epoch": 0.16793213904377743,
      "grad_norm": 0.10176027566194534,
      "learning_rate": 1.8490640653836016e-05,
      "loss": 0.0024,
      "step": 2831
    },
    {
      "epoch": 0.1679914580614545,
      "grad_norm": 0.009159309789538383,
      "learning_rate": 1.848932243606644e-05,
      "loss": 0.0002,
      "step": 2832
    },
    {
      "epoch": 0.16805077707913157,
      "grad_norm": 2.221970796585083,
      "learning_rate": 1.8488004218296864e-05,
      "loss": 0.0182,
      "step": 2833
    },
    {
      "epoch": 0.16811009609680863,
      "grad_norm": 27.809431076049805,
      "learning_rate": 1.848668600052729e-05,
      "loss": 0.0852,
      "step": 2834
    },
    {
      "epoch": 0.16816941511448572,
      "grad_norm": 6.734440326690674,
      "learning_rate": 1.8485367782757713e-05,
      "loss": 0.0762,
      "step": 2835
    },
    {
      "epoch": 0.16822873413216277,
      "grad_norm": 15.56156063079834,
      "learning_rate": 1.848404956498814e-05,
      "loss": 0.3082,
      "step": 2836
    },
    {
      "epoch": 0.16828805314983983,
      "grad_norm": 25.321060180664062,
      "learning_rate": 1.848273134721856e-05,
      "loss": 0.4836,
      "step": 2837
    },
    {
      "epoch": 0.1683473721675169,
      "grad_norm": 6.5861711502075195,
      "learning_rate": 1.8481413129448987e-05,
      "loss": 0.1432,
      "step": 2838
    },
    {
      "epoch": 0.16840669118519397,
      "grad_norm": 0.10404439270496368,
      "learning_rate": 1.8480094911679413e-05,
      "loss": 0.0016,
      "step": 2839
    },
    {
      "epoch": 0.16846601020287105,
      "grad_norm": 7.498697757720947,
      "learning_rate": 1.8478776693909835e-05,
      "loss": 0.3162,
      "step": 2840
    },
    {
      "epoch": 0.1685253292205481,
      "grad_norm": 0.4669839143753052,
      "learning_rate": 1.847745847614026e-05,
      "loss": 0.0045,
      "step": 2841
    },
    {
      "epoch": 0.16858464823822517,
      "grad_norm": 22.942733764648438,
      "learning_rate": 1.8476140258370684e-05,
      "loss": 0.4157,
      "step": 2842
    },
    {
      "epoch": 0.16864396725590225,
      "grad_norm": 14.548659324645996,
      "learning_rate": 1.847482204060111e-05,
      "loss": 0.5447,
      "step": 2843
    },
    {
      "epoch": 0.1687032862735793,
      "grad_norm": 5.349157810211182,
      "learning_rate": 1.8473503822831532e-05,
      "loss": 0.236,
      "step": 2844
    },
    {
      "epoch": 0.1687626052912564,
      "grad_norm": 9.16482925415039,
      "learning_rate": 1.8472185605061958e-05,
      "loss": 0.0667,
      "step": 2845
    },
    {
      "epoch": 0.16882192430893345,
      "grad_norm": 0.06487158685922623,
      "learning_rate": 1.847086738729238e-05,
      "loss": 0.0005,
      "step": 2846
    },
    {
      "epoch": 0.1688812433266105,
      "grad_norm": 0.20519044995307922,
      "learning_rate": 1.8469549169522806e-05,
      "loss": 0.0039,
      "step": 2847
    },
    {
      "epoch": 0.1689405623442876,
      "grad_norm": 0.4153966009616852,
      "learning_rate": 1.8468230951753232e-05,
      "loss": 0.0074,
      "step": 2848
    },
    {
      "epoch": 0.16899988136196464,
      "grad_norm": 4.128667831420898,
      "learning_rate": 1.8466912733983655e-05,
      "loss": 0.0621,
      "step": 2849
    },
    {
      "epoch": 0.1690592003796417,
      "grad_norm": 8.39908504486084,
      "learning_rate": 1.846559451621408e-05,
      "loss": 0.3045,
      "step": 2850
    },
    {
      "epoch": 0.16911851939731878,
      "grad_norm": 13.78971004486084,
      "learning_rate": 1.8464276298444507e-05,
      "loss": 0.7591,
      "step": 2851
    },
    {
      "epoch": 0.16917783841499584,
      "grad_norm": 0.0481291301548481,
      "learning_rate": 1.846295808067493e-05,
      "loss": 0.0007,
      "step": 2852
    },
    {
      "epoch": 0.16923715743267292,
      "grad_norm": 0.1108979806303978,
      "learning_rate": 1.8461639862905355e-05,
      "loss": 0.0016,
      "step": 2853
    },
    {
      "epoch": 0.16929647645034998,
      "grad_norm": 2.7996528148651123,
      "learning_rate": 1.8460321645135777e-05,
      "loss": 0.0245,
      "step": 2854
    },
    {
      "epoch": 0.16935579546802704,
      "grad_norm": 38.11765670776367,
      "learning_rate": 1.8459003427366203e-05,
      "loss": 0.6034,
      "step": 2855
    },
    {
      "epoch": 0.16941511448570412,
      "grad_norm": 0.008046452887356281,
      "learning_rate": 1.8457685209596626e-05,
      "loss": 0.0002,
      "step": 2856
    },
    {
      "epoch": 0.16947443350338118,
      "grad_norm": 7.803646564483643,
      "learning_rate": 1.845636699182705e-05,
      "loss": 0.1589,
      "step": 2857
    },
    {
      "epoch": 0.16953375252105826,
      "grad_norm": 0.0409233458340168,
      "learning_rate": 1.8455048774057474e-05,
      "loss": 0.001,
      "step": 2858
    },
    {
      "epoch": 0.16959307153873532,
      "grad_norm": 0.6878541111946106,
      "learning_rate": 1.84537305562879e-05,
      "loss": 0.0066,
      "step": 2859
    },
    {
      "epoch": 0.16965239055641237,
      "grad_norm": 14.08178424835205,
      "learning_rate": 1.8452412338518323e-05,
      "loss": 0.5707,
      "step": 2860
    },
    {
      "epoch": 0.16971170957408946,
      "grad_norm": 21.15452003479004,
      "learning_rate": 1.845109412074875e-05,
      "loss": 0.7162,
      "step": 2861
    },
    {
      "epoch": 0.16977102859176652,
      "grad_norm": 17.0068359375,
      "learning_rate": 1.8449775902979174e-05,
      "loss": 0.252,
      "step": 2862
    },
    {
      "epoch": 0.1698303476094436,
      "grad_norm": 2.126504421234131,
      "learning_rate": 1.8448457685209597e-05,
      "loss": 0.0169,
      "step": 2863
    },
    {
      "epoch": 0.16988966662712066,
      "grad_norm": 6.089209079742432,
      "learning_rate": 1.8447139467440023e-05,
      "loss": 0.0826,
      "step": 2864
    },
    {
      "epoch": 0.1699489856447977,
      "grad_norm": 2.550435781478882,
      "learning_rate": 1.844582124967045e-05,
      "loss": 0.0136,
      "step": 2865
    },
    {
      "epoch": 0.1700083046624748,
      "grad_norm": 2.142508029937744,
      "learning_rate": 1.844450303190087e-05,
      "loss": 0.0177,
      "step": 2866
    },
    {
      "epoch": 0.17006762368015185,
      "grad_norm": 0.08387661725282669,
      "learning_rate": 1.8443184814131297e-05,
      "loss": 0.0019,
      "step": 2867
    },
    {
      "epoch": 0.17012694269782894,
      "grad_norm": 2.06400990486145,
      "learning_rate": 1.844186659636172e-05,
      "loss": 0.0509,
      "step": 2868
    },
    {
      "epoch": 0.170186261715506,
      "grad_norm": 0.05719926208257675,
      "learning_rate": 1.8440548378592145e-05,
      "loss": 0.0012,
      "step": 2869
    },
    {
      "epoch": 0.17024558073318305,
      "grad_norm": 0.07772447913885117,
      "learning_rate": 1.843923016082257e-05,
      "loss": 0.001,
      "step": 2870
    },
    {
      "epoch": 0.17030489975086013,
      "grad_norm": 14.21430778503418,
      "learning_rate": 1.8437911943052994e-05,
      "loss": 1.0263,
      "step": 2871
    },
    {
      "epoch": 0.1703642187685372,
      "grad_norm": 6.140172958374023,
      "learning_rate": 1.843659372528342e-05,
      "loss": 0.0511,
      "step": 2872
    },
    {
      "epoch": 0.17042353778621425,
      "grad_norm": 2.939201593399048,
      "learning_rate": 1.8435275507513842e-05,
      "loss": 0.1425,
      "step": 2873
    },
    {
      "epoch": 0.17048285680389133,
      "grad_norm": 3.450399160385132,
      "learning_rate": 1.8433957289744268e-05,
      "loss": 0.0484,
      "step": 2874
    },
    {
      "epoch": 0.1705421758215684,
      "grad_norm": 10.058005332946777,
      "learning_rate": 1.843263907197469e-05,
      "loss": 0.4244,
      "step": 2875
    },
    {
      "epoch": 0.17060149483924547,
      "grad_norm": 0.8899358510971069,
      "learning_rate": 1.8431320854205116e-05,
      "loss": 0.0075,
      "step": 2876
    },
    {
      "epoch": 0.17066081385692253,
      "grad_norm": 0.061293572187423706,
      "learning_rate": 1.843000263643554e-05,
      "loss": 0.0012,
      "step": 2877
    },
    {
      "epoch": 0.17072013287459958,
      "grad_norm": 13.978028297424316,
      "learning_rate": 1.8428684418665965e-05,
      "loss": 0.6383,
      "step": 2878
    },
    {
      "epoch": 0.17077945189227667,
      "grad_norm": 16.666786193847656,
      "learning_rate": 1.842736620089639e-05,
      "loss": 0.3895,
      "step": 2879
    },
    {
      "epoch": 0.17083877090995372,
      "grad_norm": 3.911513328552246,
      "learning_rate": 1.8426047983126813e-05,
      "loss": 0.0958,
      "step": 2880
    },
    {
      "epoch": 0.1708980899276308,
      "grad_norm": 9.345977783203125,
      "learning_rate": 1.842472976535724e-05,
      "loss": 0.7057,
      "step": 2881
    },
    {
      "epoch": 0.17095740894530786,
      "grad_norm": 4.474913120269775,
      "learning_rate": 1.8423411547587665e-05,
      "loss": 0.1625,
      "step": 2882
    },
    {
      "epoch": 0.17101672796298492,
      "grad_norm": 3.6803271770477295,
      "learning_rate": 1.8422093329818087e-05,
      "loss": 0.0295,
      "step": 2883
    },
    {
      "epoch": 0.171076046980662,
      "grad_norm": 11.655462265014648,
      "learning_rate": 1.8420775112048513e-05,
      "loss": 0.7355,
      "step": 2884
    },
    {
      "epoch": 0.17113536599833906,
      "grad_norm": 18.54805564880371,
      "learning_rate": 1.8419456894278936e-05,
      "loss": 0.3251,
      "step": 2885
    },
    {
      "epoch": 0.17119468501601615,
      "grad_norm": 1.356032371520996,
      "learning_rate": 1.841813867650936e-05,
      "loss": 0.008,
      "step": 2886
    },
    {
      "epoch": 0.1712540040336932,
      "grad_norm": 0.24280112981796265,
      "learning_rate": 1.8416820458739787e-05,
      "loss": 0.0024,
      "step": 2887
    },
    {
      "epoch": 0.17131332305137026,
      "grad_norm": 2.313291072845459,
      "learning_rate": 1.841550224097021e-05,
      "loss": 0.0121,
      "step": 2888
    },
    {
      "epoch": 0.17137264206904734,
      "grad_norm": 1.2539138793945312,
      "learning_rate": 1.8414184023200636e-05,
      "loss": 0.0054,
      "step": 2889
    },
    {
      "epoch": 0.1714319610867244,
      "grad_norm": 1.6824002265930176,
      "learning_rate": 1.841286580543106e-05,
      "loss": 0.0107,
      "step": 2890
    },
    {
      "epoch": 0.17149128010440148,
      "grad_norm": 8.373640060424805,
      "learning_rate": 1.841154758766148e-05,
      "loss": 0.3934,
      "step": 2891
    },
    {
      "epoch": 0.17155059912207854,
      "grad_norm": 0.3168707489967346,
      "learning_rate": 1.8410229369891907e-05,
      "loss": 0.0044,
      "step": 2892
    },
    {
      "epoch": 0.1716099181397556,
      "grad_norm": 2.9001083374023438,
      "learning_rate": 1.8408911152122333e-05,
      "loss": 0.0124,
      "step": 2893
    },
    {
      "epoch": 0.17166923715743268,
      "grad_norm": 3.7953736782073975,
      "learning_rate": 1.8407592934352755e-05,
      "loss": 0.0208,
      "step": 2894
    },
    {
      "epoch": 0.17172855617510974,
      "grad_norm": 27.147615432739258,
      "learning_rate": 1.840627471658318e-05,
      "loss": 0.4473,
      "step": 2895
    },
    {
      "epoch": 0.17178787519278682,
      "grad_norm": 0.8871435523033142,
      "learning_rate": 1.8404956498813607e-05,
      "loss": 0.01,
      "step": 2896
    },
    {
      "epoch": 0.17184719421046388,
      "grad_norm": 11.896194458007812,
      "learning_rate": 1.840363828104403e-05,
      "loss": 0.2845,
      "step": 2897
    },
    {
      "epoch": 0.17190651322814093,
      "grad_norm": 18.36395835876465,
      "learning_rate": 1.8402320063274455e-05,
      "loss": 0.294,
      "step": 2898
    },
    {
      "epoch": 0.17196583224581802,
      "grad_norm": 5.099323272705078,
      "learning_rate": 1.840100184550488e-05,
      "loss": 0.1433,
      "step": 2899
    },
    {
      "epoch": 0.17202515126349507,
      "grad_norm": 7.719983100891113,
      "learning_rate": 1.8399683627735304e-05,
      "loss": 0.1514,
      "step": 2900
    },
    {
      "epoch": 0.17208447028117213,
      "grad_norm": 0.5742883682250977,
      "learning_rate": 1.839836540996573e-05,
      "loss": 0.0062,
      "step": 2901
    },
    {
      "epoch": 0.17214378929884921,
      "grad_norm": 7.473989486694336,
      "learning_rate": 1.8397047192196152e-05,
      "loss": 0.028,
      "step": 2902
    },
    {
      "epoch": 0.17220310831652627,
      "grad_norm": 15.604905128479004,
      "learning_rate": 1.8395728974426578e-05,
      "loss": 0.2089,
      "step": 2903
    },
    {
      "epoch": 0.17226242733420336,
      "grad_norm": 19.595867156982422,
      "learning_rate": 1.8394410756657e-05,
      "loss": 0.9326,
      "step": 2904
    },
    {
      "epoch": 0.1723217463518804,
      "grad_norm": 18.09929084777832,
      "learning_rate": 1.8393092538887426e-05,
      "loss": 1.3077,
      "step": 2905
    },
    {
      "epoch": 0.17238106536955747,
      "grad_norm": 8.39977741241455,
      "learning_rate": 1.839177432111785e-05,
      "loss": 0.3419,
      "step": 2906
    },
    {
      "epoch": 0.17244038438723455,
      "grad_norm": 0.038872044533491135,
      "learning_rate": 1.8390456103348275e-05,
      "loss": 0.0008,
      "step": 2907
    },
    {
      "epoch": 0.1724997034049116,
      "grad_norm": 0.03292107954621315,
      "learning_rate": 1.8389137885578697e-05,
      "loss": 0.0009,
      "step": 2908
    },
    {
      "epoch": 0.1725590224225887,
      "grad_norm": 27.273672103881836,
      "learning_rate": 1.8387819667809123e-05,
      "loss": 1.1753,
      "step": 2909
    },
    {
      "epoch": 0.17261834144026575,
      "grad_norm": 16.872600555419922,
      "learning_rate": 1.838650145003955e-05,
      "loss": 0.2912,
      "step": 2910
    },
    {
      "epoch": 0.1726776604579428,
      "grad_norm": 0.2886142134666443,
      "learning_rate": 1.838518323226997e-05,
      "loss": 0.0041,
      "step": 2911
    },
    {
      "epoch": 0.1727369794756199,
      "grad_norm": 10.787534713745117,
      "learning_rate": 1.8383865014500397e-05,
      "loss": 0.1718,
      "step": 2912
    },
    {
      "epoch": 0.17279629849329695,
      "grad_norm": 2.945895195007324,
      "learning_rate": 1.8382546796730823e-05,
      "loss": 0.0312,
      "step": 2913
    },
    {
      "epoch": 0.17285561751097403,
      "grad_norm": 6.184098243713379,
      "learning_rate": 1.8381228578961246e-05,
      "loss": 0.0755,
      "step": 2914
    },
    {
      "epoch": 0.1729149365286511,
      "grad_norm": 8.083976745605469,
      "learning_rate": 1.837991036119167e-05,
      "loss": 0.3185,
      "step": 2915
    },
    {
      "epoch": 0.17297425554632814,
      "grad_norm": 2.212860345840454,
      "learning_rate": 1.8378592143422094e-05,
      "loss": 0.0449,
      "step": 2916
    },
    {
      "epoch": 0.17303357456400523,
      "grad_norm": 9.669843673706055,
      "learning_rate": 1.837727392565252e-05,
      "loss": 0.0738,
      "step": 2917
    },
    {
      "epoch": 0.17309289358168228,
      "grad_norm": 14.354804039001465,
      "learning_rate": 1.8375955707882946e-05,
      "loss": 0.1212,
      "step": 2918
    },
    {
      "epoch": 0.17315221259935937,
      "grad_norm": 5.7802863121032715,
      "learning_rate": 1.8374637490113368e-05,
      "loss": 0.0245,
      "step": 2919
    },
    {
      "epoch": 0.17321153161703642,
      "grad_norm": 16.66920280456543,
      "learning_rate": 1.8373319272343794e-05,
      "loss": 0.0787,
      "step": 2920
    },
    {
      "epoch": 0.17327085063471348,
      "grad_norm": 3.3840253353118896,
      "learning_rate": 1.8372001054574217e-05,
      "loss": 0.0559,
      "step": 2921
    },
    {
      "epoch": 0.17333016965239056,
      "grad_norm": 16.244794845581055,
      "learning_rate": 1.8370682836804642e-05,
      "loss": 0.4407,
      "step": 2922
    },
    {
      "epoch": 0.17338948867006762,
      "grad_norm": 0.17513200640678406,
      "learning_rate": 1.8369364619035065e-05,
      "loss": 0.0016,
      "step": 2923
    },
    {
      "epoch": 0.17344880768774468,
      "grad_norm": 16.412757873535156,
      "learning_rate": 1.836804640126549e-05,
      "loss": 0.267,
      "step": 2924
    },
    {
      "epoch": 0.17350812670542176,
      "grad_norm": 35.758121490478516,
      "learning_rate": 1.8366728183495913e-05,
      "loss": 0.1315,
      "step": 2925
    },
    {
      "epoch": 0.17356744572309882,
      "grad_norm": 0.07093760371208191,
      "learning_rate": 1.836540996572634e-05,
      "loss": 0.0019,
      "step": 2926
    },
    {
      "epoch": 0.1736267647407759,
      "grad_norm": 0.37802550196647644,
      "learning_rate": 1.8364091747956765e-05,
      "loss": 0.0066,
      "step": 2927
    },
    {
      "epoch": 0.17368608375845296,
      "grad_norm": 39.96974563598633,
      "learning_rate": 1.8362773530187188e-05,
      "loss": 1.3828,
      "step": 2928
    },
    {
      "epoch": 0.17374540277613001,
      "grad_norm": 25.68241310119629,
      "learning_rate": 1.8361455312417613e-05,
      "loss": 1.198,
      "step": 2929
    },
    {
      "epoch": 0.1738047217938071,
      "grad_norm": 6.862629413604736,
      "learning_rate": 1.836013709464804e-05,
      "loss": 0.1698,
      "step": 2930
    },
    {
      "epoch": 0.17386404081148416,
      "grad_norm": 26.223648071289062,
      "learning_rate": 1.8358818876878462e-05,
      "loss": 1.296,
      "step": 2931
    },
    {
      "epoch": 0.17392335982916124,
      "grad_norm": 21.740962982177734,
      "learning_rate": 1.8357500659108888e-05,
      "loss": 0.8487,
      "step": 2932
    },
    {
      "epoch": 0.1739826788468383,
      "grad_norm": 10.956831932067871,
      "learning_rate": 1.835618244133931e-05,
      "loss": 0.2282,
      "step": 2933
    },
    {
      "epoch": 0.17404199786451535,
      "grad_norm": 0.4516242742538452,
      "learning_rate": 1.8354864223569736e-05,
      "loss": 0.0062,
      "step": 2934
    },
    {
      "epoch": 0.17410131688219244,
      "grad_norm": 16.97788429260254,
      "learning_rate": 1.835354600580016e-05,
      "loss": 0.1394,
      "step": 2935
    },
    {
      "epoch": 0.1741606358998695,
      "grad_norm": 17.260021209716797,
      "learning_rate": 1.8352227788030584e-05,
      "loss": 0.5835,
      "step": 2936
    },
    {
      "epoch": 0.17421995491754658,
      "grad_norm": 2.619493007659912,
      "learning_rate": 1.8350909570261007e-05,
      "loss": 0.0271,
      "step": 2937
    },
    {
      "epoch": 0.17427927393522363,
      "grad_norm": 0.9397082924842834,
      "learning_rate": 1.8349591352491433e-05,
      "loss": 0.0075,
      "step": 2938
    },
    {
      "epoch": 0.1743385929529007,
      "grad_norm": 2.5529463291168213,
      "learning_rate": 1.8348273134721855e-05,
      "loss": 0.0315,
      "step": 2939
    },
    {
      "epoch": 0.17439791197057777,
      "grad_norm": 10.706647872924805,
      "learning_rate": 1.834695491695228e-05,
      "loss": 0.1974,
      "step": 2940
    },
    {
      "epoch": 0.17445723098825483,
      "grad_norm": 12.98784065246582,
      "learning_rate": 1.8345636699182707e-05,
      "loss": 0.4808,
      "step": 2941
    },
    {
      "epoch": 0.17451655000593191,
      "grad_norm": 26.54852294921875,
      "learning_rate": 1.834431848141313e-05,
      "loss": 0.284,
      "step": 2942
    },
    {
      "epoch": 0.17457586902360897,
      "grad_norm": 0.14167974889278412,
      "learning_rate": 1.8343000263643555e-05,
      "loss": 0.0023,
      "step": 2943
    },
    {
      "epoch": 0.17463518804128603,
      "grad_norm": 7.1234235763549805,
      "learning_rate": 1.834168204587398e-05,
      "loss": 0.1933,
      "step": 2944
    },
    {
      "epoch": 0.1746945070589631,
      "grad_norm": 0.06497744470834732,
      "learning_rate": 1.8340363828104404e-05,
      "loss": 0.0015,
      "step": 2945
    },
    {
      "epoch": 0.17475382607664017,
      "grad_norm": 10.034181594848633,
      "learning_rate": 1.833904561033483e-05,
      "loss": 0.1563,
      "step": 2946
    },
    {
      "epoch": 0.17481314509431725,
      "grad_norm": 0.034646376967430115,
      "learning_rate": 1.8337727392565256e-05,
      "loss": 0.0009,
      "step": 2947
    },
    {
      "epoch": 0.1748724641119943,
      "grad_norm": 6.763491153717041,
      "learning_rate": 1.8336409174795678e-05,
      "loss": 0.0827,
      "step": 2948
    },
    {
      "epoch": 0.17493178312967136,
      "grad_norm": 0.15436650812625885,
      "learning_rate": 1.8335090957026104e-05,
      "loss": 0.002,
      "step": 2949
    },
    {
      "epoch": 0.17499110214734845,
      "grad_norm": 4.571141242980957,
      "learning_rate": 1.8333772739256526e-05,
      "loss": 0.041,
      "step": 2950
    },
    {
      "epoch": 0.1750504211650255,
      "grad_norm": 8.04759407043457,
      "learning_rate": 1.8332454521486952e-05,
      "loss": 0.1486,
      "step": 2951
    },
    {
      "epoch": 0.17510974018270256,
      "grad_norm": 3.4886386394500732,
      "learning_rate": 1.8331136303717375e-05,
      "loss": 0.0459,
      "step": 2952
    },
    {
      "epoch": 0.17516905920037965,
      "grad_norm": 0.1548352837562561,
      "learning_rate": 1.83298180859478e-05,
      "loss": 0.0034,
      "step": 2953
    },
    {
      "epoch": 0.1752283782180567,
      "grad_norm": 0.6652868390083313,
      "learning_rate": 1.8328499868178223e-05,
      "loss": 0.009,
      "step": 2954
    },
    {
      "epoch": 0.17528769723573379,
      "grad_norm": 9.412031173706055,
      "learning_rate": 1.832718165040865e-05,
      "loss": 0.2378,
      "step": 2955
    },
    {
      "epoch": 0.17534701625341084,
      "grad_norm": 1.9268243312835693,
      "learning_rate": 1.832586343263907e-05,
      "loss": 0.0111,
      "step": 2956
    },
    {
      "epoch": 0.1754063352710879,
      "grad_norm": 0.07170942425727844,
      "learning_rate": 1.8324545214869497e-05,
      "loss": 0.0016,
      "step": 2957
    },
    {
      "epoch": 0.17546565428876498,
      "grad_norm": 8.175857543945312,
      "learning_rate": 1.8323226997099923e-05,
      "loss": 0.4627,
      "step": 2958
    },
    {
      "epoch": 0.17552497330644204,
      "grad_norm": 3.7410264015197754,
      "learning_rate": 1.8321908779330346e-05,
      "loss": 0.0201,
      "step": 2959
    },
    {
      "epoch": 0.17558429232411912,
      "grad_norm": 28.900543212890625,
      "learning_rate": 1.8320590561560772e-05,
      "loss": 1.1572,
      "step": 2960
    },
    {
      "epoch": 0.17564361134179618,
      "grad_norm": 0.03943328186869621,
      "learning_rate": 1.8319272343791198e-05,
      "loss": 0.0009,
      "step": 2961
    },
    {
      "epoch": 0.17570293035947324,
      "grad_norm": 0.02836211770772934,
      "learning_rate": 1.831795412602162e-05,
      "loss": 0.0006,
      "step": 2962
    },
    {
      "epoch": 0.17576224937715032,
      "grad_norm": 0.874674379825592,
      "learning_rate": 1.8316635908252046e-05,
      "loss": 0.0081,
      "step": 2963
    },
    {
      "epoch": 0.17582156839482738,
      "grad_norm": 11.351701736450195,
      "learning_rate": 1.831531769048247e-05,
      "loss": 0.4184,
      "step": 2964
    },
    {
      "epoch": 0.17588088741250446,
      "grad_norm": 9.50793170928955,
      "learning_rate": 1.8313999472712894e-05,
      "loss": 0.1772,
      "step": 2965
    },
    {
      "epoch": 0.17594020643018152,
      "grad_norm": 0.40086039900779724,
      "learning_rate": 1.831268125494332e-05,
      "loss": 0.0059,
      "step": 2966
    },
    {
      "epoch": 0.17599952544785857,
      "grad_norm": 0.03715866804122925,
      "learning_rate": 1.8311363037173743e-05,
      "loss": 0.0007,
      "step": 2967
    },
    {
      "epoch": 0.17605884446553566,
      "grad_norm": 31.543642044067383,
      "learning_rate": 1.8310044819404165e-05,
      "loss": 0.3812,
      "step": 2968
    },
    {
      "epoch": 0.17611816348321271,
      "grad_norm": 0.2334275096654892,
      "learning_rate": 1.830872660163459e-05,
      "loss": 0.0024,
      "step": 2969
    },
    {
      "epoch": 0.1761774825008898,
      "grad_norm": 10.053565979003906,
      "learning_rate": 1.8307408383865014e-05,
      "loss": 0.227,
      "step": 2970
    },
    {
      "epoch": 0.17623680151856685,
      "grad_norm": 7.199882984161377,
      "learning_rate": 1.830609016609544e-05,
      "loss": 0.3674,
      "step": 2971
    },
    {
      "epoch": 0.1762961205362439,
      "grad_norm": 13.980989456176758,
      "learning_rate": 1.8304771948325865e-05,
      "loss": 0.1611,
      "step": 2972
    },
    {
      "epoch": 0.176355439553921,
      "grad_norm": 0.06157372519373894,
      "learning_rate": 1.8303453730556288e-05,
      "loss": 0.0017,
      "step": 2973
    },
    {
      "epoch": 0.17641475857159805,
      "grad_norm": 6.168224334716797,
      "learning_rate": 1.8302135512786714e-05,
      "loss": 0.2312,
      "step": 2974
    },
    {
      "epoch": 0.1764740775892751,
      "grad_norm": 23.6213321685791,
      "learning_rate": 1.830081729501714e-05,
      "loss": 0.4599,
      "step": 2975
    },
    {
      "epoch": 0.1765333966069522,
      "grad_norm": 4.171760559082031,
      "learning_rate": 1.8299499077247562e-05,
      "loss": 0.0096,
      "step": 2976
    },
    {
      "epoch": 0.17659271562462925,
      "grad_norm": 5.170891284942627,
      "learning_rate": 1.8298180859477988e-05,
      "loss": 0.1885,
      "step": 2977
    },
    {
      "epoch": 0.17665203464230633,
      "grad_norm": 5.932503700256348,
      "learning_rate": 1.8296862641708414e-05,
      "loss": 0.2125,
      "step": 2978
    },
    {
      "epoch": 0.1767113536599834,
      "grad_norm": 2.6497602462768555,
      "learning_rate": 1.8295544423938836e-05,
      "loss": 0.0198,
      "step": 2979
    },
    {
      "epoch": 0.17677067267766045,
      "grad_norm": 26.863037109375,
      "learning_rate": 1.8294226206169262e-05,
      "loss": 0.8885,
      "step": 2980
    },
    {
      "epoch": 0.17682999169533753,
      "grad_norm": 0.3040842115879059,
      "learning_rate": 1.8292907988399685e-05,
      "loss": 0.0017,
      "step": 2981
    },
    {
      "epoch": 0.17688931071301459,
      "grad_norm": 3.353341579437256,
      "learning_rate": 1.829158977063011e-05,
      "loss": 0.041,
      "step": 2982
    },
    {
      "epoch": 0.17694862973069167,
      "grad_norm": 0.6465622782707214,
      "learning_rate": 1.8290271552860533e-05,
      "loss": 0.003,
      "step": 2983
    },
    {
      "epoch": 0.17700794874836873,
      "grad_norm": 18.552745819091797,
      "learning_rate": 1.828895333509096e-05,
      "loss": 0.1162,
      "step": 2984
    },
    {
      "epoch": 0.17706726776604578,
      "grad_norm": 5.60910701751709,
      "learning_rate": 1.828763511732138e-05,
      "loss": 0.0811,
      "step": 2985
    },
    {
      "epoch": 0.17712658678372287,
      "grad_norm": 0.22945785522460938,
      "learning_rate": 1.8286316899551807e-05,
      "loss": 0.0028,
      "step": 2986
    },
    {
      "epoch": 0.17718590580139992,
      "grad_norm": 1.4709738492965698,
      "learning_rate": 1.828499868178223e-05,
      "loss": 0.0366,
      "step": 2987
    },
    {
      "epoch": 0.177245224819077,
      "grad_norm": 7.887404441833496,
      "learning_rate": 1.8283680464012656e-05,
      "loss": 0.2421,
      "step": 2988
    },
    {
      "epoch": 0.17730454383675406,
      "grad_norm": 1.0099865198135376,
      "learning_rate": 1.828236224624308e-05,
      "loss": 0.0107,
      "step": 2989
    },
    {
      "epoch": 0.17736386285443112,
      "grad_norm": 5.450201511383057,
      "learning_rate": 1.8281044028473504e-05,
      "loss": 0.2406,
      "step": 2990
    },
    {
      "epoch": 0.1774231818721082,
      "grad_norm": 8.975449562072754,
      "learning_rate": 1.827972581070393e-05,
      "loss": 0.126,
      "step": 2991
    },
    {
      "epoch": 0.17748250088978526,
      "grad_norm": 0.20388837158679962,
      "learning_rate": 1.8278407592934356e-05,
      "loss": 0.0029,
      "step": 2992
    },
    {
      "epoch": 0.17754181990746234,
      "grad_norm": 3.7458581924438477,
      "learning_rate": 1.827708937516478e-05,
      "loss": 0.0725,
      "step": 2993
    },
    {
      "epoch": 0.1776011389251394,
      "grad_norm": 0.09588733315467834,
      "learning_rate": 1.8275771157395204e-05,
      "loss": 0.0019,
      "step": 2994
    },
    {
      "epoch": 0.17766045794281646,
      "grad_norm": 24.2772274017334,
      "learning_rate": 1.827445293962563e-05,
      "loss": 0.8253,
      "step": 2995
    },
    {
      "epoch": 0.17771977696049354,
      "grad_norm": 0.037153229117393494,
      "learning_rate": 1.8273134721856053e-05,
      "loss": 0.001,
      "step": 2996
    },
    {
      "epoch": 0.1777790959781706,
      "grad_norm": 7.627035617828369,
      "learning_rate": 1.827181650408648e-05,
      "loss": 0.2129,
      "step": 2997
    },
    {
      "epoch": 0.17783841499584768,
      "grad_norm": 1.458943247795105,
      "learning_rate": 1.82704982863169e-05,
      "loss": 0.0143,
      "step": 2998
    },
    {
      "epoch": 0.17789773401352474,
      "grad_norm": 9.47538948059082,
      "learning_rate": 1.8269180068547327e-05,
      "loss": 0.14,
      "step": 2999
    },
    {
      "epoch": 0.1779570530312018,
      "grad_norm": 0.07351141422986984,
      "learning_rate": 1.826786185077775e-05,
      "loss": 0.0024,
      "step": 3000
    },
    {
      "epoch": 0.17801637204887888,
      "grad_norm": 14.856865882873535,
      "learning_rate": 1.8266543633008172e-05,
      "loss": 0.1945,
      "step": 3001
    },
    {
      "epoch": 0.17807569106655594,
      "grad_norm": 0.6979709267616272,
      "learning_rate": 1.8265225415238598e-05,
      "loss": 0.0105,
      "step": 3002
    },
    {
      "epoch": 0.178135010084233,
      "grad_norm": 3.0406501293182373,
      "learning_rate": 1.8263907197469024e-05,
      "loss": 0.0486,
      "step": 3003
    },
    {
      "epoch": 0.17819432910191008,
      "grad_norm": 10.730271339416504,
      "learning_rate": 1.8262588979699446e-05,
      "loss": 0.1434,
      "step": 3004
    },
    {
      "epoch": 0.17825364811958713,
      "grad_norm": 1.5232845544815063,
      "learning_rate": 1.8261270761929872e-05,
      "loss": 0.0115,
      "step": 3005
    },
    {
      "epoch": 0.17831296713726422,
      "grad_norm": 0.03258923068642616,
      "learning_rate": 1.8259952544160298e-05,
      "loss": 0.0007,
      "step": 3006
    },
    {
      "epoch": 0.17837228615494127,
      "grad_norm": 0.040022578090429306,
      "learning_rate": 1.825863432639072e-05,
      "loss": 0.0007,
      "step": 3007
    },
    {
      "epoch": 0.17843160517261833,
      "grad_norm": 32.89390563964844,
      "learning_rate": 1.8257316108621146e-05,
      "loss": 0.5619,
      "step": 3008
    },
    {
      "epoch": 0.1784909241902954,
      "grad_norm": 7.869905471801758,
      "learning_rate": 1.8255997890851572e-05,
      "loss": 0.1603,
      "step": 3009
    },
    {
      "epoch": 0.17855024320797247,
      "grad_norm": 0.057135988026857376,
      "learning_rate": 1.8254679673081995e-05,
      "loss": 0.0015,
      "step": 3010
    },
    {
      "epoch": 0.17860956222564955,
      "grad_norm": 0.23752275109291077,
      "learning_rate": 1.825336145531242e-05,
      "loss": 0.0043,
      "step": 3011
    },
    {
      "epoch": 0.1786688812433266,
      "grad_norm": 5.797214508056641,
      "learning_rate": 1.8252043237542843e-05,
      "loss": 0.1639,
      "step": 3012
    },
    {
      "epoch": 0.17872820026100367,
      "grad_norm": 17.928972244262695,
      "learning_rate": 1.825072501977327e-05,
      "loss": 0.5174,
      "step": 3013
    },
    {
      "epoch": 0.17878751927868075,
      "grad_norm": 18.296361923217773,
      "learning_rate": 1.824940680200369e-05,
      "loss": 0.6629,
      "step": 3014
    },
    {
      "epoch": 0.1788468382963578,
      "grad_norm": 0.15042012929916382,
      "learning_rate": 1.8248088584234117e-05,
      "loss": 0.0022,
      "step": 3015
    },
    {
      "epoch": 0.1789061573140349,
      "grad_norm": 0.9194031953811646,
      "learning_rate": 1.824677036646454e-05,
      "loss": 0.0112,
      "step": 3016
    },
    {
      "epoch": 0.17896547633171195,
      "grad_norm": 5.320298671722412,
      "learning_rate": 1.8245452148694966e-05,
      "loss": 0.0317,
      "step": 3017
    },
    {
      "epoch": 0.179024795349389,
      "grad_norm": 1.644760251045227,
      "learning_rate": 1.8244133930925388e-05,
      "loss": 0.0101,
      "step": 3018
    },
    {
      "epoch": 0.1790841143670661,
      "grad_norm": 1.4747912883758545,
      "learning_rate": 1.8242815713155814e-05,
      "loss": 0.0093,
      "step": 3019
    },
    {
      "epoch": 0.17914343338474314,
      "grad_norm": 39.93227005004883,
      "learning_rate": 1.824149749538624e-05,
      "loss": 0.402,
      "step": 3020
    },
    {
      "epoch": 0.17920275240242023,
      "grad_norm": 5.180537700653076,
      "learning_rate": 1.8240179277616662e-05,
      "loss": 0.1037,
      "step": 3021
    },
    {
      "epoch": 0.17926207142009729,
      "grad_norm": 0.24274124205112457,
      "learning_rate": 1.8238861059847088e-05,
      "loss": 0.0041,
      "step": 3022
    },
    {
      "epoch": 0.17932139043777434,
      "grad_norm": 1.3062862157821655,
      "learning_rate": 1.8237542842077514e-05,
      "loss": 0.0077,
      "step": 3023
    },
    {
      "epoch": 0.17938070945545143,
      "grad_norm": 0.13651011884212494,
      "learning_rate": 1.8236224624307937e-05,
      "loss": 0.003,
      "step": 3024
    },
    {
      "epoch": 0.17944002847312848,
      "grad_norm": 0.05107912793755531,
      "learning_rate": 1.8234906406538363e-05,
      "loss": 0.0011,
      "step": 3025
    },
    {
      "epoch": 0.17949934749080554,
      "grad_norm": 3.3109726905822754,
      "learning_rate": 1.823358818876879e-05,
      "loss": 0.0193,
      "step": 3026
    },
    {
      "epoch": 0.17955866650848262,
      "grad_norm": 4.6328535079956055,
      "learning_rate": 1.823226997099921e-05,
      "loss": 0.0827,
      "step": 3027
    },
    {
      "epoch": 0.17961798552615968,
      "grad_norm": 3.8238108158111572,
      "learning_rate": 1.8230951753229637e-05,
      "loss": 0.0827,
      "step": 3028
    },
    {
      "epoch": 0.17967730454383676,
      "grad_norm": 7.020920276641846,
      "learning_rate": 1.822963353546006e-05,
      "loss": 0.1313,
      "step": 3029
    },
    {
      "epoch": 0.17973662356151382,
      "grad_norm": 8.031659126281738,
      "learning_rate": 1.8228315317690485e-05,
      "loss": 0.1831,
      "step": 3030
    },
    {
      "epoch": 0.17979594257919088,
      "grad_norm": 0.007310525514185429,
      "learning_rate": 1.8226997099920908e-05,
      "loss": 0.0003,
      "step": 3031
    },
    {
      "epoch": 0.17985526159686796,
      "grad_norm": 48.9086799621582,
      "learning_rate": 1.8225678882151334e-05,
      "loss": 0.5363,
      "step": 3032
    },
    {
      "epoch": 0.17991458061454502,
      "grad_norm": 6.864792346954346,
      "learning_rate": 1.8224360664381756e-05,
      "loss": 0.0377,
      "step": 3033
    },
    {
      "epoch": 0.1799738996322221,
      "grad_norm": 0.07230240851640701,
      "learning_rate": 1.8223042446612182e-05,
      "loss": 0.0007,
      "step": 3034
    },
    {
      "epoch": 0.18003321864989916,
      "grad_norm": 0.024564700201153755,
      "learning_rate": 1.8221724228842604e-05,
      "loss": 0.0003,
      "step": 3035
    },
    {
      "epoch": 0.1800925376675762,
      "grad_norm": 1.6758865118026733,
      "learning_rate": 1.822040601107303e-05,
      "loss": 0.0094,
      "step": 3036
    },
    {
      "epoch": 0.1801518566852533,
      "grad_norm": 29.5509090423584,
      "learning_rate": 1.8219087793303456e-05,
      "loss": 0.3373,
      "step": 3037
    },
    {
      "epoch": 0.18021117570293035,
      "grad_norm": 9.707958221435547,
      "learning_rate": 1.821776957553388e-05,
      "loss": 0.0748,
      "step": 3038
    },
    {
      "epoch": 0.18027049472060744,
      "grad_norm": 1.670261263847351,
      "learning_rate": 1.8216451357764305e-05,
      "loss": 0.017,
      "step": 3039
    },
    {
      "epoch": 0.1803298137382845,
      "grad_norm": 2.0665464401245117,
      "learning_rate": 1.821513313999473e-05,
      "loss": 0.0122,
      "step": 3040
    },
    {
      "epoch": 0.18038913275596155,
      "grad_norm": 32.426631927490234,
      "learning_rate": 1.8213814922225153e-05,
      "loss": 2.483,
      "step": 3041
    },
    {
      "epoch": 0.18044845177363864,
      "grad_norm": 54.533905029296875,
      "learning_rate": 1.821249670445558e-05,
      "loss": 0.509,
      "step": 3042
    },
    {
      "epoch": 0.1805077707913157,
      "grad_norm": 5.881855487823486,
      "learning_rate": 1.8211178486686005e-05,
      "loss": 0.2559,
      "step": 3043
    },
    {
      "epoch": 0.18056708980899278,
      "grad_norm": 1.5875297784805298,
      "learning_rate": 1.8209860268916427e-05,
      "loss": 0.0052,
      "step": 3044
    },
    {
      "epoch": 0.18062640882666983,
      "grad_norm": 0.3208972215652466,
      "learning_rate": 1.820854205114685e-05,
      "loss": 0.0023,
      "step": 3045
    },
    {
      "epoch": 0.1806857278443469,
      "grad_norm": 0.5659656524658203,
      "learning_rate": 1.8207223833377276e-05,
      "loss": 0.0031,
      "step": 3046
    },
    {
      "epoch": 0.18074504686202397,
      "grad_norm": 8.03080940246582,
      "learning_rate": 1.8205905615607698e-05,
      "loss": 0.0607,
      "step": 3047
    },
    {
      "epoch": 0.18080436587970103,
      "grad_norm": 12.766935348510742,
      "learning_rate": 1.8204587397838124e-05,
      "loss": 0.0643,
      "step": 3048
    },
    {
      "epoch": 0.1808636848973781,
      "grad_norm": 0.07496665418148041,
      "learning_rate": 1.8203269180068546e-05,
      "loss": 0.001,
      "step": 3049
    },
    {
      "epoch": 0.18092300391505517,
      "grad_norm": 23.29660987854004,
      "learning_rate": 1.8201950962298972e-05,
      "loss": 0.2553,
      "step": 3050
    },
    {
      "epoch": 0.18098232293273223,
      "grad_norm": 20.98567008972168,
      "learning_rate": 1.8200632744529398e-05,
      "loss": 0.4167,
      "step": 3051
    },
    {
      "epoch": 0.1810416419504093,
      "grad_norm": 1.1865276098251343,
      "learning_rate": 1.819931452675982e-05,
      "loss": 0.0048,
      "step": 3052
    },
    {
      "epoch": 0.18110096096808637,
      "grad_norm": 45.61739730834961,
      "learning_rate": 1.8197996308990247e-05,
      "loss": 0.1328,
      "step": 3053
    },
    {
      "epoch": 0.18116027998576342,
      "grad_norm": 17.351030349731445,
      "learning_rate": 1.8196678091220672e-05,
      "loss": 1.1554,
      "step": 3054
    },
    {
      "epoch": 0.1812195990034405,
      "grad_norm": 0.00894036516547203,
      "learning_rate": 1.8195359873451095e-05,
      "loss": 0.0001,
      "step": 3055
    },
    {
      "epoch": 0.18127891802111756,
      "grad_norm": 5.129783630371094,
      "learning_rate": 1.819404165568152e-05,
      "loss": 0.0277,
      "step": 3056
    },
    {
      "epoch": 0.18133823703879465,
      "grad_norm": 0.018665123730897903,
      "learning_rate": 1.8192723437911947e-05,
      "loss": 0.0005,
      "step": 3057
    },
    {
      "epoch": 0.1813975560564717,
      "grad_norm": 12.991498947143555,
      "learning_rate": 1.819140522014237e-05,
      "loss": 0.1015,
      "step": 3058
    },
    {
      "epoch": 0.18145687507414876,
      "grad_norm": 6.49168586730957,
      "learning_rate": 1.8190087002372795e-05,
      "loss": 0.0781,
      "step": 3059
    },
    {
      "epoch": 0.18151619409182584,
      "grad_norm": 0.14577870070934296,
      "learning_rate": 1.8188768784603218e-05,
      "loss": 0.0024,
      "step": 3060
    },
    {
      "epoch": 0.1815755131095029,
      "grad_norm": 19.794662475585938,
      "learning_rate": 1.8187450566833643e-05,
      "loss": 0.4167,
      "step": 3061
    },
    {
      "epoch": 0.18163483212717998,
      "grad_norm": 1.1424161195755005,
      "learning_rate": 1.8186132349064066e-05,
      "loss": 0.0068,
      "step": 3062
    },
    {
      "epoch": 0.18169415114485704,
      "grad_norm": 12.532424926757812,
      "learning_rate": 1.8184814131294492e-05,
      "loss": 0.5194,
      "step": 3063
    },
    {
      "epoch": 0.1817534701625341,
      "grad_norm": 18.459728240966797,
      "learning_rate": 1.8183495913524914e-05,
      "loss": 0.4625,
      "step": 3064
    },
    {
      "epoch": 0.18181278918021118,
      "grad_norm": 26.35531997680664,
      "learning_rate": 1.818217769575534e-05,
      "loss": 2.0003,
      "step": 3065
    },
    {
      "epoch": 0.18187210819788824,
      "grad_norm": 0.008167000487446785,
      "learning_rate": 1.8180859477985763e-05,
      "loss": 0.0002,
      "step": 3066
    },
    {
      "epoch": 0.18193142721556532,
      "grad_norm": 7.9694342613220215,
      "learning_rate": 1.817954126021619e-05,
      "loss": 0.0869,
      "step": 3067
    },
    {
      "epoch": 0.18199074623324238,
      "grad_norm": 0.07471899688243866,
      "learning_rate": 1.8178223042446614e-05,
      "loss": 0.0014,
      "step": 3068
    },
    {
      "epoch": 0.18205006525091944,
      "grad_norm": 42.64321517944336,
      "learning_rate": 1.8176904824677037e-05,
      "loss": 0.785,
      "step": 3069
    },
    {
      "epoch": 0.18210938426859652,
      "grad_norm": 10.961688995361328,
      "learning_rate": 1.8175586606907463e-05,
      "loss": 0.1819,
      "step": 3070
    },
    {
      "epoch": 0.18216870328627358,
      "grad_norm": 3.295822858810425,
      "learning_rate": 1.817426838913789e-05,
      "loss": 0.063,
      "step": 3071
    },
    {
      "epoch": 0.18222802230395066,
      "grad_norm": 0.403158575296402,
      "learning_rate": 1.817295017136831e-05,
      "loss": 0.0038,
      "step": 3072
    },
    {
      "epoch": 0.18228734132162772,
      "grad_norm": 2.4652209281921387,
      "learning_rate": 1.8171631953598737e-05,
      "loss": 0.0259,
      "step": 3073
    },
    {
      "epoch": 0.18234666033930477,
      "grad_norm": 0.24660903215408325,
      "learning_rate": 1.8170313735829163e-05,
      "loss": 0.0046,
      "step": 3074
    },
    {
      "epoch": 0.18240597935698186,
      "grad_norm": 55.96663284301758,
      "learning_rate": 1.8168995518059585e-05,
      "loss": 0.3265,
      "step": 3075
    },
    {
      "epoch": 0.1824652983746589,
      "grad_norm": 21.10027313232422,
      "learning_rate": 1.816767730029001e-05,
      "loss": 0.7667,
      "step": 3076
    },
    {
      "epoch": 0.18252461739233597,
      "grad_norm": 0.006834839005023241,
      "learning_rate": 1.8166359082520434e-05,
      "loss": 0.0002,
      "step": 3077
    },
    {
      "epoch": 0.18258393641001305,
      "grad_norm": 11.061760902404785,
      "learning_rate": 1.816504086475086e-05,
      "loss": 0.3886,
      "step": 3078
    },
    {
      "epoch": 0.1826432554276901,
      "grad_norm": 7.289721965789795,
      "learning_rate": 1.8163722646981282e-05,
      "loss": 0.0225,
      "step": 3079
    },
    {
      "epoch": 0.1827025744453672,
      "grad_norm": 7.621547698974609,
      "learning_rate": 1.8162404429211705e-05,
      "loss": 0.154,
      "step": 3080
    },
    {
      "epoch": 0.18276189346304425,
      "grad_norm": 0.30190393328666687,
      "learning_rate": 1.816108621144213e-05,
      "loss": 0.002,
      "step": 3081
    },
    {
      "epoch": 0.1828212124807213,
      "grad_norm": 0.04217267408967018,
      "learning_rate": 1.8159767993672556e-05,
      "loss": 0.0007,
      "step": 3082
    },
    {
      "epoch": 0.1828805314983984,
      "grad_norm": 17.383926391601562,
      "learning_rate": 1.815844977590298e-05,
      "loss": 0.5416,
      "step": 3083
    },
    {
      "epoch": 0.18293985051607545,
      "grad_norm": 8.330094337463379,
      "learning_rate": 1.8157131558133405e-05,
      "loss": 0.1346,
      "step": 3084
    },
    {
      "epoch": 0.18299916953375253,
      "grad_norm": 0.019025083631277084,
      "learning_rate": 1.815581334036383e-05,
      "loss": 0.0006,
      "step": 3085
    },
    {
      "epoch": 0.1830584885514296,
      "grad_norm": 11.772575378417969,
      "learning_rate": 1.8154495122594253e-05,
      "loss": 0.2714,
      "step": 3086
    },
    {
      "epoch": 0.18311780756910664,
      "grad_norm": 25.38407325744629,
      "learning_rate": 1.815317690482468e-05,
      "loss": 0.1697,
      "step": 3087
    },
    {
      "epoch": 0.18317712658678373,
      "grad_norm": 0.2639903128147125,
      "learning_rate": 1.8151858687055105e-05,
      "loss": 0.0023,
      "step": 3088
    },
    {
      "epoch": 0.18323644560446078,
      "grad_norm": 13.08780574798584,
      "learning_rate": 1.8150540469285527e-05,
      "loss": 0.3693,
      "step": 3089
    },
    {
      "epoch": 0.18329576462213787,
      "grad_norm": 0.010232055559754372,
      "learning_rate": 1.8149222251515953e-05,
      "loss": 0.0002,
      "step": 3090
    },
    {
      "epoch": 0.18335508363981493,
      "grad_norm": 5.302673816680908,
      "learning_rate": 1.8147904033746376e-05,
      "loss": 0.076,
      "step": 3091
    },
    {
      "epoch": 0.18341440265749198,
      "grad_norm": 0.49546295404434204,
      "learning_rate": 1.8146585815976802e-05,
      "loss": 0.0069,
      "step": 3092
    },
    {
      "epoch": 0.18347372167516907,
      "grad_norm": 21.11577033996582,
      "learning_rate": 1.8145267598207224e-05,
      "loss": 0.1416,
      "step": 3093
    },
    {
      "epoch": 0.18353304069284612,
      "grad_norm": 0.09281943738460541,
      "learning_rate": 1.814394938043765e-05,
      "loss": 0.0011,
      "step": 3094
    },
    {
      "epoch": 0.1835923597105232,
      "grad_norm": 0.12283570319414139,
      "learning_rate": 1.8142631162668073e-05,
      "loss": 0.0022,
      "step": 3095
    },
    {
      "epoch": 0.18365167872820026,
      "grad_norm": 2.646247625350952,
      "learning_rate": 1.81413129448985e-05,
      "loss": 0.0379,
      "step": 3096
    },
    {
      "epoch": 0.18371099774587732,
      "grad_norm": 41.4331169128418,
      "learning_rate": 1.813999472712892e-05,
      "loss": 1.5276,
      "step": 3097
    },
    {
      "epoch": 0.1837703167635544,
      "grad_norm": 0.04047931730747223,
      "learning_rate": 1.8138676509359347e-05,
      "loss": 0.0013,
      "step": 3098
    },
    {
      "epoch": 0.18382963578123146,
      "grad_norm": 22.616697311401367,
      "learning_rate": 1.8137358291589773e-05,
      "loss": 0.3475,
      "step": 3099
    },
    {
      "epoch": 0.18388895479890854,
      "grad_norm": 0.29028570652008057,
      "learning_rate": 1.8136040073820195e-05,
      "loss": 0.0047,
      "step": 3100
    },
    {
      "epoch": 0.1839482738165856,
      "grad_norm": 0.022637424990534782,
      "learning_rate": 1.813472185605062e-05,
      "loss": 0.0007,
      "step": 3101
    },
    {
      "epoch": 0.18400759283426266,
      "grad_norm": 4.743149757385254,
      "learning_rate": 1.8133403638281047e-05,
      "loss": 0.0418,
      "step": 3102
    },
    {
      "epoch": 0.18406691185193974,
      "grad_norm": 21.17791748046875,
      "learning_rate": 1.813208542051147e-05,
      "loss": 0.1544,
      "step": 3103
    },
    {
      "epoch": 0.1841262308696168,
      "grad_norm": 36.804012298583984,
      "learning_rate": 1.8130767202741895e-05,
      "loss": 0.5591,
      "step": 3104
    },
    {
      "epoch": 0.18418554988729385,
      "grad_norm": 8.250218391418457,
      "learning_rate": 1.812944898497232e-05,
      "loss": 0.1398,
      "step": 3105
    },
    {
      "epoch": 0.18424486890497094,
      "grad_norm": 0.1143885999917984,
      "learning_rate": 1.8128130767202744e-05,
      "loss": 0.001,
      "step": 3106
    },
    {
      "epoch": 0.184304187922648,
      "grad_norm": 1.0886751413345337,
      "learning_rate": 1.812681254943317e-05,
      "loss": 0.0148,
      "step": 3107
    },
    {
      "epoch": 0.18436350694032508,
      "grad_norm": 20.547096252441406,
      "learning_rate": 1.8125494331663592e-05,
      "loss": 0.1233,
      "step": 3108
    },
    {
      "epoch": 0.18442282595800213,
      "grad_norm": 77.78324890136719,
      "learning_rate": 1.8124176113894018e-05,
      "loss": 2.4577,
      "step": 3109
    },
    {
      "epoch": 0.1844821449756792,
      "grad_norm": 5.248298168182373,
      "learning_rate": 1.812285789612444e-05,
      "loss": 0.13,
      "step": 3110
    },
    {
      "epoch": 0.18454146399335628,
      "grad_norm": 3.9341914653778076,
      "learning_rate": 1.8121539678354866e-05,
      "loss": 0.0254,
      "step": 3111
    },
    {
      "epoch": 0.18460078301103333,
      "grad_norm": 0.036924608051776886,
      "learning_rate": 1.812022146058529e-05,
      "loss": 0.0007,
      "step": 3112
    },
    {
      "epoch": 0.18466010202871042,
      "grad_norm": 13.785930633544922,
      "learning_rate": 1.8118903242815715e-05,
      "loss": 0.1632,
      "step": 3113
    },
    {
      "epoch": 0.18471942104638747,
      "grad_norm": 3.6571707725524902,
      "learning_rate": 1.8117585025046137e-05,
      "loss": 0.0208,
      "step": 3114
    },
    {
      "epoch": 0.18477874006406453,
      "grad_norm": 0.18358927965164185,
      "learning_rate": 1.8116266807276563e-05,
      "loss": 0.0025,
      "step": 3115
    },
    {
      "epoch": 0.1848380590817416,
      "grad_norm": 0.5533802509307861,
      "learning_rate": 1.811494858950699e-05,
      "loss": 0.0067,
      "step": 3116
    },
    {
      "epoch": 0.18489737809941867,
      "grad_norm": 0.033045798540115356,
      "learning_rate": 1.811363037173741e-05,
      "loss": 0.0007,
      "step": 3117
    },
    {
      "epoch": 0.18495669711709575,
      "grad_norm": 0.07731811702251434,
      "learning_rate": 1.8112312153967837e-05,
      "loss": 0.0018,
      "step": 3118
    },
    {
      "epoch": 0.1850160161347728,
      "grad_norm": 0.01992911845445633,
      "learning_rate": 1.8110993936198263e-05,
      "loss": 0.0004,
      "step": 3119
    },
    {
      "epoch": 0.18507533515244987,
      "grad_norm": 0.6697704195976257,
      "learning_rate": 1.8109675718428686e-05,
      "loss": 0.0055,
      "step": 3120
    },
    {
      "epoch": 0.18513465417012695,
      "grad_norm": 7.285688877105713,
      "learning_rate": 1.810835750065911e-05,
      "loss": 0.6159,
      "step": 3121
    },
    {
      "epoch": 0.185193973187804,
      "grad_norm": 1.0123015642166138,
      "learning_rate": 1.8107039282889537e-05,
      "loss": 0.0145,
      "step": 3122
    },
    {
      "epoch": 0.1852532922054811,
      "grad_norm": 0.8225608468055725,
      "learning_rate": 1.810572106511996e-05,
      "loss": 0.006,
      "step": 3123
    },
    {
      "epoch": 0.18531261122315815,
      "grad_norm": 2.5569324493408203,
      "learning_rate": 1.8104402847350382e-05,
      "loss": 0.0293,
      "step": 3124
    },
    {
      "epoch": 0.1853719302408352,
      "grad_norm": 9.058073997497559,
      "learning_rate": 1.810308462958081e-05,
      "loss": 0.0326,
      "step": 3125
    },
    {
      "epoch": 0.1854312492585123,
      "grad_norm": 2.270859956741333,
      "learning_rate": 1.810176641181123e-05,
      "loss": 0.0449,
      "step": 3126
    },
    {
      "epoch": 0.18549056827618934,
      "grad_norm": 0.009042182937264442,
      "learning_rate": 1.8100448194041657e-05,
      "loss": 0.0003,
      "step": 3127
    },
    {
      "epoch": 0.1855498872938664,
      "grad_norm": 18.80624771118164,
      "learning_rate": 1.809912997627208e-05,
      "loss": 0.4182,
      "step": 3128
    },
    {
      "epoch": 0.18560920631154348,
      "grad_norm": 3.0156359672546387,
      "learning_rate": 1.8097811758502505e-05,
      "loss": 0.032,
      "step": 3129
    },
    {
      "epoch": 0.18566852532922054,
      "grad_norm": 0.5486946105957031,
      "learning_rate": 1.809649354073293e-05,
      "loss": 0.0073,
      "step": 3130
    },
    {
      "epoch": 0.18572784434689762,
      "grad_norm": 4.320284366607666,
      "learning_rate": 1.8095175322963353e-05,
      "loss": 0.0549,
      "step": 3131
    },
    {
      "epoch": 0.18578716336457468,
      "grad_norm": 10.1475248336792,
      "learning_rate": 1.809385710519378e-05,
      "loss": 0.3137,
      "step": 3132
    },
    {
      "epoch": 0.18584648238225174,
      "grad_norm": 7.105892658233643,
      "learning_rate": 1.8092538887424205e-05,
      "loss": 0.2837,
      "step": 3133
    },
    {
      "epoch": 0.18590580139992882,
      "grad_norm": 0.03139251843094826,
      "learning_rate": 1.8091220669654628e-05,
      "loss": 0.0008,
      "step": 3134
    },
    {
      "epoch": 0.18596512041760588,
      "grad_norm": 0.5053955912590027,
      "learning_rate": 1.8089902451885054e-05,
      "loss": 0.005,
      "step": 3135
    },
    {
      "epoch": 0.18602443943528296,
      "grad_norm": 14.436129570007324,
      "learning_rate": 1.808858423411548e-05,
      "loss": 0.4014,
      "step": 3136
    },
    {
      "epoch": 0.18608375845296002,
      "grad_norm": 0.2847502827644348,
      "learning_rate": 1.8087266016345902e-05,
      "loss": 0.0043,
      "step": 3137
    },
    {
      "epoch": 0.18614307747063707,
      "grad_norm": 0.0028693689964711666,
      "learning_rate": 1.8085947798576328e-05,
      "loss": 0.0001,
      "step": 3138
    },
    {
      "epoch": 0.18620239648831416,
      "grad_norm": 4.894200801849365,
      "learning_rate": 1.808462958080675e-05,
      "loss": 0.0992,
      "step": 3139
    },
    {
      "epoch": 0.18626171550599122,
      "grad_norm": 7.3197021484375,
      "learning_rate": 1.8083311363037176e-05,
      "loss": 0.093,
      "step": 3140
    },
    {
      "epoch": 0.1863210345236683,
      "grad_norm": 0.61469566822052,
      "learning_rate": 1.80819931452676e-05,
      "loss": 0.0088,
      "step": 3141
    },
    {
      "epoch": 0.18638035354134536,
      "grad_norm": 1.211839199066162,
      "learning_rate": 1.8080674927498025e-05,
      "loss": 0.0073,
      "step": 3142
    },
    {
      "epoch": 0.1864396725590224,
      "grad_norm": 43.50532913208008,
      "learning_rate": 1.8079356709728447e-05,
      "loss": 1.4855,
      "step": 3143
    },
    {
      "epoch": 0.1864989915766995,
      "grad_norm": 0.10194773226976395,
      "learning_rate": 1.8078038491958873e-05,
      "loss": 0.002,
      "step": 3144
    },
    {
      "epoch": 0.18655831059437655,
      "grad_norm": 0.050717178732156754,
      "learning_rate": 1.8076720274189295e-05,
      "loss": 0.0012,
      "step": 3145
    },
    {
      "epoch": 0.18661762961205364,
      "grad_norm": 10.488219261169434,
      "learning_rate": 1.807540205641972e-05,
      "loss": 0.0926,
      "step": 3146
    },
    {
      "epoch": 0.1866769486297307,
      "grad_norm": 4.650278568267822,
      "learning_rate": 1.8074083838650147e-05,
      "loss": 0.065,
      "step": 3147
    },
    {
      "epoch": 0.18673626764740775,
      "grad_norm": 32.88347244262695,
      "learning_rate": 1.807276562088057e-05,
      "loss": 0.9781,
      "step": 3148
    },
    {
      "epoch": 0.18679558666508483,
      "grad_norm": 0.11202514171600342,
      "learning_rate": 1.8071447403110996e-05,
      "loss": 0.0019,
      "step": 3149
    },
    {
      "epoch": 0.1868549056827619,
      "grad_norm": 6.29433536529541,
      "learning_rate": 1.807012918534142e-05,
      "loss": 0.0314,
      "step": 3150
    },
    {
      "epoch": 0.18691422470043897,
      "grad_norm": 0.005313957575708628,
      "learning_rate": 1.8068810967571844e-05,
      "loss": 0.0002,
      "step": 3151
    },
    {
      "epoch": 0.18697354371811603,
      "grad_norm": 28.162260055541992,
      "learning_rate": 1.806749274980227e-05,
      "loss": 0.4136,
      "step": 3152
    },
    {
      "epoch": 0.1870328627357931,
      "grad_norm": 0.043954476714134216,
      "learning_rate": 1.8066174532032696e-05,
      "loss": 0.0011,
      "step": 3153
    },
    {
      "epoch": 0.18709218175347017,
      "grad_norm": 20.95368003845215,
      "learning_rate": 1.8064856314263118e-05,
      "loss": 0.4068,
      "step": 3154
    },
    {
      "epoch": 0.18715150077114723,
      "grad_norm": 4.842772960662842,
      "learning_rate": 1.8063538096493544e-05,
      "loss": 0.0846,
      "step": 3155
    },
    {
      "epoch": 0.18721081978882428,
      "grad_norm": 19.733423233032227,
      "learning_rate": 1.8062219878723967e-05,
      "loss": 0.3008,
      "step": 3156
    },
    {
      "epoch": 0.18727013880650137,
      "grad_norm": 0.011000256054103374,
      "learning_rate": 1.806090166095439e-05,
      "loss": 0.0002,
      "step": 3157
    },
    {
      "epoch": 0.18732945782417842,
      "grad_norm": 4.5226731300354,
      "learning_rate": 1.8059583443184815e-05,
      "loss": 0.0184,
      "step": 3158
    },
    {
      "epoch": 0.1873887768418555,
      "grad_norm": 0.04761524498462677,
      "learning_rate": 1.8058265225415237e-05,
      "loss": 0.0012,
      "step": 3159
    },
    {
      "epoch": 0.18744809585953257,
      "grad_norm": 2.398362398147583,
      "learning_rate": 1.8056947007645663e-05,
      "loss": 0.017,
      "step": 3160
    },
    {
      "epoch": 0.18750741487720962,
      "grad_norm": 13.542309761047363,
      "learning_rate": 1.805562878987609e-05,
      "loss": 1.112,
      "step": 3161
    },
    {
      "epoch": 0.1875667338948867,
      "grad_norm": 1.4392563104629517,
      "learning_rate": 1.8054310572106512e-05,
      "loss": 0.01,
      "step": 3162
    },
    {
      "epoch": 0.18762605291256376,
      "grad_norm": 4.065104007720947,
      "learning_rate": 1.8052992354336938e-05,
      "loss": 0.0673,
      "step": 3163
    },
    {
      "epoch": 0.18768537193024085,
      "grad_norm": 0.011388425715267658,
      "learning_rate": 1.8051674136567364e-05,
      "loss": 0.0002,
      "step": 3164
    },
    {
      "epoch": 0.1877446909479179,
      "grad_norm": 15.529675483703613,
      "learning_rate": 1.8050355918797786e-05,
      "loss": 0.299,
      "step": 3165
    },
    {
      "epoch": 0.18780400996559496,
      "grad_norm": 8.392434120178223,
      "learning_rate": 1.8049037701028212e-05,
      "loss": 0.3256,
      "step": 3166
    },
    {
      "epoch": 0.18786332898327204,
      "grad_norm": 25.61981964111328,
      "learning_rate": 1.8047719483258638e-05,
      "loss": 0.6872,
      "step": 3167
    },
    {
      "epoch": 0.1879226480009491,
      "grad_norm": 0.02633240818977356,
      "learning_rate": 1.804640126548906e-05,
      "loss": 0.0005,
      "step": 3168
    },
    {
      "epoch": 0.18798196701862618,
      "grad_norm": 23.901063919067383,
      "learning_rate": 1.8045083047719486e-05,
      "loss": 0.4056,
      "step": 3169
    },
    {
      "epoch": 0.18804128603630324,
      "grad_norm": 1.7935099601745605,
      "learning_rate": 1.804376482994991e-05,
      "loss": 0.0116,
      "step": 3170
    },
    {
      "epoch": 0.1881006050539803,
      "grad_norm": 70.95045471191406,
      "learning_rate": 1.8042446612180335e-05,
      "loss": 0.1731,
      "step": 3171
    },
    {
      "epoch": 0.18815992407165738,
      "grad_norm": 0.020405106246471405,
      "learning_rate": 1.8041128394410757e-05,
      "loss": 0.0005,
      "step": 3172
    },
    {
      "epoch": 0.18821924308933444,
      "grad_norm": 7.184200286865234,
      "learning_rate": 1.8039810176641183e-05,
      "loss": 0.0659,
      "step": 3173
    },
    {
      "epoch": 0.18827856210701152,
      "grad_norm": 0.05576006695628166,
      "learning_rate": 1.8038491958871605e-05,
      "loss": 0.0012,
      "step": 3174
    },
    {
      "epoch": 0.18833788112468858,
      "grad_norm": 8.507064819335938,
      "learning_rate": 1.803717374110203e-05,
      "loss": 0.3888,
      "step": 3175
    },
    {
      "epoch": 0.18839720014236563,
      "grad_norm": 0.05701259896159172,
      "learning_rate": 1.8035855523332454e-05,
      "loss": 0.0009,
      "step": 3176
    },
    {
      "epoch": 0.18845651916004272,
      "grad_norm": 21.467327117919922,
      "learning_rate": 1.803453730556288e-05,
      "loss": 1.6042,
      "step": 3177
    },
    {
      "epoch": 0.18851583817771977,
      "grad_norm": 39.38412857055664,
      "learning_rate": 1.8033219087793306e-05,
      "loss": 1.6896,
      "step": 3178
    },
    {
      "epoch": 0.18857515719539683,
      "grad_norm": 0.19004228711128235,
      "learning_rate": 1.8031900870023728e-05,
      "loss": 0.0028,
      "step": 3179
    },
    {
      "epoch": 0.18863447621307391,
      "grad_norm": 10.3740234375,
      "learning_rate": 1.8030582652254154e-05,
      "loss": 0.5915,
      "step": 3180
    },
    {
      "epoch": 0.18869379523075097,
      "grad_norm": 7.202060222625732,
      "learning_rate": 1.802926443448458e-05,
      "loss": 0.026,
      "step": 3181
    },
    {
      "epoch": 0.18875311424842806,
      "grad_norm": 0.022241443395614624,
      "learning_rate": 1.8027946216715002e-05,
      "loss": 0.0005,
      "step": 3182
    },
    {
      "epoch": 0.1888124332661051,
      "grad_norm": 18.45274543762207,
      "learning_rate": 1.8026627998945428e-05,
      "loss": 0.5017,
      "step": 3183
    },
    {
      "epoch": 0.18887175228378217,
      "grad_norm": 16.390871047973633,
      "learning_rate": 1.8025309781175854e-05,
      "loss": 0.6672,
      "step": 3184
    },
    {
      "epoch": 0.18893107130145925,
      "grad_norm": 1.0035449266433716,
      "learning_rate": 1.8023991563406277e-05,
      "loss": 0.027,
      "step": 3185
    },
    {
      "epoch": 0.1889903903191363,
      "grad_norm": 0.06839560717344284,
      "learning_rate": 1.8022673345636702e-05,
      "loss": 0.0018,
      "step": 3186
    },
    {
      "epoch": 0.1890497093368134,
      "grad_norm": 4.4421305656433105,
      "learning_rate": 1.8021355127867125e-05,
      "loss": 0.1456,
      "step": 3187
    },
    {
      "epoch": 0.18910902835449045,
      "grad_norm": 11.621294975280762,
      "learning_rate": 1.802003691009755e-05,
      "loss": 0.4106,
      "step": 3188
    },
    {
      "epoch": 0.1891683473721675,
      "grad_norm": 19.036273956298828,
      "learning_rate": 1.8018718692327973e-05,
      "loss": 0.3751,
      "step": 3189
    },
    {
      "epoch": 0.1892276663898446,
      "grad_norm": 10.54842472076416,
      "learning_rate": 1.80174004745584e-05,
      "loss": 0.9153,
      "step": 3190
    },
    {
      "epoch": 0.18928698540752165,
      "grad_norm": 1.4622411727905273,
      "learning_rate": 1.801608225678882e-05,
      "loss": 0.0066,
      "step": 3191
    },
    {
      "epoch": 0.18934630442519873,
      "grad_norm": 0.014497516676783562,
      "learning_rate": 1.8014764039019248e-05,
      "loss": 0.0004,
      "step": 3192
    },
    {
      "epoch": 0.1894056234428758,
      "grad_norm": 0.7419278025627136,
      "learning_rate": 1.801344582124967e-05,
      "loss": 0.0087,
      "step": 3193
    },
    {
      "epoch": 0.18946494246055284,
      "grad_norm": 3.354339122772217,
      "learning_rate": 1.8012127603480096e-05,
      "loss": 0.0377,
      "step": 3194
    },
    {
      "epoch": 0.18952426147822993,
      "grad_norm": 0.31832820177078247,
      "learning_rate": 1.8010809385710522e-05,
      "loss": 0.0023,
      "step": 3195
    },
    {
      "epoch": 0.18958358049590698,
      "grad_norm": 0.22836138308048248,
      "learning_rate": 1.8009491167940944e-05,
      "loss": 0.0043,
      "step": 3196
    },
    {
      "epoch": 0.18964289951358407,
      "grad_norm": 2.076568365097046,
      "learning_rate": 1.800817295017137e-05,
      "loss": 0.0212,
      "step": 3197
    },
    {
      "epoch": 0.18970221853126112,
      "grad_norm": 13.38390064239502,
      "learning_rate": 1.8006854732401796e-05,
      "loss": 0.1415,
      "step": 3198
    },
    {
      "epoch": 0.18976153754893818,
      "grad_norm": 25.111778259277344,
      "learning_rate": 1.800553651463222e-05,
      "loss": 0.2863,
      "step": 3199
    },
    {
      "epoch": 0.18982085656661526,
      "grad_norm": 0.6183156371116638,
      "learning_rate": 1.8004218296862644e-05,
      "loss": 0.0072,
      "step": 3200
    },
    {
      "epoch": 0.18988017558429232,
      "grad_norm": 0.31021803617477417,
      "learning_rate": 1.8002900079093067e-05,
      "loss": 0.0036,
      "step": 3201
    },
    {
      "epoch": 0.1899394946019694,
      "grad_norm": 2.051494598388672,
      "learning_rate": 1.8001581861323493e-05,
      "loss": 0.0172,
      "step": 3202
    },
    {
      "epoch": 0.18999881361964646,
      "grad_norm": 11.589944839477539,
      "learning_rate": 1.8000263643553915e-05,
      "loss": 0.1482,
      "step": 3203
    },
    {
      "epoch": 0.19005813263732352,
      "grad_norm": 4.166213035583496,
      "learning_rate": 1.799894542578434e-05,
      "loss": 0.1715,
      "step": 3204
    },
    {
      "epoch": 0.1901174516550006,
      "grad_norm": 4.598721981048584,
      "learning_rate": 1.7997627208014764e-05,
      "loss": 0.0457,
      "step": 3205
    },
    {
      "epoch": 0.19017677067267766,
      "grad_norm": 9.211336135864258,
      "learning_rate": 1.799630899024519e-05,
      "loss": 0.1675,
      "step": 3206
    },
    {
      "epoch": 0.19023608969035471,
      "grad_norm": 1.7286455631256104,
      "learning_rate": 1.7994990772475612e-05,
      "loss": 0.0038,
      "step": 3207
    },
    {
      "epoch": 0.1902954087080318,
      "grad_norm": 0.40196526050567627,
      "learning_rate": 1.7993672554706038e-05,
      "loss": 0.0063,
      "step": 3208
    },
    {
      "epoch": 0.19035472772570886,
      "grad_norm": 0.06302446126937866,
      "learning_rate": 1.7992354336936464e-05,
      "loss": 0.001,
      "step": 3209
    },
    {
      "epoch": 0.19041404674338594,
      "grad_norm": 16.37450408935547,
      "learning_rate": 1.7991036119166886e-05,
      "loss": 0.2951,
      "step": 3210
    },
    {
      "epoch": 0.190473365761063,
      "grad_norm": 16.190738677978516,
      "learning_rate": 1.7989717901397312e-05,
      "loss": 1.3642,
      "step": 3211
    },
    {
      "epoch": 0.19053268477874005,
      "grad_norm": 0.05643157660961151,
      "learning_rate": 1.7988399683627738e-05,
      "loss": 0.0014,
      "step": 3212
    },
    {
      "epoch": 0.19059200379641714,
      "grad_norm": 7.9836201667785645,
      "learning_rate": 1.798708146585816e-05,
      "loss": 0.0772,
      "step": 3213
    },
    {
      "epoch": 0.1906513228140942,
      "grad_norm": 3.0243518352508545,
      "learning_rate": 1.7985763248088586e-05,
      "loss": 0.291,
      "step": 3214
    },
    {
      "epoch": 0.19071064183177128,
      "grad_norm": 4.6888108253479,
      "learning_rate": 1.7984445030319012e-05,
      "loss": 0.1455,
      "step": 3215
    },
    {
      "epoch": 0.19076996084944833,
      "grad_norm": 4.990100860595703,
      "learning_rate": 1.7983126812549435e-05,
      "loss": 0.0342,
      "step": 3216
    },
    {
      "epoch": 0.1908292798671254,
      "grad_norm": 23.428205490112305,
      "learning_rate": 1.798180859477986e-05,
      "loss": 0.3087,
      "step": 3217
    },
    {
      "epoch": 0.19088859888480247,
      "grad_norm": 7.672694206237793,
      "learning_rate": 1.7980490377010283e-05,
      "loss": 0.0689,
      "step": 3218
    },
    {
      "epoch": 0.19094791790247953,
      "grad_norm": 13.969462394714355,
      "learning_rate": 1.797917215924071e-05,
      "loss": 0.1443,
      "step": 3219
    },
    {
      "epoch": 0.19100723692015661,
      "grad_norm": 0.2267874777317047,
      "learning_rate": 1.797785394147113e-05,
      "loss": 0.0024,
      "step": 3220
    },
    {
      "epoch": 0.19106655593783367,
      "grad_norm": 0.7534247040748596,
      "learning_rate": 1.7976535723701557e-05,
      "loss": 0.0053,
      "step": 3221
    },
    {
      "epoch": 0.19112587495551073,
      "grad_norm": 9.7234525680542,
      "learning_rate": 1.797521750593198e-05,
      "loss": 0.1211,
      "step": 3222
    },
    {
      "epoch": 0.1911851939731878,
      "grad_norm": 13.356449127197266,
      "learning_rate": 1.7973899288162406e-05,
      "loss": 0.3008,
      "step": 3223
    },
    {
      "epoch": 0.19124451299086487,
      "grad_norm": 26.06544303894043,
      "learning_rate": 1.7972581070392828e-05,
      "loss": 0.4609,
      "step": 3224
    },
    {
      "epoch": 0.19130383200854195,
      "grad_norm": 0.15241748094558716,
      "learning_rate": 1.7971262852623254e-05,
      "loss": 0.0017,
      "step": 3225
    },
    {
      "epoch": 0.191363151026219,
      "grad_norm": 5.477750301361084,
      "learning_rate": 1.796994463485368e-05,
      "loss": 0.6611,
      "step": 3226
    },
    {
      "epoch": 0.19142247004389606,
      "grad_norm": 6.491147041320801,
      "learning_rate": 1.7968626417084103e-05,
      "loss": 0.0662,
      "step": 3227
    },
    {
      "epoch": 0.19148178906157315,
      "grad_norm": 12.708555221557617,
      "learning_rate": 1.796730819931453e-05,
      "loss": 0.3429,
      "step": 3228
    },
    {
      "epoch": 0.1915411080792502,
      "grad_norm": 7.877652168273926,
      "learning_rate": 1.7965989981544954e-05,
      "loss": 0.0989,
      "step": 3229
    },
    {
      "epoch": 0.19160042709692726,
      "grad_norm": 0.225264772772789,
      "learning_rate": 1.7964671763775377e-05,
      "loss": 0.004,
      "step": 3230
    },
    {
      "epoch": 0.19165974611460435,
      "grad_norm": 3.856689453125,
      "learning_rate": 1.7963353546005803e-05,
      "loss": 0.0131,
      "step": 3231
    },
    {
      "epoch": 0.1917190651322814,
      "grad_norm": 13.300287246704102,
      "learning_rate": 1.796203532823623e-05,
      "loss": 0.0656,
      "step": 3232
    },
    {
      "epoch": 0.1917783841499585,
      "grad_norm": 7.821841716766357,
      "learning_rate": 1.796071711046665e-05,
      "loss": 0.1263,
      "step": 3233
    },
    {
      "epoch": 0.19183770316763554,
      "grad_norm": 0.13609901070594788,
      "learning_rate": 1.7959398892697077e-05,
      "loss": 0.0015,
      "step": 3234
    },
    {
      "epoch": 0.1918970221853126,
      "grad_norm": 19.304798126220703,
      "learning_rate": 1.79580806749275e-05,
      "loss": 0.2383,
      "step": 3235
    },
    {
      "epoch": 0.19195634120298968,
      "grad_norm": 37.49568176269531,
      "learning_rate": 1.7956762457157922e-05,
      "loss": 0.1269,
      "step": 3236
    },
    {
      "epoch": 0.19201566022066674,
      "grad_norm": 25.394180297851562,
      "learning_rate": 1.7955444239388348e-05,
      "loss": 0.586,
      "step": 3237
    },
    {
      "epoch": 0.19207497923834382,
      "grad_norm": 24.215490341186523,
      "learning_rate": 1.795412602161877e-05,
      "loss": 1.2892,
      "step": 3238
    },
    {
      "epoch": 0.19213429825602088,
      "grad_norm": 0.895746648311615,
      "learning_rate": 1.7952807803849196e-05,
      "loss": 0.0128,
      "step": 3239
    },
    {
      "epoch": 0.19219361727369794,
      "grad_norm": 1.9780514240264893,
      "learning_rate": 1.7951489586079622e-05,
      "loss": 0.0166,
      "step": 3240
    },
    {
      "epoch": 0.19225293629137502,
      "grad_norm": 0.06938763707876205,
      "learning_rate": 1.7950171368310045e-05,
      "loss": 0.0021,
      "step": 3241
    },
    {
      "epoch": 0.19231225530905208,
      "grad_norm": 0.04325404018163681,
      "learning_rate": 1.794885315054047e-05,
      "loss": 0.0012,
      "step": 3242
    },
    {
      "epoch": 0.19237157432672916,
      "grad_norm": 0.058611299842596054,
      "learning_rate": 1.7947534932770896e-05,
      "loss": 0.0013,
      "step": 3243
    },
    {
      "epoch": 0.19243089334440622,
      "grad_norm": 7.448526859283447,
      "learning_rate": 1.794621671500132e-05,
      "loss": 0.098,
      "step": 3244
    },
    {
      "epoch": 0.19249021236208327,
      "grad_norm": 9.749218940734863,
      "learning_rate": 1.7944898497231745e-05,
      "loss": 0.2475,
      "step": 3245
    },
    {
      "epoch": 0.19254953137976036,
      "grad_norm": 27.3529052734375,
      "learning_rate": 1.794358027946217e-05,
      "loss": 1.0468,
      "step": 3246
    },
    {
      "epoch": 0.19260885039743741,
      "grad_norm": 0.1892605423927307,
      "learning_rate": 1.7942262061692593e-05,
      "loss": 0.0018,
      "step": 3247
    },
    {
      "epoch": 0.1926681694151145,
      "grad_norm": 6.870548248291016,
      "learning_rate": 1.794094384392302e-05,
      "loss": 0.2002,
      "step": 3248
    },
    {
      "epoch": 0.19272748843279155,
      "grad_norm": 2.6911439895629883,
      "learning_rate": 1.793962562615344e-05,
      "loss": 0.0218,
      "step": 3249
    },
    {
      "epoch": 0.1927868074504686,
      "grad_norm": 24.12870979309082,
      "learning_rate": 1.7938307408383867e-05,
      "loss": 0.4285,
      "step": 3250
    },
    {
      "epoch": 0.1928461264681457,
      "grad_norm": 7.7533392906188965,
      "learning_rate": 1.793698919061429e-05,
      "loss": 0.1055,
      "step": 3251
    },
    {
      "epoch": 0.19290544548582275,
      "grad_norm": 9.305676460266113,
      "learning_rate": 1.7935670972844716e-05,
      "loss": 0.3998,
      "step": 3252
    },
    {
      "epoch": 0.1929647645034998,
      "grad_norm": 23.187314987182617,
      "learning_rate": 1.7934352755075138e-05,
      "loss": 0.2567,
      "step": 3253
    },
    {
      "epoch": 0.1930240835211769,
      "grad_norm": 17.095149993896484,
      "learning_rate": 1.7933034537305564e-05,
      "loss": 0.7313,
      "step": 3254
    },
    {
      "epoch": 0.19308340253885395,
      "grad_norm": 3.324972152709961,
      "learning_rate": 1.7931716319535987e-05,
      "loss": 0.2328,
      "step": 3255
    },
    {
      "epoch": 0.19314272155653103,
      "grad_norm": 11.32522201538086,
      "learning_rate": 1.7930398101766412e-05,
      "loss": 0.9415,
      "step": 3256
    },
    {
      "epoch": 0.1932020405742081,
      "grad_norm": 13.097846031188965,
      "learning_rate": 1.792907988399684e-05,
      "loss": 0.9492,
      "step": 3257
    },
    {
      "epoch": 0.19326135959188515,
      "grad_norm": 0.056193757802248,
      "learning_rate": 1.792776166622726e-05,
      "loss": 0.0008,
      "step": 3258
    },
    {
      "epoch": 0.19332067860956223,
      "grad_norm": 11.52550983428955,
      "learning_rate": 1.7926443448457687e-05,
      "loss": 0.1428,
      "step": 3259
    },
    {
      "epoch": 0.1933799976272393,
      "grad_norm": 8.121480941772461,
      "learning_rate": 1.7925125230688113e-05,
      "loss": 0.076,
      "step": 3260
    },
    {
      "epoch": 0.19343931664491637,
      "grad_norm": 0.8401545286178589,
      "learning_rate": 1.7923807012918535e-05,
      "loss": 0.0073,
      "step": 3261
    },
    {
      "epoch": 0.19349863566259343,
      "grad_norm": 0.6163913607597351,
      "learning_rate": 1.792248879514896e-05,
      "loss": 0.0059,
      "step": 3262
    },
    {
      "epoch": 0.19355795468027048,
      "grad_norm": 0.1014406755566597,
      "learning_rate": 1.7921170577379387e-05,
      "loss": 0.0016,
      "step": 3263
    },
    {
      "epoch": 0.19361727369794757,
      "grad_norm": 0.07032661885023117,
      "learning_rate": 1.791985235960981e-05,
      "loss": 0.0023,
      "step": 3264
    },
    {
      "epoch": 0.19367659271562462,
      "grad_norm": 0.05944683775305748,
      "learning_rate": 1.7918534141840235e-05,
      "loss": 0.0019,
      "step": 3265
    },
    {
      "epoch": 0.1937359117333017,
      "grad_norm": 33.81626892089844,
      "learning_rate": 1.7917215924070658e-05,
      "loss": 0.1809,
      "step": 3266
    },
    {
      "epoch": 0.19379523075097876,
      "grad_norm": 0.07797378301620483,
      "learning_rate": 1.7915897706301084e-05,
      "loss": 0.0014,
      "step": 3267
    },
    {
      "epoch": 0.19385454976865582,
      "grad_norm": 0.07417940348386765,
      "learning_rate": 1.7914579488531506e-05,
      "loss": 0.0027,
      "step": 3268
    },
    {
      "epoch": 0.1939138687863329,
      "grad_norm": 26.25480842590332,
      "learning_rate": 1.7913261270761932e-05,
      "loss": 0.2408,
      "step": 3269
    },
    {
      "epoch": 0.19397318780400996,
      "grad_norm": 7.863186836242676,
      "learning_rate": 1.7911943052992354e-05,
      "loss": 0.0638,
      "step": 3270
    },
    {
      "epoch": 0.19403250682168705,
      "grad_norm": 1.040420651435852,
      "learning_rate": 1.791062483522278e-05,
      "loss": 0.0179,
      "step": 3271
    },
    {
      "epoch": 0.1940918258393641,
      "grad_norm": 8.946833610534668,
      "learning_rate": 1.7909306617453203e-05,
      "loss": 0.3075,
      "step": 3272
    },
    {
      "epoch": 0.19415114485704116,
      "grad_norm": 0.04028651490807533,
      "learning_rate": 1.790798839968363e-05,
      "loss": 0.0007,
      "step": 3273
    },
    {
      "epoch": 0.19421046387471824,
      "grad_norm": 0.11838746070861816,
      "learning_rate": 1.7906670181914055e-05,
      "loss": 0.0018,
      "step": 3274
    },
    {
      "epoch": 0.1942697828923953,
      "grad_norm": 0.27888223528862,
      "learning_rate": 1.7905351964144477e-05,
      "loss": 0.0037,
      "step": 3275
    },
    {
      "epoch": 0.19432910191007238,
      "grad_norm": 0.4617763161659241,
      "learning_rate": 1.7904033746374903e-05,
      "loss": 0.0058,
      "step": 3276
    },
    {
      "epoch": 0.19438842092774944,
      "grad_norm": 3.0982160568237305,
      "learning_rate": 1.790271552860533e-05,
      "loss": 0.0292,
      "step": 3277
    },
    {
      "epoch": 0.1944477399454265,
      "grad_norm": 28.08856964111328,
      "learning_rate": 1.790139731083575e-05,
      "loss": 0.3505,
      "step": 3278
    },
    {
      "epoch": 0.19450705896310358,
      "grad_norm": 17.02985382080078,
      "learning_rate": 1.7900079093066177e-05,
      "loss": 0.9516,
      "step": 3279
    },
    {
      "epoch": 0.19456637798078064,
      "grad_norm": 0.15906473994255066,
      "learning_rate": 1.78987608752966e-05,
      "loss": 0.0021,
      "step": 3280
    },
    {
      "epoch": 0.1946256969984577,
      "grad_norm": 16.75497817993164,
      "learning_rate": 1.7897442657527026e-05,
      "loss": 0.2718,
      "step": 3281
    },
    {
      "epoch": 0.19468501601613478,
      "grad_norm": 0.8075032830238342,
      "learning_rate": 1.7896124439757448e-05,
      "loss": 0.014,
      "step": 3282
    },
    {
      "epoch": 0.19474433503381183,
      "grad_norm": 0.05736564099788666,
      "learning_rate": 1.7894806221987874e-05,
      "loss": 0.0007,
      "step": 3283
    },
    {
      "epoch": 0.19480365405148892,
      "grad_norm": 0.02936200238764286,
      "learning_rate": 1.7893488004218296e-05,
      "loss": 0.001,
      "step": 3284
    },
    {
      "epoch": 0.19486297306916597,
      "grad_norm": 10.899664878845215,
      "learning_rate": 1.7892169786448722e-05,
      "loss": 0.459,
      "step": 3285
    },
    {
      "epoch": 0.19492229208684303,
      "grad_norm": 7.177664756774902,
      "learning_rate": 1.7890851568679145e-05,
      "loss": 0.4353,
      "step": 3286
    },
    {
      "epoch": 0.19498161110452011,
      "grad_norm": 0.08522654324769974,
      "learning_rate": 1.788953335090957e-05,
      "loss": 0.0009,
      "step": 3287
    },
    {
      "epoch": 0.19504093012219717,
      "grad_norm": 0.945765495300293,
      "learning_rate": 1.7888215133139997e-05,
      "loss": 0.018,
      "step": 3288
    },
    {
      "epoch": 0.19510024913987425,
      "grad_norm": 16.879817962646484,
      "learning_rate": 1.788689691537042e-05,
      "loss": 1.0756,
      "step": 3289
    },
    {
      "epoch": 0.1951595681575513,
      "grad_norm": 3.550746202468872,
      "learning_rate": 1.7885578697600845e-05,
      "loss": 0.1342,
      "step": 3290
    },
    {
      "epoch": 0.19521888717522837,
      "grad_norm": 6.830029487609863,
      "learning_rate": 1.788426047983127e-05,
      "loss": 0.1455,
      "step": 3291
    },
    {
      "epoch": 0.19527820619290545,
      "grad_norm": 2.1331889629364014,
      "learning_rate": 1.7882942262061693e-05,
      "loss": 0.0647,
      "step": 3292
    },
    {
      "epoch": 0.1953375252105825,
      "grad_norm": 0.21015118062496185,
      "learning_rate": 1.788162404429212e-05,
      "loss": 0.0046,
      "step": 3293
    },
    {
      "epoch": 0.1953968442282596,
      "grad_norm": 0.22934144735336304,
      "learning_rate": 1.7880305826522545e-05,
      "loss": 0.0036,
      "step": 3294
    },
    {
      "epoch": 0.19545616324593665,
      "grad_norm": 0.669333815574646,
      "learning_rate": 1.7878987608752968e-05,
      "loss": 0.0082,
      "step": 3295
    },
    {
      "epoch": 0.1955154822636137,
      "grad_norm": 16.515962600708008,
      "learning_rate": 1.7877669390983393e-05,
      "loss": 0.1015,
      "step": 3296
    },
    {
      "epoch": 0.1955748012812908,
      "grad_norm": 20.34009552001953,
      "learning_rate": 1.7876351173213816e-05,
      "loss": 0.1062,
      "step": 3297
    },
    {
      "epoch": 0.19563412029896785,
      "grad_norm": 47.64055252075195,
      "learning_rate": 1.7875032955444242e-05,
      "loss": 0.6398,
      "step": 3298
    },
    {
      "epoch": 0.19569343931664493,
      "grad_norm": 0.30156832933425903,
      "learning_rate": 1.7873714737674664e-05,
      "loss": 0.0041,
      "step": 3299
    },
    {
      "epoch": 0.19575275833432199,
      "grad_norm": 12.06484317779541,
      "learning_rate": 1.787239651990509e-05,
      "loss": 0.5373,
      "step": 3300
    },
    {
      "epoch": 0.19581207735199904,
      "grad_norm": 1.5060291290283203,
      "learning_rate": 1.7871078302135513e-05,
      "loss": 0.0083,
      "step": 3301
    },
    {
      "epoch": 0.19587139636967613,
      "grad_norm": 14.467229843139648,
      "learning_rate": 1.786976008436594e-05,
      "loss": 0.3738,
      "step": 3302
    },
    {
      "epoch": 0.19593071538735318,
      "grad_norm": 0.058934081345796585,
      "learning_rate": 1.786844186659636e-05,
      "loss": 0.0007,
      "step": 3303
    },
    {
      "epoch": 0.19599003440503024,
      "grad_norm": 29.355432510375977,
      "learning_rate": 1.7867123648826787e-05,
      "loss": 0.2855,
      "step": 3304
    },
    {
      "epoch": 0.19604935342270732,
      "grad_norm": 5.960725784301758,
      "learning_rate": 1.7865805431057213e-05,
      "loss": 0.1972,
      "step": 3305
    },
    {
      "epoch": 0.19610867244038438,
      "grad_norm": 0.05560597404837608,
      "learning_rate": 1.7864487213287635e-05,
      "loss": 0.0013,
      "step": 3306
    },
    {
      "epoch": 0.19616799145806146,
      "grad_norm": 0.5552385449409485,
      "learning_rate": 1.786316899551806e-05,
      "loss": 0.0064,
      "step": 3307
    },
    {
      "epoch": 0.19622731047573852,
      "grad_norm": 0.06973055750131607,
      "learning_rate": 1.7861850777748487e-05,
      "loss": 0.0013,
      "step": 3308
    },
    {
      "epoch": 0.19628662949341558,
      "grad_norm": 0.29472091794013977,
      "learning_rate": 1.786053255997891e-05,
      "loss": 0.0048,
      "step": 3309
    },
    {
      "epoch": 0.19634594851109266,
      "grad_norm": 6.569924831390381,
      "learning_rate": 1.7859214342209335e-05,
      "loss": 0.1928,
      "step": 3310
    },
    {
      "epoch": 0.19640526752876972,
      "grad_norm": 0.06362652033567429,
      "learning_rate": 1.785789612443976e-05,
      "loss": 0.0007,
      "step": 3311
    },
    {
      "epoch": 0.1964645865464468,
      "grad_norm": 0.8033871054649353,
      "learning_rate": 1.7856577906670184e-05,
      "loss": 0.0111,
      "step": 3312
    },
    {
      "epoch": 0.19652390556412386,
      "grad_norm": 13.438543319702148,
      "learning_rate": 1.7855259688900606e-05,
      "loss": 0.1365,
      "step": 3313
    },
    {
      "epoch": 0.1965832245818009,
      "grad_norm": 1.3395109176635742,
      "learning_rate": 1.7853941471131032e-05,
      "loss": 0.0114,
      "step": 3314
    },
    {
      "epoch": 0.196642543599478,
      "grad_norm": 0.6074701547622681,
      "learning_rate": 1.7852623253361455e-05,
      "loss": 0.0058,
      "step": 3315
    },
    {
      "epoch": 0.19670186261715505,
      "grad_norm": 4.655011177062988,
      "learning_rate": 1.785130503559188e-05,
      "loss": 0.0547,
      "step": 3316
    },
    {
      "epoch": 0.19676118163483214,
      "grad_norm": 0.1003054827451706,
      "learning_rate": 1.7849986817822306e-05,
      "loss": 0.0013,
      "step": 3317
    },
    {
      "epoch": 0.1968205006525092,
      "grad_norm": 2.349648952484131,
      "learning_rate": 1.784866860005273e-05,
      "loss": 0.0154,
      "step": 3318
    },
    {
      "epoch": 0.19687981967018625,
      "grad_norm": 1.587725281715393,
      "learning_rate": 1.7847350382283155e-05,
      "loss": 0.0121,
      "step": 3319
    },
    {
      "epoch": 0.19693913868786334,
      "grad_norm": 14.52338695526123,
      "learning_rate": 1.7846032164513577e-05,
      "loss": 0.3388,
      "step": 3320
    },
    {
      "epoch": 0.1969984577055404,
      "grad_norm": 25.165674209594727,
      "learning_rate": 1.7844713946744003e-05,
      "loss": 0.0984,
      "step": 3321
    },
    {
      "epoch": 0.19705777672321748,
      "grad_norm": 6.871700763702393,
      "learning_rate": 1.784339572897443e-05,
      "loss": 0.045,
      "step": 3322
    },
    {
      "epoch": 0.19711709574089453,
      "grad_norm": 0.24778485298156738,
      "learning_rate": 1.784207751120485e-05,
      "loss": 0.005,
      "step": 3323
    },
    {
      "epoch": 0.1971764147585716,
      "grad_norm": 8.20863151550293,
      "learning_rate": 1.7840759293435277e-05,
      "loss": 0.0497,
      "step": 3324
    },
    {
      "epoch": 0.19723573377624867,
      "grad_norm": 6.996041774749756,
      "learning_rate": 1.7839441075665703e-05,
      "loss": 0.213,
      "step": 3325
    },
    {
      "epoch": 0.19729505279392573,
      "grad_norm": 3.2417023181915283,
      "learning_rate": 1.7838122857896126e-05,
      "loss": 0.3583,
      "step": 3326
    },
    {
      "epoch": 0.1973543718116028,
      "grad_norm": 6.472440719604492,
      "learning_rate": 1.7836804640126552e-05,
      "loss": 0.0805,
      "step": 3327
    },
    {
      "epoch": 0.19741369082927987,
      "grad_norm": 78.9117202758789,
      "learning_rate": 1.7835486422356974e-05,
      "loss": 0.5119,
      "step": 3328
    },
    {
      "epoch": 0.19747300984695693,
      "grad_norm": 0.04088618978857994,
      "learning_rate": 1.78341682045874e-05,
      "loss": 0.0009,
      "step": 3329
    },
    {
      "epoch": 0.197532328864634,
      "grad_norm": 0.16534681618213654,
      "learning_rate": 1.7832849986817823e-05,
      "loss": 0.0032,
      "step": 3330
    },
    {
      "epoch": 0.19759164788231107,
      "grad_norm": 4.917898178100586,
      "learning_rate": 1.783153176904825e-05,
      "loss": 0.049,
      "step": 3331
    },
    {
      "epoch": 0.19765096689998812,
      "grad_norm": 0.2880069613456726,
      "learning_rate": 1.783021355127867e-05,
      "loss": 0.0039,
      "step": 3332
    },
    {
      "epoch": 0.1977102859176652,
      "grad_norm": 14.071599006652832,
      "learning_rate": 1.7828895333509097e-05,
      "loss": 0.8195,
      "step": 3333
    },
    {
      "epoch": 0.19776960493534226,
      "grad_norm": 0.06888458132743835,
      "learning_rate": 1.782757711573952e-05,
      "loss": 0.0012,
      "step": 3334
    },
    {
      "epoch": 0.19782892395301935,
      "grad_norm": 28.86060905456543,
      "learning_rate": 1.7826258897969945e-05,
      "loss": 0.1499,
      "step": 3335
    },
    {
      "epoch": 0.1978882429706964,
      "grad_norm": 1.2066943645477295,
      "learning_rate": 1.782494068020037e-05,
      "loss": 0.0053,
      "step": 3336
    },
    {
      "epoch": 0.19794756198837346,
      "grad_norm": 6.231265068054199,
      "learning_rate": 1.7823622462430794e-05,
      "loss": 0.0887,
      "step": 3337
    },
    {
      "epoch": 0.19800688100605054,
      "grad_norm": 18.11928939819336,
      "learning_rate": 1.782230424466122e-05,
      "loss": 0.158,
      "step": 3338
    },
    {
      "epoch": 0.1980662000237276,
      "grad_norm": 0.014004616998136044,
      "learning_rate": 1.7820986026891645e-05,
      "loss": 0.0004,
      "step": 3339
    },
    {
      "epoch": 0.19812551904140469,
      "grad_norm": 4.801468849182129,
      "learning_rate": 1.7819667809122068e-05,
      "loss": 0.0236,
      "step": 3340
    },
    {
      "epoch": 0.19818483805908174,
      "grad_norm": 13.68934154510498,
      "learning_rate": 1.7818349591352494e-05,
      "loss": 0.0743,
      "step": 3341
    },
    {
      "epoch": 0.1982441570767588,
      "grad_norm": 7.670524597167969,
      "learning_rate": 1.781703137358292e-05,
      "loss": 0.096,
      "step": 3342
    },
    {
      "epoch": 0.19830347609443588,
      "grad_norm": 15.272104263305664,
      "learning_rate": 1.7815713155813342e-05,
      "loss": 0.1905,
      "step": 3343
    },
    {
      "epoch": 0.19836279511211294,
      "grad_norm": 2.2286148071289062,
      "learning_rate": 1.7814394938043768e-05,
      "loss": 0.0223,
      "step": 3344
    },
    {
      "epoch": 0.19842211412979002,
      "grad_norm": 4.26677942276001,
      "learning_rate": 1.781307672027419e-05,
      "loss": 0.0616,
      "step": 3345
    },
    {
      "epoch": 0.19848143314746708,
      "grad_norm": 1.4660110473632812,
      "learning_rate": 1.7811758502504613e-05,
      "loss": 0.0123,
      "step": 3346
    },
    {
      "epoch": 0.19854075216514414,
      "grad_norm": 3.355046033859253,
      "learning_rate": 1.781044028473504e-05,
      "loss": 0.0389,
      "step": 3347
    },
    {
      "epoch": 0.19860007118282122,
      "grad_norm": 1.519762635231018,
      "learning_rate": 1.7809122066965465e-05,
      "loss": 0.0206,
      "step": 3348
    },
    {
      "epoch": 0.19865939020049828,
      "grad_norm": 0.9196451902389526,
      "learning_rate": 1.7807803849195887e-05,
      "loss": 0.0067,
      "step": 3349
    },
    {
      "epoch": 0.19871870921817536,
      "grad_norm": 0.6946272850036621,
      "learning_rate": 1.7806485631426313e-05,
      "loss": 0.0079,
      "step": 3350
    },
    {
      "epoch": 0.19877802823585242,
      "grad_norm": 2.647221326828003,
      "learning_rate": 1.7805167413656736e-05,
      "loss": 0.0233,
      "step": 3351
    },
    {
      "epoch": 0.19883734725352947,
      "grad_norm": 6.0612664222717285,
      "learning_rate": 1.780384919588716e-05,
      "loss": 0.044,
      "step": 3352
    },
    {
      "epoch": 0.19889666627120656,
      "grad_norm": 0.744040310382843,
      "learning_rate": 1.7802530978117587e-05,
      "loss": 0.0094,
      "step": 3353
    },
    {
      "epoch": 0.1989559852888836,
      "grad_norm": 2.34493088722229,
      "learning_rate": 1.780121276034801e-05,
      "loss": 0.0172,
      "step": 3354
    },
    {
      "epoch": 0.19901530430656067,
      "grad_norm": 18.05284881591797,
      "learning_rate": 1.7799894542578436e-05,
      "loss": 0.7028,
      "step": 3355
    },
    {
      "epoch": 0.19907462332423775,
      "grad_norm": 21.74795150756836,
      "learning_rate": 1.779857632480886e-05,
      "loss": 0.3924,
      "step": 3356
    },
    {
      "epoch": 0.1991339423419148,
      "grad_norm": 7.499318599700928,
      "learning_rate": 1.7797258107039284e-05,
      "loss": 0.059,
      "step": 3357
    },
    {
      "epoch": 0.1991932613595919,
      "grad_norm": 0.09238778054714203,
      "learning_rate": 1.779593988926971e-05,
      "loss": 0.0017,
      "step": 3358
    },
    {
      "epoch": 0.19925258037726895,
      "grad_norm": 0.0069772833958268166,
      "learning_rate": 1.7794621671500133e-05,
      "loss": 0.0002,
      "step": 3359
    },
    {
      "epoch": 0.199311899394946,
      "grad_norm": 5.335641384124756,
      "learning_rate": 1.779330345373056e-05,
      "loss": 0.088,
      "step": 3360
    },
    {
      "epoch": 0.1993712184126231,
      "grad_norm": 0.09748389571905136,
      "learning_rate": 1.779198523596098e-05,
      "loss": 0.001,
      "step": 3361
    },
    {
      "epoch": 0.19943053743030015,
      "grad_norm": 0.07897605746984482,
      "learning_rate": 1.7790667018191407e-05,
      "loss": 0.0014,
      "step": 3362
    },
    {
      "epoch": 0.19948985644797723,
      "grad_norm": 1.5517518520355225,
      "learning_rate": 1.778934880042183e-05,
      "loss": 0.01,
      "step": 3363
    },
    {
      "epoch": 0.1995491754656543,
      "grad_norm": 0.02287081442773342,
      "learning_rate": 1.7788030582652255e-05,
      "loss": 0.0008,
      "step": 3364
    },
    {
      "epoch": 0.19960849448333134,
      "grad_norm": 1.5848007202148438,
      "learning_rate": 1.778671236488268e-05,
      "loss": 0.0162,
      "step": 3365
    },
    {
      "epoch": 0.19966781350100843,
      "grad_norm": 0.23165135085582733,
      "learning_rate": 1.7785394147113104e-05,
      "loss": 0.0031,
      "step": 3366
    },
    {
      "epoch": 0.19972713251868549,
      "grad_norm": 0.017455432564020157,
      "learning_rate": 1.778407592934353e-05,
      "loss": 0.0005,
      "step": 3367
    },
    {
      "epoch": 0.19978645153636257,
      "grad_norm": 10.841934204101562,
      "learning_rate": 1.7782757711573952e-05,
      "loss": 0.1333,
      "step": 3368
    },
    {
      "epoch": 0.19984577055403963,
      "grad_norm": 53.72563552856445,
      "learning_rate": 1.7781439493804378e-05,
      "loss": 1.5843,
      "step": 3369
    },
    {
      "epoch": 0.19990508957171668,
      "grad_norm": 35.54249572753906,
      "learning_rate": 1.7780121276034804e-05,
      "loss": 1.2262,
      "step": 3370
    },
    {
      "epoch": 0.19996440858939377,
      "grad_norm": 24.848249435424805,
      "learning_rate": 1.7778803058265226e-05,
      "loss": 0.3938,
      "step": 3371
    },
    {
      "epoch": 0.20002372760707082,
      "grad_norm": 0.06601978093385696,
      "learning_rate": 1.7777484840495652e-05,
      "loss": 0.0009,
      "step": 3372
    }
  ],
  "logging_steps": 1,
  "max_steps": 16858,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 3372,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.0827036487358874e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
