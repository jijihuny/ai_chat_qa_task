{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.4001334890705823,
  "eval_steps": 500,
  "global_step": 2398,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016686133822793258,
      "grad_norm": 12.055155754089355,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 4.3695,
      "step": 10
    },
    {
      "epoch": 0.0033372267645586516,
      "grad_norm": 11.911678314208984,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 4.4665,
      "step": 20
    },
    {
      "epoch": 0.005005840146837978,
      "grad_norm": 7.000014781951904,
      "learning_rate": 3e-06,
      "loss": 2.8126,
      "step": 30
    },
    {
      "epoch": 0.006674453529117303,
      "grad_norm": 13.199590682983398,
      "learning_rate": 4.000000000000001e-06,
      "loss": 4.2464,
      "step": 40
    },
    {
      "epoch": 0.00834306691139663,
      "grad_norm": 14.029197692871094,
      "learning_rate": 5e-06,
      "loss": 4.1144,
      "step": 50
    },
    {
      "epoch": 0.010011680293675955,
      "grad_norm": 9.302464485168457,
      "learning_rate": 6e-06,
      "loss": 3.3256,
      "step": 60
    },
    {
      "epoch": 0.011680293675955281,
      "grad_norm": 15.014753341674805,
      "learning_rate": 7.000000000000001e-06,
      "loss": 2.4877,
      "step": 70
    },
    {
      "epoch": 0.013348907058234607,
      "grad_norm": 9.79970932006836,
      "learning_rate": 8.000000000000001e-06,
      "loss": 3.5089,
      "step": 80
    },
    {
      "epoch": 0.015017520440513932,
      "grad_norm": 13.779865264892578,
      "learning_rate": 9e-06,
      "loss": 3.2794,
      "step": 90
    },
    {
      "epoch": 0.01668613382279326,
      "grad_norm": 11.301551818847656,
      "learning_rate": 1e-05,
      "loss": 3.6684,
      "step": 100
    },
    {
      "epoch": 0.018354747205072585,
      "grad_norm": 12.95022201538086,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 2.6943,
      "step": 110
    },
    {
      "epoch": 0.02002336058735191,
      "grad_norm": 9.577805519104004,
      "learning_rate": 1.2e-05,
      "loss": 3.3026,
      "step": 120
    },
    {
      "epoch": 0.021691973969631236,
      "grad_norm": 6.582576751708984,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 3.1611,
      "step": 130
    },
    {
      "epoch": 0.023360587351910562,
      "grad_norm": 9.120842933654785,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 2.5691,
      "step": 140
    },
    {
      "epoch": 0.025029200734189887,
      "grad_norm": 1.7658172845840454,
      "learning_rate": 1.5e-05,
      "loss": 2.146,
      "step": 150
    },
    {
      "epoch": 0.026697814116469213,
      "grad_norm": 9.7316255569458,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 2.7907,
      "step": 160
    },
    {
      "epoch": 0.02836642749874854,
      "grad_norm": 17.165279388427734,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 2.0825,
      "step": 170
    },
    {
      "epoch": 0.030035040881027864,
      "grad_norm": 7.478819847106934,
      "learning_rate": 1.8e-05,
      "loss": 2.0492,
      "step": 180
    },
    {
      "epoch": 0.03170365426330719,
      "grad_norm": 7.041898727416992,
      "learning_rate": 1.9e-05,
      "loss": 1.4729,
      "step": 190
    },
    {
      "epoch": 0.03337226764558652,
      "grad_norm": 9.482068061828613,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 200
    },
    {
      "epoch": 0.035040881027865844,
      "grad_norm": 4.689290523529053,
      "learning_rate": 2.1e-05,
      "loss": 1.3763,
      "step": 210
    },
    {
      "epoch": 0.03670949441014517,
      "grad_norm": 7.296417236328125,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.4751,
      "step": 220
    },
    {
      "epoch": 0.038378107792424496,
      "grad_norm": 11.922574996948242,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 2.0907,
      "step": 230
    },
    {
      "epoch": 0.04004672117470382,
      "grad_norm": 5.450417995452881,
      "learning_rate": 2.4e-05,
      "loss": 1.7902,
      "step": 240
    },
    {
      "epoch": 0.04171533455698315,
      "grad_norm": 7.988636493682861,
      "learning_rate": 2.5e-05,
      "loss": 2.0059,
      "step": 250
    },
    {
      "epoch": 0.04338394793926247,
      "grad_norm": 5.704071521759033,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.4196,
      "step": 260
    },
    {
      "epoch": 0.0450525613215418,
      "grad_norm": 5.168555736541748,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.4292,
      "step": 270
    },
    {
      "epoch": 0.046721174703821124,
      "grad_norm": 5.6051435470581055,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.5838,
      "step": 280
    },
    {
      "epoch": 0.04838978808610045,
      "grad_norm": 8.265167236328125,
      "learning_rate": 2.9e-05,
      "loss": 1.5459,
      "step": 290
    },
    {
      "epoch": 0.050058401468379775,
      "grad_norm": 6.930691719055176,
      "learning_rate": 3e-05,
      "loss": 1.6044,
      "step": 300
    },
    {
      "epoch": 0.0517270148506591,
      "grad_norm": 7.2446699142456055,
      "learning_rate": 3.1e-05,
      "loss": 1.6727,
      "step": 310
    },
    {
      "epoch": 0.053395628232938426,
      "grad_norm": 5.798071384429932,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.667,
      "step": 320
    },
    {
      "epoch": 0.05506424161521775,
      "grad_norm": 7.059964179992676,
      "learning_rate": 3.3e-05,
      "loss": 1.619,
      "step": 330
    },
    {
      "epoch": 0.05673285499749708,
      "grad_norm": 5.899831295013428,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.0725,
      "step": 340
    },
    {
      "epoch": 0.0584014683797764,
      "grad_norm": 5.815085411071777,
      "learning_rate": 3.5e-05,
      "loss": 1.3692,
      "step": 350
    },
    {
      "epoch": 0.06007008176205573,
      "grad_norm": 5.109336853027344,
      "learning_rate": 3.6e-05,
      "loss": 1.3641,
      "step": 360
    },
    {
      "epoch": 0.06173869514433506,
      "grad_norm": 4.846375465393066,
      "learning_rate": 3.7e-05,
      "loss": 1.5955,
      "step": 370
    },
    {
      "epoch": 0.06340730852661439,
      "grad_norm": 9.159847259521484,
      "learning_rate": 3.8e-05,
      "loss": 1.663,
      "step": 380
    },
    {
      "epoch": 0.0650759219088937,
      "grad_norm": 6.52689790725708,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.1499,
      "step": 390
    },
    {
      "epoch": 0.06674453529117304,
      "grad_norm": 8.106498718261719,
      "learning_rate": 4e-05,
      "loss": 1.0195,
      "step": 400
    },
    {
      "epoch": 0.06841314867345236,
      "grad_norm": 7.403848648071289,
      "learning_rate": 4.1e-05,
      "loss": 1.0374,
      "step": 410
    },
    {
      "epoch": 0.07008176205573169,
      "grad_norm": 10.298356056213379,
      "learning_rate": 4.2e-05,
      "loss": 1.4083,
      "step": 420
    },
    {
      "epoch": 0.07175037543801101,
      "grad_norm": 8.604483604431152,
      "learning_rate": 4.3e-05,
      "loss": 0.9467,
      "step": 430
    },
    {
      "epoch": 0.07341898882029034,
      "grad_norm": 7.670446395874023,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.5065,
      "step": 440
    },
    {
      "epoch": 0.07508760220256966,
      "grad_norm": 6.878382682800293,
      "learning_rate": 4.5e-05,
      "loss": 1.2361,
      "step": 450
    },
    {
      "epoch": 0.07675621558484899,
      "grad_norm": 9.739946365356445,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.376,
      "step": 460
    },
    {
      "epoch": 0.07842482896712831,
      "grad_norm": 2.4443745613098145,
      "learning_rate": 4.7e-05,
      "loss": 1.0508,
      "step": 470
    },
    {
      "epoch": 0.08009344234940764,
      "grad_norm": 5.105490207672119,
      "learning_rate": 4.8e-05,
      "loss": 0.9467,
      "step": 480
    },
    {
      "epoch": 0.08176205573168698,
      "grad_norm": 1.9795540571212769,
      "learning_rate": 4.9e-05,
      "loss": 1.0421,
      "step": 490
    },
    {
      "epoch": 0.0834306691139663,
      "grad_norm": 13.910361289978027,
      "learning_rate": 5e-05,
      "loss": 0.9834,
      "step": 500
    },
    {
      "epoch": 0.08509928249624563,
      "grad_norm": 5.3670220375061035,
      "learning_rate": 4.9978698023176554e-05,
      "loss": 1.2034,
      "step": 510
    },
    {
      "epoch": 0.08676789587852494,
      "grad_norm": 8.683828353881836,
      "learning_rate": 4.9957396046353105e-05,
      "loss": 0.9199,
      "step": 520
    },
    {
      "epoch": 0.08843650926080428,
      "grad_norm": 8.722528457641602,
      "learning_rate": 4.9936094069529656e-05,
      "loss": 1.3851,
      "step": 530
    },
    {
      "epoch": 0.0901051226430836,
      "grad_norm": 5.5807881355285645,
      "learning_rate": 4.99147920927062e-05,
      "loss": 1.1402,
      "step": 540
    },
    {
      "epoch": 0.09177373602536293,
      "grad_norm": 7.571084976196289,
      "learning_rate": 4.989349011588276e-05,
      "loss": 1.3795,
      "step": 550
    },
    {
      "epoch": 0.09344234940764225,
      "grad_norm": 7.37730598449707,
      "learning_rate": 4.9872188139059304e-05,
      "loss": 0.81,
      "step": 560
    },
    {
      "epoch": 0.09511096278992158,
      "grad_norm": 10.168316841125488,
      "learning_rate": 4.985088616223586e-05,
      "loss": 1.3838,
      "step": 570
    },
    {
      "epoch": 0.0967795761722009,
      "grad_norm": 10.4512357711792,
      "learning_rate": 4.9829584185412406e-05,
      "loss": 0.9509,
      "step": 580
    },
    {
      "epoch": 0.09844818955448023,
      "grad_norm": 11.75351333618164,
      "learning_rate": 4.980828220858896e-05,
      "loss": 1.142,
      "step": 590
    },
    {
      "epoch": 0.10011680293675955,
      "grad_norm": 7.678175926208496,
      "learning_rate": 4.978698023176551e-05,
      "loss": 1.0443,
      "step": 600
    },
    {
      "epoch": 0.10178541631903888,
      "grad_norm": 5.490517616271973,
      "learning_rate": 4.976567825494206e-05,
      "loss": 1.269,
      "step": 610
    },
    {
      "epoch": 0.1034540297013182,
      "grad_norm": 6.006327152252197,
      "learning_rate": 4.974437627811861e-05,
      "loss": 0.9879,
      "step": 620
    },
    {
      "epoch": 0.10512264308359753,
      "grad_norm": 8.454747200012207,
      "learning_rate": 4.972307430129516e-05,
      "loss": 1.121,
      "step": 630
    },
    {
      "epoch": 0.10679125646587685,
      "grad_norm": 4.379626750946045,
      "learning_rate": 4.9701772324471714e-05,
      "loss": 1.2119,
      "step": 640
    },
    {
      "epoch": 0.10845986984815618,
      "grad_norm": 8.560583114624023,
      "learning_rate": 4.9680470347648266e-05,
      "loss": 0.933,
      "step": 650
    },
    {
      "epoch": 0.1101284832304355,
      "grad_norm": 5.742160797119141,
      "learning_rate": 4.965916837082482e-05,
      "loss": 1.3424,
      "step": 660
    },
    {
      "epoch": 0.11179709661271484,
      "grad_norm": 8.295968055725098,
      "learning_rate": 4.963786639400136e-05,
      "loss": 1.1824,
      "step": 670
    },
    {
      "epoch": 0.11346570999499415,
      "grad_norm": 7.665077209472656,
      "learning_rate": 4.961656441717792e-05,
      "loss": 1.0148,
      "step": 680
    },
    {
      "epoch": 0.11513432337727349,
      "grad_norm": 2.2763729095458984,
      "learning_rate": 4.9595262440354464e-05,
      "loss": 1.2039,
      "step": 690
    },
    {
      "epoch": 0.1168029367595528,
      "grad_norm": 3.3238821029663086,
      "learning_rate": 4.9573960463531015e-05,
      "loss": 0.9954,
      "step": 700
    },
    {
      "epoch": 0.11847155014183214,
      "grad_norm": 5.773782253265381,
      "learning_rate": 4.9552658486707574e-05,
      "loss": 0.8885,
      "step": 710
    },
    {
      "epoch": 0.12014016352411146,
      "grad_norm": 5.722370147705078,
      "learning_rate": 4.953135650988412e-05,
      "loss": 0.843,
      "step": 720
    },
    {
      "epoch": 0.12180877690639079,
      "grad_norm": 4.529634475708008,
      "learning_rate": 4.951005453306067e-05,
      "loss": 0.9794,
      "step": 730
    },
    {
      "epoch": 0.12347739028867012,
      "grad_norm": 12.99602222442627,
      "learning_rate": 4.948875255623722e-05,
      "loss": 1.4469,
      "step": 740
    },
    {
      "epoch": 0.12514600367094944,
      "grad_norm": 5.601666450500488,
      "learning_rate": 4.946745057941377e-05,
      "loss": 1.0332,
      "step": 750
    },
    {
      "epoch": 0.12681461705322877,
      "grad_norm": 5.496005535125732,
      "learning_rate": 4.944614860259032e-05,
      "loss": 1.2491,
      "step": 760
    },
    {
      "epoch": 0.1284832304355081,
      "grad_norm": 7.067525863647461,
      "learning_rate": 4.9424846625766875e-05,
      "loss": 0.9724,
      "step": 770
    },
    {
      "epoch": 0.1301518438177874,
      "grad_norm": 6.886786460876465,
      "learning_rate": 4.9403544648943426e-05,
      "loss": 0.9522,
      "step": 780
    },
    {
      "epoch": 0.13182045720006674,
      "grad_norm": 7.650302886962891,
      "learning_rate": 4.938224267211998e-05,
      "loss": 1.2355,
      "step": 790
    },
    {
      "epoch": 0.13348907058234608,
      "grad_norm": 14.218182563781738,
      "learning_rate": 4.936094069529653e-05,
      "loss": 0.9778,
      "step": 800
    },
    {
      "epoch": 0.1351576839646254,
      "grad_norm": 5.353604316711426,
      "learning_rate": 4.933963871847307e-05,
      "loss": 1.1231,
      "step": 810
    },
    {
      "epoch": 0.1368262973469047,
      "grad_norm": 10.568121910095215,
      "learning_rate": 4.931833674164963e-05,
      "loss": 1.0955,
      "step": 820
    },
    {
      "epoch": 0.13849491072918405,
      "grad_norm": 8.434700012207031,
      "learning_rate": 4.9297034764826176e-05,
      "loss": 0.8542,
      "step": 830
    },
    {
      "epoch": 0.14016352411146338,
      "grad_norm": 11.020655632019043,
      "learning_rate": 4.927573278800273e-05,
      "loss": 1.0711,
      "step": 840
    },
    {
      "epoch": 0.1418321374937427,
      "grad_norm": 6.90020751953125,
      "learning_rate": 4.925443081117928e-05,
      "loss": 1.0857,
      "step": 850
    },
    {
      "epoch": 0.14350075087602202,
      "grad_norm": 12.6170072555542,
      "learning_rate": 4.923312883435583e-05,
      "loss": 1.1352,
      "step": 860
    },
    {
      "epoch": 0.14516936425830135,
      "grad_norm": 5.879394054412842,
      "learning_rate": 4.921182685753238e-05,
      "loss": 1.1006,
      "step": 870
    },
    {
      "epoch": 0.14683797764058068,
      "grad_norm": 8.519241333007812,
      "learning_rate": 4.919052488070893e-05,
      "loss": 0.9405,
      "step": 880
    },
    {
      "epoch": 0.14850659102286,
      "grad_norm": 12.018880844116211,
      "learning_rate": 4.9169222903885484e-05,
      "loss": 1.0162,
      "step": 890
    },
    {
      "epoch": 0.15017520440513932,
      "grad_norm": 8.296252250671387,
      "learning_rate": 4.9147920927062035e-05,
      "loss": 0.9461,
      "step": 900
    },
    {
      "epoch": 0.15184381778741865,
      "grad_norm": 1.661017656326294,
      "learning_rate": 4.9126618950238587e-05,
      "loss": 0.8436,
      "step": 910
    },
    {
      "epoch": 0.15351243116969798,
      "grad_norm": 12.7459135055542,
      "learning_rate": 4.910531697341513e-05,
      "loss": 1.0236,
      "step": 920
    },
    {
      "epoch": 0.15518104455197732,
      "grad_norm": 7.691208362579346,
      "learning_rate": 4.908401499659169e-05,
      "loss": 0.7564,
      "step": 930
    },
    {
      "epoch": 0.15684965793425662,
      "grad_norm": 5.071497917175293,
      "learning_rate": 4.9062713019768234e-05,
      "loss": 1.0305,
      "step": 940
    },
    {
      "epoch": 0.15851827131653595,
      "grad_norm": 11.30626392364502,
      "learning_rate": 4.904141104294479e-05,
      "loss": 1.1542,
      "step": 950
    },
    {
      "epoch": 0.16018688469881529,
      "grad_norm": 3.5152103900909424,
      "learning_rate": 4.9020109066121336e-05,
      "loss": 1.1834,
      "step": 960
    },
    {
      "epoch": 0.16185549808109462,
      "grad_norm": 14.171195983886719,
      "learning_rate": 4.899880708929789e-05,
      "loss": 0.8233,
      "step": 970
    },
    {
      "epoch": 0.16352411146337395,
      "grad_norm": 10.198997497558594,
      "learning_rate": 4.897750511247444e-05,
      "loss": 0.8698,
      "step": 980
    },
    {
      "epoch": 0.16519272484565325,
      "grad_norm": 6.394326686859131,
      "learning_rate": 4.895620313565099e-05,
      "loss": 0.9039,
      "step": 990
    },
    {
      "epoch": 0.1668613382279326,
      "grad_norm": 5.254652976989746,
      "learning_rate": 4.893490115882754e-05,
      "loss": 0.9891,
      "step": 1000
    },
    {
      "epoch": 0.16852995161021192,
      "grad_norm": 15.370824813842773,
      "learning_rate": 4.891359918200409e-05,
      "loss": 0.8715,
      "step": 1010
    },
    {
      "epoch": 0.17019856499249125,
      "grad_norm": 7.787691593170166,
      "learning_rate": 4.8892297205180644e-05,
      "loss": 0.7746,
      "step": 1020
    },
    {
      "epoch": 0.17186717837477056,
      "grad_norm": 11.494077682495117,
      "learning_rate": 4.887099522835719e-05,
      "loss": 0.7561,
      "step": 1030
    },
    {
      "epoch": 0.1735357917570499,
      "grad_norm": 2.676873207092285,
      "learning_rate": 4.884969325153375e-05,
      "loss": 0.8851,
      "step": 1040
    },
    {
      "epoch": 0.17520440513932922,
      "grad_norm": 7.112656593322754,
      "learning_rate": 4.882839127471029e-05,
      "loss": 0.8012,
      "step": 1050
    },
    {
      "epoch": 0.17687301852160855,
      "grad_norm": 13.557592391967773,
      "learning_rate": 4.880708929788685e-05,
      "loss": 0.8545,
      "step": 1060
    },
    {
      "epoch": 0.17854163190388786,
      "grad_norm": 8.752182006835938,
      "learning_rate": 4.8785787321063394e-05,
      "loss": 1.0867,
      "step": 1070
    },
    {
      "epoch": 0.1802102452861672,
      "grad_norm": 14.975482940673828,
      "learning_rate": 4.8764485344239946e-05,
      "loss": 1.0206,
      "step": 1080
    },
    {
      "epoch": 0.18187885866844652,
      "grad_norm": 7.262783527374268,
      "learning_rate": 4.8743183367416504e-05,
      "loss": 1.142,
      "step": 1090
    },
    {
      "epoch": 0.18354747205072586,
      "grad_norm": 11.608201026916504,
      "learning_rate": 4.872188139059305e-05,
      "loss": 0.8128,
      "step": 1100
    },
    {
      "epoch": 0.18521608543300516,
      "grad_norm": 6.071269989013672,
      "learning_rate": 4.87005794137696e-05,
      "loss": 0.7834,
      "step": 1110
    },
    {
      "epoch": 0.1868846988152845,
      "grad_norm": 5.2202324867248535,
      "learning_rate": 4.867927743694615e-05,
      "loss": 1.1932,
      "step": 1120
    },
    {
      "epoch": 0.18855331219756383,
      "grad_norm": 6.854410648345947,
      "learning_rate": 4.86579754601227e-05,
      "loss": 0.8795,
      "step": 1130
    },
    {
      "epoch": 0.19022192557984316,
      "grad_norm": 6.516758441925049,
      "learning_rate": 4.863667348329925e-05,
      "loss": 0.9976,
      "step": 1140
    },
    {
      "epoch": 0.19189053896212246,
      "grad_norm": 6.325408935546875,
      "learning_rate": 4.8615371506475805e-05,
      "loss": 0.8822,
      "step": 1150
    },
    {
      "epoch": 0.1935591523444018,
      "grad_norm": 15.320943832397461,
      "learning_rate": 4.8594069529652356e-05,
      "loss": 0.6054,
      "step": 1160
    },
    {
      "epoch": 0.19522776572668113,
      "grad_norm": 8.960070610046387,
      "learning_rate": 4.857276755282891e-05,
      "loss": 0.9196,
      "step": 1170
    },
    {
      "epoch": 0.19689637910896046,
      "grad_norm": 7.015202045440674,
      "learning_rate": 4.855146557600546e-05,
      "loss": 0.6792,
      "step": 1180
    },
    {
      "epoch": 0.19856499249123977,
      "grad_norm": 8.809013366699219,
      "learning_rate": 4.8530163599182003e-05,
      "loss": 1.2103,
      "step": 1190
    },
    {
      "epoch": 0.2002336058735191,
      "grad_norm": 5.754108905792236,
      "learning_rate": 4.850886162235856e-05,
      "loss": 1.2232,
      "step": 1200
    },
    {
      "epoch": 0.20190221925579843,
      "grad_norm": 7.985579490661621,
      "learning_rate": 4.8487559645535106e-05,
      "loss": 1.1816,
      "step": 1210
    },
    {
      "epoch": 0.20357083263807776,
      "grad_norm": 2.9442548751831055,
      "learning_rate": 4.846625766871166e-05,
      "loss": 0.8967,
      "step": 1220
    },
    {
      "epoch": 0.2052394460203571,
      "grad_norm": 9.350631713867188,
      "learning_rate": 4.844495569188821e-05,
      "loss": 0.9031,
      "step": 1230
    },
    {
      "epoch": 0.2069080594026364,
      "grad_norm": 6.945474147796631,
      "learning_rate": 4.842365371506476e-05,
      "loss": 0.9129,
      "step": 1240
    },
    {
      "epoch": 0.20857667278491573,
      "grad_norm": 10.188994407653809,
      "learning_rate": 4.840235173824131e-05,
      "loss": 0.9243,
      "step": 1250
    },
    {
      "epoch": 0.21024528616719507,
      "grad_norm": 8.459782600402832,
      "learning_rate": 4.838104976141786e-05,
      "loss": 0.5827,
      "step": 1260
    },
    {
      "epoch": 0.2119138995494744,
      "grad_norm": 5.117980003356934,
      "learning_rate": 4.8359747784594414e-05,
      "loss": 0.8088,
      "step": 1270
    },
    {
      "epoch": 0.2135825129317537,
      "grad_norm": 11.113184928894043,
      "learning_rate": 4.8338445807770965e-05,
      "loss": 1.117,
      "step": 1280
    },
    {
      "epoch": 0.21525112631403304,
      "grad_norm": 8.368911743164062,
      "learning_rate": 4.831714383094752e-05,
      "loss": 0.9381,
      "step": 1290
    },
    {
      "epoch": 0.21691973969631237,
      "grad_norm": 3.736769914627075,
      "learning_rate": 4.829584185412406e-05,
      "loss": 0.715,
      "step": 1300
    },
    {
      "epoch": 0.2185883530785917,
      "grad_norm": 5.07656192779541,
      "learning_rate": 4.827453987730062e-05,
      "loss": 1.0564,
      "step": 1310
    },
    {
      "epoch": 0.220256966460871,
      "grad_norm": 3.2761898040771484,
      "learning_rate": 4.8253237900477164e-05,
      "loss": 0.972,
      "step": 1320
    },
    {
      "epoch": 0.22192557984315034,
      "grad_norm": 3.562837600708008,
      "learning_rate": 4.8231935923653715e-05,
      "loss": 0.8114,
      "step": 1330
    },
    {
      "epoch": 0.22359419322542967,
      "grad_norm": 5.412622451782227,
      "learning_rate": 4.8210633946830267e-05,
      "loss": 0.6172,
      "step": 1340
    },
    {
      "epoch": 0.225262806607709,
      "grad_norm": 4.77771520614624,
      "learning_rate": 4.818933197000682e-05,
      "loss": 0.6817,
      "step": 1350
    },
    {
      "epoch": 0.2269314199899883,
      "grad_norm": 13.6343994140625,
      "learning_rate": 4.816802999318337e-05,
      "loss": 1.0091,
      "step": 1360
    },
    {
      "epoch": 0.22860003337226764,
      "grad_norm": 9.30815601348877,
      "learning_rate": 4.814672801635992e-05,
      "loss": 0.9068,
      "step": 1370
    },
    {
      "epoch": 0.23026864675454697,
      "grad_norm": 6.772705554962158,
      "learning_rate": 4.812542603953647e-05,
      "loss": 0.9162,
      "step": 1380
    },
    {
      "epoch": 0.2319372601368263,
      "grad_norm": 4.869386196136475,
      "learning_rate": 4.810412406271302e-05,
      "loss": 0.9245,
      "step": 1390
    },
    {
      "epoch": 0.2336058735191056,
      "grad_norm": 4.640839099884033,
      "learning_rate": 4.8082822085889575e-05,
      "loss": 0.7241,
      "step": 1400
    },
    {
      "epoch": 0.23527448690138494,
      "grad_norm": 7.260838985443115,
      "learning_rate": 4.806152010906612e-05,
      "loss": 1.1159,
      "step": 1410
    },
    {
      "epoch": 0.23694310028366428,
      "grad_norm": 8.9894437789917,
      "learning_rate": 4.804021813224268e-05,
      "loss": 0.766,
      "step": 1420
    },
    {
      "epoch": 0.2386117136659436,
      "grad_norm": 5.941689491271973,
      "learning_rate": 4.801891615541922e-05,
      "loss": 1.0211,
      "step": 1430
    },
    {
      "epoch": 0.2402803270482229,
      "grad_norm": 6.099863529205322,
      "learning_rate": 4.799761417859577e-05,
      "loss": 1.1176,
      "step": 1440
    },
    {
      "epoch": 0.24194894043050225,
      "grad_norm": 18.322364807128906,
      "learning_rate": 4.7976312201772324e-05,
      "loss": 0.9547,
      "step": 1450
    },
    {
      "epoch": 0.24361755381278158,
      "grad_norm": 4.713418006896973,
      "learning_rate": 4.7955010224948876e-05,
      "loss": 1.0038,
      "step": 1460
    },
    {
      "epoch": 0.2452861671950609,
      "grad_norm": 5.954039573669434,
      "learning_rate": 4.7933708248125434e-05,
      "loss": 0.9859,
      "step": 1470
    },
    {
      "epoch": 0.24695478057734024,
      "grad_norm": 1.3553916215896606,
      "learning_rate": 4.791240627130198e-05,
      "loss": 0.8059,
      "step": 1480
    },
    {
      "epoch": 0.24862339395961955,
      "grad_norm": 4.211933612823486,
      "learning_rate": 4.789110429447853e-05,
      "loss": 0.8068,
      "step": 1490
    },
    {
      "epoch": 0.2502920073418989,
      "grad_norm": 10.577535629272461,
      "learning_rate": 4.786980231765508e-05,
      "loss": 0.9145,
      "step": 1500
    },
    {
      "epoch": 0.2519606207241782,
      "grad_norm": 12.839938163757324,
      "learning_rate": 4.784850034083163e-05,
      "loss": 1.0051,
      "step": 1510
    },
    {
      "epoch": 0.25362923410645755,
      "grad_norm": 10.59160041809082,
      "learning_rate": 4.782719836400818e-05,
      "loss": 0.7351,
      "step": 1520
    },
    {
      "epoch": 0.25529784748873685,
      "grad_norm": 6.757431507110596,
      "learning_rate": 4.7805896387184735e-05,
      "loss": 0.9431,
      "step": 1530
    },
    {
      "epoch": 0.2569664608710162,
      "grad_norm": 11.996758460998535,
      "learning_rate": 4.7784594410361286e-05,
      "loss": 0.9925,
      "step": 1540
    },
    {
      "epoch": 0.2586350742532955,
      "grad_norm": 7.938404560089111,
      "learning_rate": 4.776329243353784e-05,
      "loss": 1.077,
      "step": 1550
    },
    {
      "epoch": 0.2603036876355748,
      "grad_norm": 7.818968772888184,
      "learning_rate": 4.774199045671439e-05,
      "loss": 0.945,
      "step": 1560
    },
    {
      "epoch": 0.2619723010178542,
      "grad_norm": 7.099704265594482,
      "learning_rate": 4.7720688479890934e-05,
      "loss": 1.0696,
      "step": 1570
    },
    {
      "epoch": 0.2636409144001335,
      "grad_norm": 4.397469997406006,
      "learning_rate": 4.769938650306749e-05,
      "loss": 0.5265,
      "step": 1580
    },
    {
      "epoch": 0.2653095277824128,
      "grad_norm": 4.42056941986084,
      "learning_rate": 4.7678084526244036e-05,
      "loss": 1.0522,
      "step": 1590
    },
    {
      "epoch": 0.26697814116469215,
      "grad_norm": 9.443928718566895,
      "learning_rate": 4.765678254942059e-05,
      "loss": 1.0368,
      "step": 1600
    },
    {
      "epoch": 0.26864675454697146,
      "grad_norm": 3.618483543395996,
      "learning_rate": 4.763548057259714e-05,
      "loss": 0.8655,
      "step": 1610
    },
    {
      "epoch": 0.2703153679292508,
      "grad_norm": 6.898618698120117,
      "learning_rate": 4.761417859577369e-05,
      "loss": 0.9831,
      "step": 1620
    },
    {
      "epoch": 0.2719839813115301,
      "grad_norm": 8.875914573669434,
      "learning_rate": 4.759287661895024e-05,
      "loss": 0.7695,
      "step": 1630
    },
    {
      "epoch": 0.2736525946938094,
      "grad_norm": 9.395337104797363,
      "learning_rate": 4.757157464212679e-05,
      "loss": 0.8331,
      "step": 1640
    },
    {
      "epoch": 0.2753212080760888,
      "grad_norm": 8.659492492675781,
      "learning_rate": 4.7550272665303344e-05,
      "loss": 0.9533,
      "step": 1650
    },
    {
      "epoch": 0.2769898214583681,
      "grad_norm": 2.3157968521118164,
      "learning_rate": 4.7528970688479896e-05,
      "loss": 0.7154,
      "step": 1660
    },
    {
      "epoch": 0.2786584348406474,
      "grad_norm": 9.683950424194336,
      "learning_rate": 4.750766871165645e-05,
      "loss": 1.0787,
      "step": 1670
    },
    {
      "epoch": 0.28032704822292676,
      "grad_norm": 11.940595626831055,
      "learning_rate": 4.748636673483299e-05,
      "loss": 1.0501,
      "step": 1680
    },
    {
      "epoch": 0.28199566160520606,
      "grad_norm": 6.150341987609863,
      "learning_rate": 4.746506475800955e-05,
      "loss": 0.6178,
      "step": 1690
    },
    {
      "epoch": 0.2836642749874854,
      "grad_norm": 9.06094741821289,
      "learning_rate": 4.7443762781186094e-05,
      "loss": 0.9085,
      "step": 1700
    },
    {
      "epoch": 0.2853328883697647,
      "grad_norm": 12.801769256591797,
      "learning_rate": 4.7422460804362645e-05,
      "loss": 0.7393,
      "step": 1710
    },
    {
      "epoch": 0.28700150175204403,
      "grad_norm": 6.536427974700928,
      "learning_rate": 4.74011588275392e-05,
      "loss": 1.0826,
      "step": 1720
    },
    {
      "epoch": 0.2886701151343234,
      "grad_norm": 4.567477703094482,
      "learning_rate": 4.737985685071575e-05,
      "loss": 0.6673,
      "step": 1730
    },
    {
      "epoch": 0.2903387285166027,
      "grad_norm": 10.086455345153809,
      "learning_rate": 4.73585548738923e-05,
      "loss": 0.9971,
      "step": 1740
    },
    {
      "epoch": 0.29200734189888206,
      "grad_norm": 5.867030620574951,
      "learning_rate": 4.733725289706885e-05,
      "loss": 0.8847,
      "step": 1750
    },
    {
      "epoch": 0.29367595528116136,
      "grad_norm": 9.277796745300293,
      "learning_rate": 4.73159509202454e-05,
      "loss": 0.6552,
      "step": 1760
    },
    {
      "epoch": 0.29534456866344067,
      "grad_norm": 10.306794166564941,
      "learning_rate": 4.729464894342195e-05,
      "loss": 0.6757,
      "step": 1770
    },
    {
      "epoch": 0.29701318204572,
      "grad_norm": 15.923295974731445,
      "learning_rate": 4.7273346966598505e-05,
      "loss": 0.7507,
      "step": 1780
    },
    {
      "epoch": 0.29868179542799933,
      "grad_norm": 10.33800220489502,
      "learning_rate": 4.725204498977505e-05,
      "loss": 1.019,
      "step": 1790
    },
    {
      "epoch": 0.30035040881027864,
      "grad_norm": 6.398563385009766,
      "learning_rate": 4.723074301295161e-05,
      "loss": 1.0343,
      "step": 1800
    },
    {
      "epoch": 0.302019022192558,
      "grad_norm": 9.485482215881348,
      "learning_rate": 4.720944103612815e-05,
      "loss": 0.7798,
      "step": 1810
    },
    {
      "epoch": 0.3036876355748373,
      "grad_norm": 12.45145034790039,
      "learning_rate": 4.71881390593047e-05,
      "loss": 0.7365,
      "step": 1820
    },
    {
      "epoch": 0.30535624895711666,
      "grad_norm": 11.77038860321045,
      "learning_rate": 4.7166837082481255e-05,
      "loss": 0.5596,
      "step": 1830
    },
    {
      "epoch": 0.30702486233939597,
      "grad_norm": 8.677770614624023,
      "learning_rate": 4.7145535105657806e-05,
      "loss": 1.013,
      "step": 1840
    },
    {
      "epoch": 0.30869347572167527,
      "grad_norm": 9.020999908447266,
      "learning_rate": 4.7124233128834364e-05,
      "loss": 0.8081,
      "step": 1850
    },
    {
      "epoch": 0.31036208910395463,
      "grad_norm": 3.7962541580200195,
      "learning_rate": 4.710293115201091e-05,
      "loss": 0.8799,
      "step": 1860
    },
    {
      "epoch": 0.31203070248623394,
      "grad_norm": 4.856635570526123,
      "learning_rate": 4.708162917518746e-05,
      "loss": 0.8808,
      "step": 1870
    },
    {
      "epoch": 0.31369931586851324,
      "grad_norm": 4.366225242614746,
      "learning_rate": 4.706032719836401e-05,
      "loss": 0.7278,
      "step": 1880
    },
    {
      "epoch": 0.3153679292507926,
      "grad_norm": 8.835206031799316,
      "learning_rate": 4.703902522154056e-05,
      "loss": 0.6885,
      "step": 1890
    },
    {
      "epoch": 0.3170365426330719,
      "grad_norm": 4.946151256561279,
      "learning_rate": 4.701772324471711e-05,
      "loss": 0.9954,
      "step": 1900
    },
    {
      "epoch": 0.31870515601535127,
      "grad_norm": 7.9252190589904785,
      "learning_rate": 4.6996421267893665e-05,
      "loss": 0.8228,
      "step": 1910
    },
    {
      "epoch": 0.32037376939763057,
      "grad_norm": 7.7471923828125,
      "learning_rate": 4.6975119291070217e-05,
      "loss": 0.7838,
      "step": 1920
    },
    {
      "epoch": 0.3220423827799099,
      "grad_norm": 7.640753269195557,
      "learning_rate": 4.695381731424676e-05,
      "loss": 0.9746,
      "step": 1930
    },
    {
      "epoch": 0.32371099616218924,
      "grad_norm": 10.363265037536621,
      "learning_rate": 4.693251533742332e-05,
      "loss": 1.0445,
      "step": 1940
    },
    {
      "epoch": 0.32537960954446854,
      "grad_norm": 6.390927791595459,
      "learning_rate": 4.6911213360599864e-05,
      "loss": 0.7316,
      "step": 1950
    },
    {
      "epoch": 0.3270482229267479,
      "grad_norm": 3.888888120651245,
      "learning_rate": 4.688991138377642e-05,
      "loss": 0.6778,
      "step": 1960
    },
    {
      "epoch": 0.3287168363090272,
      "grad_norm": 1.3898848295211792,
      "learning_rate": 4.6868609406952966e-05,
      "loss": 0.5201,
      "step": 1970
    },
    {
      "epoch": 0.3303854496913065,
      "grad_norm": 5.108023643493652,
      "learning_rate": 4.684730743012952e-05,
      "loss": 1.215,
      "step": 1980
    },
    {
      "epoch": 0.33205406307358587,
      "grad_norm": 3.7681901454925537,
      "learning_rate": 4.682600545330607e-05,
      "loss": 0.6188,
      "step": 1990
    },
    {
      "epoch": 0.3337226764558652,
      "grad_norm": 5.549030303955078,
      "learning_rate": 4.680470347648262e-05,
      "loss": 0.7019,
      "step": 2000
    },
    {
      "epoch": 0.3353912898381445,
      "grad_norm": 11.484082221984863,
      "learning_rate": 4.678340149965917e-05,
      "loss": 0.8456,
      "step": 2010
    },
    {
      "epoch": 0.33705990322042384,
      "grad_norm": 6.844860553741455,
      "learning_rate": 4.676209952283572e-05,
      "loss": 0.7103,
      "step": 2020
    },
    {
      "epoch": 0.33872851660270314,
      "grad_norm": 7.066591262817383,
      "learning_rate": 4.6740797546012274e-05,
      "loss": 0.6699,
      "step": 2030
    },
    {
      "epoch": 0.3403971299849825,
      "grad_norm": 10.624295234680176,
      "learning_rate": 4.671949556918882e-05,
      "loss": 0.798,
      "step": 2040
    },
    {
      "epoch": 0.3420657433672618,
      "grad_norm": 6.141763210296631,
      "learning_rate": 4.669819359236538e-05,
      "loss": 0.7647,
      "step": 2050
    },
    {
      "epoch": 0.3437343567495411,
      "grad_norm": 7.789971828460693,
      "learning_rate": 4.667689161554192e-05,
      "loss": 0.8635,
      "step": 2060
    },
    {
      "epoch": 0.3454029701318205,
      "grad_norm": 6.480897426605225,
      "learning_rate": 4.665558963871848e-05,
      "loss": 0.6378,
      "step": 2070
    },
    {
      "epoch": 0.3470715835140998,
      "grad_norm": 3.795684576034546,
      "learning_rate": 4.6634287661895024e-05,
      "loss": 0.8532,
      "step": 2080
    },
    {
      "epoch": 0.3487401968963791,
      "grad_norm": 10.762843132019043,
      "learning_rate": 4.6612985685071576e-05,
      "loss": 0.6584,
      "step": 2090
    },
    {
      "epoch": 0.35040881027865844,
      "grad_norm": 4.735290050506592,
      "learning_rate": 4.659168370824813e-05,
      "loss": 0.6932,
      "step": 2100
    },
    {
      "epoch": 0.35207742366093775,
      "grad_norm": 13.710530281066895,
      "learning_rate": 4.657038173142468e-05,
      "loss": 1.0869,
      "step": 2110
    },
    {
      "epoch": 0.3537460370432171,
      "grad_norm": 5.3069047927856445,
      "learning_rate": 4.654907975460123e-05,
      "loss": 0.7804,
      "step": 2120
    },
    {
      "epoch": 0.3554146504254964,
      "grad_norm": 10.247106552124023,
      "learning_rate": 4.652777777777778e-05,
      "loss": 0.6502,
      "step": 2130
    },
    {
      "epoch": 0.3570832638077757,
      "grad_norm": 0.7661415934562683,
      "learning_rate": 4.650647580095433e-05,
      "loss": 0.6462,
      "step": 2140
    },
    {
      "epoch": 0.3587518771900551,
      "grad_norm": 7.576444625854492,
      "learning_rate": 4.6485173824130884e-05,
      "loss": 0.779,
      "step": 2150
    },
    {
      "epoch": 0.3604204905723344,
      "grad_norm": 6.489376544952393,
      "learning_rate": 4.6463871847307435e-05,
      "loss": 0.8445,
      "step": 2160
    },
    {
      "epoch": 0.3620891039546137,
      "grad_norm": 8.402504920959473,
      "learning_rate": 4.644256987048398e-05,
      "loss": 0.9178,
      "step": 2170
    },
    {
      "epoch": 0.36375771733689305,
      "grad_norm": 10.369796752929688,
      "learning_rate": 4.642126789366054e-05,
      "loss": 0.7305,
      "step": 2180
    },
    {
      "epoch": 0.36542633071917235,
      "grad_norm": 7.1052141189575195,
      "learning_rate": 4.639996591683708e-05,
      "loss": 0.8199,
      "step": 2190
    },
    {
      "epoch": 0.3670949441014517,
      "grad_norm": 4.7416605949401855,
      "learning_rate": 4.637866394001363e-05,
      "loss": 0.7041,
      "step": 2200
    },
    {
      "epoch": 0.368763557483731,
      "grad_norm": 3.7465391159057617,
      "learning_rate": 4.6357361963190185e-05,
      "loss": 0.6012,
      "step": 2210
    },
    {
      "epoch": 0.3704321708660103,
      "grad_norm": 7.266491889953613,
      "learning_rate": 4.6336059986366736e-05,
      "loss": 0.7317,
      "step": 2220
    },
    {
      "epoch": 0.3721007842482897,
      "grad_norm": 3.5904736518859863,
      "learning_rate": 4.631475800954329e-05,
      "loss": 0.6366,
      "step": 2230
    },
    {
      "epoch": 0.373769397630569,
      "grad_norm": 7.939891815185547,
      "learning_rate": 4.629345603271984e-05,
      "loss": 0.9024,
      "step": 2240
    },
    {
      "epoch": 0.37543801101284835,
      "grad_norm": 5.7376627922058105,
      "learning_rate": 4.627215405589639e-05,
      "loss": 0.682,
      "step": 2250
    },
    {
      "epoch": 0.37710662439512765,
      "grad_norm": 5.6247239112854,
      "learning_rate": 4.625085207907294e-05,
      "loss": 0.6612,
      "step": 2260
    },
    {
      "epoch": 0.37877523777740696,
      "grad_norm": 4.2794623374938965,
      "learning_rate": 4.622955010224949e-05,
      "loss": 0.7075,
      "step": 2270
    },
    {
      "epoch": 0.3804438511596863,
      "grad_norm": 3.1080405712127686,
      "learning_rate": 4.620824812542604e-05,
      "loss": 1.0935,
      "step": 2280
    },
    {
      "epoch": 0.3821124645419656,
      "grad_norm": 8.822911262512207,
      "learning_rate": 4.6186946148602595e-05,
      "loss": 0.5237,
      "step": 2290
    },
    {
      "epoch": 0.38378107792424493,
      "grad_norm": 4.422133445739746,
      "learning_rate": 4.616564417177914e-05,
      "loss": 0.651,
      "step": 2300
    },
    {
      "epoch": 0.3854496913065243,
      "grad_norm": 4.939207553863525,
      "learning_rate": 4.614434219495569e-05,
      "loss": 0.5488,
      "step": 2310
    },
    {
      "epoch": 0.3871183046888036,
      "grad_norm": 5.294416427612305,
      "learning_rate": 4.612304021813225e-05,
      "loss": 0.6721,
      "step": 2320
    },
    {
      "epoch": 0.38878691807108295,
      "grad_norm": 6.780736446380615,
      "learning_rate": 4.6101738241308794e-05,
      "loss": 0.8376,
      "step": 2330
    },
    {
      "epoch": 0.39045553145336226,
      "grad_norm": 7.609031677246094,
      "learning_rate": 4.6080436264485345e-05,
      "loss": 0.6816,
      "step": 2340
    },
    {
      "epoch": 0.39212414483564156,
      "grad_norm": 4.133094310760498,
      "learning_rate": 4.6059134287661897e-05,
      "loss": 0.7729,
      "step": 2350
    },
    {
      "epoch": 0.3937927582179209,
      "grad_norm": 8.204477310180664,
      "learning_rate": 4.603783231083845e-05,
      "loss": 1.0267,
      "step": 2360
    },
    {
      "epoch": 0.39546137160020023,
      "grad_norm": 17.35120964050293,
      "learning_rate": 4.6016530334015e-05,
      "loss": 0.8972,
      "step": 2370
    },
    {
      "epoch": 0.39712998498247953,
      "grad_norm": 5.44019889831543,
      "learning_rate": 4.599522835719155e-05,
      "loss": 1.0079,
      "step": 2380
    },
    {
      "epoch": 0.3987985983647589,
      "grad_norm": 4.769240379333496,
      "learning_rate": 4.59739263803681e-05,
      "loss": 0.7596,
      "step": 2390
    }
  ],
  "logging_steps": 10,
  "max_steps": 23972,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 2398,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.795535591325696e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
