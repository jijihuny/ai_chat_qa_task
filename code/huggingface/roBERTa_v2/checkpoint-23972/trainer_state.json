{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 23972,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016686133822793258,
      "grad_norm": 12.055155754089355,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 4.3695,
      "step": 10
    },
    {
      "epoch": 0.0033372267645586516,
      "grad_norm": 11.911678314208984,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 4.4665,
      "step": 20
    },
    {
      "epoch": 0.005005840146837978,
      "grad_norm": 7.000014781951904,
      "learning_rate": 3e-06,
      "loss": 2.8126,
      "step": 30
    },
    {
      "epoch": 0.006674453529117303,
      "grad_norm": 13.199590682983398,
      "learning_rate": 4.000000000000001e-06,
      "loss": 4.2464,
      "step": 40
    },
    {
      "epoch": 0.00834306691139663,
      "grad_norm": 14.029197692871094,
      "learning_rate": 5e-06,
      "loss": 4.1144,
      "step": 50
    },
    {
      "epoch": 0.010011680293675955,
      "grad_norm": 9.302464485168457,
      "learning_rate": 6e-06,
      "loss": 3.3256,
      "step": 60
    },
    {
      "epoch": 0.011680293675955281,
      "grad_norm": 15.014753341674805,
      "learning_rate": 7.000000000000001e-06,
      "loss": 2.4877,
      "step": 70
    },
    {
      "epoch": 0.013348907058234607,
      "grad_norm": 9.79970932006836,
      "learning_rate": 8.000000000000001e-06,
      "loss": 3.5089,
      "step": 80
    },
    {
      "epoch": 0.015017520440513932,
      "grad_norm": 13.779865264892578,
      "learning_rate": 9e-06,
      "loss": 3.2794,
      "step": 90
    },
    {
      "epoch": 0.01668613382279326,
      "grad_norm": 11.301551818847656,
      "learning_rate": 1e-05,
      "loss": 3.6684,
      "step": 100
    },
    {
      "epoch": 0.018354747205072585,
      "grad_norm": 12.95022201538086,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 2.6943,
      "step": 110
    },
    {
      "epoch": 0.02002336058735191,
      "grad_norm": 9.577805519104004,
      "learning_rate": 1.2e-05,
      "loss": 3.3026,
      "step": 120
    },
    {
      "epoch": 0.021691973969631236,
      "grad_norm": 6.582576751708984,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 3.1611,
      "step": 130
    },
    {
      "epoch": 0.023360587351910562,
      "grad_norm": 9.120842933654785,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 2.5691,
      "step": 140
    },
    {
      "epoch": 0.025029200734189887,
      "grad_norm": 1.7658172845840454,
      "learning_rate": 1.5e-05,
      "loss": 2.146,
      "step": 150
    },
    {
      "epoch": 0.026697814116469213,
      "grad_norm": 9.7316255569458,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 2.7907,
      "step": 160
    },
    {
      "epoch": 0.02836642749874854,
      "grad_norm": 17.165279388427734,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 2.0825,
      "step": 170
    },
    {
      "epoch": 0.030035040881027864,
      "grad_norm": 7.478819847106934,
      "learning_rate": 1.8e-05,
      "loss": 2.0492,
      "step": 180
    },
    {
      "epoch": 0.03170365426330719,
      "grad_norm": 7.041898727416992,
      "learning_rate": 1.9e-05,
      "loss": 1.4729,
      "step": 190
    },
    {
      "epoch": 0.03337226764558652,
      "grad_norm": 9.482068061828613,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 200
    },
    {
      "epoch": 0.035040881027865844,
      "grad_norm": 4.689290523529053,
      "learning_rate": 2.1e-05,
      "loss": 1.3763,
      "step": 210
    },
    {
      "epoch": 0.03670949441014517,
      "grad_norm": 7.296417236328125,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.4751,
      "step": 220
    },
    {
      "epoch": 0.038378107792424496,
      "grad_norm": 11.922574996948242,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 2.0907,
      "step": 230
    },
    {
      "epoch": 0.04004672117470382,
      "grad_norm": 5.450417995452881,
      "learning_rate": 2.4e-05,
      "loss": 1.7902,
      "step": 240
    },
    {
      "epoch": 0.04171533455698315,
      "grad_norm": 7.988636493682861,
      "learning_rate": 2.5e-05,
      "loss": 2.0059,
      "step": 250
    },
    {
      "epoch": 0.04338394793926247,
      "grad_norm": 5.704071521759033,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.4196,
      "step": 260
    },
    {
      "epoch": 0.0450525613215418,
      "grad_norm": 5.168555736541748,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.4292,
      "step": 270
    },
    {
      "epoch": 0.046721174703821124,
      "grad_norm": 5.6051435470581055,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.5838,
      "step": 280
    },
    {
      "epoch": 0.04838978808610045,
      "grad_norm": 8.265167236328125,
      "learning_rate": 2.9e-05,
      "loss": 1.5459,
      "step": 290
    },
    {
      "epoch": 0.050058401468379775,
      "grad_norm": 6.930691719055176,
      "learning_rate": 3e-05,
      "loss": 1.6044,
      "step": 300
    },
    {
      "epoch": 0.0517270148506591,
      "grad_norm": 7.2446699142456055,
      "learning_rate": 3.1e-05,
      "loss": 1.6727,
      "step": 310
    },
    {
      "epoch": 0.053395628232938426,
      "grad_norm": 5.798071384429932,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.667,
      "step": 320
    },
    {
      "epoch": 0.05506424161521775,
      "grad_norm": 7.059964179992676,
      "learning_rate": 3.3e-05,
      "loss": 1.619,
      "step": 330
    },
    {
      "epoch": 0.05673285499749708,
      "grad_norm": 5.899831295013428,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.0725,
      "step": 340
    },
    {
      "epoch": 0.0584014683797764,
      "grad_norm": 5.815085411071777,
      "learning_rate": 3.5e-05,
      "loss": 1.3692,
      "step": 350
    },
    {
      "epoch": 0.06007008176205573,
      "grad_norm": 5.109336853027344,
      "learning_rate": 3.6e-05,
      "loss": 1.3641,
      "step": 360
    },
    {
      "epoch": 0.06173869514433506,
      "grad_norm": 4.846375465393066,
      "learning_rate": 3.7e-05,
      "loss": 1.5955,
      "step": 370
    },
    {
      "epoch": 0.06340730852661439,
      "grad_norm": 9.159847259521484,
      "learning_rate": 3.8e-05,
      "loss": 1.663,
      "step": 380
    },
    {
      "epoch": 0.0650759219088937,
      "grad_norm": 6.52689790725708,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.1499,
      "step": 390
    },
    {
      "epoch": 0.06674453529117304,
      "grad_norm": 8.106498718261719,
      "learning_rate": 4e-05,
      "loss": 1.0195,
      "step": 400
    },
    {
      "epoch": 0.06841314867345236,
      "grad_norm": 7.403848648071289,
      "learning_rate": 4.1e-05,
      "loss": 1.0374,
      "step": 410
    },
    {
      "epoch": 0.07008176205573169,
      "grad_norm": 10.298356056213379,
      "learning_rate": 4.2e-05,
      "loss": 1.4083,
      "step": 420
    },
    {
      "epoch": 0.07175037543801101,
      "grad_norm": 8.604483604431152,
      "learning_rate": 4.3e-05,
      "loss": 0.9467,
      "step": 430
    },
    {
      "epoch": 0.07341898882029034,
      "grad_norm": 7.670446395874023,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.5065,
      "step": 440
    },
    {
      "epoch": 0.07508760220256966,
      "grad_norm": 6.878382682800293,
      "learning_rate": 4.5e-05,
      "loss": 1.2361,
      "step": 450
    },
    {
      "epoch": 0.07675621558484899,
      "grad_norm": 9.739946365356445,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.376,
      "step": 460
    },
    {
      "epoch": 0.07842482896712831,
      "grad_norm": 2.4443745613098145,
      "learning_rate": 4.7e-05,
      "loss": 1.0508,
      "step": 470
    },
    {
      "epoch": 0.08009344234940764,
      "grad_norm": 5.105490207672119,
      "learning_rate": 4.8e-05,
      "loss": 0.9467,
      "step": 480
    },
    {
      "epoch": 0.08176205573168698,
      "grad_norm": 1.9795540571212769,
      "learning_rate": 4.9e-05,
      "loss": 1.0421,
      "step": 490
    },
    {
      "epoch": 0.0834306691139663,
      "grad_norm": 13.910361289978027,
      "learning_rate": 5e-05,
      "loss": 0.9834,
      "step": 500
    },
    {
      "epoch": 0.08509928249624563,
      "grad_norm": 5.3670220375061035,
      "learning_rate": 4.9978698023176554e-05,
      "loss": 1.2034,
      "step": 510
    },
    {
      "epoch": 0.08676789587852494,
      "grad_norm": 8.683828353881836,
      "learning_rate": 4.9957396046353105e-05,
      "loss": 0.9199,
      "step": 520
    },
    {
      "epoch": 0.08843650926080428,
      "grad_norm": 8.722528457641602,
      "learning_rate": 4.9936094069529656e-05,
      "loss": 1.3851,
      "step": 530
    },
    {
      "epoch": 0.0901051226430836,
      "grad_norm": 5.5807881355285645,
      "learning_rate": 4.99147920927062e-05,
      "loss": 1.1402,
      "step": 540
    },
    {
      "epoch": 0.09177373602536293,
      "grad_norm": 7.571084976196289,
      "learning_rate": 4.989349011588276e-05,
      "loss": 1.3795,
      "step": 550
    },
    {
      "epoch": 0.09344234940764225,
      "grad_norm": 7.37730598449707,
      "learning_rate": 4.9872188139059304e-05,
      "loss": 0.81,
      "step": 560
    },
    {
      "epoch": 0.09511096278992158,
      "grad_norm": 10.168316841125488,
      "learning_rate": 4.985088616223586e-05,
      "loss": 1.3838,
      "step": 570
    },
    {
      "epoch": 0.0967795761722009,
      "grad_norm": 10.4512357711792,
      "learning_rate": 4.9829584185412406e-05,
      "loss": 0.9509,
      "step": 580
    },
    {
      "epoch": 0.09844818955448023,
      "grad_norm": 11.75351333618164,
      "learning_rate": 4.980828220858896e-05,
      "loss": 1.142,
      "step": 590
    },
    {
      "epoch": 0.10011680293675955,
      "grad_norm": 7.678175926208496,
      "learning_rate": 4.978698023176551e-05,
      "loss": 1.0443,
      "step": 600
    },
    {
      "epoch": 0.10178541631903888,
      "grad_norm": 5.490517616271973,
      "learning_rate": 4.976567825494206e-05,
      "loss": 1.269,
      "step": 610
    },
    {
      "epoch": 0.1034540297013182,
      "grad_norm": 6.006327152252197,
      "learning_rate": 4.974437627811861e-05,
      "loss": 0.9879,
      "step": 620
    },
    {
      "epoch": 0.10512264308359753,
      "grad_norm": 8.454747200012207,
      "learning_rate": 4.972307430129516e-05,
      "loss": 1.121,
      "step": 630
    },
    {
      "epoch": 0.10679125646587685,
      "grad_norm": 4.379626750946045,
      "learning_rate": 4.9701772324471714e-05,
      "loss": 1.2119,
      "step": 640
    },
    {
      "epoch": 0.10845986984815618,
      "grad_norm": 8.560583114624023,
      "learning_rate": 4.9680470347648266e-05,
      "loss": 0.933,
      "step": 650
    },
    {
      "epoch": 0.1101284832304355,
      "grad_norm": 5.742160797119141,
      "learning_rate": 4.965916837082482e-05,
      "loss": 1.3424,
      "step": 660
    },
    {
      "epoch": 0.11179709661271484,
      "grad_norm": 8.295968055725098,
      "learning_rate": 4.963786639400136e-05,
      "loss": 1.1824,
      "step": 670
    },
    {
      "epoch": 0.11346570999499415,
      "grad_norm": 7.665077209472656,
      "learning_rate": 4.961656441717792e-05,
      "loss": 1.0148,
      "step": 680
    },
    {
      "epoch": 0.11513432337727349,
      "grad_norm": 2.2763729095458984,
      "learning_rate": 4.9595262440354464e-05,
      "loss": 1.2039,
      "step": 690
    },
    {
      "epoch": 0.1168029367595528,
      "grad_norm": 3.3238821029663086,
      "learning_rate": 4.9573960463531015e-05,
      "loss": 0.9954,
      "step": 700
    },
    {
      "epoch": 0.11847155014183214,
      "grad_norm": 5.773782253265381,
      "learning_rate": 4.9552658486707574e-05,
      "loss": 0.8885,
      "step": 710
    },
    {
      "epoch": 0.12014016352411146,
      "grad_norm": 5.722370147705078,
      "learning_rate": 4.953135650988412e-05,
      "loss": 0.843,
      "step": 720
    },
    {
      "epoch": 0.12180877690639079,
      "grad_norm": 4.529634475708008,
      "learning_rate": 4.951005453306067e-05,
      "loss": 0.9794,
      "step": 730
    },
    {
      "epoch": 0.12347739028867012,
      "grad_norm": 12.99602222442627,
      "learning_rate": 4.948875255623722e-05,
      "loss": 1.4469,
      "step": 740
    },
    {
      "epoch": 0.12514600367094944,
      "grad_norm": 5.601666450500488,
      "learning_rate": 4.946745057941377e-05,
      "loss": 1.0332,
      "step": 750
    },
    {
      "epoch": 0.12681461705322877,
      "grad_norm": 5.496005535125732,
      "learning_rate": 4.944614860259032e-05,
      "loss": 1.2491,
      "step": 760
    },
    {
      "epoch": 0.1284832304355081,
      "grad_norm": 7.067525863647461,
      "learning_rate": 4.9424846625766875e-05,
      "loss": 0.9724,
      "step": 770
    },
    {
      "epoch": 0.1301518438177874,
      "grad_norm": 6.886786460876465,
      "learning_rate": 4.9403544648943426e-05,
      "loss": 0.9522,
      "step": 780
    },
    {
      "epoch": 0.13182045720006674,
      "grad_norm": 7.650302886962891,
      "learning_rate": 4.938224267211998e-05,
      "loss": 1.2355,
      "step": 790
    },
    {
      "epoch": 0.13348907058234608,
      "grad_norm": 14.218182563781738,
      "learning_rate": 4.936094069529653e-05,
      "loss": 0.9778,
      "step": 800
    },
    {
      "epoch": 0.1351576839646254,
      "grad_norm": 5.353604316711426,
      "learning_rate": 4.933963871847307e-05,
      "loss": 1.1231,
      "step": 810
    },
    {
      "epoch": 0.1368262973469047,
      "grad_norm": 10.568121910095215,
      "learning_rate": 4.931833674164963e-05,
      "loss": 1.0955,
      "step": 820
    },
    {
      "epoch": 0.13849491072918405,
      "grad_norm": 8.434700012207031,
      "learning_rate": 4.9297034764826176e-05,
      "loss": 0.8542,
      "step": 830
    },
    {
      "epoch": 0.14016352411146338,
      "grad_norm": 11.020655632019043,
      "learning_rate": 4.927573278800273e-05,
      "loss": 1.0711,
      "step": 840
    },
    {
      "epoch": 0.1418321374937427,
      "grad_norm": 6.90020751953125,
      "learning_rate": 4.925443081117928e-05,
      "loss": 1.0857,
      "step": 850
    },
    {
      "epoch": 0.14350075087602202,
      "grad_norm": 12.6170072555542,
      "learning_rate": 4.923312883435583e-05,
      "loss": 1.1352,
      "step": 860
    },
    {
      "epoch": 0.14516936425830135,
      "grad_norm": 5.879394054412842,
      "learning_rate": 4.921182685753238e-05,
      "loss": 1.1006,
      "step": 870
    },
    {
      "epoch": 0.14683797764058068,
      "grad_norm": 8.519241333007812,
      "learning_rate": 4.919052488070893e-05,
      "loss": 0.9405,
      "step": 880
    },
    {
      "epoch": 0.14850659102286,
      "grad_norm": 12.018880844116211,
      "learning_rate": 4.9169222903885484e-05,
      "loss": 1.0162,
      "step": 890
    },
    {
      "epoch": 0.15017520440513932,
      "grad_norm": 8.296252250671387,
      "learning_rate": 4.9147920927062035e-05,
      "loss": 0.9461,
      "step": 900
    },
    {
      "epoch": 0.15184381778741865,
      "grad_norm": 1.661017656326294,
      "learning_rate": 4.9126618950238587e-05,
      "loss": 0.8436,
      "step": 910
    },
    {
      "epoch": 0.15351243116969798,
      "grad_norm": 12.7459135055542,
      "learning_rate": 4.910531697341513e-05,
      "loss": 1.0236,
      "step": 920
    },
    {
      "epoch": 0.15518104455197732,
      "grad_norm": 7.691208362579346,
      "learning_rate": 4.908401499659169e-05,
      "loss": 0.7564,
      "step": 930
    },
    {
      "epoch": 0.15684965793425662,
      "grad_norm": 5.071497917175293,
      "learning_rate": 4.9062713019768234e-05,
      "loss": 1.0305,
      "step": 940
    },
    {
      "epoch": 0.15851827131653595,
      "grad_norm": 11.30626392364502,
      "learning_rate": 4.904141104294479e-05,
      "loss": 1.1542,
      "step": 950
    },
    {
      "epoch": 0.16018688469881529,
      "grad_norm": 3.5152103900909424,
      "learning_rate": 4.9020109066121336e-05,
      "loss": 1.1834,
      "step": 960
    },
    {
      "epoch": 0.16185549808109462,
      "grad_norm": 14.171195983886719,
      "learning_rate": 4.899880708929789e-05,
      "loss": 0.8233,
      "step": 970
    },
    {
      "epoch": 0.16352411146337395,
      "grad_norm": 10.198997497558594,
      "learning_rate": 4.897750511247444e-05,
      "loss": 0.8698,
      "step": 980
    },
    {
      "epoch": 0.16519272484565325,
      "grad_norm": 6.394326686859131,
      "learning_rate": 4.895620313565099e-05,
      "loss": 0.9039,
      "step": 990
    },
    {
      "epoch": 0.1668613382279326,
      "grad_norm": 5.254652976989746,
      "learning_rate": 4.893490115882754e-05,
      "loss": 0.9891,
      "step": 1000
    },
    {
      "epoch": 0.16852995161021192,
      "grad_norm": 15.370824813842773,
      "learning_rate": 4.891359918200409e-05,
      "loss": 0.8715,
      "step": 1010
    },
    {
      "epoch": 0.17019856499249125,
      "grad_norm": 7.787691593170166,
      "learning_rate": 4.8892297205180644e-05,
      "loss": 0.7746,
      "step": 1020
    },
    {
      "epoch": 0.17186717837477056,
      "grad_norm": 11.494077682495117,
      "learning_rate": 4.887099522835719e-05,
      "loss": 0.7561,
      "step": 1030
    },
    {
      "epoch": 0.1735357917570499,
      "grad_norm": 2.676873207092285,
      "learning_rate": 4.884969325153375e-05,
      "loss": 0.8851,
      "step": 1040
    },
    {
      "epoch": 0.17520440513932922,
      "grad_norm": 7.112656593322754,
      "learning_rate": 4.882839127471029e-05,
      "loss": 0.8012,
      "step": 1050
    },
    {
      "epoch": 0.17687301852160855,
      "grad_norm": 13.557592391967773,
      "learning_rate": 4.880708929788685e-05,
      "loss": 0.8545,
      "step": 1060
    },
    {
      "epoch": 0.17854163190388786,
      "grad_norm": 8.752182006835938,
      "learning_rate": 4.8785787321063394e-05,
      "loss": 1.0867,
      "step": 1070
    },
    {
      "epoch": 0.1802102452861672,
      "grad_norm": 14.975482940673828,
      "learning_rate": 4.8764485344239946e-05,
      "loss": 1.0206,
      "step": 1080
    },
    {
      "epoch": 0.18187885866844652,
      "grad_norm": 7.262783527374268,
      "learning_rate": 4.8743183367416504e-05,
      "loss": 1.142,
      "step": 1090
    },
    {
      "epoch": 0.18354747205072586,
      "grad_norm": 11.608201026916504,
      "learning_rate": 4.872188139059305e-05,
      "loss": 0.8128,
      "step": 1100
    },
    {
      "epoch": 0.18521608543300516,
      "grad_norm": 6.071269989013672,
      "learning_rate": 4.87005794137696e-05,
      "loss": 0.7834,
      "step": 1110
    },
    {
      "epoch": 0.1868846988152845,
      "grad_norm": 5.2202324867248535,
      "learning_rate": 4.867927743694615e-05,
      "loss": 1.1932,
      "step": 1120
    },
    {
      "epoch": 0.18855331219756383,
      "grad_norm": 6.854410648345947,
      "learning_rate": 4.86579754601227e-05,
      "loss": 0.8795,
      "step": 1130
    },
    {
      "epoch": 0.19022192557984316,
      "grad_norm": 6.516758441925049,
      "learning_rate": 4.863667348329925e-05,
      "loss": 0.9976,
      "step": 1140
    },
    {
      "epoch": 0.19189053896212246,
      "grad_norm": 6.325408935546875,
      "learning_rate": 4.8615371506475805e-05,
      "loss": 0.8822,
      "step": 1150
    },
    {
      "epoch": 0.1935591523444018,
      "grad_norm": 15.320943832397461,
      "learning_rate": 4.8594069529652356e-05,
      "loss": 0.6054,
      "step": 1160
    },
    {
      "epoch": 0.19522776572668113,
      "grad_norm": 8.960070610046387,
      "learning_rate": 4.857276755282891e-05,
      "loss": 0.9196,
      "step": 1170
    },
    {
      "epoch": 0.19689637910896046,
      "grad_norm": 7.015202045440674,
      "learning_rate": 4.855146557600546e-05,
      "loss": 0.6792,
      "step": 1180
    },
    {
      "epoch": 0.19856499249123977,
      "grad_norm": 8.809013366699219,
      "learning_rate": 4.8530163599182003e-05,
      "loss": 1.2103,
      "step": 1190
    },
    {
      "epoch": 0.2002336058735191,
      "grad_norm": 5.754108905792236,
      "learning_rate": 4.850886162235856e-05,
      "loss": 1.2232,
      "step": 1200
    },
    {
      "epoch": 0.20190221925579843,
      "grad_norm": 7.985579490661621,
      "learning_rate": 4.8487559645535106e-05,
      "loss": 1.1816,
      "step": 1210
    },
    {
      "epoch": 0.20357083263807776,
      "grad_norm": 2.9442548751831055,
      "learning_rate": 4.846625766871166e-05,
      "loss": 0.8967,
      "step": 1220
    },
    {
      "epoch": 0.2052394460203571,
      "grad_norm": 9.350631713867188,
      "learning_rate": 4.844495569188821e-05,
      "loss": 0.9031,
      "step": 1230
    },
    {
      "epoch": 0.2069080594026364,
      "grad_norm": 6.945474147796631,
      "learning_rate": 4.842365371506476e-05,
      "loss": 0.9129,
      "step": 1240
    },
    {
      "epoch": 0.20857667278491573,
      "grad_norm": 10.188994407653809,
      "learning_rate": 4.840235173824131e-05,
      "loss": 0.9243,
      "step": 1250
    },
    {
      "epoch": 0.21024528616719507,
      "grad_norm": 8.459782600402832,
      "learning_rate": 4.838104976141786e-05,
      "loss": 0.5827,
      "step": 1260
    },
    {
      "epoch": 0.2119138995494744,
      "grad_norm": 5.117980003356934,
      "learning_rate": 4.8359747784594414e-05,
      "loss": 0.8088,
      "step": 1270
    },
    {
      "epoch": 0.2135825129317537,
      "grad_norm": 11.113184928894043,
      "learning_rate": 4.8338445807770965e-05,
      "loss": 1.117,
      "step": 1280
    },
    {
      "epoch": 0.21525112631403304,
      "grad_norm": 8.368911743164062,
      "learning_rate": 4.831714383094752e-05,
      "loss": 0.9381,
      "step": 1290
    },
    {
      "epoch": 0.21691973969631237,
      "grad_norm": 3.736769914627075,
      "learning_rate": 4.829584185412406e-05,
      "loss": 0.715,
      "step": 1300
    },
    {
      "epoch": 0.2185883530785917,
      "grad_norm": 5.07656192779541,
      "learning_rate": 4.827453987730062e-05,
      "loss": 1.0564,
      "step": 1310
    },
    {
      "epoch": 0.220256966460871,
      "grad_norm": 3.2761898040771484,
      "learning_rate": 4.8253237900477164e-05,
      "loss": 0.972,
      "step": 1320
    },
    {
      "epoch": 0.22192557984315034,
      "grad_norm": 3.562837600708008,
      "learning_rate": 4.8231935923653715e-05,
      "loss": 0.8114,
      "step": 1330
    },
    {
      "epoch": 0.22359419322542967,
      "grad_norm": 5.412622451782227,
      "learning_rate": 4.8210633946830267e-05,
      "loss": 0.6172,
      "step": 1340
    },
    {
      "epoch": 0.225262806607709,
      "grad_norm": 4.77771520614624,
      "learning_rate": 4.818933197000682e-05,
      "loss": 0.6817,
      "step": 1350
    },
    {
      "epoch": 0.2269314199899883,
      "grad_norm": 13.6343994140625,
      "learning_rate": 4.816802999318337e-05,
      "loss": 1.0091,
      "step": 1360
    },
    {
      "epoch": 0.22860003337226764,
      "grad_norm": 9.30815601348877,
      "learning_rate": 4.814672801635992e-05,
      "loss": 0.9068,
      "step": 1370
    },
    {
      "epoch": 0.23026864675454697,
      "grad_norm": 6.772705554962158,
      "learning_rate": 4.812542603953647e-05,
      "loss": 0.9162,
      "step": 1380
    },
    {
      "epoch": 0.2319372601368263,
      "grad_norm": 4.869386196136475,
      "learning_rate": 4.810412406271302e-05,
      "loss": 0.9245,
      "step": 1390
    },
    {
      "epoch": 0.2336058735191056,
      "grad_norm": 4.640839099884033,
      "learning_rate": 4.8082822085889575e-05,
      "loss": 0.7241,
      "step": 1400
    },
    {
      "epoch": 0.23527448690138494,
      "grad_norm": 7.260838985443115,
      "learning_rate": 4.806152010906612e-05,
      "loss": 1.1159,
      "step": 1410
    },
    {
      "epoch": 0.23694310028366428,
      "grad_norm": 8.9894437789917,
      "learning_rate": 4.804021813224268e-05,
      "loss": 0.766,
      "step": 1420
    },
    {
      "epoch": 0.2386117136659436,
      "grad_norm": 5.941689491271973,
      "learning_rate": 4.801891615541922e-05,
      "loss": 1.0211,
      "step": 1430
    },
    {
      "epoch": 0.2402803270482229,
      "grad_norm": 6.099863529205322,
      "learning_rate": 4.799761417859577e-05,
      "loss": 1.1176,
      "step": 1440
    },
    {
      "epoch": 0.24194894043050225,
      "grad_norm": 18.322364807128906,
      "learning_rate": 4.7976312201772324e-05,
      "loss": 0.9547,
      "step": 1450
    },
    {
      "epoch": 0.24361755381278158,
      "grad_norm": 4.713418006896973,
      "learning_rate": 4.7955010224948876e-05,
      "loss": 1.0038,
      "step": 1460
    },
    {
      "epoch": 0.2452861671950609,
      "grad_norm": 5.954039573669434,
      "learning_rate": 4.7933708248125434e-05,
      "loss": 0.9859,
      "step": 1470
    },
    {
      "epoch": 0.24695478057734024,
      "grad_norm": 1.3553916215896606,
      "learning_rate": 4.791240627130198e-05,
      "loss": 0.8059,
      "step": 1480
    },
    {
      "epoch": 0.24862339395961955,
      "grad_norm": 4.211933612823486,
      "learning_rate": 4.789110429447853e-05,
      "loss": 0.8068,
      "step": 1490
    },
    {
      "epoch": 0.2502920073418989,
      "grad_norm": 10.577535629272461,
      "learning_rate": 4.786980231765508e-05,
      "loss": 0.9145,
      "step": 1500
    },
    {
      "epoch": 0.2519606207241782,
      "grad_norm": 12.839938163757324,
      "learning_rate": 4.784850034083163e-05,
      "loss": 1.0051,
      "step": 1510
    },
    {
      "epoch": 0.25362923410645755,
      "grad_norm": 10.59160041809082,
      "learning_rate": 4.782719836400818e-05,
      "loss": 0.7351,
      "step": 1520
    },
    {
      "epoch": 0.25529784748873685,
      "grad_norm": 6.757431507110596,
      "learning_rate": 4.7805896387184735e-05,
      "loss": 0.9431,
      "step": 1530
    },
    {
      "epoch": 0.2569664608710162,
      "grad_norm": 11.996758460998535,
      "learning_rate": 4.7784594410361286e-05,
      "loss": 0.9925,
      "step": 1540
    },
    {
      "epoch": 0.2586350742532955,
      "grad_norm": 7.938404560089111,
      "learning_rate": 4.776329243353784e-05,
      "loss": 1.077,
      "step": 1550
    },
    {
      "epoch": 0.2603036876355748,
      "grad_norm": 7.818968772888184,
      "learning_rate": 4.774199045671439e-05,
      "loss": 0.945,
      "step": 1560
    },
    {
      "epoch": 0.2619723010178542,
      "grad_norm": 7.099704265594482,
      "learning_rate": 4.7720688479890934e-05,
      "loss": 1.0696,
      "step": 1570
    },
    {
      "epoch": 0.2636409144001335,
      "grad_norm": 4.397469997406006,
      "learning_rate": 4.769938650306749e-05,
      "loss": 0.5265,
      "step": 1580
    },
    {
      "epoch": 0.2653095277824128,
      "grad_norm": 4.42056941986084,
      "learning_rate": 4.7678084526244036e-05,
      "loss": 1.0522,
      "step": 1590
    },
    {
      "epoch": 0.26697814116469215,
      "grad_norm": 9.443928718566895,
      "learning_rate": 4.765678254942059e-05,
      "loss": 1.0368,
      "step": 1600
    },
    {
      "epoch": 0.26864675454697146,
      "grad_norm": 3.618483543395996,
      "learning_rate": 4.763548057259714e-05,
      "loss": 0.8655,
      "step": 1610
    },
    {
      "epoch": 0.2703153679292508,
      "grad_norm": 6.898618698120117,
      "learning_rate": 4.761417859577369e-05,
      "loss": 0.9831,
      "step": 1620
    },
    {
      "epoch": 0.2719839813115301,
      "grad_norm": 8.875914573669434,
      "learning_rate": 4.759287661895024e-05,
      "loss": 0.7695,
      "step": 1630
    },
    {
      "epoch": 0.2736525946938094,
      "grad_norm": 9.395337104797363,
      "learning_rate": 4.757157464212679e-05,
      "loss": 0.8331,
      "step": 1640
    },
    {
      "epoch": 0.2753212080760888,
      "grad_norm": 8.659492492675781,
      "learning_rate": 4.7550272665303344e-05,
      "loss": 0.9533,
      "step": 1650
    },
    {
      "epoch": 0.2769898214583681,
      "grad_norm": 2.3157968521118164,
      "learning_rate": 4.7528970688479896e-05,
      "loss": 0.7154,
      "step": 1660
    },
    {
      "epoch": 0.2786584348406474,
      "grad_norm": 9.683950424194336,
      "learning_rate": 4.750766871165645e-05,
      "loss": 1.0787,
      "step": 1670
    },
    {
      "epoch": 0.28032704822292676,
      "grad_norm": 11.940595626831055,
      "learning_rate": 4.748636673483299e-05,
      "loss": 1.0501,
      "step": 1680
    },
    {
      "epoch": 0.28199566160520606,
      "grad_norm": 6.150341987609863,
      "learning_rate": 4.746506475800955e-05,
      "loss": 0.6178,
      "step": 1690
    },
    {
      "epoch": 0.2836642749874854,
      "grad_norm": 9.06094741821289,
      "learning_rate": 4.7443762781186094e-05,
      "loss": 0.9085,
      "step": 1700
    },
    {
      "epoch": 0.2853328883697647,
      "grad_norm": 12.801769256591797,
      "learning_rate": 4.7422460804362645e-05,
      "loss": 0.7393,
      "step": 1710
    },
    {
      "epoch": 0.28700150175204403,
      "grad_norm": 6.536427974700928,
      "learning_rate": 4.74011588275392e-05,
      "loss": 1.0826,
      "step": 1720
    },
    {
      "epoch": 0.2886701151343234,
      "grad_norm": 4.567477703094482,
      "learning_rate": 4.737985685071575e-05,
      "loss": 0.6673,
      "step": 1730
    },
    {
      "epoch": 0.2903387285166027,
      "grad_norm": 10.086455345153809,
      "learning_rate": 4.73585548738923e-05,
      "loss": 0.9971,
      "step": 1740
    },
    {
      "epoch": 0.29200734189888206,
      "grad_norm": 5.867030620574951,
      "learning_rate": 4.733725289706885e-05,
      "loss": 0.8847,
      "step": 1750
    },
    {
      "epoch": 0.29367595528116136,
      "grad_norm": 9.277796745300293,
      "learning_rate": 4.73159509202454e-05,
      "loss": 0.6552,
      "step": 1760
    },
    {
      "epoch": 0.29534456866344067,
      "grad_norm": 10.306794166564941,
      "learning_rate": 4.729464894342195e-05,
      "loss": 0.6757,
      "step": 1770
    },
    {
      "epoch": 0.29701318204572,
      "grad_norm": 15.923295974731445,
      "learning_rate": 4.7273346966598505e-05,
      "loss": 0.7507,
      "step": 1780
    },
    {
      "epoch": 0.29868179542799933,
      "grad_norm": 10.33800220489502,
      "learning_rate": 4.725204498977505e-05,
      "loss": 1.019,
      "step": 1790
    },
    {
      "epoch": 0.30035040881027864,
      "grad_norm": 6.398563385009766,
      "learning_rate": 4.723074301295161e-05,
      "loss": 1.0343,
      "step": 1800
    },
    {
      "epoch": 0.302019022192558,
      "grad_norm": 9.485482215881348,
      "learning_rate": 4.720944103612815e-05,
      "loss": 0.7798,
      "step": 1810
    },
    {
      "epoch": 0.3036876355748373,
      "grad_norm": 12.45145034790039,
      "learning_rate": 4.71881390593047e-05,
      "loss": 0.7365,
      "step": 1820
    },
    {
      "epoch": 0.30535624895711666,
      "grad_norm": 11.77038860321045,
      "learning_rate": 4.7166837082481255e-05,
      "loss": 0.5596,
      "step": 1830
    },
    {
      "epoch": 0.30702486233939597,
      "grad_norm": 8.677770614624023,
      "learning_rate": 4.7145535105657806e-05,
      "loss": 1.013,
      "step": 1840
    },
    {
      "epoch": 0.30869347572167527,
      "grad_norm": 9.020999908447266,
      "learning_rate": 4.7124233128834364e-05,
      "loss": 0.8081,
      "step": 1850
    },
    {
      "epoch": 0.31036208910395463,
      "grad_norm": 3.7962541580200195,
      "learning_rate": 4.710293115201091e-05,
      "loss": 0.8799,
      "step": 1860
    },
    {
      "epoch": 0.31203070248623394,
      "grad_norm": 4.856635570526123,
      "learning_rate": 4.708162917518746e-05,
      "loss": 0.8808,
      "step": 1870
    },
    {
      "epoch": 0.31369931586851324,
      "grad_norm": 4.366225242614746,
      "learning_rate": 4.706032719836401e-05,
      "loss": 0.7278,
      "step": 1880
    },
    {
      "epoch": 0.3153679292507926,
      "grad_norm": 8.835206031799316,
      "learning_rate": 4.703902522154056e-05,
      "loss": 0.6885,
      "step": 1890
    },
    {
      "epoch": 0.3170365426330719,
      "grad_norm": 4.946151256561279,
      "learning_rate": 4.701772324471711e-05,
      "loss": 0.9954,
      "step": 1900
    },
    {
      "epoch": 0.31870515601535127,
      "grad_norm": 7.9252190589904785,
      "learning_rate": 4.6996421267893665e-05,
      "loss": 0.8228,
      "step": 1910
    },
    {
      "epoch": 0.32037376939763057,
      "grad_norm": 7.7471923828125,
      "learning_rate": 4.6975119291070217e-05,
      "loss": 0.7838,
      "step": 1920
    },
    {
      "epoch": 0.3220423827799099,
      "grad_norm": 7.640753269195557,
      "learning_rate": 4.695381731424676e-05,
      "loss": 0.9746,
      "step": 1930
    },
    {
      "epoch": 0.32371099616218924,
      "grad_norm": 10.363265037536621,
      "learning_rate": 4.693251533742332e-05,
      "loss": 1.0445,
      "step": 1940
    },
    {
      "epoch": 0.32537960954446854,
      "grad_norm": 6.390927791595459,
      "learning_rate": 4.6911213360599864e-05,
      "loss": 0.7316,
      "step": 1950
    },
    {
      "epoch": 0.3270482229267479,
      "grad_norm": 3.888888120651245,
      "learning_rate": 4.688991138377642e-05,
      "loss": 0.6778,
      "step": 1960
    },
    {
      "epoch": 0.3287168363090272,
      "grad_norm": 1.3898848295211792,
      "learning_rate": 4.6868609406952966e-05,
      "loss": 0.5201,
      "step": 1970
    },
    {
      "epoch": 0.3303854496913065,
      "grad_norm": 5.108023643493652,
      "learning_rate": 4.684730743012952e-05,
      "loss": 1.215,
      "step": 1980
    },
    {
      "epoch": 0.33205406307358587,
      "grad_norm": 3.7681901454925537,
      "learning_rate": 4.682600545330607e-05,
      "loss": 0.6188,
      "step": 1990
    },
    {
      "epoch": 0.3337226764558652,
      "grad_norm": 5.549030303955078,
      "learning_rate": 4.680470347648262e-05,
      "loss": 0.7019,
      "step": 2000
    },
    {
      "epoch": 0.3353912898381445,
      "grad_norm": 11.484082221984863,
      "learning_rate": 4.678340149965917e-05,
      "loss": 0.8456,
      "step": 2010
    },
    {
      "epoch": 0.33705990322042384,
      "grad_norm": 6.844860553741455,
      "learning_rate": 4.676209952283572e-05,
      "loss": 0.7103,
      "step": 2020
    },
    {
      "epoch": 0.33872851660270314,
      "grad_norm": 7.066591262817383,
      "learning_rate": 4.6740797546012274e-05,
      "loss": 0.6699,
      "step": 2030
    },
    {
      "epoch": 0.3403971299849825,
      "grad_norm": 10.624295234680176,
      "learning_rate": 4.671949556918882e-05,
      "loss": 0.798,
      "step": 2040
    },
    {
      "epoch": 0.3420657433672618,
      "grad_norm": 6.141763210296631,
      "learning_rate": 4.669819359236538e-05,
      "loss": 0.7647,
      "step": 2050
    },
    {
      "epoch": 0.3437343567495411,
      "grad_norm": 7.789971828460693,
      "learning_rate": 4.667689161554192e-05,
      "loss": 0.8635,
      "step": 2060
    },
    {
      "epoch": 0.3454029701318205,
      "grad_norm": 6.480897426605225,
      "learning_rate": 4.665558963871848e-05,
      "loss": 0.6378,
      "step": 2070
    },
    {
      "epoch": 0.3470715835140998,
      "grad_norm": 3.795684576034546,
      "learning_rate": 4.6634287661895024e-05,
      "loss": 0.8532,
      "step": 2080
    },
    {
      "epoch": 0.3487401968963791,
      "grad_norm": 10.762843132019043,
      "learning_rate": 4.6612985685071576e-05,
      "loss": 0.6584,
      "step": 2090
    },
    {
      "epoch": 0.35040881027865844,
      "grad_norm": 4.735290050506592,
      "learning_rate": 4.659168370824813e-05,
      "loss": 0.6932,
      "step": 2100
    },
    {
      "epoch": 0.35207742366093775,
      "grad_norm": 13.710530281066895,
      "learning_rate": 4.657038173142468e-05,
      "loss": 1.0869,
      "step": 2110
    },
    {
      "epoch": 0.3537460370432171,
      "grad_norm": 5.3069047927856445,
      "learning_rate": 4.654907975460123e-05,
      "loss": 0.7804,
      "step": 2120
    },
    {
      "epoch": 0.3554146504254964,
      "grad_norm": 10.247106552124023,
      "learning_rate": 4.652777777777778e-05,
      "loss": 0.6502,
      "step": 2130
    },
    {
      "epoch": 0.3570832638077757,
      "grad_norm": 0.7661415934562683,
      "learning_rate": 4.650647580095433e-05,
      "loss": 0.6462,
      "step": 2140
    },
    {
      "epoch": 0.3587518771900551,
      "grad_norm": 7.576444625854492,
      "learning_rate": 4.6485173824130884e-05,
      "loss": 0.779,
      "step": 2150
    },
    {
      "epoch": 0.3604204905723344,
      "grad_norm": 6.489376544952393,
      "learning_rate": 4.6463871847307435e-05,
      "loss": 0.8445,
      "step": 2160
    },
    {
      "epoch": 0.3620891039546137,
      "grad_norm": 8.402504920959473,
      "learning_rate": 4.644256987048398e-05,
      "loss": 0.9178,
      "step": 2170
    },
    {
      "epoch": 0.36375771733689305,
      "grad_norm": 10.369796752929688,
      "learning_rate": 4.642126789366054e-05,
      "loss": 0.7305,
      "step": 2180
    },
    {
      "epoch": 0.36542633071917235,
      "grad_norm": 7.1052141189575195,
      "learning_rate": 4.639996591683708e-05,
      "loss": 0.8199,
      "step": 2190
    },
    {
      "epoch": 0.3670949441014517,
      "grad_norm": 4.7416605949401855,
      "learning_rate": 4.637866394001363e-05,
      "loss": 0.7041,
      "step": 2200
    },
    {
      "epoch": 0.368763557483731,
      "grad_norm": 3.7465391159057617,
      "learning_rate": 4.6357361963190185e-05,
      "loss": 0.6012,
      "step": 2210
    },
    {
      "epoch": 0.3704321708660103,
      "grad_norm": 7.266491889953613,
      "learning_rate": 4.6336059986366736e-05,
      "loss": 0.7317,
      "step": 2220
    },
    {
      "epoch": 0.3721007842482897,
      "grad_norm": 3.5904736518859863,
      "learning_rate": 4.631475800954329e-05,
      "loss": 0.6366,
      "step": 2230
    },
    {
      "epoch": 0.373769397630569,
      "grad_norm": 7.939891815185547,
      "learning_rate": 4.629345603271984e-05,
      "loss": 0.9024,
      "step": 2240
    },
    {
      "epoch": 0.37543801101284835,
      "grad_norm": 5.7376627922058105,
      "learning_rate": 4.627215405589639e-05,
      "loss": 0.682,
      "step": 2250
    },
    {
      "epoch": 0.37710662439512765,
      "grad_norm": 5.6247239112854,
      "learning_rate": 4.625085207907294e-05,
      "loss": 0.6612,
      "step": 2260
    },
    {
      "epoch": 0.37877523777740696,
      "grad_norm": 4.2794623374938965,
      "learning_rate": 4.622955010224949e-05,
      "loss": 0.7075,
      "step": 2270
    },
    {
      "epoch": 0.3804438511596863,
      "grad_norm": 3.1080405712127686,
      "learning_rate": 4.620824812542604e-05,
      "loss": 1.0935,
      "step": 2280
    },
    {
      "epoch": 0.3821124645419656,
      "grad_norm": 8.822911262512207,
      "learning_rate": 4.6186946148602595e-05,
      "loss": 0.5237,
      "step": 2290
    },
    {
      "epoch": 0.38378107792424493,
      "grad_norm": 4.422133445739746,
      "learning_rate": 4.616564417177914e-05,
      "loss": 0.651,
      "step": 2300
    },
    {
      "epoch": 0.3854496913065243,
      "grad_norm": 4.939207553863525,
      "learning_rate": 4.614434219495569e-05,
      "loss": 0.5488,
      "step": 2310
    },
    {
      "epoch": 0.3871183046888036,
      "grad_norm": 5.294416427612305,
      "learning_rate": 4.612304021813225e-05,
      "loss": 0.6721,
      "step": 2320
    },
    {
      "epoch": 0.38878691807108295,
      "grad_norm": 6.780736446380615,
      "learning_rate": 4.6101738241308794e-05,
      "loss": 0.8376,
      "step": 2330
    },
    {
      "epoch": 0.39045553145336226,
      "grad_norm": 7.609031677246094,
      "learning_rate": 4.6080436264485345e-05,
      "loss": 0.6816,
      "step": 2340
    },
    {
      "epoch": 0.39212414483564156,
      "grad_norm": 4.133094310760498,
      "learning_rate": 4.6059134287661897e-05,
      "loss": 0.7729,
      "step": 2350
    },
    {
      "epoch": 0.3937927582179209,
      "grad_norm": 8.204477310180664,
      "learning_rate": 4.603783231083845e-05,
      "loss": 1.0267,
      "step": 2360
    },
    {
      "epoch": 0.39546137160020023,
      "grad_norm": 17.35120964050293,
      "learning_rate": 4.6016530334015e-05,
      "loss": 0.8972,
      "step": 2370
    },
    {
      "epoch": 0.39712998498247953,
      "grad_norm": 5.44019889831543,
      "learning_rate": 4.599522835719155e-05,
      "loss": 1.0079,
      "step": 2380
    },
    {
      "epoch": 0.3987985983647589,
      "grad_norm": 4.769240379333496,
      "learning_rate": 4.59739263803681e-05,
      "loss": 0.7596,
      "step": 2390
    },
    {
      "epoch": 0.4004672117470382,
      "grad_norm": 7.464847564697266,
      "learning_rate": 4.595262440354465e-05,
      "loss": 0.704,
      "step": 2400
    },
    {
      "epoch": 0.40213582512931756,
      "grad_norm": 10.745905876159668,
      "learning_rate": 4.5931322426721205e-05,
      "loss": 0.5582,
      "step": 2410
    },
    {
      "epoch": 0.40380443851159686,
      "grad_norm": 7.3920159339904785,
      "learning_rate": 4.591002044989775e-05,
      "loss": 0.7664,
      "step": 2420
    },
    {
      "epoch": 0.40547305189387617,
      "grad_norm": 8.339971542358398,
      "learning_rate": 4.588871847307431e-05,
      "loss": 0.9273,
      "step": 2430
    },
    {
      "epoch": 0.40714166527615553,
      "grad_norm": 10.794351577758789,
      "learning_rate": 4.586741649625085e-05,
      "loss": 0.9445,
      "step": 2440
    },
    {
      "epoch": 0.40881027865843483,
      "grad_norm": 11.286408424377441,
      "learning_rate": 4.584611451942741e-05,
      "loss": 0.966,
      "step": 2450
    },
    {
      "epoch": 0.4104788920407142,
      "grad_norm": 6.3461408615112305,
      "learning_rate": 4.5824812542603954e-05,
      "loss": 0.8339,
      "step": 2460
    },
    {
      "epoch": 0.4121475054229935,
      "grad_norm": 6.5962910652160645,
      "learning_rate": 4.5803510565780506e-05,
      "loss": 0.8017,
      "step": 2470
    },
    {
      "epoch": 0.4138161188052728,
      "grad_norm": 7.828488349914551,
      "learning_rate": 4.578220858895706e-05,
      "loss": 0.7322,
      "step": 2480
    },
    {
      "epoch": 0.41548473218755216,
      "grad_norm": 8.07767105102539,
      "learning_rate": 4.576090661213361e-05,
      "loss": 0.6047,
      "step": 2490
    },
    {
      "epoch": 0.41715334556983147,
      "grad_norm": 1.8017735481262207,
      "learning_rate": 4.573960463531016e-05,
      "loss": 0.8606,
      "step": 2500
    },
    {
      "epoch": 0.4188219589521108,
      "grad_norm": 2.70247220993042,
      "learning_rate": 4.571830265848671e-05,
      "loss": 0.6885,
      "step": 2510
    },
    {
      "epoch": 0.42049057233439013,
      "grad_norm": 7.9918904304504395,
      "learning_rate": 4.569700068166326e-05,
      "loss": 0.8898,
      "step": 2520
    },
    {
      "epoch": 0.42215918571666944,
      "grad_norm": 5.703439235687256,
      "learning_rate": 4.567569870483981e-05,
      "loss": 0.9081,
      "step": 2530
    },
    {
      "epoch": 0.4238277990989488,
      "grad_norm": 4.583669662475586,
      "learning_rate": 4.5654396728016365e-05,
      "loss": 0.9924,
      "step": 2540
    },
    {
      "epoch": 0.4254964124812281,
      "grad_norm": 15.184752464294434,
      "learning_rate": 4.563309475119291e-05,
      "loss": 0.9082,
      "step": 2550
    },
    {
      "epoch": 0.4271650258635074,
      "grad_norm": 7.087944507598877,
      "learning_rate": 4.561179277436947e-05,
      "loss": 0.8636,
      "step": 2560
    },
    {
      "epoch": 0.42883363924578677,
      "grad_norm": 16.176801681518555,
      "learning_rate": 4.559049079754601e-05,
      "loss": 0.9713,
      "step": 2570
    },
    {
      "epoch": 0.4305022526280661,
      "grad_norm": 7.49072265625,
      "learning_rate": 4.5569188820722564e-05,
      "loss": 0.8582,
      "step": 2580
    },
    {
      "epoch": 0.4321708660103454,
      "grad_norm": 6.530885696411133,
      "learning_rate": 4.5547886843899115e-05,
      "loss": 0.6697,
      "step": 2590
    },
    {
      "epoch": 0.43383947939262474,
      "grad_norm": 2.5926287174224854,
      "learning_rate": 4.5526584867075666e-05,
      "loss": 0.8,
      "step": 2600
    },
    {
      "epoch": 0.43550809277490404,
      "grad_norm": 8.863929748535156,
      "learning_rate": 4.550528289025222e-05,
      "loss": 0.9377,
      "step": 2610
    },
    {
      "epoch": 0.4371767061571834,
      "grad_norm": 9.06806468963623,
      "learning_rate": 4.548398091342877e-05,
      "loss": 0.8037,
      "step": 2620
    },
    {
      "epoch": 0.4388453195394627,
      "grad_norm": 4.179878234863281,
      "learning_rate": 4.546267893660532e-05,
      "loss": 0.7673,
      "step": 2630
    },
    {
      "epoch": 0.440513932921742,
      "grad_norm": 6.408902168273926,
      "learning_rate": 4.5441376959781865e-05,
      "loss": 0.8541,
      "step": 2640
    },
    {
      "epoch": 0.4421825463040214,
      "grad_norm": 5.477489471435547,
      "learning_rate": 4.542007498295842e-05,
      "loss": 0.5573,
      "step": 2650
    },
    {
      "epoch": 0.4438511596863007,
      "grad_norm": 5.637913227081299,
      "learning_rate": 4.539877300613497e-05,
      "loss": 0.768,
      "step": 2660
    },
    {
      "epoch": 0.44551977306858,
      "grad_norm": 1.1066867113113403,
      "learning_rate": 4.5377471029311525e-05,
      "loss": 0.6381,
      "step": 2670
    },
    {
      "epoch": 0.44718838645085934,
      "grad_norm": 9.95750904083252,
      "learning_rate": 4.535616905248807e-05,
      "loss": 1.0243,
      "step": 2680
    },
    {
      "epoch": 0.44885699983313865,
      "grad_norm": 13.113813400268555,
      "learning_rate": 4.533486707566462e-05,
      "loss": 0.5726,
      "step": 2690
    },
    {
      "epoch": 0.450525613215418,
      "grad_norm": 7.13749885559082,
      "learning_rate": 4.531356509884118e-05,
      "loss": 0.8445,
      "step": 2700
    },
    {
      "epoch": 0.4521942265976973,
      "grad_norm": 5.429177761077881,
      "learning_rate": 4.5292263122017724e-05,
      "loss": 1.0661,
      "step": 2710
    },
    {
      "epoch": 0.4538628399799766,
      "grad_norm": 10.41071605682373,
      "learning_rate": 4.5270961145194275e-05,
      "loss": 0.7712,
      "step": 2720
    },
    {
      "epoch": 0.455531453362256,
      "grad_norm": 3.5462491512298584,
      "learning_rate": 4.524965916837083e-05,
      "loss": 1.3715,
      "step": 2730
    },
    {
      "epoch": 0.4572000667445353,
      "grad_norm": 5.9825029373168945,
      "learning_rate": 4.522835719154738e-05,
      "loss": 0.7204,
      "step": 2740
    },
    {
      "epoch": 0.45886868012681464,
      "grad_norm": 5.247767448425293,
      "learning_rate": 4.520705521472393e-05,
      "loss": 0.9851,
      "step": 2750
    },
    {
      "epoch": 0.46053729350909395,
      "grad_norm": 10.903671264648438,
      "learning_rate": 4.518575323790048e-05,
      "loss": 0.7053,
      "step": 2760
    },
    {
      "epoch": 0.46220590689137325,
      "grad_norm": 5.295341968536377,
      "learning_rate": 4.516445126107703e-05,
      "loss": 0.6541,
      "step": 2770
    },
    {
      "epoch": 0.4638745202736526,
      "grad_norm": 7.6296844482421875,
      "learning_rate": 4.514314928425358e-05,
      "loss": 0.7307,
      "step": 2780
    },
    {
      "epoch": 0.4655431336559319,
      "grad_norm": 9.838134765625,
      "learning_rate": 4.5121847307430135e-05,
      "loss": 1.0544,
      "step": 2790
    },
    {
      "epoch": 0.4672117470382112,
      "grad_norm": 14.287303924560547,
      "learning_rate": 4.510054533060668e-05,
      "loss": 1.1315,
      "step": 2800
    },
    {
      "epoch": 0.4688803604204906,
      "grad_norm": 23.80323028564453,
      "learning_rate": 4.507924335378324e-05,
      "loss": 0.586,
      "step": 2810
    },
    {
      "epoch": 0.4705489738027699,
      "grad_norm": 10.448858261108398,
      "learning_rate": 4.505794137695978e-05,
      "loss": 0.8833,
      "step": 2820
    },
    {
      "epoch": 0.47221758718504925,
      "grad_norm": 15.47439956665039,
      "learning_rate": 4.503663940013633e-05,
      "loss": 1.1943,
      "step": 2830
    },
    {
      "epoch": 0.47388620056732855,
      "grad_norm": 11.220212936401367,
      "learning_rate": 4.5015337423312885e-05,
      "loss": 0.6328,
      "step": 2840
    },
    {
      "epoch": 0.47555481394960786,
      "grad_norm": 7.154460906982422,
      "learning_rate": 4.4994035446489436e-05,
      "loss": 0.685,
      "step": 2850
    },
    {
      "epoch": 0.4772234273318872,
      "grad_norm": 1.9077551364898682,
      "learning_rate": 4.497273346966599e-05,
      "loss": 0.527,
      "step": 2860
    },
    {
      "epoch": 0.4788920407141665,
      "grad_norm": 5.975729942321777,
      "learning_rate": 4.495143149284254e-05,
      "loss": 0.9553,
      "step": 2870
    },
    {
      "epoch": 0.4805606540964458,
      "grad_norm": 7.26579475402832,
      "learning_rate": 4.493012951601909e-05,
      "loss": 0.7634,
      "step": 2880
    },
    {
      "epoch": 0.4822292674787252,
      "grad_norm": 9.807287216186523,
      "learning_rate": 4.490882753919564e-05,
      "loss": 0.8687,
      "step": 2890
    },
    {
      "epoch": 0.4838978808610045,
      "grad_norm": 3.80379581451416,
      "learning_rate": 4.488752556237219e-05,
      "loss": 0.625,
      "step": 2900
    },
    {
      "epoch": 0.48556649424328385,
      "grad_norm": 6.512378692626953,
      "learning_rate": 4.486622358554874e-05,
      "loss": 0.5747,
      "step": 2910
    },
    {
      "epoch": 0.48723510762556316,
      "grad_norm": 8.924501419067383,
      "learning_rate": 4.4844921608725295e-05,
      "loss": 0.9141,
      "step": 2920
    },
    {
      "epoch": 0.48890372100784246,
      "grad_norm": 4.610739231109619,
      "learning_rate": 4.482361963190184e-05,
      "loss": 0.6844,
      "step": 2930
    },
    {
      "epoch": 0.4905723343901218,
      "grad_norm": 7.242586612701416,
      "learning_rate": 4.480231765507839e-05,
      "loss": 0.5807,
      "step": 2940
    },
    {
      "epoch": 0.4922409477724011,
      "grad_norm": 7.00779390335083,
      "learning_rate": 4.478101567825494e-05,
      "loss": 1.1259,
      "step": 2950
    },
    {
      "epoch": 0.4939095611546805,
      "grad_norm": 5.875826358795166,
      "learning_rate": 4.4759713701431494e-05,
      "loss": 0.7203,
      "step": 2960
    },
    {
      "epoch": 0.4955781745369598,
      "grad_norm": 12.084908485412598,
      "learning_rate": 4.4738411724608045e-05,
      "loss": 0.9004,
      "step": 2970
    },
    {
      "epoch": 0.4972467879192391,
      "grad_norm": 9.334860801696777,
      "learning_rate": 4.4717109747784596e-05,
      "loss": 0.5229,
      "step": 2980
    },
    {
      "epoch": 0.49891540130151846,
      "grad_norm": 6.018208026885986,
      "learning_rate": 4.469580777096115e-05,
      "loss": 0.7381,
      "step": 2990
    },
    {
      "epoch": 0.5005840146837978,
      "grad_norm": 8.85694408416748,
      "learning_rate": 4.46745057941377e-05,
      "loss": 0.8259,
      "step": 3000
    },
    {
      "epoch": 0.5022526280660771,
      "grad_norm": 4.89293909072876,
      "learning_rate": 4.465320381731425e-05,
      "loss": 0.6567,
      "step": 3010
    },
    {
      "epoch": 0.5039212414483564,
      "grad_norm": 5.356273651123047,
      "learning_rate": 4.4631901840490795e-05,
      "loss": 0.8368,
      "step": 3020
    },
    {
      "epoch": 0.5055898548306358,
      "grad_norm": 3.4905364513397217,
      "learning_rate": 4.461059986366735e-05,
      "loss": 0.7724,
      "step": 3030
    },
    {
      "epoch": 0.5072584682129151,
      "grad_norm": 4.61712121963501,
      "learning_rate": 4.45892978868439e-05,
      "loss": 0.7514,
      "step": 3040
    },
    {
      "epoch": 0.5089270815951944,
      "grad_norm": 4.594651699066162,
      "learning_rate": 4.4567995910020456e-05,
      "loss": 0.8799,
      "step": 3050
    },
    {
      "epoch": 0.5105956949774737,
      "grad_norm": 3.086862802505493,
      "learning_rate": 4.4546693933197e-05,
      "loss": 0.5932,
      "step": 3060
    },
    {
      "epoch": 0.512264308359753,
      "grad_norm": 6.6793670654296875,
      "learning_rate": 4.452539195637355e-05,
      "loss": 0.6143,
      "step": 3070
    },
    {
      "epoch": 0.5139329217420324,
      "grad_norm": 7.795259952545166,
      "learning_rate": 4.450408997955011e-05,
      "loss": 0.9016,
      "step": 3080
    },
    {
      "epoch": 0.5156015351243117,
      "grad_norm": 8.311861991882324,
      "learning_rate": 4.4482788002726654e-05,
      "loss": 0.6561,
      "step": 3090
    },
    {
      "epoch": 0.517270148506591,
      "grad_norm": 5.69581413269043,
      "learning_rate": 4.4461486025903206e-05,
      "loss": 0.7683,
      "step": 3100
    },
    {
      "epoch": 0.5189387618888703,
      "grad_norm": 5.7239670753479,
      "learning_rate": 4.444018404907976e-05,
      "loss": 0.7579,
      "step": 3110
    },
    {
      "epoch": 0.5206073752711496,
      "grad_norm": 9.00308895111084,
      "learning_rate": 4.441888207225631e-05,
      "loss": 0.7257,
      "step": 3120
    },
    {
      "epoch": 0.522275988653429,
      "grad_norm": 5.245102405548096,
      "learning_rate": 4.439758009543285e-05,
      "loss": 0.8876,
      "step": 3130
    },
    {
      "epoch": 0.5239446020357084,
      "grad_norm": 6.678910255432129,
      "learning_rate": 4.437627811860941e-05,
      "loss": 0.5614,
      "step": 3140
    },
    {
      "epoch": 0.5256132154179877,
      "grad_norm": 3.526228904724121,
      "learning_rate": 4.435497614178596e-05,
      "loss": 0.5574,
      "step": 3150
    },
    {
      "epoch": 0.527281828800267,
      "grad_norm": 7.789218425750732,
      "learning_rate": 4.4333674164962513e-05,
      "loss": 0.648,
      "step": 3160
    },
    {
      "epoch": 0.5289504421825463,
      "grad_norm": 8.854560852050781,
      "learning_rate": 4.4312372188139065e-05,
      "loss": 0.8562,
      "step": 3170
    },
    {
      "epoch": 0.5306190555648256,
      "grad_norm": 3.8859331607818604,
      "learning_rate": 4.429107021131561e-05,
      "loss": 0.814,
      "step": 3180
    },
    {
      "epoch": 0.532287668947105,
      "grad_norm": 2.1650381088256836,
      "learning_rate": 4.426976823449217e-05,
      "loss": 0.8992,
      "step": 3190
    },
    {
      "epoch": 0.5339562823293843,
      "grad_norm": 4.191407203674316,
      "learning_rate": 4.424846625766871e-05,
      "loss": 0.7409,
      "step": 3200
    },
    {
      "epoch": 0.5356248957116636,
      "grad_norm": 8.881800651550293,
      "learning_rate": 4.422716428084526e-05,
      "loss": 0.8351,
      "step": 3210
    },
    {
      "epoch": 0.5372935090939429,
      "grad_norm": 23.771503448486328,
      "learning_rate": 4.4205862304021815e-05,
      "loss": 0.6544,
      "step": 3220
    },
    {
      "epoch": 0.5389621224762222,
      "grad_norm": 4.4939866065979,
      "learning_rate": 4.4184560327198366e-05,
      "loss": 0.7578,
      "step": 3230
    },
    {
      "epoch": 0.5406307358585016,
      "grad_norm": 4.12946081161499,
      "learning_rate": 4.416325835037492e-05,
      "loss": 0.738,
      "step": 3240
    },
    {
      "epoch": 0.5422993492407809,
      "grad_norm": 7.04296350479126,
      "learning_rate": 4.414195637355147e-05,
      "loss": 0.7179,
      "step": 3250
    },
    {
      "epoch": 0.5439679626230602,
      "grad_norm": 10.70803451538086,
      "learning_rate": 4.412065439672802e-05,
      "loss": 0.6522,
      "step": 3260
    },
    {
      "epoch": 0.5456365760053395,
      "grad_norm": 5.622879505157471,
      "learning_rate": 4.409935241990457e-05,
      "loss": 0.5527,
      "step": 3270
    },
    {
      "epoch": 0.5473051893876189,
      "grad_norm": 5.373890399932861,
      "learning_rate": 4.407805044308112e-05,
      "loss": 0.6073,
      "step": 3280
    },
    {
      "epoch": 0.5489738027698983,
      "grad_norm": 3.2258732318878174,
      "learning_rate": 4.405674846625767e-05,
      "loss": 0.9545,
      "step": 3290
    },
    {
      "epoch": 0.5506424161521776,
      "grad_norm": 2.4356701374053955,
      "learning_rate": 4.4035446489434225e-05,
      "loss": 0.5894,
      "step": 3300
    },
    {
      "epoch": 0.5523110295344569,
      "grad_norm": 5.7462029457092285,
      "learning_rate": 4.401414451261077e-05,
      "loss": 0.6606,
      "step": 3310
    },
    {
      "epoch": 0.5539796429167362,
      "grad_norm": 9.400675773620605,
      "learning_rate": 4.399284253578732e-05,
      "loss": 0.5878,
      "step": 3320
    },
    {
      "epoch": 0.5556482562990155,
      "grad_norm": 0.8794354200363159,
      "learning_rate": 4.397154055896387e-05,
      "loss": 0.5402,
      "step": 3330
    },
    {
      "epoch": 0.5573168696812948,
      "grad_norm": 10.784392356872559,
      "learning_rate": 4.3950238582140424e-05,
      "loss": 0.7054,
      "step": 3340
    },
    {
      "epoch": 0.5589854830635742,
      "grad_norm": 4.964269161224365,
      "learning_rate": 4.3928936605316975e-05,
      "loss": 0.5962,
      "step": 3350
    },
    {
      "epoch": 0.5606540964458535,
      "grad_norm": 1.5154563188552856,
      "learning_rate": 4.3907634628493527e-05,
      "loss": 0.779,
      "step": 3360
    },
    {
      "epoch": 0.5623227098281328,
      "grad_norm": 4.975042819976807,
      "learning_rate": 4.388633265167008e-05,
      "loss": 0.4746,
      "step": 3370
    },
    {
      "epoch": 0.5639913232104121,
      "grad_norm": 4.829031944274902,
      "learning_rate": 4.386503067484663e-05,
      "loss": 0.8684,
      "step": 3380
    },
    {
      "epoch": 0.5656599365926914,
      "grad_norm": 10.773123741149902,
      "learning_rate": 4.384372869802318e-05,
      "loss": 0.7846,
      "step": 3390
    },
    {
      "epoch": 0.5673285499749708,
      "grad_norm": 8.267843246459961,
      "learning_rate": 4.3822426721199725e-05,
      "loss": 0.8021,
      "step": 3400
    },
    {
      "epoch": 0.5689971633572501,
      "grad_norm": 6.336283206939697,
      "learning_rate": 4.380112474437628e-05,
      "loss": 0.8208,
      "step": 3410
    },
    {
      "epoch": 0.5706657767395295,
      "grad_norm": 9.285051345825195,
      "learning_rate": 4.377982276755283e-05,
      "loss": 0.791,
      "step": 3420
    },
    {
      "epoch": 0.5723343901218088,
      "grad_norm": 4.062178611755371,
      "learning_rate": 4.375852079072938e-05,
      "loss": 0.6316,
      "step": 3430
    },
    {
      "epoch": 0.5740030035040881,
      "grad_norm": 5.078421115875244,
      "learning_rate": 4.373721881390593e-05,
      "loss": 0.7186,
      "step": 3440
    },
    {
      "epoch": 0.5756716168863675,
      "grad_norm": 4.903096675872803,
      "learning_rate": 4.371591683708248e-05,
      "loss": 0.8297,
      "step": 3450
    },
    {
      "epoch": 0.5773402302686468,
      "grad_norm": 9.287599563598633,
      "learning_rate": 4.369461486025904e-05,
      "loss": 0.959,
      "step": 3460
    },
    {
      "epoch": 0.5790088436509261,
      "grad_norm": 6.266707420349121,
      "learning_rate": 4.3673312883435584e-05,
      "loss": 0.4504,
      "step": 3470
    },
    {
      "epoch": 0.5806774570332054,
      "grad_norm": 6.000196933746338,
      "learning_rate": 4.3652010906612136e-05,
      "loss": 1.0367,
      "step": 3480
    },
    {
      "epoch": 0.5823460704154847,
      "grad_norm": 13.49028205871582,
      "learning_rate": 4.363070892978869e-05,
      "loss": 1.0699,
      "step": 3490
    },
    {
      "epoch": 0.5840146837977641,
      "grad_norm": 1.3922760486602783,
      "learning_rate": 4.360940695296524e-05,
      "loss": 0.7977,
      "step": 3500
    },
    {
      "epoch": 0.5856832971800434,
      "grad_norm": 10.190385818481445,
      "learning_rate": 4.358810497614178e-05,
      "loss": 0.6358,
      "step": 3510
    },
    {
      "epoch": 0.5873519105623227,
      "grad_norm": 12.609272956848145,
      "learning_rate": 4.356680299931834e-05,
      "loss": 0.6616,
      "step": 3520
    },
    {
      "epoch": 0.589020523944602,
      "grad_norm": 10.369815826416016,
      "learning_rate": 4.354550102249489e-05,
      "loss": 0.4658,
      "step": 3530
    },
    {
      "epoch": 0.5906891373268813,
      "grad_norm": 5.325717449188232,
      "learning_rate": 4.352419904567144e-05,
      "loss": 0.9338,
      "step": 3540
    },
    {
      "epoch": 0.5923577507091606,
      "grad_norm": 7.492704391479492,
      "learning_rate": 4.3502897068847995e-05,
      "loss": 0.8601,
      "step": 3550
    },
    {
      "epoch": 0.59402636409144,
      "grad_norm": 8.332467079162598,
      "learning_rate": 4.348159509202454e-05,
      "loss": 0.7151,
      "step": 3560
    },
    {
      "epoch": 0.5956949774737194,
      "grad_norm": 1.8067209720611572,
      "learning_rate": 4.34602931152011e-05,
      "loss": 0.711,
      "step": 3570
    },
    {
      "epoch": 0.5973635908559987,
      "grad_norm": 9.219263076782227,
      "learning_rate": 4.343899113837764e-05,
      "loss": 0.5708,
      "step": 3580
    },
    {
      "epoch": 0.599032204238278,
      "grad_norm": 13.367205619812012,
      "learning_rate": 4.3417689161554194e-05,
      "loss": 0.7734,
      "step": 3590
    },
    {
      "epoch": 0.6007008176205573,
      "grad_norm": 7.960381507873535,
      "learning_rate": 4.3396387184730745e-05,
      "loss": 0.481,
      "step": 3600
    },
    {
      "epoch": 0.6023694310028367,
      "grad_norm": 3.48646879196167,
      "learning_rate": 4.3375085207907296e-05,
      "loss": 0.4676,
      "step": 3610
    },
    {
      "epoch": 0.604038044385116,
      "grad_norm": 8.932222366333008,
      "learning_rate": 4.335378323108385e-05,
      "loss": 0.744,
      "step": 3620
    },
    {
      "epoch": 0.6057066577673953,
      "grad_norm": 1.437004566192627,
      "learning_rate": 4.33324812542604e-05,
      "loss": 0.5757,
      "step": 3630
    },
    {
      "epoch": 0.6073752711496746,
      "grad_norm": 16.81715965270996,
      "learning_rate": 4.331117927743695e-05,
      "loss": 0.9272,
      "step": 3640
    },
    {
      "epoch": 0.6090438845319539,
      "grad_norm": 7.365987777709961,
      "learning_rate": 4.32898773006135e-05,
      "loss": 0.9693,
      "step": 3650
    },
    {
      "epoch": 0.6107124979142333,
      "grad_norm": 2.937911033630371,
      "learning_rate": 4.326857532379005e-05,
      "loss": 0.6983,
      "step": 3660
    },
    {
      "epoch": 0.6123811112965126,
      "grad_norm": 3.4766414165496826,
      "learning_rate": 4.32472733469666e-05,
      "loss": 0.8442,
      "step": 3670
    },
    {
      "epoch": 0.6140497246787919,
      "grad_norm": 7.140987396240234,
      "learning_rate": 4.3225971370143155e-05,
      "loss": 0.8965,
      "step": 3680
    },
    {
      "epoch": 0.6157183380610712,
      "grad_norm": 2.846816301345825,
      "learning_rate": 4.32046693933197e-05,
      "loss": 0.593,
      "step": 3690
    },
    {
      "epoch": 0.6173869514433505,
      "grad_norm": 5.269775867462158,
      "learning_rate": 4.318336741649625e-05,
      "loss": 0.7344,
      "step": 3700
    },
    {
      "epoch": 0.61905556482563,
      "grad_norm": 7.564129829406738,
      "learning_rate": 4.31620654396728e-05,
      "loss": 1.1109,
      "step": 3710
    },
    {
      "epoch": 0.6207241782079093,
      "grad_norm": 4.115617752075195,
      "learning_rate": 4.3140763462849354e-05,
      "loss": 0.807,
      "step": 3720
    },
    {
      "epoch": 0.6223927915901886,
      "grad_norm": 8.914811134338379,
      "learning_rate": 4.3119461486025905e-05,
      "loss": 0.7274,
      "step": 3730
    },
    {
      "epoch": 0.6240614049724679,
      "grad_norm": 6.307037830352783,
      "learning_rate": 4.309815950920246e-05,
      "loss": 0.6101,
      "step": 3740
    },
    {
      "epoch": 0.6257300183547472,
      "grad_norm": 6.693923473358154,
      "learning_rate": 4.307685753237901e-05,
      "loss": 0.7246,
      "step": 3750
    },
    {
      "epoch": 0.6273986317370265,
      "grad_norm": 7.183480262756348,
      "learning_rate": 4.305555555555556e-05,
      "loss": 0.6983,
      "step": 3760
    },
    {
      "epoch": 0.6290672451193059,
      "grad_norm": 4.5037713050842285,
      "learning_rate": 4.303425357873211e-05,
      "loss": 0.7726,
      "step": 3770
    },
    {
      "epoch": 0.6307358585015852,
      "grad_norm": 6.928258419036865,
      "learning_rate": 4.3012951601908655e-05,
      "loss": 0.8078,
      "step": 3780
    },
    {
      "epoch": 0.6324044718838645,
      "grad_norm": 8.762860298156738,
      "learning_rate": 4.299164962508521e-05,
      "loss": 0.7574,
      "step": 3790
    },
    {
      "epoch": 0.6340730852661438,
      "grad_norm": 3.7122573852539062,
      "learning_rate": 4.297034764826176e-05,
      "loss": 1.113,
      "step": 3800
    },
    {
      "epoch": 0.6357416986484231,
      "grad_norm": 4.502765655517578,
      "learning_rate": 4.294904567143831e-05,
      "loss": 0.6806,
      "step": 3810
    },
    {
      "epoch": 0.6374103120307025,
      "grad_norm": 4.965606689453125,
      "learning_rate": 4.292774369461486e-05,
      "loss": 0.501,
      "step": 3820
    },
    {
      "epoch": 0.6390789254129818,
      "grad_norm": 3.083747148513794,
      "learning_rate": 4.290644171779141e-05,
      "loss": 0.538,
      "step": 3830
    },
    {
      "epoch": 0.6407475387952611,
      "grad_norm": 6.130108833312988,
      "learning_rate": 4.288513974096796e-05,
      "loss": 0.9588,
      "step": 3840
    },
    {
      "epoch": 0.6424161521775404,
      "grad_norm": 11.183204650878906,
      "learning_rate": 4.2863837764144514e-05,
      "loss": 0.7925,
      "step": 3850
    },
    {
      "epoch": 0.6440847655598197,
      "grad_norm": 9.033961296081543,
      "learning_rate": 4.2842535787321066e-05,
      "loss": 0.5568,
      "step": 3860
    },
    {
      "epoch": 0.6457533789420992,
      "grad_norm": 4.446171283721924,
      "learning_rate": 4.282123381049762e-05,
      "loss": 0.7081,
      "step": 3870
    },
    {
      "epoch": 0.6474219923243785,
      "grad_norm": 6.688575267791748,
      "learning_rate": 4.279993183367417e-05,
      "loss": 0.8001,
      "step": 3880
    },
    {
      "epoch": 0.6490906057066578,
      "grad_norm": 6.0021491050720215,
      "learning_rate": 4.277862985685071e-05,
      "loss": 0.608,
      "step": 3890
    },
    {
      "epoch": 0.6507592190889371,
      "grad_norm": 9.344532012939453,
      "learning_rate": 4.275732788002727e-05,
      "loss": 0.7972,
      "step": 3900
    },
    {
      "epoch": 0.6524278324712164,
      "grad_norm": 5.105352401733398,
      "learning_rate": 4.2736025903203816e-05,
      "loss": 0.6866,
      "step": 3910
    },
    {
      "epoch": 0.6540964458534958,
      "grad_norm": 0.667101263999939,
      "learning_rate": 4.271472392638037e-05,
      "loss": 0.5994,
      "step": 3920
    },
    {
      "epoch": 0.6557650592357751,
      "grad_norm": 8.293266296386719,
      "learning_rate": 4.2693421949556925e-05,
      "loss": 0.6765,
      "step": 3930
    },
    {
      "epoch": 0.6574336726180544,
      "grad_norm": 6.091841220855713,
      "learning_rate": 4.267211997273347e-05,
      "loss": 0.7459,
      "step": 3940
    },
    {
      "epoch": 0.6591022860003337,
      "grad_norm": 6.039409160614014,
      "learning_rate": 4.265081799591003e-05,
      "loss": 0.7607,
      "step": 3950
    },
    {
      "epoch": 0.660770899382613,
      "grad_norm": 2.564216375350952,
      "learning_rate": 4.262951601908657e-05,
      "loss": 0.6541,
      "step": 3960
    },
    {
      "epoch": 0.6624395127648923,
      "grad_norm": 2.593445301055908,
      "learning_rate": 4.2608214042263124e-05,
      "loss": 0.5377,
      "step": 3970
    },
    {
      "epoch": 0.6641081261471717,
      "grad_norm": 8.44690990447998,
      "learning_rate": 4.2586912065439675e-05,
      "loss": 1.0081,
      "step": 3980
    },
    {
      "epoch": 0.665776739529451,
      "grad_norm": 6.10532808303833,
      "learning_rate": 4.2565610088616226e-05,
      "loss": 0.7588,
      "step": 3990
    },
    {
      "epoch": 0.6674453529117303,
      "grad_norm": 6.348032474517822,
      "learning_rate": 4.254430811179278e-05,
      "loss": 0.949,
      "step": 4000
    },
    {
      "epoch": 0.6691139662940097,
      "grad_norm": 8.885832786560059,
      "learning_rate": 4.252300613496933e-05,
      "loss": 0.5312,
      "step": 4010
    },
    {
      "epoch": 0.670782579676289,
      "grad_norm": 6.259112358093262,
      "learning_rate": 4.250170415814588e-05,
      "loss": 0.8495,
      "step": 4020
    },
    {
      "epoch": 0.6724511930585684,
      "grad_norm": 3.0387065410614014,
      "learning_rate": 4.2480402181322425e-05,
      "loss": 0.6002,
      "step": 4030
    },
    {
      "epoch": 0.6741198064408477,
      "grad_norm": 5.885565280914307,
      "learning_rate": 4.245910020449898e-05,
      "loss": 0.6737,
      "step": 4040
    },
    {
      "epoch": 0.675788419823127,
      "grad_norm": 11.289076805114746,
      "learning_rate": 4.243779822767553e-05,
      "loss": 1.1575,
      "step": 4050
    },
    {
      "epoch": 0.6774570332054063,
      "grad_norm": 9.999051094055176,
      "learning_rate": 4.2416496250852086e-05,
      "loss": 0.7094,
      "step": 4060
    },
    {
      "epoch": 0.6791256465876856,
      "grad_norm": 7.9299235343933105,
      "learning_rate": 4.239519427402863e-05,
      "loss": 0.7829,
      "step": 4070
    },
    {
      "epoch": 0.680794259969965,
      "grad_norm": 9.301661491394043,
      "learning_rate": 4.237389229720518e-05,
      "loss": 0.9628,
      "step": 4080
    },
    {
      "epoch": 0.6824628733522443,
      "grad_norm": 11.437853813171387,
      "learning_rate": 4.235259032038173e-05,
      "loss": 0.9998,
      "step": 4090
    },
    {
      "epoch": 0.6841314867345236,
      "grad_norm": 4.55493688583374,
      "learning_rate": 4.2331288343558284e-05,
      "loss": 0.4362,
      "step": 4100
    },
    {
      "epoch": 0.6858001001168029,
      "grad_norm": 7.243223190307617,
      "learning_rate": 4.2309986366734835e-05,
      "loss": 0.6433,
      "step": 4110
    },
    {
      "epoch": 0.6874687134990822,
      "grad_norm": 2.93525767326355,
      "learning_rate": 4.228868438991139e-05,
      "loss": 0.5778,
      "step": 4120
    },
    {
      "epoch": 0.6891373268813615,
      "grad_norm": 8.642040252685547,
      "learning_rate": 4.226738241308794e-05,
      "loss": 0.8699,
      "step": 4130
    },
    {
      "epoch": 0.690805940263641,
      "grad_norm": 1.8188085556030273,
      "learning_rate": 4.224608043626448e-05,
      "loss": 0.8268,
      "step": 4140
    },
    {
      "epoch": 0.6924745536459203,
      "grad_norm": 7.2352752685546875,
      "learning_rate": 4.222477845944104e-05,
      "loss": 0.4464,
      "step": 4150
    },
    {
      "epoch": 0.6941431670281996,
      "grad_norm": 8.987089157104492,
      "learning_rate": 4.2203476482617585e-05,
      "loss": 0.5024,
      "step": 4160
    },
    {
      "epoch": 0.6958117804104789,
      "grad_norm": 12.73311996459961,
      "learning_rate": 4.2182174505794143e-05,
      "loss": 0.9632,
      "step": 4170
    },
    {
      "epoch": 0.6974803937927582,
      "grad_norm": 9.871585845947266,
      "learning_rate": 4.216087252897069e-05,
      "loss": 0.7035,
      "step": 4180
    },
    {
      "epoch": 0.6991490071750376,
      "grad_norm": 7.942930221557617,
      "learning_rate": 4.213957055214724e-05,
      "loss": 0.6375,
      "step": 4190
    },
    {
      "epoch": 0.7008176205573169,
      "grad_norm": 6.049046993255615,
      "learning_rate": 4.211826857532379e-05,
      "loss": 0.6483,
      "step": 4200
    },
    {
      "epoch": 0.7024862339395962,
      "grad_norm": 9.923908233642578,
      "learning_rate": 4.209696659850034e-05,
      "loss": 0.8694,
      "step": 4210
    },
    {
      "epoch": 0.7041548473218755,
      "grad_norm": 5.461172580718994,
      "learning_rate": 4.207566462167689e-05,
      "loss": 0.5718,
      "step": 4220
    },
    {
      "epoch": 0.7058234607041548,
      "grad_norm": 10.776086807250977,
      "learning_rate": 4.2054362644853445e-05,
      "loss": 0.8369,
      "step": 4230
    },
    {
      "epoch": 0.7074920740864342,
      "grad_norm": 4.959886074066162,
      "learning_rate": 4.2033060668029996e-05,
      "loss": 0.6308,
      "step": 4240
    },
    {
      "epoch": 0.7091606874687135,
      "grad_norm": 15.118078231811523,
      "learning_rate": 4.201175869120655e-05,
      "loss": 0.5623,
      "step": 4250
    },
    {
      "epoch": 0.7108293008509928,
      "grad_norm": 8.131327629089355,
      "learning_rate": 4.19904567143831e-05,
      "loss": 0.7075,
      "step": 4260
    },
    {
      "epoch": 0.7124979142332721,
      "grad_norm": 9.716422080993652,
      "learning_rate": 4.196915473755964e-05,
      "loss": 0.6262,
      "step": 4270
    },
    {
      "epoch": 0.7141665276155514,
      "grad_norm": 3.214151382446289,
      "learning_rate": 4.19478527607362e-05,
      "loss": 0.6034,
      "step": 4280
    },
    {
      "epoch": 0.7158351409978309,
      "grad_norm": 4.074402332305908,
      "learning_rate": 4.1926550783912746e-05,
      "loss": 0.7989,
      "step": 4290
    },
    {
      "epoch": 0.7175037543801102,
      "grad_norm": 7.163739204406738,
      "learning_rate": 4.19052488070893e-05,
      "loss": 0.9275,
      "step": 4300
    },
    {
      "epoch": 0.7191723677623895,
      "grad_norm": 7.367880821228027,
      "learning_rate": 4.1883946830265855e-05,
      "loss": 0.5448,
      "step": 4310
    },
    {
      "epoch": 0.7208409811446688,
      "grad_norm": 8.09970760345459,
      "learning_rate": 4.18626448534424e-05,
      "loss": 0.5303,
      "step": 4320
    },
    {
      "epoch": 0.7225095945269481,
      "grad_norm": 6.714642524719238,
      "learning_rate": 4.184134287661895e-05,
      "loss": 0.5531,
      "step": 4330
    },
    {
      "epoch": 0.7241782079092274,
      "grad_norm": 1.1687126159667969,
      "learning_rate": 4.18200408997955e-05,
      "loss": 0.6571,
      "step": 4340
    },
    {
      "epoch": 0.7258468212915068,
      "grad_norm": 6.26352071762085,
      "learning_rate": 4.1798738922972054e-05,
      "loss": 0.6732,
      "step": 4350
    },
    {
      "epoch": 0.7275154346737861,
      "grad_norm": 10.937356948852539,
      "learning_rate": 4.1777436946148605e-05,
      "loss": 0.608,
      "step": 4360
    },
    {
      "epoch": 0.7291840480560654,
      "grad_norm": 4.903808116912842,
      "learning_rate": 4.1756134969325156e-05,
      "loss": 0.6646,
      "step": 4370
    },
    {
      "epoch": 0.7308526614383447,
      "grad_norm": 6.324450492858887,
      "learning_rate": 4.173483299250171e-05,
      "loss": 0.5736,
      "step": 4380
    },
    {
      "epoch": 0.732521274820624,
      "grad_norm": 6.476178169250488,
      "learning_rate": 4.171353101567826e-05,
      "loss": 0.5545,
      "step": 4390
    },
    {
      "epoch": 0.7341898882029034,
      "grad_norm": 2.108445882797241,
      "learning_rate": 4.169222903885481e-05,
      "loss": 0.5225,
      "step": 4400
    },
    {
      "epoch": 0.7358585015851827,
      "grad_norm": 8.852609634399414,
      "learning_rate": 4.1670927062031355e-05,
      "loss": 0.4368,
      "step": 4410
    },
    {
      "epoch": 0.737527114967462,
      "grad_norm": 4.862251281738281,
      "learning_rate": 4.164962508520791e-05,
      "loss": 0.3635,
      "step": 4420
    },
    {
      "epoch": 0.7391957283497413,
      "grad_norm": 12.884478569030762,
      "learning_rate": 4.162832310838446e-05,
      "loss": 0.8591,
      "step": 4430
    },
    {
      "epoch": 0.7408643417320206,
      "grad_norm": 8.639510154724121,
      "learning_rate": 4.160702113156101e-05,
      "loss": 0.7237,
      "step": 4440
    },
    {
      "epoch": 0.7425329551143001,
      "grad_norm": 5.25611686706543,
      "learning_rate": 4.158571915473756e-05,
      "loss": 1.0857,
      "step": 4450
    },
    {
      "epoch": 0.7442015684965794,
      "grad_norm": 6.877534866333008,
      "learning_rate": 4.156441717791411e-05,
      "loss": 0.4412,
      "step": 4460
    },
    {
      "epoch": 0.7458701818788587,
      "grad_norm": 9.823124885559082,
      "learning_rate": 4.154311520109066e-05,
      "loss": 0.5941,
      "step": 4470
    },
    {
      "epoch": 0.747538795261138,
      "grad_norm": 8.539118766784668,
      "learning_rate": 4.1521813224267214e-05,
      "loss": 0.8642,
      "step": 4480
    },
    {
      "epoch": 0.7492074086434173,
      "grad_norm": 8.372551918029785,
      "learning_rate": 4.1500511247443766e-05,
      "loss": 0.941,
      "step": 4490
    },
    {
      "epoch": 0.7508760220256967,
      "grad_norm": 3.806669235229492,
      "learning_rate": 4.147920927062032e-05,
      "loss": 0.6955,
      "step": 4500
    },
    {
      "epoch": 0.752544635407976,
      "grad_norm": 4.872559547424316,
      "learning_rate": 4.145790729379687e-05,
      "loss": 0.7113,
      "step": 4510
    },
    {
      "epoch": 0.7542132487902553,
      "grad_norm": 6.39990758895874,
      "learning_rate": 4.143660531697341e-05,
      "loss": 0.5752,
      "step": 4520
    },
    {
      "epoch": 0.7558818621725346,
      "grad_norm": 9.073878288269043,
      "learning_rate": 4.141530334014997e-05,
      "loss": 0.8966,
      "step": 4530
    },
    {
      "epoch": 0.7575504755548139,
      "grad_norm": 8.259525299072266,
      "learning_rate": 4.1394001363326516e-05,
      "loss": 0.7544,
      "step": 4540
    },
    {
      "epoch": 0.7592190889370932,
      "grad_norm": 9.095061302185059,
      "learning_rate": 4.1372699386503074e-05,
      "loss": 0.6377,
      "step": 4550
    },
    {
      "epoch": 0.7608877023193726,
      "grad_norm": 3.3842735290527344,
      "learning_rate": 4.135139740967962e-05,
      "loss": 0.4586,
      "step": 4560
    },
    {
      "epoch": 0.7625563157016519,
      "grad_norm": 11.09359073638916,
      "learning_rate": 4.133009543285617e-05,
      "loss": 0.8225,
      "step": 4570
    },
    {
      "epoch": 0.7642249290839312,
      "grad_norm": 7.516799449920654,
      "learning_rate": 4.130879345603272e-05,
      "loss": 0.9417,
      "step": 4580
    },
    {
      "epoch": 0.7658935424662106,
      "grad_norm": 8.702359199523926,
      "learning_rate": 4.128749147920927e-05,
      "loss": 0.8059,
      "step": 4590
    },
    {
      "epoch": 0.7675621558484899,
      "grad_norm": 4.871837139129639,
      "learning_rate": 4.1266189502385823e-05,
      "loss": 0.6167,
      "step": 4600
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 5.978969573974609,
      "learning_rate": 4.1244887525562375e-05,
      "loss": 0.6297,
      "step": 4610
    },
    {
      "epoch": 0.7708993826130486,
      "grad_norm": 7.267699241638184,
      "learning_rate": 4.1223585548738926e-05,
      "loss": 0.5319,
      "step": 4620
    },
    {
      "epoch": 0.7725679959953279,
      "grad_norm": 5.294400691986084,
      "learning_rate": 4.120228357191547e-05,
      "loss": 0.9719,
      "step": 4630
    },
    {
      "epoch": 0.7742366093776072,
      "grad_norm": 7.456960201263428,
      "learning_rate": 4.118098159509203e-05,
      "loss": 0.8657,
      "step": 4640
    },
    {
      "epoch": 0.7759052227598865,
      "grad_norm": 7.910400390625,
      "learning_rate": 4.115967961826857e-05,
      "loss": 0.8676,
      "step": 4650
    },
    {
      "epoch": 0.7775738361421659,
      "grad_norm": 4.210155963897705,
      "learning_rate": 4.113837764144513e-05,
      "loss": 0.6633,
      "step": 4660
    },
    {
      "epoch": 0.7792424495244452,
      "grad_norm": 7.547813892364502,
      "learning_rate": 4.1117075664621676e-05,
      "loss": 0.75,
      "step": 4670
    },
    {
      "epoch": 0.7809110629067245,
      "grad_norm": 6.9306440353393555,
      "learning_rate": 4.109577368779823e-05,
      "loss": 0.6064,
      "step": 4680
    },
    {
      "epoch": 0.7825796762890038,
      "grad_norm": 4.675894260406494,
      "learning_rate": 4.1074471710974785e-05,
      "loss": 0.4711,
      "step": 4690
    },
    {
      "epoch": 0.7842482896712831,
      "grad_norm": 12.16272258758545,
      "learning_rate": 4.105316973415133e-05,
      "loss": 0.5497,
      "step": 4700
    },
    {
      "epoch": 0.7859169030535625,
      "grad_norm": 16.51823616027832,
      "learning_rate": 4.103186775732788e-05,
      "loss": 0.7037,
      "step": 4710
    },
    {
      "epoch": 0.7875855164358418,
      "grad_norm": 5.109891414642334,
      "learning_rate": 4.101056578050443e-05,
      "loss": 0.721,
      "step": 4720
    },
    {
      "epoch": 0.7892541298181212,
      "grad_norm": 6.841188907623291,
      "learning_rate": 4.0989263803680984e-05,
      "loss": 0.6291,
      "step": 4730
    },
    {
      "epoch": 0.7909227432004005,
      "grad_norm": 9.411490440368652,
      "learning_rate": 4.096796182685753e-05,
      "loss": 0.826,
      "step": 4740
    },
    {
      "epoch": 0.7925913565826798,
      "grad_norm": 6.792606353759766,
      "learning_rate": 4.094665985003409e-05,
      "loss": 0.4422,
      "step": 4750
    },
    {
      "epoch": 0.7942599699649591,
      "grad_norm": 10.627891540527344,
      "learning_rate": 4.092535787321064e-05,
      "loss": 0.8627,
      "step": 4760
    },
    {
      "epoch": 0.7959285833472385,
      "grad_norm": 3.5424792766571045,
      "learning_rate": 4.090405589638719e-05,
      "loss": 0.5445,
      "step": 4770
    },
    {
      "epoch": 0.7975971967295178,
      "grad_norm": 7.913875579833984,
      "learning_rate": 4.088275391956374e-05,
      "loss": 0.4995,
      "step": 4780
    },
    {
      "epoch": 0.7992658101117971,
      "grad_norm": 4.588155746459961,
      "learning_rate": 4.0861451942740285e-05,
      "loss": 0.7042,
      "step": 4790
    },
    {
      "epoch": 0.8009344234940764,
      "grad_norm": 2.9795498847961426,
      "learning_rate": 4.084014996591684e-05,
      "loss": 0.4652,
      "step": 4800
    },
    {
      "epoch": 0.8026030368763557,
      "grad_norm": 6.137686729431152,
      "learning_rate": 4.081884798909339e-05,
      "loss": 0.6991,
      "step": 4810
    },
    {
      "epoch": 0.8042716502586351,
      "grad_norm": 4.010180473327637,
      "learning_rate": 4.079754601226994e-05,
      "loss": 0.6801,
      "step": 4820
    },
    {
      "epoch": 0.8059402636409144,
      "grad_norm": 4.369078636169434,
      "learning_rate": 4.077624403544649e-05,
      "loss": 0.8245,
      "step": 4830
    },
    {
      "epoch": 0.8076088770231937,
      "grad_norm": 11.5368013381958,
      "learning_rate": 4.075494205862304e-05,
      "loss": 1.1635,
      "step": 4840
    },
    {
      "epoch": 0.809277490405473,
      "grad_norm": 6.361441612243652,
      "learning_rate": 4.073364008179959e-05,
      "loss": 0.2785,
      "step": 4850
    },
    {
      "epoch": 0.8109461037877523,
      "grad_norm": 1.2545968294143677,
      "learning_rate": 4.0712338104976144e-05,
      "loss": 0.6861,
      "step": 4860
    },
    {
      "epoch": 0.8126147171700318,
      "grad_norm": 8.083144187927246,
      "learning_rate": 4.0691036128152696e-05,
      "loss": 0.7574,
      "step": 4870
    },
    {
      "epoch": 0.8142833305523111,
      "grad_norm": 7.553082466125488,
      "learning_rate": 4.066973415132925e-05,
      "loss": 0.6912,
      "step": 4880
    },
    {
      "epoch": 0.8159519439345904,
      "grad_norm": 5.209771156311035,
      "learning_rate": 4.06484321745058e-05,
      "loss": 0.7128,
      "step": 4890
    },
    {
      "epoch": 0.8176205573168697,
      "grad_norm": 13.643183708190918,
      "learning_rate": 4.062713019768234e-05,
      "loss": 0.6429,
      "step": 4900
    },
    {
      "epoch": 0.819289170699149,
      "grad_norm": 13.913156509399414,
      "learning_rate": 4.06058282208589e-05,
      "loss": 0.7242,
      "step": 4910
    },
    {
      "epoch": 0.8209577840814284,
      "grad_norm": 5.303826808929443,
      "learning_rate": 4.0584526244035446e-05,
      "loss": 0.766,
      "step": 4920
    },
    {
      "epoch": 0.8226263974637077,
      "grad_norm": 3.849189281463623,
      "learning_rate": 4.0563224267212e-05,
      "loss": 0.5202,
      "step": 4930
    },
    {
      "epoch": 0.824295010845987,
      "grad_norm": 5.480501651763916,
      "learning_rate": 4.054192229038855e-05,
      "loss": 0.6775,
      "step": 4940
    },
    {
      "epoch": 0.8259636242282663,
      "grad_norm": 7.440640449523926,
      "learning_rate": 4.05206203135651e-05,
      "loss": 0.5223,
      "step": 4950
    },
    {
      "epoch": 0.8276322376105456,
      "grad_norm": 6.773004055023193,
      "learning_rate": 4.049931833674165e-05,
      "loss": 0.6429,
      "step": 4960
    },
    {
      "epoch": 0.8293008509928249,
      "grad_norm": 17.210622787475586,
      "learning_rate": 4.04780163599182e-05,
      "loss": 0.78,
      "step": 4970
    },
    {
      "epoch": 0.8309694643751043,
      "grad_norm": 9.461005210876465,
      "learning_rate": 4.0456714383094754e-05,
      "loss": 0.8105,
      "step": 4980
    },
    {
      "epoch": 0.8326380777573836,
      "grad_norm": 6.6932549476623535,
      "learning_rate": 4.0435412406271305e-05,
      "loss": 1.1653,
      "step": 4990
    },
    {
      "epoch": 0.8343066911396629,
      "grad_norm": 9.994537353515625,
      "learning_rate": 4.0414110429447856e-05,
      "loss": 0.8045,
      "step": 5000
    },
    {
      "epoch": 0.8359753045219422,
      "grad_norm": 6.114248752593994,
      "learning_rate": 4.03928084526244e-05,
      "loss": 0.8358,
      "step": 5010
    },
    {
      "epoch": 0.8376439179042215,
      "grad_norm": 3.5686943531036377,
      "learning_rate": 4.037150647580096e-05,
      "loss": 0.5322,
      "step": 5020
    },
    {
      "epoch": 0.839312531286501,
      "grad_norm": 3.305602550506592,
      "learning_rate": 4.0350204498977503e-05,
      "loss": 0.6466,
      "step": 5030
    },
    {
      "epoch": 0.8409811446687803,
      "grad_norm": 6.408198356628418,
      "learning_rate": 4.0328902522154055e-05,
      "loss": 0.6449,
      "step": 5040
    },
    {
      "epoch": 0.8426497580510596,
      "grad_norm": 13.212977409362793,
      "learning_rate": 4.0307600545330606e-05,
      "loss": 0.7199,
      "step": 5050
    },
    {
      "epoch": 0.8443183714333389,
      "grad_norm": 7.085546493530273,
      "learning_rate": 4.028629856850716e-05,
      "loss": 0.8435,
      "step": 5060
    },
    {
      "epoch": 0.8459869848156182,
      "grad_norm": 7.736124515533447,
      "learning_rate": 4.0264996591683716e-05,
      "loss": 1.1025,
      "step": 5070
    },
    {
      "epoch": 0.8476555981978976,
      "grad_norm": 4.315603733062744,
      "learning_rate": 4.024369461486026e-05,
      "loss": 0.7415,
      "step": 5080
    },
    {
      "epoch": 0.8493242115801769,
      "grad_norm": 5.228132247924805,
      "learning_rate": 4.022239263803681e-05,
      "loss": 0.6567,
      "step": 5090
    },
    {
      "epoch": 0.8509928249624562,
      "grad_norm": 9.190413475036621,
      "learning_rate": 4.020109066121336e-05,
      "loss": 0.7975,
      "step": 5100
    },
    {
      "epoch": 0.8526614383447355,
      "grad_norm": 5.6962385177612305,
      "learning_rate": 4.0179788684389914e-05,
      "loss": 0.6476,
      "step": 5110
    },
    {
      "epoch": 0.8543300517270148,
      "grad_norm": 2.631312847137451,
      "learning_rate": 4.015848670756646e-05,
      "loss": 0.5019,
      "step": 5120
    },
    {
      "epoch": 0.8559986651092941,
      "grad_norm": 10.998701095581055,
      "learning_rate": 4.013718473074302e-05,
      "loss": 0.8213,
      "step": 5130
    },
    {
      "epoch": 0.8576672784915735,
      "grad_norm": 4.476869106292725,
      "learning_rate": 4.011588275391957e-05,
      "loss": 0.7319,
      "step": 5140
    },
    {
      "epoch": 0.8593358918738528,
      "grad_norm": 9.150187492370605,
      "learning_rate": 4.009458077709612e-05,
      "loss": 0.5819,
      "step": 5150
    },
    {
      "epoch": 0.8610045052561321,
      "grad_norm": 7.1300177574157715,
      "learning_rate": 4.007327880027267e-05,
      "loss": 0.5257,
      "step": 5160
    },
    {
      "epoch": 0.8626731186384115,
      "grad_norm": 6.205604553222656,
      "learning_rate": 4.0051976823449215e-05,
      "loss": 0.9618,
      "step": 5170
    },
    {
      "epoch": 0.8643417320206908,
      "grad_norm": 8.86750316619873,
      "learning_rate": 4.0030674846625773e-05,
      "loss": 0.8152,
      "step": 5180
    },
    {
      "epoch": 0.8660103454029702,
      "grad_norm": 7.690610885620117,
      "learning_rate": 4.000937286980232e-05,
      "loss": 0.68,
      "step": 5190
    },
    {
      "epoch": 0.8676789587852495,
      "grad_norm": 3.9835286140441895,
      "learning_rate": 3.998807089297887e-05,
      "loss": 0.7588,
      "step": 5200
    },
    {
      "epoch": 0.8693475721675288,
      "grad_norm": 2.502793073654175,
      "learning_rate": 3.996676891615542e-05,
      "loss": 0.5734,
      "step": 5210
    },
    {
      "epoch": 0.8710161855498081,
      "grad_norm": 3.6077799797058105,
      "learning_rate": 3.994546693933197e-05,
      "loss": 0.7394,
      "step": 5220
    },
    {
      "epoch": 0.8726847989320874,
      "grad_norm": 4.495758056640625,
      "learning_rate": 3.992416496250852e-05,
      "loss": 0.5086,
      "step": 5230
    },
    {
      "epoch": 0.8743534123143668,
      "grad_norm": 5.819901943206787,
      "learning_rate": 3.9902862985685075e-05,
      "loss": 0.4372,
      "step": 5240
    },
    {
      "epoch": 0.8760220256966461,
      "grad_norm": 9.174017906188965,
      "learning_rate": 3.9881561008861626e-05,
      "loss": 0.5426,
      "step": 5250
    },
    {
      "epoch": 0.8776906390789254,
      "grad_norm": 5.728267669677734,
      "learning_rate": 3.986025903203818e-05,
      "loss": 0.6011,
      "step": 5260
    },
    {
      "epoch": 0.8793592524612047,
      "grad_norm": 11.243270874023438,
      "learning_rate": 3.983895705521473e-05,
      "loss": 0.7131,
      "step": 5270
    },
    {
      "epoch": 0.881027865843484,
      "grad_norm": 7.2624077796936035,
      "learning_rate": 3.981765507839127e-05,
      "loss": 0.837,
      "step": 5280
    },
    {
      "epoch": 0.8826964792257634,
      "grad_norm": 15.410677909851074,
      "learning_rate": 3.979635310156783e-05,
      "loss": 0.7819,
      "step": 5290
    },
    {
      "epoch": 0.8843650926080427,
      "grad_norm": 4.9948201179504395,
      "learning_rate": 3.9775051124744376e-05,
      "loss": 0.4872,
      "step": 5300
    },
    {
      "epoch": 0.886033705990322,
      "grad_norm": 11.277363777160645,
      "learning_rate": 3.975374914792093e-05,
      "loss": 0.9607,
      "step": 5310
    },
    {
      "epoch": 0.8877023193726014,
      "grad_norm": 9.392916679382324,
      "learning_rate": 3.973244717109748e-05,
      "loss": 0.8722,
      "step": 5320
    },
    {
      "epoch": 0.8893709327548807,
      "grad_norm": 4.632662296295166,
      "learning_rate": 3.971114519427403e-05,
      "loss": 0.583,
      "step": 5330
    },
    {
      "epoch": 0.89103954613716,
      "grad_norm": 12.589158058166504,
      "learning_rate": 3.968984321745058e-05,
      "loss": 0.8729,
      "step": 5340
    },
    {
      "epoch": 0.8927081595194394,
      "grad_norm": 7.155453681945801,
      "learning_rate": 3.966854124062713e-05,
      "loss": 0.6203,
      "step": 5350
    },
    {
      "epoch": 0.8943767729017187,
      "grad_norm": 9.711152076721191,
      "learning_rate": 3.9647239263803684e-05,
      "loss": 0.7692,
      "step": 5360
    },
    {
      "epoch": 0.896045386283998,
      "grad_norm": 8.618765830993652,
      "learning_rate": 3.9625937286980235e-05,
      "loss": 0.9834,
      "step": 5370
    },
    {
      "epoch": 0.8977139996662773,
      "grad_norm": 21.52099609375,
      "learning_rate": 3.9604635310156786e-05,
      "loss": 0.5869,
      "step": 5380
    },
    {
      "epoch": 0.8993826130485566,
      "grad_norm": 8.747952461242676,
      "learning_rate": 3.958333333333333e-05,
      "loss": 0.8227,
      "step": 5390
    },
    {
      "epoch": 0.901051226430836,
      "grad_norm": 10.506210327148438,
      "learning_rate": 3.956203135650989e-05,
      "loss": 1.0179,
      "step": 5400
    },
    {
      "epoch": 0.9027198398131153,
      "grad_norm": 6.565392971038818,
      "learning_rate": 3.9540729379686434e-05,
      "loss": 0.5993,
      "step": 5410
    },
    {
      "epoch": 0.9043884531953946,
      "grad_norm": 4.1620869636535645,
      "learning_rate": 3.9519427402862985e-05,
      "loss": 0.7142,
      "step": 5420
    },
    {
      "epoch": 0.9060570665776739,
      "grad_norm": 4.8562750816345215,
      "learning_rate": 3.9498125426039536e-05,
      "loss": 0.4364,
      "step": 5430
    },
    {
      "epoch": 0.9077256799599532,
      "grad_norm": 5.589602470397949,
      "learning_rate": 3.947682344921609e-05,
      "loss": 0.5838,
      "step": 5440
    },
    {
      "epoch": 0.9093942933422327,
      "grad_norm": 9.907262802124023,
      "learning_rate": 3.9455521472392646e-05,
      "loss": 0.5153,
      "step": 5450
    },
    {
      "epoch": 0.911062906724512,
      "grad_norm": 3.6757144927978516,
      "learning_rate": 3.943421949556919e-05,
      "loss": 0.6815,
      "step": 5460
    },
    {
      "epoch": 0.9127315201067913,
      "grad_norm": 5.992738246917725,
      "learning_rate": 3.941291751874574e-05,
      "loss": 0.611,
      "step": 5470
    },
    {
      "epoch": 0.9144001334890706,
      "grad_norm": 5.381848335266113,
      "learning_rate": 3.939161554192229e-05,
      "loss": 0.7277,
      "step": 5480
    },
    {
      "epoch": 0.9160687468713499,
      "grad_norm": 10.144418716430664,
      "learning_rate": 3.9370313565098844e-05,
      "loss": 0.6102,
      "step": 5490
    },
    {
      "epoch": 0.9177373602536293,
      "grad_norm": 8.351442337036133,
      "learning_rate": 3.934901158827539e-05,
      "loss": 0.8789,
      "step": 5500
    },
    {
      "epoch": 0.9194059736359086,
      "grad_norm": 6.370941638946533,
      "learning_rate": 3.932770961145195e-05,
      "loss": 0.7821,
      "step": 5510
    },
    {
      "epoch": 0.9210745870181879,
      "grad_norm": 6.09891939163208,
      "learning_rate": 3.930640763462849e-05,
      "loss": 0.6984,
      "step": 5520
    },
    {
      "epoch": 0.9227432004004672,
      "grad_norm": 7.602884292602539,
      "learning_rate": 3.928510565780504e-05,
      "loss": 0.5258,
      "step": 5530
    },
    {
      "epoch": 0.9244118137827465,
      "grad_norm": 4.333096027374268,
      "learning_rate": 3.92638036809816e-05,
      "loss": 0.4684,
      "step": 5540
    },
    {
      "epoch": 0.9260804271650258,
      "grad_norm": 8.45220947265625,
      "learning_rate": 3.9242501704158145e-05,
      "loss": 0.7005,
      "step": 5550
    },
    {
      "epoch": 0.9277490405473052,
      "grad_norm": 5.317202568054199,
      "learning_rate": 3.9221199727334704e-05,
      "loss": 0.4434,
      "step": 5560
    },
    {
      "epoch": 0.9294176539295845,
      "grad_norm": 6.002959251403809,
      "learning_rate": 3.919989775051125e-05,
      "loss": 0.5697,
      "step": 5570
    },
    {
      "epoch": 0.9310862673118638,
      "grad_norm": 11.944580078125,
      "learning_rate": 3.91785957736878e-05,
      "loss": 0.7272,
      "step": 5580
    },
    {
      "epoch": 0.9327548806941431,
      "grad_norm": 2.909165859222412,
      "learning_rate": 3.915729379686435e-05,
      "loss": 0.4679,
      "step": 5590
    },
    {
      "epoch": 0.9344234940764224,
      "grad_norm": 8.003212928771973,
      "learning_rate": 3.91359918200409e-05,
      "loss": 0.657,
      "step": 5600
    },
    {
      "epoch": 0.9360921074587019,
      "grad_norm": 8.912395477294922,
      "learning_rate": 3.9114689843217453e-05,
      "loss": 0.8196,
      "step": 5610
    },
    {
      "epoch": 0.9377607208409812,
      "grad_norm": 10.56026554107666,
      "learning_rate": 3.9093387866394005e-05,
      "loss": 0.8723,
      "step": 5620
    },
    {
      "epoch": 0.9394293342232605,
      "grad_norm": 3.2825167179107666,
      "learning_rate": 3.9072085889570556e-05,
      "loss": 0.6074,
      "step": 5630
    },
    {
      "epoch": 0.9410979476055398,
      "grad_norm": 7.866597652435303,
      "learning_rate": 3.905078391274711e-05,
      "loss": 0.6997,
      "step": 5640
    },
    {
      "epoch": 0.9427665609878191,
      "grad_norm": 3.4545888900756836,
      "learning_rate": 3.902948193592366e-05,
      "loss": 0.6029,
      "step": 5650
    },
    {
      "epoch": 0.9444351743700985,
      "grad_norm": 6.406834125518799,
      "learning_rate": 3.90081799591002e-05,
      "loss": 0.5674,
      "step": 5660
    },
    {
      "epoch": 0.9461037877523778,
      "grad_norm": 8.456024169921875,
      "learning_rate": 3.898687798227676e-05,
      "loss": 1.0087,
      "step": 5670
    },
    {
      "epoch": 0.9477724011346571,
      "grad_norm": 2.2167649269104004,
      "learning_rate": 3.8965576005453306e-05,
      "loss": 0.5554,
      "step": 5680
    },
    {
      "epoch": 0.9494410145169364,
      "grad_norm": 4.261198043823242,
      "learning_rate": 3.894427402862986e-05,
      "loss": 0.6524,
      "step": 5690
    },
    {
      "epoch": 0.9511096278992157,
      "grad_norm": 8.01386547088623,
      "learning_rate": 3.892297205180641e-05,
      "loss": 0.9691,
      "step": 5700
    },
    {
      "epoch": 0.9527782412814951,
      "grad_norm": 8.178125381469727,
      "learning_rate": 3.890167007498296e-05,
      "loss": 0.7116,
      "step": 5710
    },
    {
      "epoch": 0.9544468546637744,
      "grad_norm": 6.532522201538086,
      "learning_rate": 3.888036809815951e-05,
      "loss": 0.624,
      "step": 5720
    },
    {
      "epoch": 0.9561154680460537,
      "grad_norm": 9.305240631103516,
      "learning_rate": 3.885906612133606e-05,
      "loss": 0.5837,
      "step": 5730
    },
    {
      "epoch": 0.957784081428333,
      "grad_norm": 5.13653039932251,
      "learning_rate": 3.8837764144512614e-05,
      "loss": 0.5712,
      "step": 5740
    },
    {
      "epoch": 0.9594526948106123,
      "grad_norm": 4.876650333404541,
      "learning_rate": 3.8816462167689165e-05,
      "loss": 0.5474,
      "step": 5750
    },
    {
      "epoch": 0.9611213081928917,
      "grad_norm": 13.566899299621582,
      "learning_rate": 3.8795160190865717e-05,
      "loss": 0.5989,
      "step": 5760
    },
    {
      "epoch": 0.9627899215751711,
      "grad_norm": 5.612605094909668,
      "learning_rate": 3.877385821404226e-05,
      "loss": 0.6628,
      "step": 5770
    },
    {
      "epoch": 0.9644585349574504,
      "grad_norm": 7.087528228759766,
      "learning_rate": 3.875255623721882e-05,
      "loss": 0.7394,
      "step": 5780
    },
    {
      "epoch": 0.9661271483397297,
      "grad_norm": 5.69182825088501,
      "learning_rate": 3.8731254260395364e-05,
      "loss": 0.6527,
      "step": 5790
    },
    {
      "epoch": 0.967795761722009,
      "grad_norm": 7.981171131134033,
      "learning_rate": 3.8709952283571915e-05,
      "loss": 1.0885,
      "step": 5800
    },
    {
      "epoch": 0.9694643751042883,
      "grad_norm": 7.01986837387085,
      "learning_rate": 3.8688650306748466e-05,
      "loss": 0.7263,
      "step": 5810
    },
    {
      "epoch": 0.9711329884865677,
      "grad_norm": 6.18464469909668,
      "learning_rate": 3.866734832992502e-05,
      "loss": 0.6154,
      "step": 5820
    },
    {
      "epoch": 0.972801601868847,
      "grad_norm": 7.219038963317871,
      "learning_rate": 3.864604635310157e-05,
      "loss": 0.8784,
      "step": 5830
    },
    {
      "epoch": 0.9744702152511263,
      "grad_norm": 7.147895336151123,
      "learning_rate": 3.862474437627812e-05,
      "loss": 0.6445,
      "step": 5840
    },
    {
      "epoch": 0.9761388286334056,
      "grad_norm": 1.3653048276901245,
      "learning_rate": 3.860344239945467e-05,
      "loss": 0.5937,
      "step": 5850
    },
    {
      "epoch": 0.9778074420156849,
      "grad_norm": 2.871269702911377,
      "learning_rate": 3.858214042263122e-05,
      "loss": 0.4439,
      "step": 5860
    },
    {
      "epoch": 0.9794760553979643,
      "grad_norm": 6.522500991821289,
      "learning_rate": 3.8560838445807774e-05,
      "loss": 0.6028,
      "step": 5870
    },
    {
      "epoch": 0.9811446687802436,
      "grad_norm": 11.192880630493164,
      "learning_rate": 3.853953646898432e-05,
      "loss": 0.4032,
      "step": 5880
    },
    {
      "epoch": 0.982813282162523,
      "grad_norm": 6.832179069519043,
      "learning_rate": 3.851823449216088e-05,
      "loss": 0.6175,
      "step": 5890
    },
    {
      "epoch": 0.9844818955448023,
      "grad_norm": 5.334899425506592,
      "learning_rate": 3.849693251533742e-05,
      "loss": 0.7307,
      "step": 5900
    },
    {
      "epoch": 0.9861505089270816,
      "grad_norm": 4.785776138305664,
      "learning_rate": 3.847563053851397e-05,
      "loss": 0.4224,
      "step": 5910
    },
    {
      "epoch": 0.987819122309361,
      "grad_norm": 4.659121990203857,
      "learning_rate": 3.845432856169053e-05,
      "loss": 0.5516,
      "step": 5920
    },
    {
      "epoch": 0.9894877356916403,
      "grad_norm": 4.3518805503845215,
      "learning_rate": 3.8433026584867076e-05,
      "loss": 0.6912,
      "step": 5930
    },
    {
      "epoch": 0.9911563490739196,
      "grad_norm": 11.588605880737305,
      "learning_rate": 3.8411724608043634e-05,
      "loss": 0.8729,
      "step": 5940
    },
    {
      "epoch": 0.9928249624561989,
      "grad_norm": 2.8313825130462646,
      "learning_rate": 3.839042263122018e-05,
      "loss": 0.4936,
      "step": 5950
    },
    {
      "epoch": 0.9944935758384782,
      "grad_norm": 1.295936942100525,
      "learning_rate": 3.836912065439673e-05,
      "loss": 1.0575,
      "step": 5960
    },
    {
      "epoch": 0.9961621892207575,
      "grad_norm": 5.885469913482666,
      "learning_rate": 3.834781867757328e-05,
      "loss": 0.8524,
      "step": 5970
    },
    {
      "epoch": 0.9978308026030369,
      "grad_norm": 5.300264835357666,
      "learning_rate": 3.832651670074983e-05,
      "loss": 0.9004,
      "step": 5980
    },
    {
      "epoch": 0.9994994159853162,
      "grad_norm": 4.336697578430176,
      "learning_rate": 3.8305214723926384e-05,
      "loss": 0.7755,
      "step": 5990
    },
    {
      "epoch": 1.0011680293675955,
      "grad_norm": 5.374946594238281,
      "learning_rate": 3.8283912747102935e-05,
      "loss": 1.0092,
      "step": 6000
    },
    {
      "epoch": 1.002836642749875,
      "grad_norm": 13.538224220275879,
      "learning_rate": 3.8262610770279486e-05,
      "loss": 0.8447,
      "step": 6010
    },
    {
      "epoch": 1.0045052561321541,
      "grad_norm": 7.171322345733643,
      "learning_rate": 3.824130879345603e-05,
      "loss": 0.7054,
      "step": 6020
    },
    {
      "epoch": 1.0061738695144335,
      "grad_norm": 9.475341796875,
      "learning_rate": 3.822000681663259e-05,
      "loss": 0.7357,
      "step": 6030
    },
    {
      "epoch": 1.0078424828967127,
      "grad_norm": 1.9368295669555664,
      "learning_rate": 3.8198704839809133e-05,
      "loss": 0.7679,
      "step": 6040
    },
    {
      "epoch": 1.0095110962789922,
      "grad_norm": 1.81779944896698,
      "learning_rate": 3.817740286298569e-05,
      "loss": 0.8321,
      "step": 6050
    },
    {
      "epoch": 1.0111797096612716,
      "grad_norm": 4.569530010223389,
      "learning_rate": 3.8156100886162236e-05,
      "loss": 0.4538,
      "step": 6060
    },
    {
      "epoch": 1.0128483230435508,
      "grad_norm": 4.245853900909424,
      "learning_rate": 3.813479890933879e-05,
      "loss": 0.5643,
      "step": 6070
    },
    {
      "epoch": 1.0145169364258302,
      "grad_norm": 8.67273235321045,
      "learning_rate": 3.811349693251534e-05,
      "loss": 0.726,
      "step": 6080
    },
    {
      "epoch": 1.0161855498081094,
      "grad_norm": 0.696869969367981,
      "learning_rate": 3.809219495569189e-05,
      "loss": 0.3949,
      "step": 6090
    },
    {
      "epoch": 1.0178541631903888,
      "grad_norm": 1.8328641653060913,
      "learning_rate": 3.807089297886844e-05,
      "loss": 0.5064,
      "step": 6100
    },
    {
      "epoch": 1.0195227765726682,
      "grad_norm": 5.915022373199463,
      "learning_rate": 3.804959100204499e-05,
      "loss": 0.8124,
      "step": 6110
    },
    {
      "epoch": 1.0211913899549474,
      "grad_norm": 9.905023574829102,
      "learning_rate": 3.8028289025221544e-05,
      "loss": 0.9459,
      "step": 6120
    },
    {
      "epoch": 1.0228600033372268,
      "grad_norm": 8.825831413269043,
      "learning_rate": 3.800698704839809e-05,
      "loss": 0.8078,
      "step": 6130
    },
    {
      "epoch": 1.024528616719506,
      "grad_norm": 4.524229526519775,
      "learning_rate": 3.798568507157465e-05,
      "loss": 0.5991,
      "step": 6140
    },
    {
      "epoch": 1.0261972301017854,
      "grad_norm": 7.027403831481934,
      "learning_rate": 3.796438309475119e-05,
      "loss": 0.5608,
      "step": 6150
    },
    {
      "epoch": 1.0278658434840648,
      "grad_norm": 13.857187271118164,
      "learning_rate": 3.794308111792775e-05,
      "loss": 0.6976,
      "step": 6160
    },
    {
      "epoch": 1.029534456866344,
      "grad_norm": 1.990322470664978,
      "learning_rate": 3.7921779141104294e-05,
      "loss": 0.5526,
      "step": 6170
    },
    {
      "epoch": 1.0312030702486235,
      "grad_norm": 7.5649895668029785,
      "learning_rate": 3.7900477164280845e-05,
      "loss": 0.5831,
      "step": 6180
    },
    {
      "epoch": 1.0328716836309026,
      "grad_norm": 4.529258728027344,
      "learning_rate": 3.78791751874574e-05,
      "loss": 0.6738,
      "step": 6190
    },
    {
      "epoch": 1.034540297013182,
      "grad_norm": 4.342246055603027,
      "learning_rate": 3.785787321063395e-05,
      "loss": 0.6826,
      "step": 6200
    },
    {
      "epoch": 1.0362089103954615,
      "grad_norm": 8.65554141998291,
      "learning_rate": 3.78365712338105e-05,
      "loss": 0.7767,
      "step": 6210
    },
    {
      "epoch": 1.0378775237777407,
      "grad_norm": 3.967540740966797,
      "learning_rate": 3.781526925698705e-05,
      "loss": 0.8327,
      "step": 6220
    },
    {
      "epoch": 1.03954613716002,
      "grad_norm": 4.833505153656006,
      "learning_rate": 3.77939672801636e-05,
      "loss": 0.5086,
      "step": 6230
    },
    {
      "epoch": 1.0412147505422993,
      "grad_norm": 5.758260250091553,
      "learning_rate": 3.777266530334015e-05,
      "loss": 0.7085,
      "step": 6240
    },
    {
      "epoch": 1.0428833639245787,
      "grad_norm": 8.68689250946045,
      "learning_rate": 3.7751363326516705e-05,
      "loss": 0.7104,
      "step": 6250
    },
    {
      "epoch": 1.044551977306858,
      "grad_norm": 1.2792319059371948,
      "learning_rate": 3.773006134969325e-05,
      "loss": 0.3728,
      "step": 6260
    },
    {
      "epoch": 1.0462205906891373,
      "grad_norm": 3.806483268737793,
      "learning_rate": 3.770875937286981e-05,
      "loss": 0.5215,
      "step": 6270
    },
    {
      "epoch": 1.0478892040714167,
      "grad_norm": 4.537534236907959,
      "learning_rate": 3.768745739604635e-05,
      "loss": 0.7684,
      "step": 6280
    },
    {
      "epoch": 1.049557817453696,
      "grad_norm": 9.518585205078125,
      "learning_rate": 3.76661554192229e-05,
      "loss": 0.5682,
      "step": 6290
    },
    {
      "epoch": 1.0512264308359753,
      "grad_norm": 3.418015241622925,
      "learning_rate": 3.764485344239946e-05,
      "loss": 0.6903,
      "step": 6300
    },
    {
      "epoch": 1.0528950442182545,
      "grad_norm": 11.803902626037598,
      "learning_rate": 3.7623551465576006e-05,
      "loss": 0.5303,
      "step": 6310
    },
    {
      "epoch": 1.054563657600534,
      "grad_norm": 3.404094934463501,
      "learning_rate": 3.760224948875256e-05,
      "loss": 0.6294,
      "step": 6320
    },
    {
      "epoch": 1.0562322709828134,
      "grad_norm": 15.93364143371582,
      "learning_rate": 3.758094751192911e-05,
      "loss": 0.6061,
      "step": 6330
    },
    {
      "epoch": 1.0579008843650926,
      "grad_norm": 8.045159339904785,
      "learning_rate": 3.755964553510566e-05,
      "loss": 0.6405,
      "step": 6340
    },
    {
      "epoch": 1.059569497747372,
      "grad_norm": 7.330000400543213,
      "learning_rate": 3.753834355828221e-05,
      "loss": 0.5606,
      "step": 6350
    },
    {
      "epoch": 1.0612381111296512,
      "grad_norm": 12.553983688354492,
      "learning_rate": 3.751704158145876e-05,
      "loss": 0.4008,
      "step": 6360
    },
    {
      "epoch": 1.0629067245119306,
      "grad_norm": 3.549842596054077,
      "learning_rate": 3.7495739604635314e-05,
      "loss": 0.5925,
      "step": 6370
    },
    {
      "epoch": 1.06457533789421,
      "grad_norm": 10.390632629394531,
      "learning_rate": 3.7474437627811865e-05,
      "loss": 0.406,
      "step": 6380
    },
    {
      "epoch": 1.0662439512764892,
      "grad_norm": 8.135400772094727,
      "learning_rate": 3.7453135650988416e-05,
      "loss": 0.4865,
      "step": 6390
    },
    {
      "epoch": 1.0679125646587686,
      "grad_norm": 4.331161975860596,
      "learning_rate": 3.743183367416496e-05,
      "loss": 0.6061,
      "step": 6400
    },
    {
      "epoch": 1.0695811780410478,
      "grad_norm": 17.440366744995117,
      "learning_rate": 3.741053169734152e-05,
      "loss": 0.4884,
      "step": 6410
    },
    {
      "epoch": 1.0712497914233272,
      "grad_norm": 4.943343162536621,
      "learning_rate": 3.7389229720518064e-05,
      "loss": 0.7891,
      "step": 6420
    },
    {
      "epoch": 1.0729184048056066,
      "grad_norm": 14.76405143737793,
      "learning_rate": 3.7367927743694615e-05,
      "loss": 0.5094,
      "step": 6430
    },
    {
      "epoch": 1.0745870181878858,
      "grad_norm": 5.039114475250244,
      "learning_rate": 3.7346625766871166e-05,
      "loss": 0.4866,
      "step": 6440
    },
    {
      "epoch": 1.0762556315701652,
      "grad_norm": 4.9691290855407715,
      "learning_rate": 3.732532379004772e-05,
      "loss": 0.5368,
      "step": 6450
    },
    {
      "epoch": 1.0779242449524444,
      "grad_norm": 6.873760223388672,
      "learning_rate": 3.730402181322427e-05,
      "loss": 0.6677,
      "step": 6460
    },
    {
      "epoch": 1.0795928583347238,
      "grad_norm": 9.760710716247559,
      "learning_rate": 3.728271983640082e-05,
      "loss": 0.9027,
      "step": 6470
    },
    {
      "epoch": 1.0812614717170033,
      "grad_norm": 3.5461392402648926,
      "learning_rate": 3.726141785957737e-05,
      "loss": 0.6164,
      "step": 6480
    },
    {
      "epoch": 1.0829300850992825,
      "grad_norm": 7.976288795471191,
      "learning_rate": 3.724011588275392e-05,
      "loss": 0.4679,
      "step": 6490
    },
    {
      "epoch": 1.0845986984815619,
      "grad_norm": 4.073061466217041,
      "learning_rate": 3.7218813905930474e-05,
      "loss": 0.5063,
      "step": 6500
    },
    {
      "epoch": 1.086267311863841,
      "grad_norm": 6.462550640106201,
      "learning_rate": 3.719751192910702e-05,
      "loss": 0.7184,
      "step": 6510
    },
    {
      "epoch": 1.0879359252461205,
      "grad_norm": 11.96576976776123,
      "learning_rate": 3.717620995228358e-05,
      "loss": 0.6945,
      "step": 6520
    },
    {
      "epoch": 1.0896045386284,
      "grad_norm": 13.607446670532227,
      "learning_rate": 3.715490797546012e-05,
      "loss": 0.8052,
      "step": 6530
    },
    {
      "epoch": 1.091273152010679,
      "grad_norm": 9.588977813720703,
      "learning_rate": 3.713360599863668e-05,
      "loss": 0.6373,
      "step": 6540
    },
    {
      "epoch": 1.0929417653929585,
      "grad_norm": 1.755293846130371,
      "learning_rate": 3.7112304021813224e-05,
      "loss": 0.3134,
      "step": 6550
    },
    {
      "epoch": 1.0946103787752377,
      "grad_norm": 7.734462261199951,
      "learning_rate": 3.7091002044989775e-05,
      "loss": 0.6569,
      "step": 6560
    },
    {
      "epoch": 1.0962789921575171,
      "grad_norm": 1.383083462715149,
      "learning_rate": 3.706970006816633e-05,
      "loss": 0.2354,
      "step": 6570
    },
    {
      "epoch": 1.0979476055397965,
      "grad_norm": 10.910091400146484,
      "learning_rate": 3.704839809134288e-05,
      "loss": 0.9163,
      "step": 6580
    },
    {
      "epoch": 1.0996162189220757,
      "grad_norm": 14.028646469116211,
      "learning_rate": 3.702709611451943e-05,
      "loss": 0.3814,
      "step": 6590
    },
    {
      "epoch": 1.1012848323043551,
      "grad_norm": 5.5066609382629395,
      "learning_rate": 3.700579413769598e-05,
      "loss": 0.4891,
      "step": 6600
    },
    {
      "epoch": 1.1029534456866343,
      "grad_norm": 6.57949686050415,
      "learning_rate": 3.698449216087253e-05,
      "loss": 0.4059,
      "step": 6610
    },
    {
      "epoch": 1.1046220590689138,
      "grad_norm": 7.122279167175293,
      "learning_rate": 3.696319018404908e-05,
      "loss": 0.8008,
      "step": 6620
    },
    {
      "epoch": 1.106290672451193,
      "grad_norm": 7.192483901977539,
      "learning_rate": 3.6941888207225635e-05,
      "loss": 0.6814,
      "step": 6630
    },
    {
      "epoch": 1.1079592858334724,
      "grad_norm": 6.323932647705078,
      "learning_rate": 3.692058623040218e-05,
      "loss": 0.3882,
      "step": 6640
    },
    {
      "epoch": 1.1096278992157518,
      "grad_norm": 8.028552055358887,
      "learning_rate": 3.689928425357874e-05,
      "loss": 0.8305,
      "step": 6650
    },
    {
      "epoch": 1.111296512598031,
      "grad_norm": 4.735127925872803,
      "learning_rate": 3.687798227675528e-05,
      "loss": 0.5609,
      "step": 6660
    },
    {
      "epoch": 1.1129651259803104,
      "grad_norm": 5.254373550415039,
      "learning_rate": 3.685668029993183e-05,
      "loss": 0.7171,
      "step": 6670
    },
    {
      "epoch": 1.1146337393625898,
      "grad_norm": 4.408944606781006,
      "learning_rate": 3.683537832310839e-05,
      "loss": 0.6458,
      "step": 6680
    },
    {
      "epoch": 1.116302352744869,
      "grad_norm": 4.268215179443359,
      "learning_rate": 3.6814076346284936e-05,
      "loss": 0.4308,
      "step": 6690
    },
    {
      "epoch": 1.1179709661271484,
      "grad_norm": 4.247426509857178,
      "learning_rate": 3.679277436946149e-05,
      "loss": 0.5246,
      "step": 6700
    },
    {
      "epoch": 1.1196395795094276,
      "grad_norm": 7.157942771911621,
      "learning_rate": 3.677147239263804e-05,
      "loss": 0.4986,
      "step": 6710
    },
    {
      "epoch": 1.121308192891707,
      "grad_norm": 4.990433692932129,
      "learning_rate": 3.675017041581459e-05,
      "loss": 0.5936,
      "step": 6720
    },
    {
      "epoch": 1.1229768062739862,
      "grad_norm": 12.001702308654785,
      "learning_rate": 3.6728868438991134e-05,
      "loss": 0.7533,
      "step": 6730
    },
    {
      "epoch": 1.1246454196562656,
      "grad_norm": 7.522315979003906,
      "learning_rate": 3.670756646216769e-05,
      "loss": 0.6657,
      "step": 6740
    },
    {
      "epoch": 1.126314033038545,
      "grad_norm": 9.230464935302734,
      "learning_rate": 3.6686264485344244e-05,
      "loss": 0.6489,
      "step": 6750
    },
    {
      "epoch": 1.1279826464208242,
      "grad_norm": 1.2969410419464111,
      "learning_rate": 3.6664962508520795e-05,
      "loss": 0.5905,
      "step": 6760
    },
    {
      "epoch": 1.1296512598031037,
      "grad_norm": 8.823436737060547,
      "learning_rate": 3.6643660531697347e-05,
      "loss": 0.5623,
      "step": 6770
    },
    {
      "epoch": 1.1313198731853829,
      "grad_norm": 2.039334297180176,
      "learning_rate": 3.662235855487389e-05,
      "loss": 0.5531,
      "step": 6780
    },
    {
      "epoch": 1.1329884865676623,
      "grad_norm": 9.40267562866211,
      "learning_rate": 3.660105657805045e-05,
      "loss": 0.8879,
      "step": 6790
    },
    {
      "epoch": 1.1346570999499417,
      "grad_norm": 5.656665802001953,
      "learning_rate": 3.6579754601226994e-05,
      "loss": 0.4651,
      "step": 6800
    },
    {
      "epoch": 1.1363257133322209,
      "grad_norm": 7.052081108093262,
      "learning_rate": 3.6558452624403545e-05,
      "loss": 0.4042,
      "step": 6810
    },
    {
      "epoch": 1.1379943267145003,
      "grad_norm": 7.350648880004883,
      "learning_rate": 3.6537150647580096e-05,
      "loss": 0.9515,
      "step": 6820
    },
    {
      "epoch": 1.1396629400967795,
      "grad_norm": 7.639624118804932,
      "learning_rate": 3.651584867075665e-05,
      "loss": 0.8204,
      "step": 6830
    },
    {
      "epoch": 1.141331553479059,
      "grad_norm": 4.469156265258789,
      "learning_rate": 3.64945466939332e-05,
      "loss": 0.7851,
      "step": 6840
    },
    {
      "epoch": 1.1430001668613383,
      "grad_norm": 4.416633605957031,
      "learning_rate": 3.647324471710975e-05,
      "loss": 0.5788,
      "step": 6850
    },
    {
      "epoch": 1.1446687802436175,
      "grad_norm": 5.527399063110352,
      "learning_rate": 3.64519427402863e-05,
      "loss": 0.7662,
      "step": 6860
    },
    {
      "epoch": 1.146337393625897,
      "grad_norm": 9.548369407653809,
      "learning_rate": 3.643064076346285e-05,
      "loss": 0.7932,
      "step": 6870
    },
    {
      "epoch": 1.1480060070081761,
      "grad_norm": 2.7713897228240967,
      "learning_rate": 3.6409338786639404e-05,
      "loss": 0.498,
      "step": 6880
    },
    {
      "epoch": 1.1496746203904555,
      "grad_norm": 5.374054908752441,
      "learning_rate": 3.638803680981595e-05,
      "loss": 0.4535,
      "step": 6890
    },
    {
      "epoch": 1.151343233772735,
      "grad_norm": 2.4161317348480225,
      "learning_rate": 3.636673483299251e-05,
      "loss": 0.6984,
      "step": 6900
    },
    {
      "epoch": 1.1530118471550141,
      "grad_norm": 3.794311761856079,
      "learning_rate": 3.634543285616905e-05,
      "loss": 0.7508,
      "step": 6910
    },
    {
      "epoch": 1.1546804605372936,
      "grad_norm": 9.70964241027832,
      "learning_rate": 3.63241308793456e-05,
      "loss": 0.6453,
      "step": 6920
    },
    {
      "epoch": 1.1563490739195728,
      "grad_norm": 5.058757781982422,
      "learning_rate": 3.6302828902522154e-05,
      "loss": 0.461,
      "step": 6930
    },
    {
      "epoch": 1.1580176873018522,
      "grad_norm": 5.127359867095947,
      "learning_rate": 3.6281526925698706e-05,
      "loss": 0.7034,
      "step": 6940
    },
    {
      "epoch": 1.1596863006841316,
      "grad_norm": 5.916182518005371,
      "learning_rate": 3.626022494887526e-05,
      "loss": 0.6677,
      "step": 6950
    },
    {
      "epoch": 1.1613549140664108,
      "grad_norm": 6.05897331237793,
      "learning_rate": 3.623892297205181e-05,
      "loss": 0.9871,
      "step": 6960
    },
    {
      "epoch": 1.1630235274486902,
      "grad_norm": 2.5839531421661377,
      "learning_rate": 3.621762099522836e-05,
      "loss": 0.457,
      "step": 6970
    },
    {
      "epoch": 1.1646921408309694,
      "grad_norm": 3.7286715507507324,
      "learning_rate": 3.619631901840491e-05,
      "loss": 0.5693,
      "step": 6980
    },
    {
      "epoch": 1.1663607542132488,
      "grad_norm": 1.9645737409591675,
      "learning_rate": 3.617501704158146e-05,
      "loss": 0.6683,
      "step": 6990
    },
    {
      "epoch": 1.168029367595528,
      "grad_norm": 4.315593719482422,
      "learning_rate": 3.615371506475801e-05,
      "loss": 0.6747,
      "step": 7000
    },
    {
      "epoch": 1.1696979809778074,
      "grad_norm": 11.413714408874512,
      "learning_rate": 3.6132413087934565e-05,
      "loss": 0.7119,
      "step": 7010
    },
    {
      "epoch": 1.1713665943600868,
      "grad_norm": 7.082180500030518,
      "learning_rate": 3.611111111111111e-05,
      "loss": 0.5657,
      "step": 7020
    },
    {
      "epoch": 1.173035207742366,
      "grad_norm": 11.14974594116211,
      "learning_rate": 3.608980913428766e-05,
      "loss": 0.5988,
      "step": 7030
    },
    {
      "epoch": 1.1747038211246454,
      "grad_norm": 4.396084785461426,
      "learning_rate": 3.606850715746421e-05,
      "loss": 0.6484,
      "step": 7040
    },
    {
      "epoch": 1.1763724345069249,
      "grad_norm": 6.6631035804748535,
      "learning_rate": 3.6047205180640763e-05,
      "loss": 0.5979,
      "step": 7050
    },
    {
      "epoch": 1.178041047889204,
      "grad_norm": 10.596138954162598,
      "learning_rate": 3.602590320381732e-05,
      "loss": 0.599,
      "step": 7060
    },
    {
      "epoch": 1.1797096612714835,
      "grad_norm": 8.649216651916504,
      "learning_rate": 3.6004601226993866e-05,
      "loss": 1.0748,
      "step": 7070
    },
    {
      "epoch": 1.1813782746537627,
      "grad_norm": 10.33381175994873,
      "learning_rate": 3.598329925017042e-05,
      "loss": 0.6896,
      "step": 7080
    },
    {
      "epoch": 1.183046888036042,
      "grad_norm": 5.234814643859863,
      "learning_rate": 3.596199727334697e-05,
      "loss": 0.5335,
      "step": 7090
    },
    {
      "epoch": 1.1847155014183213,
      "grad_norm": 8.948088645935059,
      "learning_rate": 3.594069529652352e-05,
      "loss": 0.6576,
      "step": 7100
    },
    {
      "epoch": 1.1863841148006007,
      "grad_norm": 10.271075248718262,
      "learning_rate": 3.5919393319700065e-05,
      "loss": 0.676,
      "step": 7110
    },
    {
      "epoch": 1.18805272818288,
      "grad_norm": 3.508049249649048,
      "learning_rate": 3.589809134287662e-05,
      "loss": 0.62,
      "step": 7120
    },
    {
      "epoch": 1.1897213415651593,
      "grad_norm": 5.7778401374816895,
      "learning_rate": 3.587678936605317e-05,
      "loss": 0.7665,
      "step": 7130
    },
    {
      "epoch": 1.1913899549474387,
      "grad_norm": 7.086795806884766,
      "learning_rate": 3.5855487389229725e-05,
      "loss": 0.8002,
      "step": 7140
    },
    {
      "epoch": 1.1930585683297181,
      "grad_norm": 4.585603713989258,
      "learning_rate": 3.583418541240628e-05,
      "loss": 0.6344,
      "step": 7150
    },
    {
      "epoch": 1.1947271817119973,
      "grad_norm": 10.239252090454102,
      "learning_rate": 3.581288343558282e-05,
      "loss": 0.4321,
      "step": 7160
    },
    {
      "epoch": 1.1963957950942767,
      "grad_norm": 9.314529418945312,
      "learning_rate": 3.579158145875938e-05,
      "loss": 0.7614,
      "step": 7170
    },
    {
      "epoch": 1.198064408476556,
      "grad_norm": 1.0419179201126099,
      "learning_rate": 3.5770279481935924e-05,
      "loss": 0.5,
      "step": 7180
    },
    {
      "epoch": 1.1997330218588353,
      "grad_norm": 4.463251113891602,
      "learning_rate": 3.5748977505112475e-05,
      "loss": 0.7678,
      "step": 7190
    },
    {
      "epoch": 1.2014016352411145,
      "grad_norm": 7.095434188842773,
      "learning_rate": 3.5727675528289027e-05,
      "loss": 0.6124,
      "step": 7200
    },
    {
      "epoch": 1.203070248623394,
      "grad_norm": 3.0591468811035156,
      "learning_rate": 3.570637355146558e-05,
      "loss": 0.6569,
      "step": 7210
    },
    {
      "epoch": 1.2047388620056734,
      "grad_norm": 4.009960651397705,
      "learning_rate": 3.568507157464213e-05,
      "loss": 0.6906,
      "step": 7220
    },
    {
      "epoch": 1.2064074753879526,
      "grad_norm": 5.86364221572876,
      "learning_rate": 3.566376959781868e-05,
      "loss": 0.6614,
      "step": 7230
    },
    {
      "epoch": 1.208076088770232,
      "grad_norm": 11.338386535644531,
      "learning_rate": 3.564246762099523e-05,
      "loss": 0.535,
      "step": 7240
    },
    {
      "epoch": 1.2097447021525112,
      "grad_norm": 12.546670913696289,
      "learning_rate": 3.562116564417178e-05,
      "loss": 0.3669,
      "step": 7250
    },
    {
      "epoch": 1.2114133155347906,
      "grad_norm": 8.431734085083008,
      "learning_rate": 3.5599863667348335e-05,
      "loss": 0.58,
      "step": 7260
    },
    {
      "epoch": 1.21308192891707,
      "grad_norm": 5.878488540649414,
      "learning_rate": 3.557856169052488e-05,
      "loss": 0.7176,
      "step": 7270
    },
    {
      "epoch": 1.2147505422993492,
      "grad_norm": 9.647154808044434,
      "learning_rate": 3.555725971370144e-05,
      "loss": 0.6577,
      "step": 7280
    },
    {
      "epoch": 1.2164191556816286,
      "grad_norm": 2.966520309448242,
      "learning_rate": 3.553595773687798e-05,
      "loss": 0.784,
      "step": 7290
    },
    {
      "epoch": 1.2180877690639078,
      "grad_norm": 5.559872150421143,
      "learning_rate": 3.551465576005453e-05,
      "loss": 1.1233,
      "step": 7300
    },
    {
      "epoch": 1.2197563824461872,
      "grad_norm": 5.0459675788879395,
      "learning_rate": 3.5493353783231084e-05,
      "loss": 0.6239,
      "step": 7310
    },
    {
      "epoch": 1.2214249958284666,
      "grad_norm": 7.173463821411133,
      "learning_rate": 3.5472051806407636e-05,
      "loss": 0.6379,
      "step": 7320
    },
    {
      "epoch": 1.2230936092107458,
      "grad_norm": 8.204900741577148,
      "learning_rate": 3.545074982958419e-05,
      "loss": 0.5892,
      "step": 7330
    },
    {
      "epoch": 1.2247622225930253,
      "grad_norm": 5.583600997924805,
      "learning_rate": 3.542944785276074e-05,
      "loss": 0.6358,
      "step": 7340
    },
    {
      "epoch": 1.2264308359753044,
      "grad_norm": 7.2032904624938965,
      "learning_rate": 3.540814587593729e-05,
      "loss": 0.6666,
      "step": 7350
    },
    {
      "epoch": 1.2280994493575839,
      "grad_norm": 7.465412139892578,
      "learning_rate": 3.538684389911384e-05,
      "loss": 0.543,
      "step": 7360
    },
    {
      "epoch": 1.229768062739863,
      "grad_norm": 3.3939058780670166,
      "learning_rate": 3.536554192229039e-05,
      "loss": 0.6687,
      "step": 7370
    },
    {
      "epoch": 1.2314366761221425,
      "grad_norm": 1.2408862113952637,
      "learning_rate": 3.534423994546694e-05,
      "loss": 0.6308,
      "step": 7380
    },
    {
      "epoch": 1.2331052895044219,
      "grad_norm": 6.897663116455078,
      "learning_rate": 3.5322937968643495e-05,
      "loss": 0.7142,
      "step": 7390
    },
    {
      "epoch": 1.234773902886701,
      "grad_norm": 4.448601245880127,
      "learning_rate": 3.530163599182004e-05,
      "loss": 0.4822,
      "step": 7400
    },
    {
      "epoch": 1.2364425162689805,
      "grad_norm": 0.7527143955230713,
      "learning_rate": 3.528033401499659e-05,
      "loss": 0.3028,
      "step": 7410
    },
    {
      "epoch": 1.23811112965126,
      "grad_norm": 3.9726107120513916,
      "learning_rate": 3.525903203817314e-05,
      "loss": 0.6313,
      "step": 7420
    },
    {
      "epoch": 1.239779743033539,
      "grad_norm": 7.673214435577393,
      "learning_rate": 3.5237730061349694e-05,
      "loss": 0.479,
      "step": 7430
    },
    {
      "epoch": 1.2414483564158185,
      "grad_norm": 10.951626777648926,
      "learning_rate": 3.521642808452625e-05,
      "loss": 0.8505,
      "step": 7440
    },
    {
      "epoch": 1.2431169697980977,
      "grad_norm": 7.510569095611572,
      "learning_rate": 3.5195126107702796e-05,
      "loss": 0.6805,
      "step": 7450
    },
    {
      "epoch": 1.2447855831803771,
      "grad_norm": 9.328713417053223,
      "learning_rate": 3.517382413087935e-05,
      "loss": 0.8433,
      "step": 7460
    },
    {
      "epoch": 1.2464541965626563,
      "grad_norm": 2.3765006065368652,
      "learning_rate": 3.51525221540559e-05,
      "loss": 0.5929,
      "step": 7470
    },
    {
      "epoch": 1.2481228099449357,
      "grad_norm": 7.9521026611328125,
      "learning_rate": 3.513122017723245e-05,
      "loss": 0.3743,
      "step": 7480
    },
    {
      "epoch": 1.2497914233272152,
      "grad_norm": 11.668896675109863,
      "learning_rate": 3.5109918200408995e-05,
      "loss": 0.567,
      "step": 7490
    },
    {
      "epoch": 1.2514600367094944,
      "grad_norm": 8.029791831970215,
      "learning_rate": 3.508861622358555e-05,
      "loss": 0.5257,
      "step": 7500
    },
    {
      "epoch": 1.2531286500917738,
      "grad_norm": 2.1989424228668213,
      "learning_rate": 3.50673142467621e-05,
      "loss": 0.5445,
      "step": 7510
    },
    {
      "epoch": 1.2547972634740532,
      "grad_norm": 6.79125452041626,
      "learning_rate": 3.504601226993865e-05,
      "loss": 0.4542,
      "step": 7520
    },
    {
      "epoch": 1.2564658768563324,
      "grad_norm": 6.68700647354126,
      "learning_rate": 3.502471029311521e-05,
      "loss": 0.5461,
      "step": 7530
    },
    {
      "epoch": 1.2581344902386118,
      "grad_norm": 1.6994255781173706,
      "learning_rate": 3.500340831629175e-05,
      "loss": 0.6659,
      "step": 7540
    },
    {
      "epoch": 1.259803103620891,
      "grad_norm": 1.9643442630767822,
      "learning_rate": 3.498210633946831e-05,
      "loss": 0.8916,
      "step": 7550
    },
    {
      "epoch": 1.2614717170031704,
      "grad_norm": 6.440840721130371,
      "learning_rate": 3.4960804362644854e-05,
      "loss": 0.435,
      "step": 7560
    },
    {
      "epoch": 1.2631403303854496,
      "grad_norm": 6.294175624847412,
      "learning_rate": 3.4939502385821405e-05,
      "loss": 0.6012,
      "step": 7570
    },
    {
      "epoch": 1.264808943767729,
      "grad_norm": 13.192429542541504,
      "learning_rate": 3.491820040899796e-05,
      "loss": 0.6153,
      "step": 7580
    },
    {
      "epoch": 1.2664775571500084,
      "grad_norm": 3.5880069732666016,
      "learning_rate": 3.489689843217451e-05,
      "loss": 0.4031,
      "step": 7590
    },
    {
      "epoch": 1.2681461705322876,
      "grad_norm": 10.811549186706543,
      "learning_rate": 3.487559645535106e-05,
      "loss": 0.6215,
      "step": 7600
    },
    {
      "epoch": 1.269814783914567,
      "grad_norm": 2.981245517730713,
      "learning_rate": 3.485429447852761e-05,
      "loss": 0.5621,
      "step": 7610
    },
    {
      "epoch": 1.2714833972968465,
      "grad_norm": 3.4594509601593018,
      "learning_rate": 3.483299250170416e-05,
      "loss": 0.5458,
      "step": 7620
    },
    {
      "epoch": 1.2731520106791256,
      "grad_norm": 9.071195602416992,
      "learning_rate": 3.481169052488071e-05,
      "loss": 0.5487,
      "step": 7630
    },
    {
      "epoch": 1.2748206240614048,
      "grad_norm": 4.412954330444336,
      "learning_rate": 3.4790388548057265e-05,
      "loss": 0.4577,
      "step": 7640
    },
    {
      "epoch": 1.2764892374436843,
      "grad_norm": 9.974410057067871,
      "learning_rate": 3.476908657123381e-05,
      "loss": 0.6936,
      "step": 7650
    },
    {
      "epoch": 1.2781578508259637,
      "grad_norm": 4.994160175323486,
      "learning_rate": 3.474778459441037e-05,
      "loss": 0.7816,
      "step": 7660
    },
    {
      "epoch": 1.2798264642082429,
      "grad_norm": 5.522905349731445,
      "learning_rate": 3.472648261758691e-05,
      "loss": 0.3739,
      "step": 7670
    },
    {
      "epoch": 1.2814950775905223,
      "grad_norm": 7.749030590057373,
      "learning_rate": 3.470518064076346e-05,
      "loss": 0.7047,
      "step": 7680
    },
    {
      "epoch": 1.2831636909728017,
      "grad_norm": 6.478409290313721,
      "learning_rate": 3.4683878663940015e-05,
      "loss": 0.7439,
      "step": 7690
    },
    {
      "epoch": 1.284832304355081,
      "grad_norm": 6.615057468414307,
      "learning_rate": 3.4662576687116566e-05,
      "loss": 0.3824,
      "step": 7700
    },
    {
      "epoch": 1.2865009177373603,
      "grad_norm": 11.134163856506348,
      "learning_rate": 3.464127471029312e-05,
      "loss": 0.563,
      "step": 7710
    },
    {
      "epoch": 1.2881695311196395,
      "grad_norm": 18.005348205566406,
      "learning_rate": 3.461997273346967e-05,
      "loss": 0.5812,
      "step": 7720
    },
    {
      "epoch": 1.289838144501919,
      "grad_norm": 1.329520583152771,
      "learning_rate": 3.459867075664622e-05,
      "loss": 0.4143,
      "step": 7730
    },
    {
      "epoch": 1.291506757884198,
      "grad_norm": 8.088467597961426,
      "learning_rate": 3.457736877982277e-05,
      "loss": 0.5002,
      "step": 7740
    },
    {
      "epoch": 1.2931753712664775,
      "grad_norm": 9.61352252960205,
      "learning_rate": 3.455606680299932e-05,
      "loss": 0.6966,
      "step": 7750
    },
    {
      "epoch": 1.294843984648757,
      "grad_norm": 8.665962219238281,
      "learning_rate": 3.453476482617587e-05,
      "loss": 0.8084,
      "step": 7760
    },
    {
      "epoch": 1.2965125980310361,
      "grad_norm": 8.269156455993652,
      "learning_rate": 3.4513462849352425e-05,
      "loss": 0.7399,
      "step": 7770
    },
    {
      "epoch": 1.2981812114133156,
      "grad_norm": 4.055342197418213,
      "learning_rate": 3.449216087252897e-05,
      "loss": 0.6207,
      "step": 7780
    },
    {
      "epoch": 1.299849824795595,
      "grad_norm": 4.412633895874023,
      "learning_rate": 3.447085889570552e-05,
      "loss": 0.6736,
      "step": 7790
    },
    {
      "epoch": 1.3015184381778742,
      "grad_norm": 11.866539001464844,
      "learning_rate": 3.444955691888207e-05,
      "loss": 0.662,
      "step": 7800
    },
    {
      "epoch": 1.3031870515601536,
      "grad_norm": 7.681331157684326,
      "learning_rate": 3.4428254942058624e-05,
      "loss": 0.8469,
      "step": 7810
    },
    {
      "epoch": 1.3048556649424328,
      "grad_norm": 7.7690205574035645,
      "learning_rate": 3.4406952965235175e-05,
      "loss": 0.6742,
      "step": 7820
    },
    {
      "epoch": 1.3065242783247122,
      "grad_norm": 8.509383201599121,
      "learning_rate": 3.4385650988411726e-05,
      "loss": 1.0217,
      "step": 7830
    },
    {
      "epoch": 1.3081928917069914,
      "grad_norm": 3.006300687789917,
      "learning_rate": 3.436434901158828e-05,
      "loss": 0.7692,
      "step": 7840
    },
    {
      "epoch": 1.3098615050892708,
      "grad_norm": 6.513629913330078,
      "learning_rate": 3.434304703476483e-05,
      "loss": 0.6085,
      "step": 7850
    },
    {
      "epoch": 1.3115301184715502,
      "grad_norm": 4.841976642608643,
      "learning_rate": 3.432174505794138e-05,
      "loss": 0.679,
      "step": 7860
    },
    {
      "epoch": 1.3131987318538294,
      "grad_norm": 14.25003719329834,
      "learning_rate": 3.4300443081117925e-05,
      "loss": 0.8291,
      "step": 7870
    },
    {
      "epoch": 1.3148673452361088,
      "grad_norm": 3.8958845138549805,
      "learning_rate": 3.427914110429448e-05,
      "loss": 0.7613,
      "step": 7880
    },
    {
      "epoch": 1.3165359586183882,
      "grad_norm": 4.429243564605713,
      "learning_rate": 3.425783912747103e-05,
      "loss": 0.7853,
      "step": 7890
    },
    {
      "epoch": 1.3182045720006674,
      "grad_norm": 8.772050857543945,
      "learning_rate": 3.423653715064758e-05,
      "loss": 0.8032,
      "step": 7900
    },
    {
      "epoch": 1.3198731853829468,
      "grad_norm": 5.376317977905273,
      "learning_rate": 3.421523517382414e-05,
      "loss": 0.6922,
      "step": 7910
    },
    {
      "epoch": 1.321541798765226,
      "grad_norm": 4.730961799621582,
      "learning_rate": 3.419393319700068e-05,
      "loss": 0.5242,
      "step": 7920
    },
    {
      "epoch": 1.3232104121475055,
      "grad_norm": 6.306545734405518,
      "learning_rate": 3.417263122017723e-05,
      "loss": 0.8051,
      "step": 7930
    },
    {
      "epoch": 1.3248790255297846,
      "grad_norm": 4.52564811706543,
      "learning_rate": 3.4151329243353784e-05,
      "loss": 0.5424,
      "step": 7940
    },
    {
      "epoch": 1.326547638912064,
      "grad_norm": 5.624623775482178,
      "learning_rate": 3.4130027266530336e-05,
      "loss": 0.5221,
      "step": 7950
    },
    {
      "epoch": 1.3282162522943435,
      "grad_norm": 8.512168884277344,
      "learning_rate": 3.410872528970689e-05,
      "loss": 0.5883,
      "step": 7960
    },
    {
      "epoch": 1.3298848656766227,
      "grad_norm": 6.592508792877197,
      "learning_rate": 3.408742331288344e-05,
      "loss": 0.584,
      "step": 7970
    },
    {
      "epoch": 1.331553479058902,
      "grad_norm": 7.4322004318237305,
      "learning_rate": 3.406612133605999e-05,
      "loss": 0.7251,
      "step": 7980
    },
    {
      "epoch": 1.3332220924411815,
      "grad_norm": 0.9539530277252197,
      "learning_rate": 3.404481935923654e-05,
      "loss": 0.426,
      "step": 7990
    },
    {
      "epoch": 1.3348907058234607,
      "grad_norm": 11.277975082397461,
      "learning_rate": 3.402351738241309e-05,
      "loss": 0.7599,
      "step": 8000
    },
    {
      "epoch": 1.33655931920574,
      "grad_norm": 5.26953125,
      "learning_rate": 3.400221540558964e-05,
      "loss": 0.4548,
      "step": 8010
    },
    {
      "epoch": 1.3382279325880193,
      "grad_norm": 8.273087501525879,
      "learning_rate": 3.3980913428766195e-05,
      "loss": 0.5426,
      "step": 8020
    },
    {
      "epoch": 1.3398965459702987,
      "grad_norm": 6.567383289337158,
      "learning_rate": 3.395961145194274e-05,
      "loss": 0.5902,
      "step": 8030
    },
    {
      "epoch": 1.341565159352578,
      "grad_norm": 5.1387200355529785,
      "learning_rate": 3.39383094751193e-05,
      "loss": 0.4896,
      "step": 8040
    },
    {
      "epoch": 1.3432337727348573,
      "grad_norm": 3.3559603691101074,
      "learning_rate": 3.391700749829584e-05,
      "loss": 0.8754,
      "step": 8050
    },
    {
      "epoch": 1.3449023861171367,
      "grad_norm": 12.123994827270508,
      "learning_rate": 3.3895705521472393e-05,
      "loss": 0.587,
      "step": 8060
    },
    {
      "epoch": 1.346570999499416,
      "grad_norm": 6.148636817932129,
      "learning_rate": 3.3874403544648945e-05,
      "loss": 0.6295,
      "step": 8070
    },
    {
      "epoch": 1.3482396128816954,
      "grad_norm": 2.1247739791870117,
      "learning_rate": 3.3853101567825496e-05,
      "loss": 0.4381,
      "step": 8080
    },
    {
      "epoch": 1.3499082262639748,
      "grad_norm": 3.8700451850891113,
      "learning_rate": 3.383179959100205e-05,
      "loss": 0.6148,
      "step": 8090
    },
    {
      "epoch": 1.351576839646254,
      "grad_norm": 1.874157428741455,
      "learning_rate": 3.38104976141786e-05,
      "loss": 0.4065,
      "step": 8100
    },
    {
      "epoch": 1.3532454530285332,
      "grad_norm": 0.13495682179927826,
      "learning_rate": 3.378919563735515e-05,
      "loss": 0.8201,
      "step": 8110
    },
    {
      "epoch": 1.3549140664108126,
      "grad_norm": 9.310791969299316,
      "learning_rate": 3.3767893660531695e-05,
      "loss": 0.5686,
      "step": 8120
    },
    {
      "epoch": 1.356582679793092,
      "grad_norm": 7.913043022155762,
      "learning_rate": 3.374659168370825e-05,
      "loss": 0.6539,
      "step": 8130
    },
    {
      "epoch": 1.3582512931753712,
      "grad_norm": 1.745361328125,
      "learning_rate": 3.37252897068848e-05,
      "loss": 0.6673,
      "step": 8140
    },
    {
      "epoch": 1.3599199065576506,
      "grad_norm": 7.911966800689697,
      "learning_rate": 3.3703987730061355e-05,
      "loss": 0.5043,
      "step": 8150
    },
    {
      "epoch": 1.36158851993993,
      "grad_norm": 9.95453929901123,
      "learning_rate": 3.36826857532379e-05,
      "loss": 0.4084,
      "step": 8160
    },
    {
      "epoch": 1.3632571333222092,
      "grad_norm": 8.832788467407227,
      "learning_rate": 3.366138377641445e-05,
      "loss": 0.617,
      "step": 8170
    },
    {
      "epoch": 1.3649257467044886,
      "grad_norm": 4.105202674865723,
      "learning_rate": 3.3640081799591e-05,
      "loss": 0.5218,
      "step": 8180
    },
    {
      "epoch": 1.3665943600867678,
      "grad_norm": 3.429629325866699,
      "learning_rate": 3.3618779822767554e-05,
      "loss": 0.7211,
      "step": 8190
    },
    {
      "epoch": 1.3682629734690472,
      "grad_norm": 2.5207204818725586,
      "learning_rate": 3.3597477845944105e-05,
      "loss": 0.3775,
      "step": 8200
    },
    {
      "epoch": 1.3699315868513264,
      "grad_norm": 3.319451332092285,
      "learning_rate": 3.3576175869120657e-05,
      "loss": 0.341,
      "step": 8210
    },
    {
      "epoch": 1.3716002002336058,
      "grad_norm": 6.204531192779541,
      "learning_rate": 3.355487389229721e-05,
      "loss": 0.7652,
      "step": 8220
    },
    {
      "epoch": 1.3732688136158853,
      "grad_norm": 5.474119186401367,
      "learning_rate": 3.353357191547375e-05,
      "loss": 0.6674,
      "step": 8230
    },
    {
      "epoch": 1.3749374269981645,
      "grad_norm": 3.5407679080963135,
      "learning_rate": 3.351226993865031e-05,
      "loss": 0.3622,
      "step": 8240
    },
    {
      "epoch": 1.3766060403804439,
      "grad_norm": 8.376006126403809,
      "learning_rate": 3.3490967961826855e-05,
      "loss": 0.9103,
      "step": 8250
    },
    {
      "epoch": 1.3782746537627233,
      "grad_norm": 1.886756420135498,
      "learning_rate": 3.346966598500341e-05,
      "loss": 0.7364,
      "step": 8260
    },
    {
      "epoch": 1.3799432671450025,
      "grad_norm": 0.32379868626594543,
      "learning_rate": 3.344836400817996e-05,
      "loss": 0.5476,
      "step": 8270
    },
    {
      "epoch": 1.381611880527282,
      "grad_norm": 6.644929885864258,
      "learning_rate": 3.342706203135651e-05,
      "loss": 0.5887,
      "step": 8280
    },
    {
      "epoch": 1.383280493909561,
      "grad_norm": 5.6287384033203125,
      "learning_rate": 3.340576005453307e-05,
      "loss": 0.6094,
      "step": 8290
    },
    {
      "epoch": 1.3849491072918405,
      "grad_norm": 8.596138954162598,
      "learning_rate": 3.338445807770961e-05,
      "loss": 0.5436,
      "step": 8300
    },
    {
      "epoch": 1.3866177206741197,
      "grad_norm": 3.051163673400879,
      "learning_rate": 3.336315610088616e-05,
      "loss": 0.6911,
      "step": 8310
    },
    {
      "epoch": 1.3882863340563991,
      "grad_norm": 1.942934513092041,
      "learning_rate": 3.3341854124062714e-05,
      "loss": 0.4785,
      "step": 8320
    },
    {
      "epoch": 1.3899549474386785,
      "grad_norm": 10.663559913635254,
      "learning_rate": 3.3320552147239266e-05,
      "loss": 0.6457,
      "step": 8330
    },
    {
      "epoch": 1.3916235608209577,
      "grad_norm": 5.178856372833252,
      "learning_rate": 3.329925017041582e-05,
      "loss": 0.7173,
      "step": 8340
    },
    {
      "epoch": 1.3932921742032371,
      "grad_norm": 13.978004455566406,
      "learning_rate": 3.327794819359237e-05,
      "loss": 0.6356,
      "step": 8350
    },
    {
      "epoch": 1.3949607875855166,
      "grad_norm": 6.657058238983154,
      "learning_rate": 3.325664621676892e-05,
      "loss": 0.6422,
      "step": 8360
    },
    {
      "epoch": 1.3966294009677958,
      "grad_norm": 3.5396764278411865,
      "learning_rate": 3.323534423994547e-05,
      "loss": 0.4051,
      "step": 8370
    },
    {
      "epoch": 1.3982980143500752,
      "grad_norm": 2.3687942028045654,
      "learning_rate": 3.321404226312202e-05,
      "loss": 0.4517,
      "step": 8380
    },
    {
      "epoch": 1.3999666277323544,
      "grad_norm": 4.17796516418457,
      "learning_rate": 3.319274028629857e-05,
      "loss": 0.4182,
      "step": 8390
    },
    {
      "epoch": 1.4016352411146338,
      "grad_norm": 17.489177703857422,
      "learning_rate": 3.3171438309475125e-05,
      "loss": 0.5658,
      "step": 8400
    },
    {
      "epoch": 1.403303854496913,
      "grad_norm": 8.039362907409668,
      "learning_rate": 3.315013633265167e-05,
      "loss": 0.2832,
      "step": 8410
    },
    {
      "epoch": 1.4049724678791924,
      "grad_norm": 11.380532264709473,
      "learning_rate": 3.312883435582822e-05,
      "loss": 0.9273,
      "step": 8420
    },
    {
      "epoch": 1.4066410812614718,
      "grad_norm": 15.082452774047852,
      "learning_rate": 3.310753237900477e-05,
      "loss": 0.7602,
      "step": 8430
    },
    {
      "epoch": 1.408309694643751,
      "grad_norm": 7.5229082107543945,
      "learning_rate": 3.3086230402181324e-05,
      "loss": 0.5158,
      "step": 8440
    },
    {
      "epoch": 1.4099783080260304,
      "grad_norm": 4.808210372924805,
      "learning_rate": 3.3064928425357875e-05,
      "loss": 0.6771,
      "step": 8450
    },
    {
      "epoch": 1.4116469214083098,
      "grad_norm": 7.191981792449951,
      "learning_rate": 3.3043626448534426e-05,
      "loss": 0.9119,
      "step": 8460
    },
    {
      "epoch": 1.413315534790589,
      "grad_norm": 2.9206044673919678,
      "learning_rate": 3.302232447171098e-05,
      "loss": 0.6035,
      "step": 8470
    },
    {
      "epoch": 1.4149841481728682,
      "grad_norm": 3.622389793395996,
      "learning_rate": 3.300102249488753e-05,
      "loss": 0.5959,
      "step": 8480
    },
    {
      "epoch": 1.4166527615551476,
      "grad_norm": 9.402924537658691,
      "learning_rate": 3.297972051806408e-05,
      "loss": 0.7355,
      "step": 8490
    },
    {
      "epoch": 1.418321374937427,
      "grad_norm": 4.524724006652832,
      "learning_rate": 3.2958418541240625e-05,
      "loss": 0.4083,
      "step": 8500
    },
    {
      "epoch": 1.4199899883197062,
      "grad_norm": 4.885047912597656,
      "learning_rate": 3.293711656441718e-05,
      "loss": 0.5737,
      "step": 8510
    },
    {
      "epoch": 1.4216586017019857,
      "grad_norm": 9.000179290771484,
      "learning_rate": 3.291581458759373e-05,
      "loss": 0.6167,
      "step": 8520
    },
    {
      "epoch": 1.423327215084265,
      "grad_norm": 3.677793502807617,
      "learning_rate": 3.289451261077028e-05,
      "loss": 0.6309,
      "step": 8530
    },
    {
      "epoch": 1.4249958284665443,
      "grad_norm": 8.820743560791016,
      "learning_rate": 3.287321063394683e-05,
      "loss": 0.685,
      "step": 8540
    },
    {
      "epoch": 1.4266644418488237,
      "grad_norm": 10.081793785095215,
      "learning_rate": 3.285190865712338e-05,
      "loss": 0.6211,
      "step": 8550
    },
    {
      "epoch": 1.4283330552311029,
      "grad_norm": 1.7561029195785522,
      "learning_rate": 3.283060668029993e-05,
      "loss": 0.3408,
      "step": 8560
    },
    {
      "epoch": 1.4300016686133823,
      "grad_norm": 8.373208045959473,
      "learning_rate": 3.2809304703476484e-05,
      "loss": 0.6968,
      "step": 8570
    },
    {
      "epoch": 1.4316702819956615,
      "grad_norm": 2.874149799346924,
      "learning_rate": 3.2788002726653035e-05,
      "loss": 0.415,
      "step": 8580
    },
    {
      "epoch": 1.433338895377941,
      "grad_norm": 8.328850746154785,
      "learning_rate": 3.276670074982959e-05,
      "loss": 1.0151,
      "step": 8590
    },
    {
      "epoch": 1.4350075087602203,
      "grad_norm": 4.614090442657471,
      "learning_rate": 3.274539877300614e-05,
      "loss": 0.8282,
      "step": 8600
    },
    {
      "epoch": 1.4366761221424995,
      "grad_norm": 3.4453911781311035,
      "learning_rate": 3.272409679618268e-05,
      "loss": 0.7179,
      "step": 8610
    },
    {
      "epoch": 1.438344735524779,
      "grad_norm": 7.855764389038086,
      "learning_rate": 3.270279481935924e-05,
      "loss": 0.4326,
      "step": 8620
    },
    {
      "epoch": 1.4400133489070583,
      "grad_norm": 1.3439610004425049,
      "learning_rate": 3.2681492842535785e-05,
      "loss": 0.54,
      "step": 8630
    },
    {
      "epoch": 1.4416819622893375,
      "grad_norm": 0.21829967200756073,
      "learning_rate": 3.266019086571234e-05,
      "loss": 0.8181,
      "step": 8640
    },
    {
      "epoch": 1.443350575671617,
      "grad_norm": 3.332728862762451,
      "learning_rate": 3.263888888888889e-05,
      "loss": 0.6468,
      "step": 8650
    },
    {
      "epoch": 1.4450191890538961,
      "grad_norm": 1.7673839330673218,
      "learning_rate": 3.261758691206544e-05,
      "loss": 0.7624,
      "step": 8660
    },
    {
      "epoch": 1.4466878024361756,
      "grad_norm": 3.3501555919647217,
      "learning_rate": 3.2596284935242e-05,
      "loss": 0.4528,
      "step": 8670
    },
    {
      "epoch": 1.4483564158184548,
      "grad_norm": 10.811310768127441,
      "learning_rate": 3.257498295841854e-05,
      "loss": 0.6699,
      "step": 8680
    },
    {
      "epoch": 1.4500250292007342,
      "grad_norm": 10.368925094604492,
      "learning_rate": 3.255368098159509e-05,
      "loss": 0.5426,
      "step": 8690
    },
    {
      "epoch": 1.4516936425830136,
      "grad_norm": 6.9403977394104,
      "learning_rate": 3.2532379004771645e-05,
      "loss": 0.4729,
      "step": 8700
    },
    {
      "epoch": 1.4533622559652928,
      "grad_norm": 5.923768043518066,
      "learning_rate": 3.2511077027948196e-05,
      "loss": 0.4924,
      "step": 8710
    },
    {
      "epoch": 1.4550308693475722,
      "grad_norm": 4.750648021697998,
      "learning_rate": 3.248977505112474e-05,
      "loss": 0.6209,
      "step": 8720
    },
    {
      "epoch": 1.4566994827298516,
      "grad_norm": 7.779639720916748,
      "learning_rate": 3.24684730743013e-05,
      "loss": 0.3693,
      "step": 8730
    },
    {
      "epoch": 1.4583680961121308,
      "grad_norm": 11.908980369567871,
      "learning_rate": 3.244717109747784e-05,
      "loss": 0.6692,
      "step": 8740
    },
    {
      "epoch": 1.4600367094944102,
      "grad_norm": 6.640372276306152,
      "learning_rate": 3.24258691206544e-05,
      "loss": 0.4494,
      "step": 8750
    },
    {
      "epoch": 1.4617053228766894,
      "grad_norm": 4.303104400634766,
      "learning_rate": 3.240456714383095e-05,
      "loss": 0.3602,
      "step": 8760
    },
    {
      "epoch": 1.4633739362589688,
      "grad_norm": 3.8626718521118164,
      "learning_rate": 3.23832651670075e-05,
      "loss": 0.6799,
      "step": 8770
    },
    {
      "epoch": 1.465042549641248,
      "grad_norm": 12.676007270812988,
      "learning_rate": 3.2361963190184055e-05,
      "loss": 0.4482,
      "step": 8780
    },
    {
      "epoch": 1.4667111630235274,
      "grad_norm": 14.700623512268066,
      "learning_rate": 3.23406612133606e-05,
      "loss": 0.7211,
      "step": 8790
    },
    {
      "epoch": 1.4683797764058069,
      "grad_norm": 17.84679412841797,
      "learning_rate": 3.231935923653715e-05,
      "loss": 0.5886,
      "step": 8800
    },
    {
      "epoch": 1.470048389788086,
      "grad_norm": 8.474899291992188,
      "learning_rate": 3.22980572597137e-05,
      "loss": 0.4059,
      "step": 8810
    },
    {
      "epoch": 1.4717170031703655,
      "grad_norm": 7.242010116577148,
      "learning_rate": 3.2276755282890254e-05,
      "loss": 1.0522,
      "step": 8820
    },
    {
      "epoch": 1.4733856165526449,
      "grad_norm": 11.685423851013184,
      "learning_rate": 3.2255453306066805e-05,
      "loss": 0.8163,
      "step": 8830
    },
    {
      "epoch": 1.475054229934924,
      "grad_norm": 4.961813449859619,
      "learning_rate": 3.2234151329243356e-05,
      "loss": 0.2975,
      "step": 8840
    },
    {
      "epoch": 1.4767228433172033,
      "grad_norm": 9.108125686645508,
      "learning_rate": 3.221284935241991e-05,
      "loss": 0.6511,
      "step": 8850
    },
    {
      "epoch": 1.4783914566994827,
      "grad_norm": 9.666248321533203,
      "learning_rate": 3.219154737559646e-05,
      "loss": 0.7486,
      "step": 8860
    },
    {
      "epoch": 1.480060070081762,
      "grad_norm": 5.374334812164307,
      "learning_rate": 3.217024539877301e-05,
      "loss": 0.343,
      "step": 8870
    },
    {
      "epoch": 1.4817286834640413,
      "grad_norm": 3.600764036178589,
      "learning_rate": 3.2148943421949555e-05,
      "loss": 0.5654,
      "step": 8880
    },
    {
      "epoch": 1.4833972968463207,
      "grad_norm": 0.7321490049362183,
      "learning_rate": 3.212764144512611e-05,
      "loss": 0.7029,
      "step": 8890
    },
    {
      "epoch": 1.4850659102286001,
      "grad_norm": 8.611417770385742,
      "learning_rate": 3.210633946830266e-05,
      "loss": 0.9093,
      "step": 8900
    },
    {
      "epoch": 1.4867345236108793,
      "grad_norm": 5.071789741516113,
      "learning_rate": 3.208503749147921e-05,
      "loss": 0.6833,
      "step": 8910
    },
    {
      "epoch": 1.4884031369931587,
      "grad_norm": 4.312652111053467,
      "learning_rate": 3.206373551465576e-05,
      "loss": 0.673,
      "step": 8920
    },
    {
      "epoch": 1.490071750375438,
      "grad_norm": 3.216688632965088,
      "learning_rate": 3.204243353783231e-05,
      "loss": 0.5486,
      "step": 8930
    },
    {
      "epoch": 1.4917403637577173,
      "grad_norm": 8.382513046264648,
      "learning_rate": 3.202113156100886e-05,
      "loss": 0.7633,
      "step": 8940
    },
    {
      "epoch": 1.4934089771399965,
      "grad_norm": 2.1479108333587646,
      "learning_rate": 3.1999829584185414e-05,
      "loss": 0.5055,
      "step": 8950
    },
    {
      "epoch": 1.495077590522276,
      "grad_norm": 2.4514904022216797,
      "learning_rate": 3.1978527607361966e-05,
      "loss": 0.4025,
      "step": 8960
    },
    {
      "epoch": 1.4967462039045554,
      "grad_norm": 3.183171272277832,
      "learning_rate": 3.195722563053852e-05,
      "loss": 0.7229,
      "step": 8970
    },
    {
      "epoch": 1.4984148172868346,
      "grad_norm": 7.426604270935059,
      "learning_rate": 3.193592365371507e-05,
      "loss": 0.5359,
      "step": 8980
    },
    {
      "epoch": 1.500083430669114,
      "grad_norm": 6.63726282119751,
      "learning_rate": 3.191462167689161e-05,
      "loss": 1.2171,
      "step": 8990
    },
    {
      "epoch": 1.5017520440513934,
      "grad_norm": 9.155046463012695,
      "learning_rate": 3.189331970006817e-05,
      "loss": 0.5327,
      "step": 9000
    },
    {
      "epoch": 1.5034206574336726,
      "grad_norm": 8.820505142211914,
      "learning_rate": 3.1872017723244715e-05,
      "loss": 0.8004,
      "step": 9010
    },
    {
      "epoch": 1.5050892708159518,
      "grad_norm": 8.250025749206543,
      "learning_rate": 3.185071574642127e-05,
      "loss": 0.7308,
      "step": 9020
    },
    {
      "epoch": 1.5067578841982314,
      "grad_norm": 8.459835052490234,
      "learning_rate": 3.182941376959782e-05,
      "loss": 0.4927,
      "step": 9030
    },
    {
      "epoch": 1.5084264975805106,
      "grad_norm": 12.788475036621094,
      "learning_rate": 3.180811179277437e-05,
      "loss": 0.8262,
      "step": 9040
    },
    {
      "epoch": 1.5100951109627898,
      "grad_norm": 7.627364635467529,
      "learning_rate": 3.178680981595093e-05,
      "loss": 0.5389,
      "step": 9050
    },
    {
      "epoch": 1.5117637243450692,
      "grad_norm": 5.160128593444824,
      "learning_rate": 3.176550783912747e-05,
      "loss": 0.5113,
      "step": 9060
    },
    {
      "epoch": 1.5134323377273486,
      "grad_norm": 7.357578754425049,
      "learning_rate": 3.174420586230402e-05,
      "loss": 0.5582,
      "step": 9070
    },
    {
      "epoch": 1.5151009511096278,
      "grad_norm": 4.4765214920043945,
      "learning_rate": 3.1722903885480575e-05,
      "loss": 0.6229,
      "step": 9080
    },
    {
      "epoch": 1.5167695644919073,
      "grad_norm": 6.135571002960205,
      "learning_rate": 3.1701601908657126e-05,
      "loss": 0.5935,
      "step": 9090
    },
    {
      "epoch": 1.5184381778741867,
      "grad_norm": 8.773470878601074,
      "learning_rate": 3.168029993183367e-05,
      "loss": 0.5157,
      "step": 9100
    },
    {
      "epoch": 1.5201067912564659,
      "grad_norm": 7.115835666656494,
      "learning_rate": 3.165899795501023e-05,
      "loss": 0.5129,
      "step": 9110
    },
    {
      "epoch": 1.521775404638745,
      "grad_norm": 11.73090934753418,
      "learning_rate": 3.163769597818677e-05,
      "loss": 0.7512,
      "step": 9120
    },
    {
      "epoch": 1.5234440180210247,
      "grad_norm": 2.566441535949707,
      "learning_rate": 3.1616394001363325e-05,
      "loss": 0.4315,
      "step": 9130
    },
    {
      "epoch": 1.5251126314033039,
      "grad_norm": 18.555818557739258,
      "learning_rate": 3.159509202453988e-05,
      "loss": 0.7115,
      "step": 9140
    },
    {
      "epoch": 1.526781244785583,
      "grad_norm": 6.614068031311035,
      "learning_rate": 3.157379004771643e-05,
      "loss": 0.5848,
      "step": 9150
    },
    {
      "epoch": 1.5284498581678625,
      "grad_norm": 4.307902812957764,
      "learning_rate": 3.1552488070892985e-05,
      "loss": 0.5803,
      "step": 9160
    },
    {
      "epoch": 1.530118471550142,
      "grad_norm": 7.952247142791748,
      "learning_rate": 3.153118609406953e-05,
      "loss": 0.5801,
      "step": 9170
    },
    {
      "epoch": 1.531787084932421,
      "grad_norm": 8.330987930297852,
      "learning_rate": 3.150988411724608e-05,
      "loss": 0.6589,
      "step": 9180
    },
    {
      "epoch": 1.5334556983147005,
      "grad_norm": 12.13907241821289,
      "learning_rate": 3.148858214042263e-05,
      "loss": 0.7627,
      "step": 9190
    },
    {
      "epoch": 1.53512431169698,
      "grad_norm": 1.6922581195831299,
      "learning_rate": 3.1467280163599184e-05,
      "loss": 0.5483,
      "step": 9200
    },
    {
      "epoch": 1.5367929250792591,
      "grad_norm": 14.7852201461792,
      "learning_rate": 3.1445978186775735e-05,
      "loss": 0.698,
      "step": 9210
    },
    {
      "epoch": 1.5384615384615383,
      "grad_norm": 7.403207302093506,
      "learning_rate": 3.1424676209952287e-05,
      "loss": 0.598,
      "step": 9220
    },
    {
      "epoch": 1.5401301518438177,
      "grad_norm": 3.7980165481567383,
      "learning_rate": 3.140337423312884e-05,
      "loss": 0.877,
      "step": 9230
    },
    {
      "epoch": 1.5417987652260972,
      "grad_norm": 10.884892463684082,
      "learning_rate": 3.138207225630539e-05,
      "loss": 0.6204,
      "step": 9240
    },
    {
      "epoch": 1.5434673786083764,
      "grad_norm": 1.7849979400634766,
      "learning_rate": 3.136077027948194e-05,
      "loss": 0.5948,
      "step": 9250
    },
    {
      "epoch": 1.5451359919906558,
      "grad_norm": 2.6192729473114014,
      "learning_rate": 3.1339468302658485e-05,
      "loss": 0.4915,
      "step": 9260
    },
    {
      "epoch": 1.5468046053729352,
      "grad_norm": 6.416302680969238,
      "learning_rate": 3.131816632583504e-05,
      "loss": 0.7305,
      "step": 9270
    },
    {
      "epoch": 1.5484732187552144,
      "grad_norm": 6.573675632476807,
      "learning_rate": 3.129686434901159e-05,
      "loss": 0.4857,
      "step": 9280
    },
    {
      "epoch": 1.5501418321374938,
      "grad_norm": 7.614989280700684,
      "learning_rate": 3.127556237218814e-05,
      "loss": 0.7393,
      "step": 9290
    },
    {
      "epoch": 1.5518104455197732,
      "grad_norm": 7.870978832244873,
      "learning_rate": 3.125426039536469e-05,
      "loss": 0.4166,
      "step": 9300
    },
    {
      "epoch": 1.5534790589020524,
      "grad_norm": 8.271153450012207,
      "learning_rate": 3.123295841854124e-05,
      "loss": 0.5567,
      "step": 9310
    },
    {
      "epoch": 1.5551476722843316,
      "grad_norm": 6.844205379486084,
      "learning_rate": 3.121165644171779e-05,
      "loss": 0.4685,
      "step": 9320
    },
    {
      "epoch": 1.556816285666611,
      "grad_norm": 4.315903663635254,
      "learning_rate": 3.1190354464894344e-05,
      "loss": 0.5525,
      "step": 9330
    },
    {
      "epoch": 1.5584848990488904,
      "grad_norm": 14.129106521606445,
      "learning_rate": 3.1169052488070896e-05,
      "loss": 1.2183,
      "step": 9340
    },
    {
      "epoch": 1.5601535124311696,
      "grad_norm": 7.0326104164123535,
      "learning_rate": 3.114775051124745e-05,
      "loss": 0.7165,
      "step": 9350
    },
    {
      "epoch": 1.561822125813449,
      "grad_norm": 7.263661861419678,
      "learning_rate": 3.1126448534424e-05,
      "loss": 0.5109,
      "step": 9360
    },
    {
      "epoch": 1.5634907391957285,
      "grad_norm": 6.543117046356201,
      "learning_rate": 3.110514655760054e-05,
      "loss": 0.6376,
      "step": 9370
    },
    {
      "epoch": 1.5651593525780076,
      "grad_norm": 4.477919578552246,
      "learning_rate": 3.10838445807771e-05,
      "loss": 0.6468,
      "step": 9380
    },
    {
      "epoch": 1.5668279659602868,
      "grad_norm": 0.21305158734321594,
      "learning_rate": 3.1062542603953646e-05,
      "loss": 0.4269,
      "step": 9390
    },
    {
      "epoch": 1.5684965793425665,
      "grad_norm": 3.5977249145507812,
      "learning_rate": 3.10412406271302e-05,
      "loss": 0.5423,
      "step": 9400
    },
    {
      "epoch": 1.5701651927248457,
      "grad_norm": 3.0719103813171387,
      "learning_rate": 3.101993865030675e-05,
      "loss": 0.3765,
      "step": 9410
    },
    {
      "epoch": 1.5718338061071249,
      "grad_norm": 17.071300506591797,
      "learning_rate": 3.09986366734833e-05,
      "loss": 0.9107,
      "step": 9420
    },
    {
      "epoch": 1.5735024194894043,
      "grad_norm": 4.411855697631836,
      "learning_rate": 3.097733469665985e-05,
      "loss": 0.5751,
      "step": 9430
    },
    {
      "epoch": 1.5751710328716837,
      "grad_norm": 2.5973312854766846,
      "learning_rate": 3.09560327198364e-05,
      "loss": 0.3022,
      "step": 9440
    },
    {
      "epoch": 1.576839646253963,
      "grad_norm": 5.411478042602539,
      "learning_rate": 3.0934730743012954e-05,
      "loss": 0.5279,
      "step": 9450
    },
    {
      "epoch": 1.5785082596362423,
      "grad_norm": 16.947586059570312,
      "learning_rate": 3.0913428766189505e-05,
      "loss": 0.8612,
      "step": 9460
    },
    {
      "epoch": 1.5801768730185217,
      "grad_norm": 10.726524353027344,
      "learning_rate": 3.0892126789366056e-05,
      "loss": 0.7718,
      "step": 9470
    },
    {
      "epoch": 1.581845486400801,
      "grad_norm": 5.916514873504639,
      "learning_rate": 3.08708248125426e-05,
      "loss": 0.4057,
      "step": 9480
    },
    {
      "epoch": 1.58351409978308,
      "grad_norm": 9.402974128723145,
      "learning_rate": 3.084952283571916e-05,
      "loss": 0.792,
      "step": 9490
    },
    {
      "epoch": 1.5851827131653597,
      "grad_norm": 15.781068801879883,
      "learning_rate": 3.0828220858895703e-05,
      "loss": 0.4392,
      "step": 9500
    },
    {
      "epoch": 1.586851326547639,
      "grad_norm": 2.748289108276367,
      "learning_rate": 3.0806918882072255e-05,
      "loss": 0.6199,
      "step": 9510
    },
    {
      "epoch": 1.5885199399299181,
      "grad_norm": 9.747245788574219,
      "learning_rate": 3.078561690524881e-05,
      "loss": 0.9082,
      "step": 9520
    },
    {
      "epoch": 1.5901885533121976,
      "grad_norm": 5.962884426116943,
      "learning_rate": 3.076431492842536e-05,
      "loss": 0.6359,
      "step": 9530
    },
    {
      "epoch": 1.591857166694477,
      "grad_norm": 10.573715209960938,
      "learning_rate": 3.0743012951601915e-05,
      "loss": 0.7453,
      "step": 9540
    },
    {
      "epoch": 1.5935257800767562,
      "grad_norm": 4.3139328956604,
      "learning_rate": 3.072171097477846e-05,
      "loss": 0.5799,
      "step": 9550
    },
    {
      "epoch": 1.5951943934590356,
      "grad_norm": 5.432448387145996,
      "learning_rate": 3.070040899795501e-05,
      "loss": 0.3759,
      "step": 9560
    },
    {
      "epoch": 1.596863006841315,
      "grad_norm": 2.596191644668579,
      "learning_rate": 3.067910702113156e-05,
      "loss": 0.4705,
      "step": 9570
    },
    {
      "epoch": 1.5985316202235942,
      "grad_norm": 3.0379514694213867,
      "learning_rate": 3.0657805044308114e-05,
      "loss": 0.394,
      "step": 9580
    },
    {
      "epoch": 1.6002002336058734,
      "grad_norm": 3.1148767471313477,
      "learning_rate": 3.0636503067484665e-05,
      "loss": 0.5843,
      "step": 9590
    },
    {
      "epoch": 1.6018688469881528,
      "grad_norm": 6.787603855133057,
      "learning_rate": 3.061520109066122e-05,
      "loss": 0.309,
      "step": 9600
    },
    {
      "epoch": 1.6035374603704322,
      "grad_norm": 10.106114387512207,
      "learning_rate": 3.059389911383777e-05,
      "loss": 0.8139,
      "step": 9610
    },
    {
      "epoch": 1.6052060737527114,
      "grad_norm": 11.956754684448242,
      "learning_rate": 3.057259713701431e-05,
      "loss": 0.83,
      "step": 9620
    },
    {
      "epoch": 1.6068746871349908,
      "grad_norm": 3.7332239151000977,
      "learning_rate": 3.055129516019087e-05,
      "loss": 0.6229,
      "step": 9630
    },
    {
      "epoch": 1.6085433005172702,
      "grad_norm": 4.89230489730835,
      "learning_rate": 3.0529993183367415e-05,
      "loss": 0.6092,
      "step": 9640
    },
    {
      "epoch": 1.6102119138995494,
      "grad_norm": 6.4700188636779785,
      "learning_rate": 3.050869120654397e-05,
      "loss": 0.839,
      "step": 9650
    },
    {
      "epoch": 1.6118805272818288,
      "grad_norm": 8.622596740722656,
      "learning_rate": 3.048738922972052e-05,
      "loss": 0.5755,
      "step": 9660
    },
    {
      "epoch": 1.6135491406641083,
      "grad_norm": 3.716906785964966,
      "learning_rate": 3.046608725289707e-05,
      "loss": 0.4277,
      "step": 9670
    },
    {
      "epoch": 1.6152177540463875,
      "grad_norm": 8.185111045837402,
      "learning_rate": 3.0444785276073624e-05,
      "loss": 0.6075,
      "step": 9680
    },
    {
      "epoch": 1.6168863674286666,
      "grad_norm": 12.614288330078125,
      "learning_rate": 3.0423483299250172e-05,
      "loss": 0.4947,
      "step": 9690
    },
    {
      "epoch": 1.618554980810946,
      "grad_norm": 8.820324897766113,
      "learning_rate": 3.040218132242672e-05,
      "loss": 0.5528,
      "step": 9700
    },
    {
      "epoch": 1.6202235941932255,
      "grad_norm": 6.951489448547363,
      "learning_rate": 3.0380879345603275e-05,
      "loss": 0.547,
      "step": 9710
    },
    {
      "epoch": 1.6218922075755047,
      "grad_norm": 9.778212547302246,
      "learning_rate": 3.0359577368779822e-05,
      "loss": 0.7684,
      "step": 9720
    },
    {
      "epoch": 1.623560820957784,
      "grad_norm": 14.361490249633789,
      "learning_rate": 3.0338275391956374e-05,
      "loss": 0.5244,
      "step": 9730
    },
    {
      "epoch": 1.6252294343400635,
      "grad_norm": 11.928483009338379,
      "learning_rate": 3.0316973415132925e-05,
      "loss": 0.8927,
      "step": 9740
    },
    {
      "epoch": 1.6268980477223427,
      "grad_norm": 11.418351173400879,
      "learning_rate": 3.0295671438309476e-05,
      "loss": 0.349,
      "step": 9750
    },
    {
      "epoch": 1.628566661104622,
      "grad_norm": 9.28530216217041,
      "learning_rate": 3.027436946148603e-05,
      "loss": 0.4735,
      "step": 9760
    },
    {
      "epoch": 1.6302352744869015,
      "grad_norm": 9.326608657836914,
      "learning_rate": 3.025306748466258e-05,
      "loss": 0.6465,
      "step": 9770
    },
    {
      "epoch": 1.6319038878691807,
      "grad_norm": 10.606550216674805,
      "learning_rate": 3.0231765507839127e-05,
      "loss": 0.7158,
      "step": 9780
    },
    {
      "epoch": 1.63357250125146,
      "grad_norm": 8.48009967803955,
      "learning_rate": 3.0210463531015682e-05,
      "loss": 0.4541,
      "step": 9790
    },
    {
      "epoch": 1.6352411146337393,
      "grad_norm": 11.236242294311523,
      "learning_rate": 3.018916155419223e-05,
      "loss": 0.5019,
      "step": 9800
    },
    {
      "epoch": 1.6369097280160188,
      "grad_norm": 15.122669219970703,
      "learning_rate": 3.0167859577368778e-05,
      "loss": 0.5514,
      "step": 9810
    },
    {
      "epoch": 1.638578341398298,
      "grad_norm": 8.765739440917969,
      "learning_rate": 3.0146557600545332e-05,
      "loss": 0.3919,
      "step": 9820
    },
    {
      "epoch": 1.6402469547805774,
      "grad_norm": 6.456867218017578,
      "learning_rate": 3.012525562372188e-05,
      "loss": 0.6771,
      "step": 9830
    },
    {
      "epoch": 1.6419155681628568,
      "grad_norm": 5.471393585205078,
      "learning_rate": 3.0103953646898435e-05,
      "loss": 0.5843,
      "step": 9840
    },
    {
      "epoch": 1.643584181545136,
      "grad_norm": 5.630269527435303,
      "learning_rate": 3.0082651670074986e-05,
      "loss": 0.6505,
      "step": 9850
    },
    {
      "epoch": 1.6452527949274152,
      "grad_norm": 7.932456016540527,
      "learning_rate": 3.0061349693251534e-05,
      "loss": 0.613,
      "step": 9860
    },
    {
      "epoch": 1.6469214083096948,
      "grad_norm": 1.2512809038162231,
      "learning_rate": 3.004004771642809e-05,
      "loss": 0.5774,
      "step": 9870
    },
    {
      "epoch": 1.648590021691974,
      "grad_norm": 0.6271213293075562,
      "learning_rate": 3.0018745739604637e-05,
      "loss": 0.4986,
      "step": 9880
    },
    {
      "epoch": 1.6502586350742532,
      "grad_norm": 6.476260185241699,
      "learning_rate": 2.9997443762781185e-05,
      "loss": 0.8952,
      "step": 9890
    },
    {
      "epoch": 1.6519272484565326,
      "grad_norm": 4.948751449584961,
      "learning_rate": 2.997614178595774e-05,
      "loss": 0.5877,
      "step": 9900
    },
    {
      "epoch": 1.653595861838812,
      "grad_norm": 11.030789375305176,
      "learning_rate": 2.9954839809134288e-05,
      "loss": 0.5907,
      "step": 9910
    },
    {
      "epoch": 1.6552644752210912,
      "grad_norm": 2.6102917194366455,
      "learning_rate": 2.993353783231084e-05,
      "loss": 0.4656,
      "step": 9920
    },
    {
      "epoch": 1.6569330886033706,
      "grad_norm": 9.695969581604004,
      "learning_rate": 2.991223585548739e-05,
      "loss": 0.7925,
      "step": 9930
    },
    {
      "epoch": 1.65860170198565,
      "grad_norm": 9.71259593963623,
      "learning_rate": 2.989093387866394e-05,
      "loss": 0.7573,
      "step": 9940
    },
    {
      "epoch": 1.6602703153679292,
      "grad_norm": 8.891251564025879,
      "learning_rate": 2.9869631901840496e-05,
      "loss": 0.501,
      "step": 9950
    },
    {
      "epoch": 1.6619389287502084,
      "grad_norm": 1.0867644548416138,
      "learning_rate": 2.9848329925017044e-05,
      "loss": 0.3982,
      "step": 9960
    },
    {
      "epoch": 1.6636075421324878,
      "grad_norm": 10.17703628540039,
      "learning_rate": 2.9827027948193592e-05,
      "loss": 0.3404,
      "step": 9970
    },
    {
      "epoch": 1.6652761555147673,
      "grad_norm": 5.6265387535095215,
      "learning_rate": 2.9805725971370147e-05,
      "loss": 0.5374,
      "step": 9980
    },
    {
      "epoch": 1.6669447688970465,
      "grad_norm": 8.63183879852295,
      "learning_rate": 2.9784423994546695e-05,
      "loss": 0.5958,
      "step": 9990
    },
    {
      "epoch": 1.6686133822793259,
      "grad_norm": 2.908292531967163,
      "learning_rate": 2.9763122017723243e-05,
      "loss": 0.5456,
      "step": 10000
    },
    {
      "epoch": 1.6702819956616053,
      "grad_norm": 10.281736373901367,
      "learning_rate": 2.9741820040899797e-05,
      "loss": 0.4965,
      "step": 10010
    },
    {
      "epoch": 1.6719506090438845,
      "grad_norm": 4.522469997406006,
      "learning_rate": 2.9720518064076345e-05,
      "loss": 0.4061,
      "step": 10020
    },
    {
      "epoch": 1.673619222426164,
      "grad_norm": 5.333430290222168,
      "learning_rate": 2.9699216087252897e-05,
      "loss": 0.385,
      "step": 10030
    },
    {
      "epoch": 1.6752878358084433,
      "grad_norm": 7.758179187774658,
      "learning_rate": 2.967791411042945e-05,
      "loss": 0.8616,
      "step": 10040
    },
    {
      "epoch": 1.6769564491907225,
      "grad_norm": 2.8060100078582764,
      "learning_rate": 2.9656612133606e-05,
      "loss": 0.4814,
      "step": 10050
    },
    {
      "epoch": 1.6786250625730017,
      "grad_norm": 10.365041732788086,
      "learning_rate": 2.9635310156782554e-05,
      "loss": 0.5385,
      "step": 10060
    },
    {
      "epoch": 1.6802936759552811,
      "grad_norm": 10.830699920654297,
      "learning_rate": 2.9614008179959102e-05,
      "loss": 0.8568,
      "step": 10070
    },
    {
      "epoch": 1.6819622893375605,
      "grad_norm": 0.497829794883728,
      "learning_rate": 2.959270620313565e-05,
      "loss": 0.4842,
      "step": 10080
    },
    {
      "epoch": 1.6836309027198397,
      "grad_norm": 7.996775150299072,
      "learning_rate": 2.9571404226312205e-05,
      "loss": 0.6433,
      "step": 10090
    },
    {
      "epoch": 1.6852995161021191,
      "grad_norm": 5.698049068450928,
      "learning_rate": 2.9550102249488753e-05,
      "loss": 0.9541,
      "step": 10100
    },
    {
      "epoch": 1.6869681294843986,
      "grad_norm": 11.2415189743042,
      "learning_rate": 2.9528800272665304e-05,
      "loss": 0.4235,
      "step": 10110
    },
    {
      "epoch": 1.6886367428666778,
      "grad_norm": 11.176098823547363,
      "learning_rate": 2.9507498295841855e-05,
      "loss": 0.7133,
      "step": 10120
    },
    {
      "epoch": 1.6903053562489572,
      "grad_norm": 4.008163928985596,
      "learning_rate": 2.9486196319018407e-05,
      "loss": 0.4437,
      "step": 10130
    },
    {
      "epoch": 1.6919739696312366,
      "grad_norm": 0.15347211062908173,
      "learning_rate": 2.946489434219496e-05,
      "loss": 0.5495,
      "step": 10140
    },
    {
      "epoch": 1.6936425830135158,
      "grad_norm": 7.809494972229004,
      "learning_rate": 2.944359236537151e-05,
      "loss": 0.743,
      "step": 10150
    },
    {
      "epoch": 1.695311196395795,
      "grad_norm": 7.280704021453857,
      "learning_rate": 2.9422290388548057e-05,
      "loss": 0.6168,
      "step": 10160
    },
    {
      "epoch": 1.6969798097780744,
      "grad_norm": 1.6373629570007324,
      "learning_rate": 2.9400988411724612e-05,
      "loss": 0.6266,
      "step": 10170
    },
    {
      "epoch": 1.6986484231603538,
      "grad_norm": 4.534719467163086,
      "learning_rate": 2.937968643490116e-05,
      "loss": 0.5207,
      "step": 10180
    },
    {
      "epoch": 1.700317036542633,
      "grad_norm": 13.129566192626953,
      "learning_rate": 2.9358384458077708e-05,
      "loss": 0.7775,
      "step": 10190
    },
    {
      "epoch": 1.7019856499249124,
      "grad_norm": 6.208026885986328,
      "learning_rate": 2.9337082481254263e-05,
      "loss": 0.4542,
      "step": 10200
    },
    {
      "epoch": 1.7036542633071918,
      "grad_norm": 8.047687530517578,
      "learning_rate": 2.931578050443081e-05,
      "loss": 1.0197,
      "step": 10210
    },
    {
      "epoch": 1.705322876689471,
      "grad_norm": 3.058518409729004,
      "learning_rate": 2.9294478527607362e-05,
      "loss": 0.5823,
      "step": 10220
    },
    {
      "epoch": 1.7069914900717502,
      "grad_norm": 5.460329532623291,
      "learning_rate": 2.9273176550783917e-05,
      "loss": 0.4105,
      "step": 10230
    },
    {
      "epoch": 1.7086601034540299,
      "grad_norm": 9.102624893188477,
      "learning_rate": 2.9251874573960464e-05,
      "loss": 0.5022,
      "step": 10240
    },
    {
      "epoch": 1.710328716836309,
      "grad_norm": 6.483457565307617,
      "learning_rate": 2.923057259713702e-05,
      "loss": 0.4685,
      "step": 10250
    },
    {
      "epoch": 1.7119973302185882,
      "grad_norm": 3.486149549484253,
      "learning_rate": 2.9209270620313567e-05,
      "loss": 0.4579,
      "step": 10260
    },
    {
      "epoch": 1.7136659436008677,
      "grad_norm": 7.998251438140869,
      "learning_rate": 2.9187968643490115e-05,
      "loss": 0.6886,
      "step": 10270
    },
    {
      "epoch": 1.715334556983147,
      "grad_norm": 0.18450714647769928,
      "learning_rate": 2.916666666666667e-05,
      "loss": 0.4903,
      "step": 10280
    },
    {
      "epoch": 1.7170031703654263,
      "grad_norm": 6.745783805847168,
      "learning_rate": 2.9145364689843218e-05,
      "loss": 0.5178,
      "step": 10290
    },
    {
      "epoch": 1.7186717837477057,
      "grad_norm": 9.882035255432129,
      "learning_rate": 2.912406271301977e-05,
      "loss": 0.5795,
      "step": 10300
    },
    {
      "epoch": 1.720340397129985,
      "grad_norm": 1.8027377128601074,
      "learning_rate": 2.910276073619632e-05,
      "loss": 0.7671,
      "step": 10310
    },
    {
      "epoch": 1.7220090105122643,
      "grad_norm": 5.638763427734375,
      "learning_rate": 2.908145875937287e-05,
      "loss": 0.7214,
      "step": 10320
    },
    {
      "epoch": 1.7236776238945435,
      "grad_norm": 2.022662401199341,
      "learning_rate": 2.9060156782549426e-05,
      "loss": 0.8863,
      "step": 10330
    },
    {
      "epoch": 1.7253462372768231,
      "grad_norm": 3.5070858001708984,
      "learning_rate": 2.9038854805725974e-05,
      "loss": 0.756,
      "step": 10340
    },
    {
      "epoch": 1.7270148506591023,
      "grad_norm": 8.975205421447754,
      "learning_rate": 2.9017552828902522e-05,
      "loss": 0.5931,
      "step": 10350
    },
    {
      "epoch": 1.7286834640413815,
      "grad_norm": 19.723791122436523,
      "learning_rate": 2.8996250852079077e-05,
      "loss": 0.6128,
      "step": 10360
    },
    {
      "epoch": 1.730352077423661,
      "grad_norm": 5.739619731903076,
      "learning_rate": 2.8974948875255625e-05,
      "loss": 0.4792,
      "step": 10370
    },
    {
      "epoch": 1.7320206908059403,
      "grad_norm": 2.5797433853149414,
      "learning_rate": 2.8953646898432173e-05,
      "loss": 0.6405,
      "step": 10380
    },
    {
      "epoch": 1.7336893041882195,
      "grad_norm": 4.863265514373779,
      "learning_rate": 2.8932344921608728e-05,
      "loss": 0.9447,
      "step": 10390
    },
    {
      "epoch": 1.735357917570499,
      "grad_norm": 7.469916820526123,
      "learning_rate": 2.8911042944785276e-05,
      "loss": 0.6589,
      "step": 10400
    },
    {
      "epoch": 1.7370265309527784,
      "grad_norm": 10.135943412780762,
      "learning_rate": 2.8889740967961827e-05,
      "loss": 0.7335,
      "step": 10410
    },
    {
      "epoch": 1.7386951443350576,
      "grad_norm": 7.15086030960083,
      "learning_rate": 2.886843899113838e-05,
      "loss": 0.5317,
      "step": 10420
    },
    {
      "epoch": 1.7403637577173368,
      "grad_norm": 12.565216064453125,
      "learning_rate": 2.884713701431493e-05,
      "loss": 0.6201,
      "step": 10430
    },
    {
      "epoch": 1.7420323710996162,
      "grad_norm": 1.3789668083190918,
      "learning_rate": 2.8825835037491484e-05,
      "loss": 0.5265,
      "step": 10440
    },
    {
      "epoch": 1.7437009844818956,
      "grad_norm": 7.048779487609863,
      "learning_rate": 2.8804533060668032e-05,
      "loss": 0.6096,
      "step": 10450
    },
    {
      "epoch": 1.7453695978641748,
      "grad_norm": 5.979588985443115,
      "learning_rate": 2.878323108384458e-05,
      "loss": 0.7076,
      "step": 10460
    },
    {
      "epoch": 1.7470382112464542,
      "grad_norm": 7.404950141906738,
      "learning_rate": 2.8761929107021135e-05,
      "loss": 0.5136,
      "step": 10470
    },
    {
      "epoch": 1.7487068246287336,
      "grad_norm": 6.117426872253418,
      "learning_rate": 2.8740627130197683e-05,
      "loss": 0.4167,
      "step": 10480
    },
    {
      "epoch": 1.7503754380110128,
      "grad_norm": 6.382055759429932,
      "learning_rate": 2.8719325153374234e-05,
      "loss": 0.6094,
      "step": 10490
    },
    {
      "epoch": 1.7520440513932922,
      "grad_norm": 11.04975700378418,
      "learning_rate": 2.8698023176550785e-05,
      "loss": 0.557,
      "step": 10500
    },
    {
      "epoch": 1.7537126647755716,
      "grad_norm": 4.27220344543457,
      "learning_rate": 2.8676721199727337e-05,
      "loss": 0.5602,
      "step": 10510
    },
    {
      "epoch": 1.7553812781578508,
      "grad_norm": 9.478187561035156,
      "learning_rate": 2.8655419222903885e-05,
      "loss": 0.597,
      "step": 10520
    },
    {
      "epoch": 1.75704989154013,
      "grad_norm": 2.402026891708374,
      "learning_rate": 2.863411724608044e-05,
      "loss": 0.5139,
      "step": 10530
    },
    {
      "epoch": 1.7587185049224094,
      "grad_norm": 3.392568588256836,
      "learning_rate": 2.8612815269256987e-05,
      "loss": 0.8615,
      "step": 10540
    },
    {
      "epoch": 1.7603871183046889,
      "grad_norm": 5.688475608825684,
      "learning_rate": 2.8591513292433542e-05,
      "loss": 0.4132,
      "step": 10550
    },
    {
      "epoch": 1.762055731686968,
      "grad_norm": 16.1141414642334,
      "learning_rate": 2.857021131561009e-05,
      "loss": 0.6301,
      "step": 10560
    },
    {
      "epoch": 1.7637243450692475,
      "grad_norm": 5.404414653778076,
      "learning_rate": 2.8548909338786638e-05,
      "loss": 0.5989,
      "step": 10570
    },
    {
      "epoch": 1.7653929584515269,
      "grad_norm": 16.43512725830078,
      "learning_rate": 2.8527607361963193e-05,
      "loss": 0.5801,
      "step": 10580
    },
    {
      "epoch": 1.767061571833806,
      "grad_norm": 12.788108825683594,
      "learning_rate": 2.850630538513974e-05,
      "loss": 0.5528,
      "step": 10590
    },
    {
      "epoch": 1.7687301852160853,
      "grad_norm": 2.4457576274871826,
      "learning_rate": 2.8485003408316292e-05,
      "loss": 0.7582,
      "step": 10600
    },
    {
      "epoch": 1.770398798598365,
      "grad_norm": 11.660794258117676,
      "learning_rate": 2.8463701431492847e-05,
      "loss": 0.7675,
      "step": 10610
    },
    {
      "epoch": 1.772067411980644,
      "grad_norm": 4.917978286743164,
      "learning_rate": 2.8442399454669395e-05,
      "loss": 0.5562,
      "step": 10620
    },
    {
      "epoch": 1.7737360253629233,
      "grad_norm": 4.057318687438965,
      "learning_rate": 2.842109747784595e-05,
      "loss": 1.0563,
      "step": 10630
    },
    {
      "epoch": 1.7754046387452027,
      "grad_norm": 1.6464701890945435,
      "learning_rate": 2.8399795501022497e-05,
      "loss": 0.6572,
      "step": 10640
    },
    {
      "epoch": 1.7770732521274821,
      "grad_norm": 6.413885116577148,
      "learning_rate": 2.8378493524199045e-05,
      "loss": 0.3438,
      "step": 10650
    },
    {
      "epoch": 1.7787418655097613,
      "grad_norm": 9.243247985839844,
      "learning_rate": 2.83571915473756e-05,
      "loss": 0.4877,
      "step": 10660
    },
    {
      "epoch": 1.7804104788920407,
      "grad_norm": 11.49670124053955,
      "learning_rate": 2.8335889570552148e-05,
      "loss": 0.7866,
      "step": 10670
    },
    {
      "epoch": 1.7820790922743202,
      "grad_norm": 12.247901916503906,
      "learning_rate": 2.83145875937287e-05,
      "loss": 0.6279,
      "step": 10680
    },
    {
      "epoch": 1.7837477056565993,
      "grad_norm": 4.4054856300354,
      "learning_rate": 2.829328561690525e-05,
      "loss": 0.5693,
      "step": 10690
    },
    {
      "epoch": 1.7854163190388785,
      "grad_norm": 8.280196189880371,
      "learning_rate": 2.8271983640081802e-05,
      "loss": 0.6179,
      "step": 10700
    },
    {
      "epoch": 1.7870849324211582,
      "grad_norm": 0.7711713314056396,
      "learning_rate": 2.825068166325835e-05,
      "loss": 0.4229,
      "step": 10710
    },
    {
      "epoch": 1.7887535458034374,
      "grad_norm": 9.81059741973877,
      "learning_rate": 2.8229379686434904e-05,
      "loss": 0.6915,
      "step": 10720
    },
    {
      "epoch": 1.7904221591857166,
      "grad_norm": 2.133312225341797,
      "learning_rate": 2.8208077709611452e-05,
      "loss": 0.5644,
      "step": 10730
    },
    {
      "epoch": 1.792090772567996,
      "grad_norm": 5.2942914962768555,
      "learning_rate": 2.8186775732788007e-05,
      "loss": 0.5572,
      "step": 10740
    },
    {
      "epoch": 1.7937593859502754,
      "grad_norm": 15.872344017028809,
      "learning_rate": 2.8165473755964555e-05,
      "loss": 0.7198,
      "step": 10750
    },
    {
      "epoch": 1.7954279993325546,
      "grad_norm": 3.902937650680542,
      "learning_rate": 2.8144171779141103e-05,
      "loss": 0.6971,
      "step": 10760
    },
    {
      "epoch": 1.797096612714834,
      "grad_norm": 5.449068069458008,
      "learning_rate": 2.8122869802317658e-05,
      "loss": 0.4433,
      "step": 10770
    },
    {
      "epoch": 1.7987652260971134,
      "grad_norm": 6.02102518081665,
      "learning_rate": 2.8101567825494206e-05,
      "loss": 0.7242,
      "step": 10780
    },
    {
      "epoch": 1.8004338394793926,
      "grad_norm": 10.914822578430176,
      "learning_rate": 2.8080265848670757e-05,
      "loss": 0.656,
      "step": 10790
    },
    {
      "epoch": 1.8021024528616718,
      "grad_norm": 0.9265561103820801,
      "learning_rate": 2.8058963871847312e-05,
      "loss": 0.5643,
      "step": 10800
    },
    {
      "epoch": 1.8037710662439512,
      "grad_norm": 3.290001153945923,
      "learning_rate": 2.803766189502386e-05,
      "loss": 0.4933,
      "step": 10810
    },
    {
      "epoch": 1.8054396796262306,
      "grad_norm": 5.390014171600342,
      "learning_rate": 2.8016359918200408e-05,
      "loss": 0.6248,
      "step": 10820
    },
    {
      "epoch": 1.8071082930085098,
      "grad_norm": 7.025228977203369,
      "learning_rate": 2.7995057941376962e-05,
      "loss": 0.7171,
      "step": 10830
    },
    {
      "epoch": 1.8087769063907893,
      "grad_norm": 1.5035008192062378,
      "learning_rate": 2.797375596455351e-05,
      "loss": 0.3695,
      "step": 10840
    },
    {
      "epoch": 1.8104455197730687,
      "grad_norm": 11.008944511413574,
      "learning_rate": 2.7952453987730065e-05,
      "loss": 0.794,
      "step": 10850
    },
    {
      "epoch": 1.8121141331553479,
      "grad_norm": 4.8571391105651855,
      "learning_rate": 2.7931152010906613e-05,
      "loss": 0.5603,
      "step": 10860
    },
    {
      "epoch": 1.8137827465376273,
      "grad_norm": 1.3382558822631836,
      "learning_rate": 2.7909850034083164e-05,
      "loss": 0.4259,
      "step": 10870
    },
    {
      "epoch": 1.8154513599199067,
      "grad_norm": 2.4287784099578857,
      "learning_rate": 2.7888548057259716e-05,
      "loss": 0.4055,
      "step": 10880
    },
    {
      "epoch": 1.8171199733021859,
      "grad_norm": 12.000529289245605,
      "learning_rate": 2.7867246080436267e-05,
      "loss": 0.7297,
      "step": 10890
    },
    {
      "epoch": 1.818788586684465,
      "grad_norm": 7.276248931884766,
      "learning_rate": 2.7845944103612815e-05,
      "loss": 0.5478,
      "step": 10900
    },
    {
      "epoch": 1.8204572000667445,
      "grad_norm": 12.071920394897461,
      "learning_rate": 2.782464212678937e-05,
      "loss": 0.4825,
      "step": 10910
    },
    {
      "epoch": 1.822125813449024,
      "grad_norm": 8.043177604675293,
      "learning_rate": 2.7803340149965918e-05,
      "loss": 0.9512,
      "step": 10920
    },
    {
      "epoch": 1.823794426831303,
      "grad_norm": 6.475778102874756,
      "learning_rate": 2.7782038173142472e-05,
      "loss": 0.5424,
      "step": 10930
    },
    {
      "epoch": 1.8254630402135825,
      "grad_norm": 0.583655059337616,
      "learning_rate": 2.776073619631902e-05,
      "loss": 0.4948,
      "step": 10940
    },
    {
      "epoch": 1.827131653595862,
      "grad_norm": 8.299691200256348,
      "learning_rate": 2.7739434219495568e-05,
      "loss": 0.638,
      "step": 10950
    },
    {
      "epoch": 1.8288002669781411,
      "grad_norm": 9.447907447814941,
      "learning_rate": 2.7718132242672123e-05,
      "loss": 0.5373,
      "step": 10960
    },
    {
      "epoch": 1.8304688803604203,
      "grad_norm": 12.956201553344727,
      "learning_rate": 2.769683026584867e-05,
      "loss": 0.639,
      "step": 10970
    },
    {
      "epoch": 1.8321374937427,
      "grad_norm": 6.628962516784668,
      "learning_rate": 2.7675528289025222e-05,
      "loss": 0.546,
      "step": 10980
    },
    {
      "epoch": 1.8338061071249792,
      "grad_norm": 1.1513140201568604,
      "learning_rate": 2.7654226312201777e-05,
      "loss": 0.5534,
      "step": 10990
    },
    {
      "epoch": 1.8354747205072584,
      "grad_norm": 5.592545509338379,
      "learning_rate": 2.7632924335378325e-05,
      "loss": 0.6686,
      "step": 11000
    },
    {
      "epoch": 1.8371433338895378,
      "grad_norm": 11.68380355834961,
      "learning_rate": 2.7611622358554873e-05,
      "loss": 0.7813,
      "step": 11010
    },
    {
      "epoch": 1.8388119472718172,
      "grad_norm": 5.098140239715576,
      "learning_rate": 2.7590320381731427e-05,
      "loss": 0.5239,
      "step": 11020
    },
    {
      "epoch": 1.8404805606540964,
      "grad_norm": 8.583306312561035,
      "learning_rate": 2.7569018404907975e-05,
      "loss": 0.6193,
      "step": 11030
    },
    {
      "epoch": 1.8421491740363758,
      "grad_norm": 13.979496955871582,
      "learning_rate": 2.754771642808453e-05,
      "loss": 0.7214,
      "step": 11040
    },
    {
      "epoch": 1.8438177874186552,
      "grad_norm": 9.43800163269043,
      "learning_rate": 2.7526414451261078e-05,
      "loss": 0.8549,
      "step": 11050
    },
    {
      "epoch": 1.8454864008009344,
      "grad_norm": 12.993525505065918,
      "learning_rate": 2.7505112474437626e-05,
      "loss": 0.4506,
      "step": 11060
    },
    {
      "epoch": 1.8471550141832136,
      "grad_norm": 1.012054443359375,
      "learning_rate": 2.748381049761418e-05,
      "loss": 0.3806,
      "step": 11070
    },
    {
      "epoch": 1.8488236275654932,
      "grad_norm": 7.780065536499023,
      "learning_rate": 2.7462508520790732e-05,
      "loss": 0.5633,
      "step": 11080
    },
    {
      "epoch": 1.8504922409477724,
      "grad_norm": 0.7357267737388611,
      "learning_rate": 2.744120654396728e-05,
      "loss": 0.4206,
      "step": 11090
    },
    {
      "epoch": 1.8521608543300516,
      "grad_norm": 3.0379295349121094,
      "learning_rate": 2.7419904567143835e-05,
      "loss": 0.5442,
      "step": 11100
    },
    {
      "epoch": 1.853829467712331,
      "grad_norm": 13.328064918518066,
      "learning_rate": 2.7398602590320383e-05,
      "loss": 0.617,
      "step": 11110
    },
    {
      "epoch": 1.8554980810946105,
      "grad_norm": 1.8596233129501343,
      "learning_rate": 2.737730061349693e-05,
      "loss": 0.4341,
      "step": 11120
    },
    {
      "epoch": 1.8571666944768896,
      "grad_norm": 7.64577054977417,
      "learning_rate": 2.7355998636673485e-05,
      "loss": 0.6768,
      "step": 11130
    },
    {
      "epoch": 1.858835307859169,
      "grad_norm": 12.400761604309082,
      "learning_rate": 2.7334696659850033e-05,
      "loss": 0.5877,
      "step": 11140
    },
    {
      "epoch": 1.8605039212414485,
      "grad_norm": 8.731733322143555,
      "learning_rate": 2.7313394683026588e-05,
      "loss": 0.4189,
      "step": 11150
    },
    {
      "epoch": 1.8621725346237277,
      "grad_norm": 3.8799290657043457,
      "learning_rate": 2.7292092706203136e-05,
      "loss": 0.6235,
      "step": 11160
    },
    {
      "epoch": 1.8638411480060069,
      "grad_norm": 8.26113224029541,
      "learning_rate": 2.7270790729379687e-05,
      "loss": 0.5904,
      "step": 11170
    },
    {
      "epoch": 1.8655097613882863,
      "grad_norm": 6.936022758483887,
      "learning_rate": 2.7249488752556242e-05,
      "loss": 0.3286,
      "step": 11180
    },
    {
      "epoch": 1.8671783747705657,
      "grad_norm": 3.855494976043701,
      "learning_rate": 2.722818677573279e-05,
      "loss": 0.3914,
      "step": 11190
    },
    {
      "epoch": 1.868846988152845,
      "grad_norm": 9.96683120727539,
      "learning_rate": 2.7206884798909338e-05,
      "loss": 0.6344,
      "step": 11200
    },
    {
      "epoch": 1.8705156015351243,
      "grad_norm": 11.302999496459961,
      "learning_rate": 2.7185582822085892e-05,
      "loss": 0.557,
      "step": 11210
    },
    {
      "epoch": 1.8721842149174037,
      "grad_norm": 11.98402214050293,
      "learning_rate": 2.716428084526244e-05,
      "loss": 0.5497,
      "step": 11220
    },
    {
      "epoch": 1.873852828299683,
      "grad_norm": 6.015153884887695,
      "learning_rate": 2.7142978868438995e-05,
      "loss": 0.8403,
      "step": 11230
    },
    {
      "epoch": 1.8755214416819623,
      "grad_norm": 7.967526435852051,
      "learning_rate": 2.7121676891615543e-05,
      "loss": 0.6237,
      "step": 11240
    },
    {
      "epoch": 1.8771900550642417,
      "grad_norm": 7.257206439971924,
      "learning_rate": 2.710037491479209e-05,
      "loss": 0.7807,
      "step": 11250
    },
    {
      "epoch": 1.878858668446521,
      "grad_norm": 13.089550971984863,
      "learning_rate": 2.7079072937968646e-05,
      "loss": 0.4347,
      "step": 11260
    },
    {
      "epoch": 1.8805272818288001,
      "grad_norm": 4.673247814178467,
      "learning_rate": 2.7057770961145197e-05,
      "loss": 0.7334,
      "step": 11270
    },
    {
      "epoch": 1.8821958952110796,
      "grad_norm": 5.522902011871338,
      "learning_rate": 2.7036468984321745e-05,
      "loss": 0.5391,
      "step": 11280
    },
    {
      "epoch": 1.883864508593359,
      "grad_norm": 6.904984951019287,
      "learning_rate": 2.70151670074983e-05,
      "loss": 0.582,
      "step": 11290
    },
    {
      "epoch": 1.8855331219756382,
      "grad_norm": 8.806720733642578,
      "learning_rate": 2.6993865030674848e-05,
      "loss": 0.7992,
      "step": 11300
    },
    {
      "epoch": 1.8872017353579176,
      "grad_norm": 6.741366386413574,
      "learning_rate": 2.6972563053851396e-05,
      "loss": 0.6333,
      "step": 11310
    },
    {
      "epoch": 1.888870348740197,
      "grad_norm": 4.7396135330200195,
      "learning_rate": 2.695126107702795e-05,
      "loss": 0.5861,
      "step": 11320
    },
    {
      "epoch": 1.8905389621224762,
      "grad_norm": 6.248992443084717,
      "learning_rate": 2.6929959100204498e-05,
      "loss": 0.9482,
      "step": 11330
    },
    {
      "epoch": 1.8922075755047556,
      "grad_norm": 7.570301055908203,
      "learning_rate": 2.6908657123381053e-05,
      "loss": 0.5705,
      "step": 11340
    },
    {
      "epoch": 1.893876188887035,
      "grad_norm": 8.017674446105957,
      "learning_rate": 2.68873551465576e-05,
      "loss": 0.4768,
      "step": 11350
    },
    {
      "epoch": 1.8955448022693142,
      "grad_norm": 13.230324745178223,
      "learning_rate": 2.6866053169734152e-05,
      "loss": 0.4903,
      "step": 11360
    },
    {
      "epoch": 1.8972134156515934,
      "grad_norm": 7.2906646728515625,
      "learning_rate": 2.6844751192910707e-05,
      "loss": 0.7789,
      "step": 11370
    },
    {
      "epoch": 1.8988820290338728,
      "grad_norm": 7.871485710144043,
      "learning_rate": 2.6823449216087255e-05,
      "loss": 0.7516,
      "step": 11380
    },
    {
      "epoch": 1.9005506424161522,
      "grad_norm": 1.193405270576477,
      "learning_rate": 2.6802147239263803e-05,
      "loss": 0.5009,
      "step": 11390
    },
    {
      "epoch": 1.9022192557984314,
      "grad_norm": 3.2832398414611816,
      "learning_rate": 2.6780845262440358e-05,
      "loss": 0.5387,
      "step": 11400
    },
    {
      "epoch": 1.9038878691807108,
      "grad_norm": 5.397151470184326,
      "learning_rate": 2.6759543285616906e-05,
      "loss": 0.5071,
      "step": 11410
    },
    {
      "epoch": 1.9055564825629903,
      "grad_norm": 3.213564872741699,
      "learning_rate": 2.6738241308793453e-05,
      "loss": 0.7177,
      "step": 11420
    },
    {
      "epoch": 1.9072250959452695,
      "grad_norm": 8.378090858459473,
      "learning_rate": 2.6716939331970008e-05,
      "loss": 0.6934,
      "step": 11430
    },
    {
      "epoch": 1.9088937093275486,
      "grad_norm": 4.285092830657959,
      "learning_rate": 2.6695637355146556e-05,
      "loss": 0.5246,
      "step": 11440
    },
    {
      "epoch": 1.9105623227098283,
      "grad_norm": 9.331666946411133,
      "learning_rate": 2.667433537832311e-05,
      "loss": 0.6731,
      "step": 11450
    },
    {
      "epoch": 1.9122309360921075,
      "grad_norm": 5.7757487297058105,
      "learning_rate": 2.6653033401499662e-05,
      "loss": 0.5239,
      "step": 11460
    },
    {
      "epoch": 1.9138995494743867,
      "grad_norm": 5.6742844581604,
      "learning_rate": 2.663173142467621e-05,
      "loss": 0.5067,
      "step": 11470
    },
    {
      "epoch": 1.915568162856666,
      "grad_norm": 2.8064770698547363,
      "learning_rate": 2.6610429447852765e-05,
      "loss": 0.3704,
      "step": 11480
    },
    {
      "epoch": 1.9172367762389455,
      "grad_norm": 7.095961093902588,
      "learning_rate": 2.6589127471029313e-05,
      "loss": 0.5854,
      "step": 11490
    },
    {
      "epoch": 1.9189053896212247,
      "grad_norm": 2.2089908123016357,
      "learning_rate": 2.656782549420586e-05,
      "loss": 0.3977,
      "step": 11500
    },
    {
      "epoch": 1.9205740030035041,
      "grad_norm": 9.049036979675293,
      "learning_rate": 2.6546523517382415e-05,
      "loss": 0.8894,
      "step": 11510
    },
    {
      "epoch": 1.9222426163857835,
      "grad_norm": 4.692685127258301,
      "learning_rate": 2.6525221540558963e-05,
      "loss": 0.7355,
      "step": 11520
    },
    {
      "epoch": 1.9239112297680627,
      "grad_norm": 10.06353759765625,
      "learning_rate": 2.6503919563735518e-05,
      "loss": 0.551,
      "step": 11530
    },
    {
      "epoch": 1.925579843150342,
      "grad_norm": 4.884902477264404,
      "learning_rate": 2.6482617586912066e-05,
      "loss": 0.6458,
      "step": 11540
    },
    {
      "epoch": 1.9272484565326216,
      "grad_norm": 1.1838490962982178,
      "learning_rate": 2.6461315610088617e-05,
      "loss": 0.4901,
      "step": 11550
    },
    {
      "epoch": 1.9289170699149008,
      "grad_norm": 2.0917370319366455,
      "learning_rate": 2.6440013633265172e-05,
      "loss": 0.3675,
      "step": 11560
    },
    {
      "epoch": 1.93058568329718,
      "grad_norm": 7.127318382263184,
      "learning_rate": 2.641871165644172e-05,
      "loss": 0.6606,
      "step": 11570
    },
    {
      "epoch": 1.9322542966794594,
      "grad_norm": 13.944512367248535,
      "learning_rate": 2.6397409679618268e-05,
      "loss": 0.6883,
      "step": 11580
    },
    {
      "epoch": 1.9339229100617388,
      "grad_norm": 1.783181071281433,
      "learning_rate": 2.6376107702794823e-05,
      "loss": 0.7656,
      "step": 11590
    },
    {
      "epoch": 1.935591523444018,
      "grad_norm": 11.182962417602539,
      "learning_rate": 2.635480572597137e-05,
      "loss": 0.6354,
      "step": 11600
    },
    {
      "epoch": 1.9372601368262974,
      "grad_norm": 13.978933334350586,
      "learning_rate": 2.633350374914792e-05,
      "loss": 0.7624,
      "step": 11610
    },
    {
      "epoch": 1.9389287502085768,
      "grad_norm": 6.12726354598999,
      "learning_rate": 2.6312201772324473e-05,
      "loss": 0.6037,
      "step": 11620
    },
    {
      "epoch": 1.940597363590856,
      "grad_norm": 1.869924783706665,
      "learning_rate": 2.629089979550102e-05,
      "loss": 0.5417,
      "step": 11630
    },
    {
      "epoch": 1.9422659769731352,
      "grad_norm": 5.226812839508057,
      "learning_rate": 2.6269597818677576e-05,
      "loss": 0.5286,
      "step": 11640
    },
    {
      "epoch": 1.9439345903554146,
      "grad_norm": 5.104292869567871,
      "learning_rate": 2.6248295841854127e-05,
      "loss": 0.6587,
      "step": 11650
    },
    {
      "epoch": 1.945603203737694,
      "grad_norm": 3.0732779502868652,
      "learning_rate": 2.6226993865030675e-05,
      "loss": 0.6088,
      "step": 11660
    },
    {
      "epoch": 1.9472718171199732,
      "grad_norm": 8.703959465026855,
      "learning_rate": 2.620569188820723e-05,
      "loss": 0.3745,
      "step": 11670
    },
    {
      "epoch": 1.9489404305022526,
      "grad_norm": 10.600788116455078,
      "learning_rate": 2.6184389911383778e-05,
      "loss": 0.7441,
      "step": 11680
    },
    {
      "epoch": 1.950609043884532,
      "grad_norm": 1.4410812854766846,
      "learning_rate": 2.6163087934560326e-05,
      "loss": 0.4604,
      "step": 11690
    },
    {
      "epoch": 1.9522776572668112,
      "grad_norm": 2.3584721088409424,
      "learning_rate": 2.614178595773688e-05,
      "loss": 0.4113,
      "step": 11700
    },
    {
      "epoch": 1.9539462706490907,
      "grad_norm": 7.9622802734375,
      "learning_rate": 2.612048398091343e-05,
      "loss": 0.509,
      "step": 11710
    },
    {
      "epoch": 1.95561488403137,
      "grad_norm": 9.836729049682617,
      "learning_rate": 2.609918200408998e-05,
      "loss": 0.4537,
      "step": 11720
    },
    {
      "epoch": 1.9572834974136493,
      "grad_norm": 14.751102447509766,
      "learning_rate": 2.607788002726653e-05,
      "loss": 0.7532,
      "step": 11730
    },
    {
      "epoch": 1.9589521107959285,
      "grad_norm": 8.3240385055542,
      "learning_rate": 2.6056578050443082e-05,
      "loss": 0.7209,
      "step": 11740
    },
    {
      "epoch": 1.9606207241782079,
      "grad_norm": 2.2937276363372803,
      "learning_rate": 2.6035276073619637e-05,
      "loss": 0.2982,
      "step": 11750
    },
    {
      "epoch": 1.9622893375604873,
      "grad_norm": 8.641708374023438,
      "learning_rate": 2.6013974096796185e-05,
      "loss": 0.6745,
      "step": 11760
    },
    {
      "epoch": 1.9639579509427665,
      "grad_norm": 5.8348283767700195,
      "learning_rate": 2.5992672119972733e-05,
      "loss": 0.4207,
      "step": 11770
    },
    {
      "epoch": 1.965626564325046,
      "grad_norm": 2.7787039279937744,
      "learning_rate": 2.5971370143149288e-05,
      "loss": 0.5732,
      "step": 11780
    },
    {
      "epoch": 1.9672951777073253,
      "grad_norm": 8.676706314086914,
      "learning_rate": 2.5950068166325836e-05,
      "loss": 0.5311,
      "step": 11790
    },
    {
      "epoch": 1.9689637910896045,
      "grad_norm": 3.2985408306121826,
      "learning_rate": 2.5928766189502384e-05,
      "loss": 0.3672,
      "step": 11800
    },
    {
      "epoch": 1.9706324044718837,
      "grad_norm": 7.060450077056885,
      "learning_rate": 2.590746421267894e-05,
      "loss": 0.5111,
      "step": 11810
    },
    {
      "epoch": 1.9723010178541633,
      "grad_norm": 8.342052459716797,
      "learning_rate": 2.5886162235855486e-05,
      "loss": 0.9109,
      "step": 11820
    },
    {
      "epoch": 1.9739696312364425,
      "grad_norm": 3.1965436935424805,
      "learning_rate": 2.586486025903204e-05,
      "loss": 0.3055,
      "step": 11830
    },
    {
      "epoch": 1.9756382446187217,
      "grad_norm": 2.4976742267608643,
      "learning_rate": 2.5843558282208592e-05,
      "loss": 0.414,
      "step": 11840
    },
    {
      "epoch": 1.9773068580010011,
      "grad_norm": 7.932590961456299,
      "learning_rate": 2.582225630538514e-05,
      "loss": 0.8706,
      "step": 11850
    },
    {
      "epoch": 1.9789754713832806,
      "grad_norm": 9.706235885620117,
      "learning_rate": 2.5800954328561695e-05,
      "loss": 0.5863,
      "step": 11860
    },
    {
      "epoch": 1.9806440847655598,
      "grad_norm": 12.90772819519043,
      "learning_rate": 2.5779652351738243e-05,
      "loss": 0.4256,
      "step": 11870
    },
    {
      "epoch": 1.9823126981478392,
      "grad_norm": 1.6066176891326904,
      "learning_rate": 2.575835037491479e-05,
      "loss": 0.5593,
      "step": 11880
    },
    {
      "epoch": 1.9839813115301186,
      "grad_norm": 3.6320762634277344,
      "learning_rate": 2.5737048398091346e-05,
      "loss": 0.4828,
      "step": 11890
    },
    {
      "epoch": 1.9856499249123978,
      "grad_norm": 1.8442095518112183,
      "learning_rate": 2.5715746421267894e-05,
      "loss": 0.4877,
      "step": 11900
    },
    {
      "epoch": 1.987318538294677,
      "grad_norm": 14.181127548217773,
      "learning_rate": 2.5694444444444445e-05,
      "loss": 0.7109,
      "step": 11910
    },
    {
      "epoch": 1.9889871516769566,
      "grad_norm": 1.298028826713562,
      "learning_rate": 2.5673142467620996e-05,
      "loss": 0.4586,
      "step": 11920
    },
    {
      "epoch": 1.9906557650592358,
      "grad_norm": 8.27596378326416,
      "learning_rate": 2.5651840490797547e-05,
      "loss": 0.618,
      "step": 11930
    },
    {
      "epoch": 1.992324378441515,
      "grad_norm": 12.391458511352539,
      "learning_rate": 2.5630538513974102e-05,
      "loss": 0.601,
      "step": 11940
    },
    {
      "epoch": 1.9939929918237944,
      "grad_norm": 5.91458797454834,
      "learning_rate": 2.560923653715065e-05,
      "loss": 0.5092,
      "step": 11950
    },
    {
      "epoch": 1.9956616052060738,
      "grad_norm": 11.167746543884277,
      "learning_rate": 2.5587934560327198e-05,
      "loss": 0.517,
      "step": 11960
    },
    {
      "epoch": 1.997330218588353,
      "grad_norm": 6.90272331237793,
      "learning_rate": 2.5566632583503753e-05,
      "loss": 0.6517,
      "step": 11970
    },
    {
      "epoch": 1.9989988319706324,
      "grad_norm": 6.041019916534424,
      "learning_rate": 2.55453306066803e-05,
      "loss": 0.4761,
      "step": 11980
    },
    {
      "epoch": 2.000667445352912,
      "grad_norm": 3.8367743492126465,
      "learning_rate": 2.552402862985685e-05,
      "loss": 0.6682,
      "step": 11990
    },
    {
      "epoch": 2.002336058735191,
      "grad_norm": 4.162703514099121,
      "learning_rate": 2.5502726653033403e-05,
      "loss": 0.4028,
      "step": 12000
    },
    {
      "epoch": 2.0040046721174702,
      "grad_norm": 14.186470985412598,
      "learning_rate": 2.548142467620995e-05,
      "loss": 0.6277,
      "step": 12010
    },
    {
      "epoch": 2.00567328549975,
      "grad_norm": 8.741341590881348,
      "learning_rate": 2.5460122699386503e-05,
      "loss": 0.5818,
      "step": 12020
    },
    {
      "epoch": 2.007341898882029,
      "grad_norm": 1.780522346496582,
      "learning_rate": 2.5438820722563057e-05,
      "loss": 0.2782,
      "step": 12030
    },
    {
      "epoch": 2.0090105122643083,
      "grad_norm": 15.327637672424316,
      "learning_rate": 2.5417518745739605e-05,
      "loss": 0.5956,
      "step": 12040
    },
    {
      "epoch": 2.010679125646588,
      "grad_norm": 12.693575859069824,
      "learning_rate": 2.539621676891616e-05,
      "loss": 0.6907,
      "step": 12050
    },
    {
      "epoch": 2.012347739028867,
      "grad_norm": 4.891194820404053,
      "learning_rate": 2.5374914792092708e-05,
      "loss": 0.4497,
      "step": 12060
    },
    {
      "epoch": 2.0140163524111463,
      "grad_norm": 7.883111476898193,
      "learning_rate": 2.5353612815269256e-05,
      "loss": 0.4944,
      "step": 12070
    },
    {
      "epoch": 2.0156849657934255,
      "grad_norm": 4.408793926239014,
      "learning_rate": 2.533231083844581e-05,
      "loss": 0.5716,
      "step": 12080
    },
    {
      "epoch": 2.017353579175705,
      "grad_norm": 5.686966896057129,
      "learning_rate": 2.531100886162236e-05,
      "loss": 0.4213,
      "step": 12090
    },
    {
      "epoch": 2.0190221925579843,
      "grad_norm": 2.596097230911255,
      "learning_rate": 2.528970688479891e-05,
      "loss": 0.3729,
      "step": 12100
    },
    {
      "epoch": 2.0206908059402635,
      "grad_norm": 2.947553873062134,
      "learning_rate": 2.526840490797546e-05,
      "loss": 0.5451,
      "step": 12110
    },
    {
      "epoch": 2.022359419322543,
      "grad_norm": 6.4590888023376465,
      "learning_rate": 2.5247102931152013e-05,
      "loss": 0.6872,
      "step": 12120
    },
    {
      "epoch": 2.0240280327048223,
      "grad_norm": 0.28481513261795044,
      "learning_rate": 2.5225800954328567e-05,
      "loss": 0.7571,
      "step": 12130
    },
    {
      "epoch": 2.0256966460871015,
      "grad_norm": 7.530422687530518,
      "learning_rate": 2.5204498977505115e-05,
      "loss": 0.453,
      "step": 12140
    },
    {
      "epoch": 2.0273652594693807,
      "grad_norm": 6.341362476348877,
      "learning_rate": 2.5183197000681663e-05,
      "loss": 0.7052,
      "step": 12150
    },
    {
      "epoch": 2.0290338728516604,
      "grad_norm": 8.907991409301758,
      "learning_rate": 2.5161895023858218e-05,
      "loss": 0.5698,
      "step": 12160
    },
    {
      "epoch": 2.0307024862339396,
      "grad_norm": 12.99517822265625,
      "learning_rate": 2.5140593047034766e-05,
      "loss": 0.4613,
      "step": 12170
    },
    {
      "epoch": 2.0323710996162188,
      "grad_norm": 12.919214248657227,
      "learning_rate": 2.5119291070211314e-05,
      "loss": 0.4703,
      "step": 12180
    },
    {
      "epoch": 2.0340397129984984,
      "grad_norm": 18.373794555664062,
      "learning_rate": 2.509798909338787e-05,
      "loss": 0.8548,
      "step": 12190
    },
    {
      "epoch": 2.0357083263807776,
      "grad_norm": 12.941751480102539,
      "learning_rate": 2.5076687116564416e-05,
      "loss": 0.427,
      "step": 12200
    },
    {
      "epoch": 2.037376939763057,
      "grad_norm": 5.688244342803955,
      "learning_rate": 2.5055385139740968e-05,
      "loss": 0.7901,
      "step": 12210
    },
    {
      "epoch": 2.0390455531453364,
      "grad_norm": 0.7614021897315979,
      "learning_rate": 2.5034083162917522e-05,
      "loss": 0.6793,
      "step": 12220
    },
    {
      "epoch": 2.0407141665276156,
      "grad_norm": 5.7423014640808105,
      "learning_rate": 2.501278118609407e-05,
      "loss": 0.5085,
      "step": 12230
    },
    {
      "epoch": 2.042382779909895,
      "grad_norm": 18.812910079956055,
      "learning_rate": 2.4991479209270622e-05,
      "loss": 0.5423,
      "step": 12240
    },
    {
      "epoch": 2.044051393292174,
      "grad_norm": 7.639048099517822,
      "learning_rate": 2.4970177232447173e-05,
      "loss": 0.539,
      "step": 12250
    },
    {
      "epoch": 2.0457200066744536,
      "grad_norm": 7.073643684387207,
      "learning_rate": 2.4948875255623724e-05,
      "loss": 0.5534,
      "step": 12260
    },
    {
      "epoch": 2.047388620056733,
      "grad_norm": 4.239982604980469,
      "learning_rate": 2.4927573278800272e-05,
      "loss": 0.5518,
      "step": 12270
    },
    {
      "epoch": 2.049057233439012,
      "grad_norm": 13.295586585998535,
      "learning_rate": 2.4906271301976824e-05,
      "loss": 0.4393,
      "step": 12280
    },
    {
      "epoch": 2.0507258468212917,
      "grad_norm": 8.348016738891602,
      "learning_rate": 2.4884969325153375e-05,
      "loss": 0.4512,
      "step": 12290
    },
    {
      "epoch": 2.052394460203571,
      "grad_norm": 4.569064140319824,
      "learning_rate": 2.4863667348329926e-05,
      "loss": 0.4924,
      "step": 12300
    },
    {
      "epoch": 2.05406307358585,
      "grad_norm": 14.737363815307617,
      "learning_rate": 2.4842365371506478e-05,
      "loss": 0.3757,
      "step": 12310
    },
    {
      "epoch": 2.0557316869681297,
      "grad_norm": 10.93381118774414,
      "learning_rate": 2.482106339468303e-05,
      "loss": 0.4356,
      "step": 12320
    },
    {
      "epoch": 2.057400300350409,
      "grad_norm": 4.222966194152832,
      "learning_rate": 2.479976141785958e-05,
      "loss": 0.4101,
      "step": 12330
    },
    {
      "epoch": 2.059068913732688,
      "grad_norm": 6.155494689941406,
      "learning_rate": 2.4778459441036128e-05,
      "loss": 0.5009,
      "step": 12340
    },
    {
      "epoch": 2.0607375271149673,
      "grad_norm": 2.7632882595062256,
      "learning_rate": 2.475715746421268e-05,
      "loss": 0.4981,
      "step": 12350
    },
    {
      "epoch": 2.062406140497247,
      "grad_norm": 4.02683162689209,
      "learning_rate": 2.473585548738923e-05,
      "loss": 0.6176,
      "step": 12360
    },
    {
      "epoch": 2.064074753879526,
      "grad_norm": 6.922466278076172,
      "learning_rate": 2.4714553510565782e-05,
      "loss": 0.65,
      "step": 12370
    },
    {
      "epoch": 2.0657433672618053,
      "grad_norm": 5.625187397003174,
      "learning_rate": 2.469325153374233e-05,
      "loss": 0.9026,
      "step": 12380
    },
    {
      "epoch": 2.067411980644085,
      "grad_norm": 1.0019031763076782,
      "learning_rate": 2.467194955691888e-05,
      "loss": 0.5825,
      "step": 12390
    },
    {
      "epoch": 2.069080594026364,
      "grad_norm": 9.458734512329102,
      "learning_rate": 2.4650647580095433e-05,
      "loss": 0.4731,
      "step": 12400
    },
    {
      "epoch": 2.0707492074086433,
      "grad_norm": 2.3224880695343018,
      "learning_rate": 2.4629345603271988e-05,
      "loss": 0.4249,
      "step": 12410
    },
    {
      "epoch": 2.072417820790923,
      "grad_norm": 7.020013332366943,
      "learning_rate": 2.4608043626448535e-05,
      "loss": 0.5688,
      "step": 12420
    },
    {
      "epoch": 2.074086434173202,
      "grad_norm": 7.550470352172852,
      "learning_rate": 2.4586741649625087e-05,
      "loss": 0.8108,
      "step": 12430
    },
    {
      "epoch": 2.0757550475554813,
      "grad_norm": 7.151927471160889,
      "learning_rate": 2.4565439672801638e-05,
      "loss": 0.8777,
      "step": 12440
    },
    {
      "epoch": 2.0774236609377605,
      "grad_norm": 8.78616714477539,
      "learning_rate": 2.4544137695978186e-05,
      "loss": 0.6024,
      "step": 12450
    },
    {
      "epoch": 2.07909227432004,
      "grad_norm": 6.150145053863525,
      "learning_rate": 2.4522835719154737e-05,
      "loss": 0.6363,
      "step": 12460
    },
    {
      "epoch": 2.0807608877023194,
      "grad_norm": 1.1926350593566895,
      "learning_rate": 2.450153374233129e-05,
      "loss": 0.4711,
      "step": 12470
    },
    {
      "epoch": 2.0824295010845986,
      "grad_norm": 3.1370627880096436,
      "learning_rate": 2.448023176550784e-05,
      "loss": 0.4211,
      "step": 12480
    },
    {
      "epoch": 2.084098114466878,
      "grad_norm": 2.243422269821167,
      "learning_rate": 2.445892978868439e-05,
      "loss": 0.5541,
      "step": 12490
    },
    {
      "epoch": 2.0857667278491574,
      "grad_norm": 12.640222549438477,
      "learning_rate": 2.4437627811860943e-05,
      "loss": 0.4236,
      "step": 12500
    },
    {
      "epoch": 2.0874353412314366,
      "grad_norm": 2.364929437637329,
      "learning_rate": 2.4416325835037494e-05,
      "loss": 0.2484,
      "step": 12510
    },
    {
      "epoch": 2.089103954613716,
      "grad_norm": 10.946349143981934,
      "learning_rate": 2.4395023858214045e-05,
      "loss": 0.6814,
      "step": 12520
    },
    {
      "epoch": 2.0907725679959954,
      "grad_norm": 7.025642395019531,
      "learning_rate": 2.4373721881390593e-05,
      "loss": 0.3572,
      "step": 12530
    },
    {
      "epoch": 2.0924411813782746,
      "grad_norm": 7.414419174194336,
      "learning_rate": 2.4352419904567145e-05,
      "loss": 0.412,
      "step": 12540
    },
    {
      "epoch": 2.094109794760554,
      "grad_norm": 2.377911329269409,
      "learning_rate": 2.4331117927743696e-05,
      "loss": 0.6236,
      "step": 12550
    },
    {
      "epoch": 2.0957784081428334,
      "grad_norm": 5.59388542175293,
      "learning_rate": 2.4309815950920247e-05,
      "loss": 0.7535,
      "step": 12560
    },
    {
      "epoch": 2.0974470215251126,
      "grad_norm": 3.774824619293213,
      "learning_rate": 2.4288513974096795e-05,
      "loss": 0.5001,
      "step": 12570
    },
    {
      "epoch": 2.099115634907392,
      "grad_norm": 8.563779830932617,
      "learning_rate": 2.4267211997273347e-05,
      "loss": 0.73,
      "step": 12580
    },
    {
      "epoch": 2.1007842482896715,
      "grad_norm": 6.722067356109619,
      "learning_rate": 2.4245910020449898e-05,
      "loss": 0.4601,
      "step": 12590
    },
    {
      "epoch": 2.1024528616719507,
      "grad_norm": 5.883053779602051,
      "learning_rate": 2.422460804362645e-05,
      "loss": 0.623,
      "step": 12600
    },
    {
      "epoch": 2.10412147505423,
      "grad_norm": 3.4178788661956787,
      "learning_rate": 2.4203306066803e-05,
      "loss": 0.4473,
      "step": 12610
    },
    {
      "epoch": 2.105790088436509,
      "grad_norm": 3.3628554344177246,
      "learning_rate": 2.4182004089979552e-05,
      "loss": 0.4249,
      "step": 12620
    },
    {
      "epoch": 2.1074587018187887,
      "grad_norm": 7.805354595184326,
      "learning_rate": 2.4160702113156103e-05,
      "loss": 0.6043,
      "step": 12630
    },
    {
      "epoch": 2.109127315201068,
      "grad_norm": 4.279355525970459,
      "learning_rate": 2.413940013633265e-05,
      "loss": 0.5814,
      "step": 12640
    },
    {
      "epoch": 2.110795928583347,
      "grad_norm": 6.756079196929932,
      "learning_rate": 2.4118098159509202e-05,
      "loss": 0.4766,
      "step": 12650
    },
    {
      "epoch": 2.1124645419656267,
      "grad_norm": 2.001124143600464,
      "learning_rate": 2.4096796182685754e-05,
      "loss": 0.4649,
      "step": 12660
    },
    {
      "epoch": 2.114133155347906,
      "grad_norm": 0.8352165818214417,
      "learning_rate": 2.4075494205862305e-05,
      "loss": 0.4725,
      "step": 12670
    },
    {
      "epoch": 2.115801768730185,
      "grad_norm": 5.745327472686768,
      "learning_rate": 2.4054192229038856e-05,
      "loss": 0.4793,
      "step": 12680
    },
    {
      "epoch": 2.1174703821124647,
      "grad_norm": 8.170348167419434,
      "learning_rate": 2.4032890252215408e-05,
      "loss": 0.6395,
      "step": 12690
    },
    {
      "epoch": 2.119138995494744,
      "grad_norm": 12.434511184692383,
      "learning_rate": 2.401158827539196e-05,
      "loss": 0.7515,
      "step": 12700
    },
    {
      "epoch": 2.120807608877023,
      "grad_norm": 3.3916187286376953,
      "learning_rate": 2.399028629856851e-05,
      "loss": 0.6201,
      "step": 12710
    },
    {
      "epoch": 2.1224762222593023,
      "grad_norm": 6.458469867706299,
      "learning_rate": 2.396898432174506e-05,
      "loss": 0.5042,
      "step": 12720
    },
    {
      "epoch": 2.124144835641582,
      "grad_norm": 5.744049072265625,
      "learning_rate": 2.394768234492161e-05,
      "loss": 0.5635,
      "step": 12730
    },
    {
      "epoch": 2.125813449023861,
      "grad_norm": 1.8865758180618286,
      "learning_rate": 2.392638036809816e-05,
      "loss": 0.5099,
      "step": 12740
    },
    {
      "epoch": 2.1274820624061404,
      "grad_norm": 8.739212036132812,
      "learning_rate": 2.3905078391274712e-05,
      "loss": 0.8641,
      "step": 12750
    },
    {
      "epoch": 2.12915067578842,
      "grad_norm": 2.1598286628723145,
      "learning_rate": 2.388377641445126e-05,
      "loss": 0.4271,
      "step": 12760
    },
    {
      "epoch": 2.130819289170699,
      "grad_norm": 8.991613388061523,
      "learning_rate": 2.386247443762781e-05,
      "loss": 0.4175,
      "step": 12770
    },
    {
      "epoch": 2.1324879025529784,
      "grad_norm": 6.842810153961182,
      "learning_rate": 2.3841172460804363e-05,
      "loss": 0.3694,
      "step": 12780
    },
    {
      "epoch": 2.134156515935258,
      "grad_norm": 1.7037659883499146,
      "learning_rate": 2.3819870483980914e-05,
      "loss": 0.6415,
      "step": 12790
    },
    {
      "epoch": 2.135825129317537,
      "grad_norm": 8.281326293945312,
      "learning_rate": 2.3798568507157466e-05,
      "loss": 0.5008,
      "step": 12800
    },
    {
      "epoch": 2.1374937426998164,
      "grad_norm": 8.53256607055664,
      "learning_rate": 2.3777266530334017e-05,
      "loss": 0.4756,
      "step": 12810
    },
    {
      "epoch": 2.1391623560820956,
      "grad_norm": 1.4870010614395142,
      "learning_rate": 2.3755964553510568e-05,
      "loss": 0.7255,
      "step": 12820
    },
    {
      "epoch": 2.1408309694643752,
      "grad_norm": 5.461845874786377,
      "learning_rate": 2.3734662576687116e-05,
      "loss": 0.4823,
      "step": 12830
    },
    {
      "epoch": 2.1424995828466544,
      "grad_norm": 13.580111503601074,
      "learning_rate": 2.3713360599863668e-05,
      "loss": 0.4783,
      "step": 12840
    },
    {
      "epoch": 2.1441681962289336,
      "grad_norm": 17.7171688079834,
      "learning_rate": 2.369205862304022e-05,
      "loss": 0.5628,
      "step": 12850
    },
    {
      "epoch": 2.1458368096112133,
      "grad_norm": 12.22330379486084,
      "learning_rate": 2.367075664621677e-05,
      "loss": 0.5647,
      "step": 12860
    },
    {
      "epoch": 2.1475054229934925,
      "grad_norm": 10.744745254516602,
      "learning_rate": 2.364945466939332e-05,
      "loss": 0.2999,
      "step": 12870
    },
    {
      "epoch": 2.1491740363757716,
      "grad_norm": 3.637953996658325,
      "learning_rate": 2.3628152692569873e-05,
      "loss": 0.7332,
      "step": 12880
    },
    {
      "epoch": 2.150842649758051,
      "grad_norm": 5.956653594970703,
      "learning_rate": 2.3606850715746424e-05,
      "loss": 0.4237,
      "step": 12890
    },
    {
      "epoch": 2.1525112631403305,
      "grad_norm": 12.397068977355957,
      "learning_rate": 2.3585548738922976e-05,
      "loss": 0.5977,
      "step": 12900
    },
    {
      "epoch": 2.1541798765226097,
      "grad_norm": 10.182942390441895,
      "learning_rate": 2.3564246762099523e-05,
      "loss": 0.7495,
      "step": 12910
    },
    {
      "epoch": 2.155848489904889,
      "grad_norm": 7.909827709197998,
      "learning_rate": 2.3542944785276075e-05,
      "loss": 0.3484,
      "step": 12920
    },
    {
      "epoch": 2.1575171032871685,
      "grad_norm": 2.0388829708099365,
      "learning_rate": 2.3521642808452626e-05,
      "loss": 0.5925,
      "step": 12930
    },
    {
      "epoch": 2.1591857166694477,
      "grad_norm": 6.9217119216918945,
      "learning_rate": 2.3500340831629174e-05,
      "loss": 0.6792,
      "step": 12940
    },
    {
      "epoch": 2.160854330051727,
      "grad_norm": 10.4379243850708,
      "learning_rate": 2.3479038854805725e-05,
      "loss": 0.4222,
      "step": 12950
    },
    {
      "epoch": 2.1625229434340065,
      "grad_norm": 2.8365488052368164,
      "learning_rate": 2.3457736877982277e-05,
      "loss": 0.5876,
      "step": 12960
    },
    {
      "epoch": 2.1641915568162857,
      "grad_norm": 8.202917098999023,
      "learning_rate": 2.3436434901158828e-05,
      "loss": 0.6844,
      "step": 12970
    },
    {
      "epoch": 2.165860170198565,
      "grad_norm": 10.568673133850098,
      "learning_rate": 2.341513292433538e-05,
      "loss": 0.5005,
      "step": 12980
    },
    {
      "epoch": 2.1675287835808446,
      "grad_norm": 3.3091869354248047,
      "learning_rate": 2.339383094751193e-05,
      "loss": 0.4666,
      "step": 12990
    },
    {
      "epoch": 2.1691973969631237,
      "grad_norm": 1.0353646278381348,
      "learning_rate": 2.3372528970688482e-05,
      "loss": 0.7425,
      "step": 13000
    },
    {
      "epoch": 2.170866010345403,
      "grad_norm": 2.8577330112457275,
      "learning_rate": 2.3351226993865033e-05,
      "loss": 0.5393,
      "step": 13010
    },
    {
      "epoch": 2.172534623727682,
      "grad_norm": 2.694959878921509,
      "learning_rate": 2.332992501704158e-05,
      "loss": 0.4671,
      "step": 13020
    },
    {
      "epoch": 2.1742032371099618,
      "grad_norm": 9.500311851501465,
      "learning_rate": 2.3308623040218133e-05,
      "loss": 0.7369,
      "step": 13030
    },
    {
      "epoch": 2.175871850492241,
      "grad_norm": 7.983854293823242,
      "learning_rate": 2.3287321063394684e-05,
      "loss": 0.5222,
      "step": 13040
    },
    {
      "epoch": 2.17754046387452,
      "grad_norm": 14.958597183227539,
      "learning_rate": 2.3266019086571235e-05,
      "loss": 0.5331,
      "step": 13050
    },
    {
      "epoch": 2.1792090772568,
      "grad_norm": 2.575300693511963,
      "learning_rate": 2.3244717109747787e-05,
      "loss": 0.4692,
      "step": 13060
    },
    {
      "epoch": 2.180877690639079,
      "grad_norm": 5.254853248596191,
      "learning_rate": 2.3223415132924338e-05,
      "loss": 0.4989,
      "step": 13070
    },
    {
      "epoch": 2.182546304021358,
      "grad_norm": 6.056183815002441,
      "learning_rate": 2.320211315610089e-05,
      "loss": 0.3362,
      "step": 13080
    },
    {
      "epoch": 2.1842149174036374,
      "grad_norm": 6.28759241104126,
      "learning_rate": 2.3180811179277437e-05,
      "loss": 0.6194,
      "step": 13090
    },
    {
      "epoch": 2.185883530785917,
      "grad_norm": 14.888075828552246,
      "learning_rate": 2.315950920245399e-05,
      "loss": 0.5216,
      "step": 13100
    },
    {
      "epoch": 2.187552144168196,
      "grad_norm": 4.086630821228027,
      "learning_rate": 2.313820722563054e-05,
      "loss": 0.4822,
      "step": 13110
    },
    {
      "epoch": 2.1892207575504754,
      "grad_norm": 10.736702919006348,
      "learning_rate": 2.311690524880709e-05,
      "loss": 0.6046,
      "step": 13120
    },
    {
      "epoch": 2.190889370932755,
      "grad_norm": 2.025290012359619,
      "learning_rate": 2.309560327198364e-05,
      "loss": 0.41,
      "step": 13130
    },
    {
      "epoch": 2.1925579843150342,
      "grad_norm": 6.574837684631348,
      "learning_rate": 2.307430129516019e-05,
      "loss": 0.5152,
      "step": 13140
    },
    {
      "epoch": 2.1942265976973134,
      "grad_norm": 3.3533477783203125,
      "learning_rate": 2.3052999318336742e-05,
      "loss": 0.4581,
      "step": 13150
    },
    {
      "epoch": 2.195895211079593,
      "grad_norm": 5.875707149505615,
      "learning_rate": 2.3031697341513293e-05,
      "loss": 0.6201,
      "step": 13160
    },
    {
      "epoch": 2.1975638244618723,
      "grad_norm": 4.815243244171143,
      "learning_rate": 2.3010395364689844e-05,
      "loss": 0.3258,
      "step": 13170
    },
    {
      "epoch": 2.1992324378441515,
      "grad_norm": 1.9412932395935059,
      "learning_rate": 2.2989093387866396e-05,
      "loss": 0.6105,
      "step": 13180
    },
    {
      "epoch": 2.2009010512264306,
      "grad_norm": 7.0165019035339355,
      "learning_rate": 2.2967791411042947e-05,
      "loss": 0.773,
      "step": 13190
    },
    {
      "epoch": 2.2025696646087103,
      "grad_norm": 4.451877117156982,
      "learning_rate": 2.29464894342195e-05,
      "loss": 0.4305,
      "step": 13200
    },
    {
      "epoch": 2.2042382779909895,
      "grad_norm": 3.5724520683288574,
      "learning_rate": 2.2925187457396046e-05,
      "loss": 0.6354,
      "step": 13210
    },
    {
      "epoch": 2.2059068913732687,
      "grad_norm": 3.5874335765838623,
      "learning_rate": 2.2903885480572598e-05,
      "loss": 0.5299,
      "step": 13220
    },
    {
      "epoch": 2.2075755047555483,
      "grad_norm": 7.402770519256592,
      "learning_rate": 2.288258350374915e-05,
      "loss": 0.4335,
      "step": 13230
    },
    {
      "epoch": 2.2092441181378275,
      "grad_norm": 0.40766119956970215,
      "learning_rate": 2.2861281526925697e-05,
      "loss": 0.3192,
      "step": 13240
    },
    {
      "epoch": 2.2109127315201067,
      "grad_norm": 7.82407283782959,
      "learning_rate": 2.2839979550102252e-05,
      "loss": 0.4115,
      "step": 13250
    },
    {
      "epoch": 2.212581344902386,
      "grad_norm": 6.915604114532471,
      "learning_rate": 2.2818677573278803e-05,
      "loss": 0.4487,
      "step": 13260
    },
    {
      "epoch": 2.2142499582846655,
      "grad_norm": 2.4587042331695557,
      "learning_rate": 2.2797375596455354e-05,
      "loss": 0.6329,
      "step": 13270
    },
    {
      "epoch": 2.2159185716669447,
      "grad_norm": 14.228825569152832,
      "learning_rate": 2.2776073619631902e-05,
      "loss": 0.7748,
      "step": 13280
    },
    {
      "epoch": 2.217587185049224,
      "grad_norm": 6.17348575592041,
      "learning_rate": 2.2754771642808454e-05,
      "loss": 0.4427,
      "step": 13290
    },
    {
      "epoch": 2.2192557984315036,
      "grad_norm": 1.3742755651474,
      "learning_rate": 2.2733469665985005e-05,
      "loss": 0.5917,
      "step": 13300
    },
    {
      "epoch": 2.2209244118137828,
      "grad_norm": 7.0827717781066895,
      "learning_rate": 2.2712167689161556e-05,
      "loss": 0.5264,
      "step": 13310
    },
    {
      "epoch": 2.222593025196062,
      "grad_norm": 9.088719367980957,
      "learning_rate": 2.2690865712338104e-05,
      "loss": 0.5877,
      "step": 13320
    },
    {
      "epoch": 2.2242616385783416,
      "grad_norm": 13.289702415466309,
      "learning_rate": 2.2669563735514656e-05,
      "loss": 0.5979,
      "step": 13330
    },
    {
      "epoch": 2.2259302519606208,
      "grad_norm": 2.226699113845825,
      "learning_rate": 2.2648261758691207e-05,
      "loss": 0.6371,
      "step": 13340
    },
    {
      "epoch": 2.2275988653429,
      "grad_norm": 5.018858432769775,
      "learning_rate": 2.2626959781867758e-05,
      "loss": 0.6693,
      "step": 13350
    },
    {
      "epoch": 2.2292674787251796,
      "grad_norm": 13.160627365112305,
      "learning_rate": 2.260565780504431e-05,
      "loss": 0.6328,
      "step": 13360
    },
    {
      "epoch": 2.230936092107459,
      "grad_norm": 8.084263801574707,
      "learning_rate": 2.258435582822086e-05,
      "loss": 0.4673,
      "step": 13370
    },
    {
      "epoch": 2.232604705489738,
      "grad_norm": 1.6497457027435303,
      "learning_rate": 2.2563053851397412e-05,
      "loss": 0.4645,
      "step": 13380
    },
    {
      "epoch": 2.234273318872017,
      "grad_norm": 1.840442419052124,
      "learning_rate": 2.254175187457396e-05,
      "loss": 0.3156,
      "step": 13390
    },
    {
      "epoch": 2.235941932254297,
      "grad_norm": 9.914693832397461,
      "learning_rate": 2.252044989775051e-05,
      "loss": 0.4373,
      "step": 13400
    },
    {
      "epoch": 2.237610545636576,
      "grad_norm": 7.233997344970703,
      "learning_rate": 2.2499147920927063e-05,
      "loss": 0.6254,
      "step": 13410
    },
    {
      "epoch": 2.239279159018855,
      "grad_norm": 10.47616195678711,
      "learning_rate": 2.2477845944103614e-05,
      "loss": 0.407,
      "step": 13420
    },
    {
      "epoch": 2.240947772401135,
      "grad_norm": 3.3367390632629395,
      "learning_rate": 2.2456543967280162e-05,
      "loss": 0.4496,
      "step": 13430
    },
    {
      "epoch": 2.242616385783414,
      "grad_norm": 8.257811546325684,
      "learning_rate": 2.2435241990456717e-05,
      "loss": 0.6987,
      "step": 13440
    },
    {
      "epoch": 2.2442849991656932,
      "grad_norm": 6.102403163909912,
      "learning_rate": 2.2413940013633268e-05,
      "loss": 0.4137,
      "step": 13450
    },
    {
      "epoch": 2.2459536125479724,
      "grad_norm": 7.960425853729248,
      "learning_rate": 2.239263803680982e-05,
      "loss": 0.5026,
      "step": 13460
    },
    {
      "epoch": 2.247622225930252,
      "grad_norm": 6.235080242156982,
      "learning_rate": 2.2371336059986367e-05,
      "loss": 0.4393,
      "step": 13470
    },
    {
      "epoch": 2.2492908393125313,
      "grad_norm": 2.342047691345215,
      "learning_rate": 2.235003408316292e-05,
      "loss": 0.5567,
      "step": 13480
    },
    {
      "epoch": 2.2509594526948105,
      "grad_norm": 10.85355281829834,
      "learning_rate": 2.232873210633947e-05,
      "loss": 0.3505,
      "step": 13490
    },
    {
      "epoch": 2.25262806607709,
      "grad_norm": 9.128728866577148,
      "learning_rate": 2.230743012951602e-05,
      "loss": 0.4811,
      "step": 13500
    },
    {
      "epoch": 2.2542966794593693,
      "grad_norm": 5.30757999420166,
      "learning_rate": 2.228612815269257e-05,
      "loss": 0.4553,
      "step": 13510
    },
    {
      "epoch": 2.2559652928416485,
      "grad_norm": 4.137683391571045,
      "learning_rate": 2.226482617586912e-05,
      "loss": 0.619,
      "step": 13520
    },
    {
      "epoch": 2.257633906223928,
      "grad_norm": 2.4711050987243652,
      "learning_rate": 2.2243524199045672e-05,
      "loss": 0.5726,
      "step": 13530
    },
    {
      "epoch": 2.2593025196062073,
      "grad_norm": 7.95661735534668,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 0.4558,
      "step": 13540
    },
    {
      "epoch": 2.2609711329884865,
      "grad_norm": 6.568255424499512,
      "learning_rate": 2.2200920245398775e-05,
      "loss": 0.4269,
      "step": 13550
    },
    {
      "epoch": 2.2626397463707657,
      "grad_norm": 2.067863702774048,
      "learning_rate": 2.2179618268575326e-05,
      "loss": 0.5491,
      "step": 13560
    },
    {
      "epoch": 2.2643083597530453,
      "grad_norm": 7.620333194732666,
      "learning_rate": 2.2158316291751877e-05,
      "loss": 0.5219,
      "step": 13570
    },
    {
      "epoch": 2.2659769731353245,
      "grad_norm": 7.872384548187256,
      "learning_rate": 2.2137014314928425e-05,
      "loss": 0.5835,
      "step": 13580
    },
    {
      "epoch": 2.2676455865176037,
      "grad_norm": 11.334502220153809,
      "learning_rate": 2.2115712338104977e-05,
      "loss": 0.5582,
      "step": 13590
    },
    {
      "epoch": 2.2693141998998834,
      "grad_norm": 11.834273338317871,
      "learning_rate": 2.2094410361281528e-05,
      "loss": 0.6048,
      "step": 13600
    },
    {
      "epoch": 2.2709828132821626,
      "grad_norm": 9.871572494506836,
      "learning_rate": 2.207310838445808e-05,
      "loss": 0.3216,
      "step": 13610
    },
    {
      "epoch": 2.2726514266644418,
      "grad_norm": 0.6099595427513123,
      "learning_rate": 2.2051806407634627e-05,
      "loss": 0.4696,
      "step": 13620
    },
    {
      "epoch": 2.274320040046721,
      "grad_norm": 7.252748012542725,
      "learning_rate": 2.2030504430811182e-05,
      "loss": 0.5044,
      "step": 13630
    },
    {
      "epoch": 2.2759886534290006,
      "grad_norm": 10.550479888916016,
      "learning_rate": 2.2009202453987733e-05,
      "loss": 0.6116,
      "step": 13640
    },
    {
      "epoch": 2.27765726681128,
      "grad_norm": 8.13786792755127,
      "learning_rate": 2.1987900477164285e-05,
      "loss": 0.5495,
      "step": 13650
    },
    {
      "epoch": 2.279325880193559,
      "grad_norm": 9.829028129577637,
      "learning_rate": 2.1966598500340832e-05,
      "loss": 0.5897,
      "step": 13660
    },
    {
      "epoch": 2.2809944935758386,
      "grad_norm": 7.016475200653076,
      "learning_rate": 2.1945296523517384e-05,
      "loss": 0.5535,
      "step": 13670
    },
    {
      "epoch": 2.282663106958118,
      "grad_norm": 7.445830345153809,
      "learning_rate": 2.1923994546693935e-05,
      "loss": 0.5889,
      "step": 13680
    },
    {
      "epoch": 2.284331720340397,
      "grad_norm": 6.717894554138184,
      "learning_rate": 2.1902692569870483e-05,
      "loss": 0.4334,
      "step": 13690
    },
    {
      "epoch": 2.2860003337226766,
      "grad_norm": 10.38194465637207,
      "learning_rate": 2.1881390593047034e-05,
      "loss": 0.5072,
      "step": 13700
    },
    {
      "epoch": 2.287668947104956,
      "grad_norm": 15.577435493469238,
      "learning_rate": 2.1860088616223586e-05,
      "loss": 0.3597,
      "step": 13710
    },
    {
      "epoch": 2.289337560487235,
      "grad_norm": 0.2886430621147156,
      "learning_rate": 2.1838786639400137e-05,
      "loss": 0.729,
      "step": 13720
    },
    {
      "epoch": 2.2910061738695147,
      "grad_norm": 4.0316996574401855,
      "learning_rate": 2.181748466257669e-05,
      "loss": 0.5804,
      "step": 13730
    },
    {
      "epoch": 2.292674787251794,
      "grad_norm": 7.4194769859313965,
      "learning_rate": 2.179618268575324e-05,
      "loss": 0.3976,
      "step": 13740
    },
    {
      "epoch": 2.294343400634073,
      "grad_norm": 6.894708156585693,
      "learning_rate": 2.177488070892979e-05,
      "loss": 0.7717,
      "step": 13750
    },
    {
      "epoch": 2.2960120140163522,
      "grad_norm": 4.606409549713135,
      "learning_rate": 2.1753578732106342e-05,
      "loss": 0.4387,
      "step": 13760
    },
    {
      "epoch": 2.297680627398632,
      "grad_norm": 13.875804901123047,
      "learning_rate": 2.173227675528289e-05,
      "loss": 0.7842,
      "step": 13770
    },
    {
      "epoch": 2.299349240780911,
      "grad_norm": 8.494665145874023,
      "learning_rate": 2.171097477845944e-05,
      "loss": 0.3103,
      "step": 13780
    },
    {
      "epoch": 2.3010178541631903,
      "grad_norm": 8.158863067626953,
      "learning_rate": 2.1689672801635993e-05,
      "loss": 0.4248,
      "step": 13790
    },
    {
      "epoch": 2.30268646754547,
      "grad_norm": 8.338669776916504,
      "learning_rate": 2.1668370824812544e-05,
      "loss": 0.7883,
      "step": 13800
    },
    {
      "epoch": 2.304355080927749,
      "grad_norm": 10.743446350097656,
      "learning_rate": 2.1647068847989092e-05,
      "loss": 0.3496,
      "step": 13810
    },
    {
      "epoch": 2.3060236943100283,
      "grad_norm": 5.827617645263672,
      "learning_rate": 2.1625766871165647e-05,
      "loss": 0.6389,
      "step": 13820
    },
    {
      "epoch": 2.3076923076923075,
      "grad_norm": 4.6904706954956055,
      "learning_rate": 2.1604464894342198e-05,
      "loss": 0.6347,
      "step": 13830
    },
    {
      "epoch": 2.309360921074587,
      "grad_norm": 6.461751461029053,
      "learning_rate": 2.1583162917518746e-05,
      "loss": 0.4438,
      "step": 13840
    },
    {
      "epoch": 2.3110295344568663,
      "grad_norm": 17.73653221130371,
      "learning_rate": 2.1561860940695298e-05,
      "loss": 0.5142,
      "step": 13850
    },
    {
      "epoch": 2.3126981478391455,
      "grad_norm": 3.9493417739868164,
      "learning_rate": 2.154055896387185e-05,
      "loss": 0.6224,
      "step": 13860
    },
    {
      "epoch": 2.314366761221425,
      "grad_norm": 0.9332907795906067,
      "learning_rate": 2.15192569870484e-05,
      "loss": 0.5539,
      "step": 13870
    },
    {
      "epoch": 2.3160353746037043,
      "grad_norm": 10.436158180236816,
      "learning_rate": 2.1497955010224948e-05,
      "loss": 0.501,
      "step": 13880
    },
    {
      "epoch": 2.3177039879859835,
      "grad_norm": 9.904646873474121,
      "learning_rate": 2.14766530334015e-05,
      "loss": 0.4039,
      "step": 13890
    },
    {
      "epoch": 2.319372601368263,
      "grad_norm": 4.443920135498047,
      "learning_rate": 2.145535105657805e-05,
      "loss": 0.282,
      "step": 13900
    },
    {
      "epoch": 2.3210412147505424,
      "grad_norm": 1.6318610906600952,
      "learning_rate": 2.1434049079754602e-05,
      "loss": 0.4659,
      "step": 13910
    },
    {
      "epoch": 2.3227098281328216,
      "grad_norm": 4.710101127624512,
      "learning_rate": 2.1412747102931153e-05,
      "loss": 0.6753,
      "step": 13920
    },
    {
      "epoch": 2.3243784415151008,
      "grad_norm": 6.856459617614746,
      "learning_rate": 2.1391445126107705e-05,
      "loss": 0.758,
      "step": 13930
    },
    {
      "epoch": 2.3260470548973804,
      "grad_norm": 3.8705661296844482,
      "learning_rate": 2.1370143149284256e-05,
      "loss": 0.5328,
      "step": 13940
    },
    {
      "epoch": 2.3277156682796596,
      "grad_norm": 8.5818452835083,
      "learning_rate": 2.1348841172460807e-05,
      "loss": 0.5212,
      "step": 13950
    },
    {
      "epoch": 2.329384281661939,
      "grad_norm": 3.6129119396209717,
      "learning_rate": 2.1327539195637355e-05,
      "loss": 0.6314,
      "step": 13960
    },
    {
      "epoch": 2.3310528950442184,
      "grad_norm": 2.989680051803589,
      "learning_rate": 2.1306237218813907e-05,
      "loss": 0.3181,
      "step": 13970
    },
    {
      "epoch": 2.3327215084264976,
      "grad_norm": 9.437374114990234,
      "learning_rate": 2.1284935241990458e-05,
      "loss": 0.4313,
      "step": 13980
    },
    {
      "epoch": 2.334390121808777,
      "grad_norm": 5.3309712409973145,
      "learning_rate": 2.1263633265167006e-05,
      "loss": 0.5024,
      "step": 13990
    },
    {
      "epoch": 2.336058735191056,
      "grad_norm": 11.689990997314453,
      "learning_rate": 2.1242331288343557e-05,
      "loss": 0.6449,
      "step": 14000
    },
    {
      "epoch": 2.3377273485733356,
      "grad_norm": 11.866998672485352,
      "learning_rate": 2.122102931152011e-05,
      "loss": 0.4776,
      "step": 14010
    },
    {
      "epoch": 2.339395961955615,
      "grad_norm": 15.0510835647583,
      "learning_rate": 2.1199727334696663e-05,
      "loss": 0.4554,
      "step": 14020
    },
    {
      "epoch": 2.341064575337894,
      "grad_norm": 10.44144344329834,
      "learning_rate": 2.117842535787321e-05,
      "loss": 0.517,
      "step": 14030
    },
    {
      "epoch": 2.3427331887201737,
      "grad_norm": 7.706904411315918,
      "learning_rate": 2.1157123381049763e-05,
      "loss": 0.6977,
      "step": 14040
    },
    {
      "epoch": 2.344401802102453,
      "grad_norm": 4.523712635040283,
      "learning_rate": 2.1135821404226314e-05,
      "loss": 0.6906,
      "step": 14050
    },
    {
      "epoch": 2.346070415484732,
      "grad_norm": 2.2393364906311035,
      "learning_rate": 2.1114519427402865e-05,
      "loss": 0.5903,
      "step": 14060
    },
    {
      "epoch": 2.3477390288670117,
      "grad_norm": 6.74041748046875,
      "learning_rate": 2.1093217450579413e-05,
      "loss": 1.046,
      "step": 14070
    },
    {
      "epoch": 2.349407642249291,
      "grad_norm": 9.099632263183594,
      "learning_rate": 2.1071915473755965e-05,
      "loss": 0.4142,
      "step": 14080
    },
    {
      "epoch": 2.35107625563157,
      "grad_norm": 4.173635959625244,
      "learning_rate": 2.1050613496932516e-05,
      "loss": 0.7565,
      "step": 14090
    },
    {
      "epoch": 2.3527448690138497,
      "grad_norm": 3.815950632095337,
      "learning_rate": 2.1029311520109067e-05,
      "loss": 0.4255,
      "step": 14100
    },
    {
      "epoch": 2.354413482396129,
      "grad_norm": 4.44741678237915,
      "learning_rate": 2.100800954328562e-05,
      "loss": 0.3723,
      "step": 14110
    },
    {
      "epoch": 2.356082095778408,
      "grad_norm": 1.5353567600250244,
      "learning_rate": 2.098670756646217e-05,
      "loss": 0.599,
      "step": 14120
    },
    {
      "epoch": 2.3577507091606873,
      "grad_norm": 0.2403869777917862,
      "learning_rate": 2.096540558963872e-05,
      "loss": 0.4032,
      "step": 14130
    },
    {
      "epoch": 2.359419322542967,
      "grad_norm": 9.058250427246094,
      "learning_rate": 2.094410361281527e-05,
      "loss": 0.6462,
      "step": 14140
    },
    {
      "epoch": 2.361087935925246,
      "grad_norm": 6.683839321136475,
      "learning_rate": 2.092280163599182e-05,
      "loss": 0.8038,
      "step": 14150
    },
    {
      "epoch": 2.3627565493075253,
      "grad_norm": 9.384918212890625,
      "learning_rate": 2.0901499659168372e-05,
      "loss": 0.5075,
      "step": 14160
    },
    {
      "epoch": 2.364425162689805,
      "grad_norm": 6.143031120300293,
      "learning_rate": 2.0880197682344923e-05,
      "loss": 0.5336,
      "step": 14170
    },
    {
      "epoch": 2.366093776072084,
      "grad_norm": 9.785111427307129,
      "learning_rate": 2.085889570552147e-05,
      "loss": 0.6309,
      "step": 14180
    },
    {
      "epoch": 2.3677623894543633,
      "grad_norm": 12.084087371826172,
      "learning_rate": 2.0837593728698022e-05,
      "loss": 0.5389,
      "step": 14190
    },
    {
      "epoch": 2.3694310028366425,
      "grad_norm": 5.767510890960693,
      "learning_rate": 2.0816291751874574e-05,
      "loss": 0.6155,
      "step": 14200
    },
    {
      "epoch": 2.371099616218922,
      "grad_norm": 9.143580436706543,
      "learning_rate": 2.079498977505113e-05,
      "loss": 0.5428,
      "step": 14210
    },
    {
      "epoch": 2.3727682296012014,
      "grad_norm": 8.45649242401123,
      "learning_rate": 2.0773687798227676e-05,
      "loss": 0.6012,
      "step": 14220
    },
    {
      "epoch": 2.3744368429834806,
      "grad_norm": 2.5262105464935303,
      "learning_rate": 2.0752385821404228e-05,
      "loss": 0.3706,
      "step": 14230
    },
    {
      "epoch": 2.37610545636576,
      "grad_norm": 19.687484741210938,
      "learning_rate": 2.073108384458078e-05,
      "loss": 0.2999,
      "step": 14240
    },
    {
      "epoch": 2.3777740697480394,
      "grad_norm": 4.756596088409424,
      "learning_rate": 2.070978186775733e-05,
      "loss": 0.5483,
      "step": 14250
    },
    {
      "epoch": 2.3794426831303186,
      "grad_norm": 10.998579025268555,
      "learning_rate": 2.0688479890933878e-05,
      "loss": 0.502,
      "step": 14260
    },
    {
      "epoch": 2.3811112965125982,
      "grad_norm": 7.564642906188965,
      "learning_rate": 2.066717791411043e-05,
      "loss": 0.7253,
      "step": 14270
    },
    {
      "epoch": 2.3827799098948774,
      "grad_norm": 7.082239627838135,
      "learning_rate": 2.064587593728698e-05,
      "loss": 0.6458,
      "step": 14280
    },
    {
      "epoch": 2.3844485232771566,
      "grad_norm": 14.164307594299316,
      "learning_rate": 2.0624573960463532e-05,
      "loss": 0.6019,
      "step": 14290
    },
    {
      "epoch": 2.3861171366594363,
      "grad_norm": 4.498689651489258,
      "learning_rate": 2.0603271983640084e-05,
      "loss": 0.5911,
      "step": 14300
    },
    {
      "epoch": 2.3877857500417154,
      "grad_norm": 3.372525691986084,
      "learning_rate": 2.0581970006816635e-05,
      "loss": 0.6594,
      "step": 14310
    },
    {
      "epoch": 2.3894543634239946,
      "grad_norm": 8.275764465332031,
      "learning_rate": 2.0560668029993186e-05,
      "loss": 0.6151,
      "step": 14320
    },
    {
      "epoch": 2.391122976806274,
      "grad_norm": 6.056221961975098,
      "learning_rate": 2.0539366053169734e-05,
      "loss": 0.5978,
      "step": 14330
    },
    {
      "epoch": 2.3927915901885535,
      "grad_norm": 6.863025188446045,
      "learning_rate": 2.0518064076346286e-05,
      "loss": 0.3959,
      "step": 14340
    },
    {
      "epoch": 2.3944602035708327,
      "grad_norm": 6.720818519592285,
      "learning_rate": 2.0496762099522837e-05,
      "loss": 0.6078,
      "step": 14350
    },
    {
      "epoch": 2.396128816953112,
      "grad_norm": 7.364116191864014,
      "learning_rate": 2.0475460122699388e-05,
      "loss": 0.7605,
      "step": 14360
    },
    {
      "epoch": 2.397797430335391,
      "grad_norm": 5.044676303863525,
      "learning_rate": 2.0454158145875936e-05,
      "loss": 0.5216,
      "step": 14370
    },
    {
      "epoch": 2.3994660437176707,
      "grad_norm": 8.183341026306152,
      "learning_rate": 2.0432856169052487e-05,
      "loss": 0.5616,
      "step": 14380
    },
    {
      "epoch": 2.40113465709995,
      "grad_norm": 5.272016525268555,
      "learning_rate": 2.041155419222904e-05,
      "loss": 0.5014,
      "step": 14390
    },
    {
      "epoch": 2.402803270482229,
      "grad_norm": 3.576300621032715,
      "learning_rate": 2.0390252215405593e-05,
      "loss": 0.3199,
      "step": 14400
    },
    {
      "epoch": 2.4044718838645087,
      "grad_norm": 8.644652366638184,
      "learning_rate": 2.036895023858214e-05,
      "loss": 0.7143,
      "step": 14410
    },
    {
      "epoch": 2.406140497246788,
      "grad_norm": 4.14621639251709,
      "learning_rate": 2.0347648261758693e-05,
      "loss": 0.5203,
      "step": 14420
    },
    {
      "epoch": 2.407809110629067,
      "grad_norm": 9.672074317932129,
      "learning_rate": 2.0326346284935244e-05,
      "loss": 0.3575,
      "step": 14430
    },
    {
      "epoch": 2.4094777240113467,
      "grad_norm": 4.18858003616333,
      "learning_rate": 2.0305044308111792e-05,
      "loss": 0.5583,
      "step": 14440
    },
    {
      "epoch": 2.411146337393626,
      "grad_norm": 10.161283493041992,
      "learning_rate": 2.0283742331288343e-05,
      "loss": 0.6188,
      "step": 14450
    },
    {
      "epoch": 2.412814950775905,
      "grad_norm": 2.8396027088165283,
      "learning_rate": 2.0262440354464895e-05,
      "loss": 0.3842,
      "step": 14460
    },
    {
      "epoch": 2.4144835641581848,
      "grad_norm": 8.42267894744873,
      "learning_rate": 2.0241138377641446e-05,
      "loss": 0.7285,
      "step": 14470
    },
    {
      "epoch": 2.416152177540464,
      "grad_norm": 2.810020923614502,
      "learning_rate": 2.0219836400817997e-05,
      "loss": 0.7311,
      "step": 14480
    },
    {
      "epoch": 2.417820790922743,
      "grad_norm": 2.877525568008423,
      "learning_rate": 2.019853442399455e-05,
      "loss": 0.3994,
      "step": 14490
    },
    {
      "epoch": 2.4194894043050224,
      "grad_norm": 11.400708198547363,
      "learning_rate": 2.01772324471711e-05,
      "loss": 0.5735,
      "step": 14500
    },
    {
      "epoch": 2.421158017687302,
      "grad_norm": 11.435088157653809,
      "learning_rate": 2.015593047034765e-05,
      "loss": 0.6372,
      "step": 14510
    },
    {
      "epoch": 2.422826631069581,
      "grad_norm": 2.600750207901001,
      "learning_rate": 2.01346284935242e-05,
      "loss": 0.3449,
      "step": 14520
    },
    {
      "epoch": 2.4244952444518604,
      "grad_norm": 7.517061233520508,
      "learning_rate": 2.011332651670075e-05,
      "loss": 0.4765,
      "step": 14530
    },
    {
      "epoch": 2.42616385783414,
      "grad_norm": 6.977991104125977,
      "learning_rate": 2.0092024539877302e-05,
      "loss": 0.4972,
      "step": 14540
    },
    {
      "epoch": 2.427832471216419,
      "grad_norm": 14.124423027038574,
      "learning_rate": 2.0070722563053853e-05,
      "loss": 0.526,
      "step": 14550
    },
    {
      "epoch": 2.4295010845986984,
      "grad_norm": 7.046823501586914,
      "learning_rate": 2.00494205862304e-05,
      "loss": 0.3979,
      "step": 14560
    },
    {
      "epoch": 2.4311696979809776,
      "grad_norm": 10.182618141174316,
      "learning_rate": 2.0028118609406953e-05,
      "loss": 0.3561,
      "step": 14570
    },
    {
      "epoch": 2.4328383113632572,
      "grad_norm": 2.3328802585601807,
      "learning_rate": 2.0006816632583504e-05,
      "loss": 0.2448,
      "step": 14580
    },
    {
      "epoch": 2.4345069247455364,
      "grad_norm": 9.778514862060547,
      "learning_rate": 1.9985514655760055e-05,
      "loss": 0.5228,
      "step": 14590
    },
    {
      "epoch": 2.4361755381278156,
      "grad_norm": 1.7233253717422485,
      "learning_rate": 1.9964212678936607e-05,
      "loss": 0.4409,
      "step": 14600
    },
    {
      "epoch": 2.4378441515100953,
      "grad_norm": 6.033242702484131,
      "learning_rate": 1.9942910702113158e-05,
      "loss": 0.515,
      "step": 14610
    },
    {
      "epoch": 2.4395127648923745,
      "grad_norm": 9.26954460144043,
      "learning_rate": 1.992160872528971e-05,
      "loss": 0.5457,
      "step": 14620
    },
    {
      "epoch": 2.4411813782746536,
      "grad_norm": 5.933469295501709,
      "learning_rate": 1.9900306748466257e-05,
      "loss": 0.7891,
      "step": 14630
    },
    {
      "epoch": 2.4428499916569333,
      "grad_norm": 10.52901840209961,
      "learning_rate": 1.987900477164281e-05,
      "loss": 0.7899,
      "step": 14640
    },
    {
      "epoch": 2.4445186050392125,
      "grad_norm": 2.5500354766845703,
      "learning_rate": 1.985770279481936e-05,
      "loss": 0.7606,
      "step": 14650
    },
    {
      "epoch": 2.4461872184214917,
      "grad_norm": 4.351262092590332,
      "learning_rate": 1.983640081799591e-05,
      "loss": 0.3848,
      "step": 14660
    },
    {
      "epoch": 2.4478558318037713,
      "grad_norm": 8.569928169250488,
      "learning_rate": 1.9815098841172462e-05,
      "loss": 0.5898,
      "step": 14670
    },
    {
      "epoch": 2.4495244451860505,
      "grad_norm": 3.078124761581421,
      "learning_rate": 1.9793796864349014e-05,
      "loss": 0.6065,
      "step": 14680
    },
    {
      "epoch": 2.4511930585683297,
      "grad_norm": 7.153619766235352,
      "learning_rate": 1.9772494887525565e-05,
      "loss": 0.5329,
      "step": 14690
    },
    {
      "epoch": 2.452861671950609,
      "grad_norm": 7.2406325340271,
      "learning_rate": 1.9751192910702116e-05,
      "loss": 0.4334,
      "step": 14700
    },
    {
      "epoch": 2.4545302853328885,
      "grad_norm": 9.242321014404297,
      "learning_rate": 1.9729890933878664e-05,
      "loss": 0.6206,
      "step": 14710
    },
    {
      "epoch": 2.4561988987151677,
      "grad_norm": 2.535071611404419,
      "learning_rate": 1.9708588957055216e-05,
      "loss": 0.461,
      "step": 14720
    },
    {
      "epoch": 2.457867512097447,
      "grad_norm": 3.750258207321167,
      "learning_rate": 1.9687286980231767e-05,
      "loss": 0.3578,
      "step": 14730
    },
    {
      "epoch": 2.459536125479726,
      "grad_norm": 13.112589836120605,
      "learning_rate": 1.9665985003408315e-05,
      "loss": 0.6175,
      "step": 14740
    },
    {
      "epoch": 2.4612047388620057,
      "grad_norm": 5.501185894012451,
      "learning_rate": 1.9644683026584866e-05,
      "loss": 0.7755,
      "step": 14750
    },
    {
      "epoch": 2.462873352244285,
      "grad_norm": 6.482693672180176,
      "learning_rate": 1.9623381049761418e-05,
      "loss": 0.6591,
      "step": 14760
    },
    {
      "epoch": 2.464541965626564,
      "grad_norm": 5.009889125823975,
      "learning_rate": 1.960207907293797e-05,
      "loss": 0.3348,
      "step": 14770
    },
    {
      "epoch": 2.4662105790088438,
      "grad_norm": 2.384525775909424,
      "learning_rate": 1.958077709611452e-05,
      "loss": 0.3449,
      "step": 14780
    },
    {
      "epoch": 2.467879192391123,
      "grad_norm": 8.626617431640625,
      "learning_rate": 1.955947511929107e-05,
      "loss": 0.6132,
      "step": 14790
    },
    {
      "epoch": 2.469547805773402,
      "grad_norm": 6.945192337036133,
      "learning_rate": 1.9538173142467623e-05,
      "loss": 0.6115,
      "step": 14800
    },
    {
      "epoch": 2.471216419155682,
      "grad_norm": 7.385819911956787,
      "learning_rate": 1.9516871165644174e-05,
      "loss": 0.5455,
      "step": 14810
    },
    {
      "epoch": 2.472885032537961,
      "grad_norm": 9.285290718078613,
      "learning_rate": 1.9495569188820722e-05,
      "loss": 0.432,
      "step": 14820
    },
    {
      "epoch": 2.47455364592024,
      "grad_norm": 1.8759186267852783,
      "learning_rate": 1.9474267211997274e-05,
      "loss": 0.5735,
      "step": 14830
    },
    {
      "epoch": 2.47622225930252,
      "grad_norm": 6.098407745361328,
      "learning_rate": 1.9452965235173825e-05,
      "loss": 0.3831,
      "step": 14840
    },
    {
      "epoch": 2.477890872684799,
      "grad_norm": 8.74697208404541,
      "learning_rate": 1.9431663258350376e-05,
      "loss": 0.6677,
      "step": 14850
    },
    {
      "epoch": 2.479559486067078,
      "grad_norm": 4.887656211853027,
      "learning_rate": 1.9410361281526928e-05,
      "loss": 0.8008,
      "step": 14860
    },
    {
      "epoch": 2.4812280994493574,
      "grad_norm": 10.133333206176758,
      "learning_rate": 1.938905930470348e-05,
      "loss": 0.6266,
      "step": 14870
    },
    {
      "epoch": 2.482896712831637,
      "grad_norm": 2.4784657955169678,
      "learning_rate": 1.936775732788003e-05,
      "loss": 0.5405,
      "step": 14880
    },
    {
      "epoch": 2.4845653262139162,
      "grad_norm": 5.6495442390441895,
      "learning_rate": 1.9346455351056578e-05,
      "loss": 0.4822,
      "step": 14890
    },
    {
      "epoch": 2.4862339395961954,
      "grad_norm": 6.802266597747803,
      "learning_rate": 1.932515337423313e-05,
      "loss": 0.4544,
      "step": 14900
    },
    {
      "epoch": 2.487902552978475,
      "grad_norm": 6.565819263458252,
      "learning_rate": 1.930385139740968e-05,
      "loss": 0.6142,
      "step": 14910
    },
    {
      "epoch": 2.4895711663607543,
      "grad_norm": 9.220205307006836,
      "learning_rate": 1.9282549420586232e-05,
      "loss": 0.465,
      "step": 14920
    },
    {
      "epoch": 2.4912397797430335,
      "grad_norm": 2.3738932609558105,
      "learning_rate": 1.926124744376278e-05,
      "loss": 0.4167,
      "step": 14930
    },
    {
      "epoch": 2.4929083931253126,
      "grad_norm": 7.048452377319336,
      "learning_rate": 1.923994546693933e-05,
      "loss": 0.5339,
      "step": 14940
    },
    {
      "epoch": 2.4945770065075923,
      "grad_norm": 6.744619369506836,
      "learning_rate": 1.9218643490115883e-05,
      "loss": 0.6317,
      "step": 14950
    },
    {
      "epoch": 2.4962456198898715,
      "grad_norm": 0.42951130867004395,
      "learning_rate": 1.9197341513292434e-05,
      "loss": 0.4469,
      "step": 14960
    },
    {
      "epoch": 2.4979142332721507,
      "grad_norm": 11.550333976745605,
      "learning_rate": 1.9176039536468985e-05,
      "loss": 0.5752,
      "step": 14970
    },
    {
      "epoch": 2.4995828466544303,
      "grad_norm": 14.466115951538086,
      "learning_rate": 1.9154737559645537e-05,
      "loss": 0.5007,
      "step": 14980
    },
    {
      "epoch": 2.5012514600367095,
      "grad_norm": 1.3144464492797852,
      "learning_rate": 1.9133435582822088e-05,
      "loss": 0.4898,
      "step": 14990
    },
    {
      "epoch": 2.5029200734189887,
      "grad_norm": 12.923011779785156,
      "learning_rate": 1.911213360599864e-05,
      "loss": 0.6953,
      "step": 15000
    },
    {
      "epoch": 2.5045886868012683,
      "grad_norm": 4.819092273712158,
      "learning_rate": 1.9090831629175187e-05,
      "loss": 0.4394,
      "step": 15010
    },
    {
      "epoch": 2.5062573001835475,
      "grad_norm": 1.786250352859497,
      "learning_rate": 1.906952965235174e-05,
      "loss": 0.312,
      "step": 15020
    },
    {
      "epoch": 2.5079259135658267,
      "grad_norm": 6.23142147064209,
      "learning_rate": 1.904822767552829e-05,
      "loss": 0.467,
      "step": 15030
    },
    {
      "epoch": 2.5095945269481064,
      "grad_norm": 5.897035121917725,
      "learning_rate": 1.9026925698704838e-05,
      "loss": 0.3847,
      "step": 15040
    },
    {
      "epoch": 2.5112631403303856,
      "grad_norm": 9.313064575195312,
      "learning_rate": 1.9005623721881393e-05,
      "loss": 0.4626,
      "step": 15050
    },
    {
      "epoch": 2.5129317537126648,
      "grad_norm": 17.019628524780273,
      "learning_rate": 1.8984321745057944e-05,
      "loss": 0.4327,
      "step": 15060
    },
    {
      "epoch": 2.514600367094944,
      "grad_norm": 6.403646945953369,
      "learning_rate": 1.8963019768234495e-05,
      "loss": 0.5361,
      "step": 15070
    },
    {
      "epoch": 2.5162689804772236,
      "grad_norm": 9.947307586669922,
      "learning_rate": 1.8941717791411043e-05,
      "loss": 0.5084,
      "step": 15080
    },
    {
      "epoch": 2.5179375938595028,
      "grad_norm": 2.538274049758911,
      "learning_rate": 1.8920415814587595e-05,
      "loss": 0.5426,
      "step": 15090
    },
    {
      "epoch": 2.519606207241782,
      "grad_norm": 13.711299896240234,
      "learning_rate": 1.8899113837764146e-05,
      "loss": 0.6761,
      "step": 15100
    },
    {
      "epoch": 2.521274820624061,
      "grad_norm": 3.9957470893859863,
      "learning_rate": 1.8877811860940697e-05,
      "loss": 0.3871,
      "step": 15110
    },
    {
      "epoch": 2.522943434006341,
      "grad_norm": 9.686710357666016,
      "learning_rate": 1.8856509884117245e-05,
      "loss": 0.4702,
      "step": 15120
    },
    {
      "epoch": 2.52461204738862,
      "grad_norm": 8.102943420410156,
      "learning_rate": 1.8835207907293796e-05,
      "loss": 0.4192,
      "step": 15130
    },
    {
      "epoch": 2.526280660770899,
      "grad_norm": 2.7387797832489014,
      "learning_rate": 1.8813905930470348e-05,
      "loss": 0.5041,
      "step": 15140
    },
    {
      "epoch": 2.527949274153179,
      "grad_norm": 7.091102600097656,
      "learning_rate": 1.87926039536469e-05,
      "loss": 0.6402,
      "step": 15150
    },
    {
      "epoch": 2.529617887535458,
      "grad_norm": 8.409760475158691,
      "learning_rate": 1.877130197682345e-05,
      "loss": 0.5702,
      "step": 15160
    },
    {
      "epoch": 2.531286500917737,
      "grad_norm": 11.569634437561035,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 0.5599,
      "step": 15170
    },
    {
      "epoch": 2.532955114300017,
      "grad_norm": 18.72032356262207,
      "learning_rate": 1.8728698023176553e-05,
      "loss": 0.82,
      "step": 15180
    },
    {
      "epoch": 2.534623727682296,
      "grad_norm": 1.6070632934570312,
      "learning_rate": 1.87073960463531e-05,
      "loss": 0.4667,
      "step": 15190
    },
    {
      "epoch": 2.5362923410645752,
      "grad_norm": 5.568280220031738,
      "learning_rate": 1.8686094069529652e-05,
      "loss": 0.4093,
      "step": 15200
    },
    {
      "epoch": 2.537960954446855,
      "grad_norm": 6.626762390136719,
      "learning_rate": 1.8664792092706204e-05,
      "loss": 0.3125,
      "step": 15210
    },
    {
      "epoch": 2.539629567829134,
      "grad_norm": 9.32747745513916,
      "learning_rate": 1.8643490115882755e-05,
      "loss": 0.2582,
      "step": 15220
    },
    {
      "epoch": 2.5412981812114133,
      "grad_norm": 3.7970778942108154,
      "learning_rate": 1.8622188139059303e-05,
      "loss": 0.7183,
      "step": 15230
    },
    {
      "epoch": 2.542966794593693,
      "grad_norm": 9.455649375915527,
      "learning_rate": 1.8600886162235858e-05,
      "loss": 0.4738,
      "step": 15240
    },
    {
      "epoch": 2.544635407975972,
      "grad_norm": 11.331625938415527,
      "learning_rate": 1.857958418541241e-05,
      "loss": 0.6592,
      "step": 15250
    },
    {
      "epoch": 2.5463040213582513,
      "grad_norm": 7.998076915740967,
      "learning_rate": 1.855828220858896e-05,
      "loss": 0.6167,
      "step": 15260
    },
    {
      "epoch": 2.5479726347405305,
      "grad_norm": 11.270665168762207,
      "learning_rate": 1.8536980231765508e-05,
      "loss": 0.6428,
      "step": 15270
    },
    {
      "epoch": 2.5496412481228097,
      "grad_norm": 2.285168409347534,
      "learning_rate": 1.851567825494206e-05,
      "loss": 0.5731,
      "step": 15280
    },
    {
      "epoch": 2.5513098615050893,
      "grad_norm": 5.1311564445495605,
      "learning_rate": 1.849437627811861e-05,
      "loss": 0.5017,
      "step": 15290
    },
    {
      "epoch": 2.5529784748873685,
      "grad_norm": 3.873725414276123,
      "learning_rate": 1.8473074301295162e-05,
      "loss": 0.3077,
      "step": 15300
    },
    {
      "epoch": 2.5546470882696477,
      "grad_norm": 8.292540550231934,
      "learning_rate": 1.845177232447171e-05,
      "loss": 0.5267,
      "step": 15310
    },
    {
      "epoch": 2.5563157016519273,
      "grad_norm": 9.751287460327148,
      "learning_rate": 1.843047034764826e-05,
      "loss": 0.6218,
      "step": 15320
    },
    {
      "epoch": 2.5579843150342065,
      "grad_norm": 8.278902053833008,
      "learning_rate": 1.8409168370824813e-05,
      "loss": 0.5469,
      "step": 15330
    },
    {
      "epoch": 2.5596529284164857,
      "grad_norm": 4.19534158706665,
      "learning_rate": 1.8387866394001364e-05,
      "loss": 0.7333,
      "step": 15340
    },
    {
      "epoch": 2.5613215417987654,
      "grad_norm": 7.531599521636963,
      "learning_rate": 1.8366564417177915e-05,
      "loss": 0.4988,
      "step": 15350
    },
    {
      "epoch": 2.5629901551810446,
      "grad_norm": 9.09699535369873,
      "learning_rate": 1.8345262440354467e-05,
      "loss": 0.5113,
      "step": 15360
    },
    {
      "epoch": 2.5646587685633238,
      "grad_norm": 2.6113643646240234,
      "learning_rate": 1.8323960463531018e-05,
      "loss": 0.614,
      "step": 15370
    },
    {
      "epoch": 2.5663273819456034,
      "grad_norm": 5.993854522705078,
      "learning_rate": 1.8302658486707566e-05,
      "loss": 0.3872,
      "step": 15380
    },
    {
      "epoch": 2.5679959953278826,
      "grad_norm": 4.96619176864624,
      "learning_rate": 1.8281356509884117e-05,
      "loss": 0.3985,
      "step": 15390
    },
    {
      "epoch": 2.569664608710162,
      "grad_norm": 9.666243553161621,
      "learning_rate": 1.826005453306067e-05,
      "loss": 0.4567,
      "step": 15400
    },
    {
      "epoch": 2.5713332220924414,
      "grad_norm": 10.471216201782227,
      "learning_rate": 1.823875255623722e-05,
      "loss": 0.4106,
      "step": 15410
    },
    {
      "epoch": 2.5730018354747206,
      "grad_norm": 1.525052785873413,
      "learning_rate": 1.8217450579413768e-05,
      "loss": 0.4086,
      "step": 15420
    },
    {
      "epoch": 2.574670448857,
      "grad_norm": 9.29492473602295,
      "learning_rate": 1.8196148602590323e-05,
      "loss": 0.467,
      "step": 15430
    },
    {
      "epoch": 2.576339062239279,
      "grad_norm": 4.637003421783447,
      "learning_rate": 1.8174846625766874e-05,
      "loss": 0.422,
      "step": 15440
    },
    {
      "epoch": 2.5780076756215586,
      "grad_norm": 7.4437785148620605,
      "learning_rate": 1.8153544648943425e-05,
      "loss": 0.4918,
      "step": 15450
    },
    {
      "epoch": 2.579676289003838,
      "grad_norm": 16.29684829711914,
      "learning_rate": 1.8132242672119973e-05,
      "loss": 0.5758,
      "step": 15460
    },
    {
      "epoch": 2.581344902386117,
      "grad_norm": 8.646158218383789,
      "learning_rate": 1.8110940695296525e-05,
      "loss": 0.3422,
      "step": 15470
    },
    {
      "epoch": 2.583013515768396,
      "grad_norm": 4.601890563964844,
      "learning_rate": 1.8089638718473076e-05,
      "loss": 0.6566,
      "step": 15480
    },
    {
      "epoch": 2.584682129150676,
      "grad_norm": 6.450811862945557,
      "learning_rate": 1.8068336741649624e-05,
      "loss": 0.4762,
      "step": 15490
    },
    {
      "epoch": 2.586350742532955,
      "grad_norm": 2.9566831588745117,
      "learning_rate": 1.8047034764826175e-05,
      "loss": 0.4508,
      "step": 15500
    },
    {
      "epoch": 2.5880193559152342,
      "grad_norm": 6.318184852600098,
      "learning_rate": 1.8025732788002727e-05,
      "loss": 0.5036,
      "step": 15510
    },
    {
      "epoch": 2.589687969297514,
      "grad_norm": 10.443085670471191,
      "learning_rate": 1.8004430811179278e-05,
      "loss": 0.3544,
      "step": 15520
    },
    {
      "epoch": 2.591356582679793,
      "grad_norm": 13.031764030456543,
      "learning_rate": 1.798312883435583e-05,
      "loss": 0.671,
      "step": 15530
    },
    {
      "epoch": 2.5930251960620723,
      "grad_norm": 2.3899717330932617,
      "learning_rate": 1.796182685753238e-05,
      "loss": 0.3482,
      "step": 15540
    },
    {
      "epoch": 2.594693809444352,
      "grad_norm": 4.456630229949951,
      "learning_rate": 1.7940524880708932e-05,
      "loss": 0.4037,
      "step": 15550
    },
    {
      "epoch": 2.596362422826631,
      "grad_norm": 8.953022003173828,
      "learning_rate": 1.7919222903885483e-05,
      "loss": 0.5859,
      "step": 15560
    },
    {
      "epoch": 2.5980310362089103,
      "grad_norm": 4.095673561096191,
      "learning_rate": 1.789792092706203e-05,
      "loss": 0.5075,
      "step": 15570
    },
    {
      "epoch": 2.59969964959119,
      "grad_norm": 6.091678619384766,
      "learning_rate": 1.7876618950238582e-05,
      "loss": 0.6023,
      "step": 15580
    },
    {
      "epoch": 2.601368262973469,
      "grad_norm": 5.607409477233887,
      "learning_rate": 1.7855316973415134e-05,
      "loss": 0.5011,
      "step": 15590
    },
    {
      "epoch": 2.6030368763557483,
      "grad_norm": 2.9346656799316406,
      "learning_rate": 1.7834014996591685e-05,
      "loss": 0.3453,
      "step": 15600
    },
    {
      "epoch": 2.604705489738028,
      "grad_norm": 4.189365863800049,
      "learning_rate": 1.7812713019768233e-05,
      "loss": 0.4512,
      "step": 15610
    },
    {
      "epoch": 2.606374103120307,
      "grad_norm": 7.855368614196777,
      "learning_rate": 1.7791411042944784e-05,
      "loss": 0.6064,
      "step": 15620
    },
    {
      "epoch": 2.6080427165025863,
      "grad_norm": 9.43651294708252,
      "learning_rate": 1.777010906612134e-05,
      "loss": 0.8596,
      "step": 15630
    },
    {
      "epoch": 2.6097113298848655,
      "grad_norm": 8.970636367797852,
      "learning_rate": 1.7748807089297887e-05,
      "loss": 0.6409,
      "step": 15640
    },
    {
      "epoch": 2.6113799432671447,
      "grad_norm": 9.26462459564209,
      "learning_rate": 1.772750511247444e-05,
      "loss": 0.2988,
      "step": 15650
    },
    {
      "epoch": 2.6130485566494244,
      "grad_norm": 2.967334747314453,
      "learning_rate": 1.770620313565099e-05,
      "loss": 0.3207,
      "step": 15660
    },
    {
      "epoch": 2.6147171700317036,
      "grad_norm": 9.12269401550293,
      "learning_rate": 1.768490115882754e-05,
      "loss": 0.7114,
      "step": 15670
    },
    {
      "epoch": 2.6163857834139828,
      "grad_norm": 3.041598320007324,
      "learning_rate": 1.766359918200409e-05,
      "loss": 0.2708,
      "step": 15680
    },
    {
      "epoch": 2.6180543967962624,
      "grad_norm": 13.673577308654785,
      "learning_rate": 1.764229720518064e-05,
      "loss": 0.4559,
      "step": 15690
    },
    {
      "epoch": 2.6197230101785416,
      "grad_norm": 7.830970287322998,
      "learning_rate": 1.762099522835719e-05,
      "loss": 0.4727,
      "step": 15700
    },
    {
      "epoch": 2.621391623560821,
      "grad_norm": 9.333783149719238,
      "learning_rate": 1.7599693251533743e-05,
      "loss": 0.4716,
      "step": 15710
    },
    {
      "epoch": 2.6230602369431004,
      "grad_norm": 7.365431308746338,
      "learning_rate": 1.7578391274710294e-05,
      "loss": 0.5484,
      "step": 15720
    },
    {
      "epoch": 2.6247288503253796,
      "grad_norm": 15.041814804077148,
      "learning_rate": 1.7557089297886846e-05,
      "loss": 0.6464,
      "step": 15730
    },
    {
      "epoch": 2.626397463707659,
      "grad_norm": 5.754903316497803,
      "learning_rate": 1.7535787321063397e-05,
      "loss": 0.4363,
      "step": 15740
    },
    {
      "epoch": 2.6280660770899384,
      "grad_norm": 11.327482223510742,
      "learning_rate": 1.751448534423995e-05,
      "loss": 0.3675,
      "step": 15750
    },
    {
      "epoch": 2.6297346904722176,
      "grad_norm": 0.8958194255828857,
      "learning_rate": 1.7493183367416496e-05,
      "loss": 0.5386,
      "step": 15760
    },
    {
      "epoch": 2.631403303854497,
      "grad_norm": 9.439261436462402,
      "learning_rate": 1.7471881390593048e-05,
      "loss": 0.7797,
      "step": 15770
    },
    {
      "epoch": 2.6330719172367765,
      "grad_norm": 6.741166591644287,
      "learning_rate": 1.74505794137696e-05,
      "loss": 0.4339,
      "step": 15780
    },
    {
      "epoch": 2.6347405306190557,
      "grad_norm": 3.203801393508911,
      "learning_rate": 1.7429277436946147e-05,
      "loss": 0.3537,
      "step": 15790
    },
    {
      "epoch": 2.636409144001335,
      "grad_norm": 9.485466003417969,
      "learning_rate": 1.7407975460122698e-05,
      "loss": 0.5575,
      "step": 15800
    },
    {
      "epoch": 2.638077757383614,
      "grad_norm": 11.239176750183105,
      "learning_rate": 1.738667348329925e-05,
      "loss": 0.7304,
      "step": 15810
    },
    {
      "epoch": 2.6397463707658937,
      "grad_norm": 7.789951801300049,
      "learning_rate": 1.7365371506475804e-05,
      "loss": 0.4481,
      "step": 15820
    },
    {
      "epoch": 2.641414984148173,
      "grad_norm": 7.539767742156982,
      "learning_rate": 1.7344069529652352e-05,
      "loss": 0.5293,
      "step": 15830
    },
    {
      "epoch": 2.643083597530452,
      "grad_norm": 11.143632888793945,
      "learning_rate": 1.7322767552828903e-05,
      "loss": 0.7679,
      "step": 15840
    },
    {
      "epoch": 2.6447522109127313,
      "grad_norm": 14.443768501281738,
      "learning_rate": 1.7301465576005455e-05,
      "loss": 0.4605,
      "step": 15850
    },
    {
      "epoch": 2.646420824295011,
      "grad_norm": 21.972171783447266,
      "learning_rate": 1.7280163599182006e-05,
      "loss": 0.4627,
      "step": 15860
    },
    {
      "epoch": 2.64808943767729,
      "grad_norm": 5.081539630889893,
      "learning_rate": 1.7258861622358554e-05,
      "loss": 0.686,
      "step": 15870
    },
    {
      "epoch": 2.6497580510595693,
      "grad_norm": 4.494253158569336,
      "learning_rate": 1.7237559645535105e-05,
      "loss": 0.7853,
      "step": 15880
    },
    {
      "epoch": 2.651426664441849,
      "grad_norm": 2.1378982067108154,
      "learning_rate": 1.7216257668711657e-05,
      "loss": 0.3917,
      "step": 15890
    },
    {
      "epoch": 2.653095277824128,
      "grad_norm": 1.7785167694091797,
      "learning_rate": 1.7194955691888208e-05,
      "loss": 0.4865,
      "step": 15900
    },
    {
      "epoch": 2.6547638912064073,
      "grad_norm": 1.5101196765899658,
      "learning_rate": 1.717365371506476e-05,
      "loss": 0.4051,
      "step": 15910
    },
    {
      "epoch": 2.656432504588687,
      "grad_norm": 7.412766456604004,
      "learning_rate": 1.715235173824131e-05,
      "loss": 0.5352,
      "step": 15920
    },
    {
      "epoch": 2.658101117970966,
      "grad_norm": 7.552366733551025,
      "learning_rate": 1.7131049761417862e-05,
      "loss": 0.7513,
      "step": 15930
    },
    {
      "epoch": 2.6597697313532453,
      "grad_norm": 4.760387897491455,
      "learning_rate": 1.710974778459441e-05,
      "loss": 0.6192,
      "step": 15940
    },
    {
      "epoch": 2.661438344735525,
      "grad_norm": 6.960901260375977,
      "learning_rate": 1.708844580777096e-05,
      "loss": 0.4112,
      "step": 15950
    },
    {
      "epoch": 2.663106958117804,
      "grad_norm": 9.279844284057617,
      "learning_rate": 1.7067143830947513e-05,
      "loss": 0.4845,
      "step": 15960
    },
    {
      "epoch": 2.6647755715000834,
      "grad_norm": 7.49318265914917,
      "learning_rate": 1.7045841854124064e-05,
      "loss": 0.5174,
      "step": 15970
    },
    {
      "epoch": 2.666444184882363,
      "grad_norm": 4.9857001304626465,
      "learning_rate": 1.7024539877300612e-05,
      "loss": 0.6247,
      "step": 15980
    },
    {
      "epoch": 2.668112798264642,
      "grad_norm": 2.7395169734954834,
      "learning_rate": 1.7003237900477163e-05,
      "loss": 0.4074,
      "step": 15990
    },
    {
      "epoch": 2.6697814116469214,
      "grad_norm": 12.144691467285156,
      "learning_rate": 1.6981935923653715e-05,
      "loss": 0.6296,
      "step": 16000
    },
    {
      "epoch": 2.6714500250292006,
      "grad_norm": 17.367328643798828,
      "learning_rate": 1.696063394683027e-05,
      "loss": 0.56,
      "step": 16010
    },
    {
      "epoch": 2.67311863841148,
      "grad_norm": 2.4037301540374756,
      "learning_rate": 1.6939331970006817e-05,
      "loss": 0.6421,
      "step": 16020
    },
    {
      "epoch": 2.6747872517937594,
      "grad_norm": 8.378253936767578,
      "learning_rate": 1.691802999318337e-05,
      "loss": 0.5805,
      "step": 16030
    },
    {
      "epoch": 2.6764558651760386,
      "grad_norm": 4.161113262176514,
      "learning_rate": 1.689672801635992e-05,
      "loss": 0.3246,
      "step": 16040
    },
    {
      "epoch": 2.678124478558318,
      "grad_norm": 12.841883659362793,
      "learning_rate": 1.687542603953647e-05,
      "loss": 0.366,
      "step": 16050
    },
    {
      "epoch": 2.6797930919405974,
      "grad_norm": 16.73142433166504,
      "learning_rate": 1.685412406271302e-05,
      "loss": 0.7304,
      "step": 16060
    },
    {
      "epoch": 2.6814617053228766,
      "grad_norm": 9.41549301147461,
      "learning_rate": 1.683282208588957e-05,
      "loss": 0.5727,
      "step": 16070
    },
    {
      "epoch": 2.683130318705156,
      "grad_norm": 9.280864715576172,
      "learning_rate": 1.6811520109066122e-05,
      "loss": 0.7288,
      "step": 16080
    },
    {
      "epoch": 2.6847989320874355,
      "grad_norm": 4.625895023345947,
      "learning_rate": 1.6790218132242673e-05,
      "loss": 0.4737,
      "step": 16090
    },
    {
      "epoch": 2.6864675454697147,
      "grad_norm": 8.685924530029297,
      "learning_rate": 1.6768916155419224e-05,
      "loss": 0.7022,
      "step": 16100
    },
    {
      "epoch": 2.688136158851994,
      "grad_norm": 1.827699899673462,
      "learning_rate": 1.6747614178595776e-05,
      "loss": 0.4247,
      "step": 16110
    },
    {
      "epoch": 2.6898047722342735,
      "grad_norm": 7.435196876525879,
      "learning_rate": 1.6726312201772327e-05,
      "loss": 0.4184,
      "step": 16120
    },
    {
      "epoch": 2.6914733856165527,
      "grad_norm": 8.877111434936523,
      "learning_rate": 1.6705010224948875e-05,
      "loss": 0.6128,
      "step": 16130
    },
    {
      "epoch": 2.693141998998832,
      "grad_norm": 5.845722198486328,
      "learning_rate": 1.6683708248125426e-05,
      "loss": 0.516,
      "step": 16140
    },
    {
      "epoch": 2.6948106123811115,
      "grad_norm": 11.422572135925293,
      "learning_rate": 1.6662406271301978e-05,
      "loss": 0.7086,
      "step": 16150
    },
    {
      "epoch": 2.6964792257633907,
      "grad_norm": 11.763328552246094,
      "learning_rate": 1.664110429447853e-05,
      "loss": 0.5597,
      "step": 16160
    },
    {
      "epoch": 2.69814783914567,
      "grad_norm": 8.423702239990234,
      "learning_rate": 1.6619802317655077e-05,
      "loss": 0.4747,
      "step": 16170
    },
    {
      "epoch": 2.6998164525279496,
      "grad_norm": 12.885936737060547,
      "learning_rate": 1.659850034083163e-05,
      "loss": 0.4672,
      "step": 16180
    },
    {
      "epoch": 2.7014850659102287,
      "grad_norm": 5.81057596206665,
      "learning_rate": 1.657719836400818e-05,
      "loss": 0.3149,
      "step": 16190
    },
    {
      "epoch": 2.703153679292508,
      "grad_norm": 13.797612190246582,
      "learning_rate": 1.6555896387184734e-05,
      "loss": 0.5076,
      "step": 16200
    },
    {
      "epoch": 2.704822292674787,
      "grad_norm": 4.91392183303833,
      "learning_rate": 1.6534594410361282e-05,
      "loss": 0.3367,
      "step": 16210
    },
    {
      "epoch": 2.7064909060570663,
      "grad_norm": 6.669650077819824,
      "learning_rate": 1.6513292433537834e-05,
      "loss": 0.9297,
      "step": 16220
    },
    {
      "epoch": 2.708159519439346,
      "grad_norm": 10.261019706726074,
      "learning_rate": 1.6491990456714385e-05,
      "loss": 0.5266,
      "step": 16230
    },
    {
      "epoch": 2.709828132821625,
      "grad_norm": 10.171897888183594,
      "learning_rate": 1.6470688479890933e-05,
      "loss": 0.6168,
      "step": 16240
    },
    {
      "epoch": 2.7114967462039044,
      "grad_norm": 8.15678596496582,
      "learning_rate": 1.6449386503067484e-05,
      "loss": 0.5128,
      "step": 16250
    },
    {
      "epoch": 2.713165359586184,
      "grad_norm": 4.154895305633545,
      "learning_rate": 1.6428084526244036e-05,
      "loss": 0.3508,
      "step": 16260
    },
    {
      "epoch": 2.714833972968463,
      "grad_norm": 1.491623878479004,
      "learning_rate": 1.6406782549420587e-05,
      "loss": 0.3265,
      "step": 16270
    },
    {
      "epoch": 2.7165025863507424,
      "grad_norm": 12.513566970825195,
      "learning_rate": 1.6385480572597138e-05,
      "loss": 0.5966,
      "step": 16280
    },
    {
      "epoch": 2.718171199733022,
      "grad_norm": 4.453824043273926,
      "learning_rate": 1.636417859577369e-05,
      "loss": 0.3735,
      "step": 16290
    },
    {
      "epoch": 2.719839813115301,
      "grad_norm": 6.55167818069458,
      "learning_rate": 1.634287661895024e-05,
      "loss": 0.4273,
      "step": 16300
    },
    {
      "epoch": 2.7215084264975804,
      "grad_norm": 1.271162986755371,
      "learning_rate": 1.6321574642126792e-05,
      "loss": 0.4666,
      "step": 16310
    },
    {
      "epoch": 2.72317703987986,
      "grad_norm": 8.835387229919434,
      "learning_rate": 1.630027266530334e-05,
      "loss": 0.4009,
      "step": 16320
    },
    {
      "epoch": 2.7248456532621392,
      "grad_norm": 9.387311935424805,
      "learning_rate": 1.627897068847989e-05,
      "loss": 0.6991,
      "step": 16330
    },
    {
      "epoch": 2.7265142666444184,
      "grad_norm": 9.778569221496582,
      "learning_rate": 1.6257668711656443e-05,
      "loss": 0.57,
      "step": 16340
    },
    {
      "epoch": 2.728182880026698,
      "grad_norm": 4.377457141876221,
      "learning_rate": 1.6236366734832994e-05,
      "loss": 0.6163,
      "step": 16350
    },
    {
      "epoch": 2.7298514934089773,
      "grad_norm": 11.248703956604004,
      "learning_rate": 1.6215064758009542e-05,
      "loss": 0.4832,
      "step": 16360
    },
    {
      "epoch": 2.7315201067912565,
      "grad_norm": 2.513213634490967,
      "learning_rate": 1.6193762781186093e-05,
      "loss": 0.306,
      "step": 16370
    },
    {
      "epoch": 2.7331887201735356,
      "grad_norm": 3.1726558208465576,
      "learning_rate": 1.6172460804362645e-05,
      "loss": 0.4296,
      "step": 16380
    },
    {
      "epoch": 2.7348573335558153,
      "grad_norm": 9.555340766906738,
      "learning_rate": 1.6151158827539196e-05,
      "loss": 0.526,
      "step": 16390
    },
    {
      "epoch": 2.7365259469380945,
      "grad_norm": 10.272353172302246,
      "learning_rate": 1.6129856850715747e-05,
      "loss": 0.5741,
      "step": 16400
    },
    {
      "epoch": 2.7381945603203737,
      "grad_norm": 11.597908973693848,
      "learning_rate": 1.61085548738923e-05,
      "loss": 0.5764,
      "step": 16410
    },
    {
      "epoch": 2.739863173702653,
      "grad_norm": 4.925652503967285,
      "learning_rate": 1.608725289706885e-05,
      "loss": 0.6536,
      "step": 16420
    },
    {
      "epoch": 2.7415317870849325,
      "grad_norm": 14.212209701538086,
      "learning_rate": 1.6065950920245398e-05,
      "loss": 0.6718,
      "step": 16430
    },
    {
      "epoch": 2.7432004004672117,
      "grad_norm": 3.3683722019195557,
      "learning_rate": 1.604464894342195e-05,
      "loss": 0.6603,
      "step": 16440
    },
    {
      "epoch": 2.744869013849491,
      "grad_norm": 17.481237411499023,
      "learning_rate": 1.60233469665985e-05,
      "loss": 0.5163,
      "step": 16450
    },
    {
      "epoch": 2.7465376272317705,
      "grad_norm": 1.4374791383743286,
      "learning_rate": 1.6002044989775052e-05,
      "loss": 0.3193,
      "step": 16460
    },
    {
      "epoch": 2.7482062406140497,
      "grad_norm": 4.082062721252441,
      "learning_rate": 1.5980743012951603e-05,
      "loss": 0.3311,
      "step": 16470
    },
    {
      "epoch": 2.749874853996329,
      "grad_norm": 13.772332191467285,
      "learning_rate": 1.5959441036128155e-05,
      "loss": 0.6819,
      "step": 16480
    },
    {
      "epoch": 2.7515434673786086,
      "grad_norm": 4.579165458679199,
      "learning_rate": 1.5938139059304706e-05,
      "loss": 0.4839,
      "step": 16490
    },
    {
      "epoch": 2.7532120807608877,
      "grad_norm": 7.720444202423096,
      "learning_rate": 1.5916837082481257e-05,
      "loss": 0.5774,
      "step": 16500
    },
    {
      "epoch": 2.754880694143167,
      "grad_norm": 7.064278602600098,
      "learning_rate": 1.5895535105657805e-05,
      "loss": 0.5463,
      "step": 16510
    },
    {
      "epoch": 2.7565493075254466,
      "grad_norm": 7.0993499755859375,
      "learning_rate": 1.5874233128834357e-05,
      "loss": 0.6116,
      "step": 16520
    },
    {
      "epoch": 2.7582179209077258,
      "grad_norm": 3.2702407836914062,
      "learning_rate": 1.5852931152010908e-05,
      "loss": 0.5645,
      "step": 16530
    },
    {
      "epoch": 2.759886534290005,
      "grad_norm": 6.835800647735596,
      "learning_rate": 1.5831629175187456e-05,
      "loss": 0.3907,
      "step": 16540
    },
    {
      "epoch": 2.7615551476722846,
      "grad_norm": 2.123067617416382,
      "learning_rate": 1.5810327198364007e-05,
      "loss": 0.5469,
      "step": 16550
    },
    {
      "epoch": 2.763223761054564,
      "grad_norm": 5.377809047698975,
      "learning_rate": 1.578902522154056e-05,
      "loss": 0.6805,
      "step": 16560
    },
    {
      "epoch": 2.764892374436843,
      "grad_norm": 5.368576526641846,
      "learning_rate": 1.576772324471711e-05,
      "loss": 0.713,
      "step": 16570
    },
    {
      "epoch": 2.766560987819122,
      "grad_norm": 2.550097703933716,
      "learning_rate": 1.574642126789366e-05,
      "loss": 0.3827,
      "step": 16580
    },
    {
      "epoch": 2.7682296012014014,
      "grad_norm": 3.954512357711792,
      "learning_rate": 1.5725119291070212e-05,
      "loss": 0.5407,
      "step": 16590
    },
    {
      "epoch": 2.769898214583681,
      "grad_norm": 11.095760345458984,
      "learning_rate": 1.5703817314246764e-05,
      "loss": 0.562,
      "step": 16600
    },
    {
      "epoch": 2.77156682796596,
      "grad_norm": 1.0061991214752197,
      "learning_rate": 1.5682515337423315e-05,
      "loss": 0.8415,
      "step": 16610
    },
    {
      "epoch": 2.7732354413482394,
      "grad_norm": 8.540018081665039,
      "learning_rate": 1.5661213360599863e-05,
      "loss": 0.4563,
      "step": 16620
    },
    {
      "epoch": 2.774904054730519,
      "grad_norm": 0.8468987345695496,
      "learning_rate": 1.5639911383776414e-05,
      "loss": 0.4805,
      "step": 16630
    },
    {
      "epoch": 2.7765726681127982,
      "grad_norm": 9.30815601348877,
      "learning_rate": 1.5618609406952966e-05,
      "loss": 0.4317,
      "step": 16640
    },
    {
      "epoch": 2.7782412814950774,
      "grad_norm": 1.2963765859603882,
      "learning_rate": 1.5597307430129517e-05,
      "loss": 0.4564,
      "step": 16650
    },
    {
      "epoch": 2.779909894877357,
      "grad_norm": 2.628162384033203,
      "learning_rate": 1.557600545330607e-05,
      "loss": 0.4524,
      "step": 16660
    },
    {
      "epoch": 2.7815785082596363,
      "grad_norm": 3.1525471210479736,
      "learning_rate": 1.555470347648262e-05,
      "loss": 0.3706,
      "step": 16670
    },
    {
      "epoch": 2.7832471216419155,
      "grad_norm": 10.947587013244629,
      "learning_rate": 1.553340149965917e-05,
      "loss": 0.5594,
      "step": 16680
    },
    {
      "epoch": 2.784915735024195,
      "grad_norm": 7.298094749450684,
      "learning_rate": 1.551209952283572e-05,
      "loss": 0.7464,
      "step": 16690
    },
    {
      "epoch": 2.7865843484064743,
      "grad_norm": 7.8140482902526855,
      "learning_rate": 1.549079754601227e-05,
      "loss": 0.7151,
      "step": 16700
    },
    {
      "epoch": 2.7882529617887535,
      "grad_norm": 9.879986763000488,
      "learning_rate": 1.546949556918882e-05,
      "loss": 0.578,
      "step": 16710
    },
    {
      "epoch": 2.789921575171033,
      "grad_norm": 7.017724514007568,
      "learning_rate": 1.5448193592365373e-05,
      "loss": 0.6705,
      "step": 16720
    },
    {
      "epoch": 2.7915901885533123,
      "grad_norm": 5.1667022705078125,
      "learning_rate": 1.542689161554192e-05,
      "loss": 0.2902,
      "step": 16730
    },
    {
      "epoch": 2.7932588019355915,
      "grad_norm": 2.361213445663452,
      "learning_rate": 1.5405589638718472e-05,
      "loss": 0.5519,
      "step": 16740
    },
    {
      "epoch": 2.7949274153178707,
      "grad_norm": 6.350761413574219,
      "learning_rate": 1.5384287661895024e-05,
      "loss": 0.4169,
      "step": 16750
    },
    {
      "epoch": 2.7965960287001503,
      "grad_norm": 8.299983024597168,
      "learning_rate": 1.5362985685071575e-05,
      "loss": 0.7339,
      "step": 16760
    },
    {
      "epoch": 2.7982646420824295,
      "grad_norm": 5.601474761962891,
      "learning_rate": 1.5341683708248126e-05,
      "loss": 0.5327,
      "step": 16770
    },
    {
      "epoch": 2.7999332554647087,
      "grad_norm": 11.839405059814453,
      "learning_rate": 1.5320381731424678e-05,
      "loss": 0.6522,
      "step": 16780
    },
    {
      "epoch": 2.801601868846988,
      "grad_norm": 4.7770304679870605,
      "learning_rate": 1.529907975460123e-05,
      "loss": 0.7492,
      "step": 16790
    },
    {
      "epoch": 2.8032704822292676,
      "grad_norm": 5.609739303588867,
      "learning_rate": 1.527777777777778e-05,
      "loss": 0.5403,
      "step": 16800
    },
    {
      "epoch": 2.8049390956115468,
      "grad_norm": 2.2726593017578125,
      "learning_rate": 1.5256475800954328e-05,
      "loss": 0.539,
      "step": 16810
    },
    {
      "epoch": 2.806607708993826,
      "grad_norm": 5.0081892013549805,
      "learning_rate": 1.523517382413088e-05,
      "loss": 0.7375,
      "step": 16820
    },
    {
      "epoch": 2.8082763223761056,
      "grad_norm": 4.40825891494751,
      "learning_rate": 1.521387184730743e-05,
      "loss": 0.2663,
      "step": 16830
    },
    {
      "epoch": 2.8099449357583848,
      "grad_norm": 13.726273536682129,
      "learning_rate": 1.519256987048398e-05,
      "loss": 0.634,
      "step": 16840
    },
    {
      "epoch": 2.811613549140664,
      "grad_norm": 3.309905767440796,
      "learning_rate": 1.5171267893660532e-05,
      "loss": 0.5206,
      "step": 16850
    },
    {
      "epoch": 2.8132821625229436,
      "grad_norm": 3.9195353984832764,
      "learning_rate": 1.5149965916837083e-05,
      "loss": 0.4073,
      "step": 16860
    },
    {
      "epoch": 2.814950775905223,
      "grad_norm": 0.708142101764679,
      "learning_rate": 1.5128663940013634e-05,
      "loss": 0.6842,
      "step": 16870
    },
    {
      "epoch": 2.816619389287502,
      "grad_norm": 7.119955539703369,
      "learning_rate": 1.5107361963190184e-05,
      "loss": 0.58,
      "step": 16880
    },
    {
      "epoch": 2.8182880026697816,
      "grad_norm": 4.600697040557861,
      "learning_rate": 1.5086059986366735e-05,
      "loss": 0.5124,
      "step": 16890
    },
    {
      "epoch": 2.819956616052061,
      "grad_norm": 2.6969358921051025,
      "learning_rate": 1.5064758009543287e-05,
      "loss": 0.6701,
      "step": 16900
    },
    {
      "epoch": 2.82162522943434,
      "grad_norm": 5.196833610534668,
      "learning_rate": 1.5043456032719838e-05,
      "loss": 0.4726,
      "step": 16910
    },
    {
      "epoch": 2.8232938428166197,
      "grad_norm": 6.984055519104004,
      "learning_rate": 1.5022154055896388e-05,
      "loss": 0.5459,
      "step": 16920
    },
    {
      "epoch": 2.824962456198899,
      "grad_norm": 3.5109288692474365,
      "learning_rate": 1.5000852079072939e-05,
      "loss": 0.2776,
      "step": 16930
    },
    {
      "epoch": 2.826631069581178,
      "grad_norm": 8.979833602905273,
      "learning_rate": 1.497955010224949e-05,
      "loss": 0.5043,
      "step": 16940
    },
    {
      "epoch": 2.8282996829634572,
      "grad_norm": 12.635648727416992,
      "learning_rate": 1.4958248125426042e-05,
      "loss": 0.7374,
      "step": 16950
    },
    {
      "epoch": 2.8299682963457364,
      "grad_norm": 8.566157341003418,
      "learning_rate": 1.493694614860259e-05,
      "loss": 0.9515,
      "step": 16960
    },
    {
      "epoch": 2.831636909728016,
      "grad_norm": 3.7903525829315186,
      "learning_rate": 1.4915644171779141e-05,
      "loss": 0.606,
      "step": 16970
    },
    {
      "epoch": 2.8333055231102953,
      "grad_norm": 6.7888617515563965,
      "learning_rate": 1.4894342194955694e-05,
      "loss": 0.6111,
      "step": 16980
    },
    {
      "epoch": 2.8349741364925745,
      "grad_norm": 8.770487785339355,
      "learning_rate": 1.4873040218132242e-05,
      "loss": 0.6104,
      "step": 16990
    },
    {
      "epoch": 2.836642749874854,
      "grad_norm": 8.25041675567627,
      "learning_rate": 1.4851738241308793e-05,
      "loss": 0.6803,
      "step": 17000
    },
    {
      "epoch": 2.8383113632571333,
      "grad_norm": 1.7264947891235352,
      "learning_rate": 1.4830436264485345e-05,
      "loss": 0.2047,
      "step": 17010
    },
    {
      "epoch": 2.8399799766394125,
      "grad_norm": 11.293725967407227,
      "learning_rate": 1.4809134287661896e-05,
      "loss": 0.6456,
      "step": 17020
    },
    {
      "epoch": 2.841648590021692,
      "grad_norm": 11.819718360900879,
      "learning_rate": 1.4787832310838446e-05,
      "loss": 0.5329,
      "step": 17030
    },
    {
      "epoch": 2.8433172034039713,
      "grad_norm": 0.23228704929351807,
      "learning_rate": 1.4766530334014997e-05,
      "loss": 0.3352,
      "step": 17040
    },
    {
      "epoch": 2.8449858167862505,
      "grad_norm": 8.482902526855469,
      "learning_rate": 1.4745228357191548e-05,
      "loss": 0.5973,
      "step": 17050
    },
    {
      "epoch": 2.84665443016853,
      "grad_norm": 4.520404815673828,
      "learning_rate": 1.47239263803681e-05,
      "loss": 0.3742,
      "step": 17060
    },
    {
      "epoch": 2.8483230435508093,
      "grad_norm": 1.7727665901184082,
      "learning_rate": 1.4702624403544649e-05,
      "loss": 0.4697,
      "step": 17070
    },
    {
      "epoch": 2.8499916569330885,
      "grad_norm": 9.927495956420898,
      "learning_rate": 1.46813224267212e-05,
      "loss": 0.5126,
      "step": 17080
    },
    {
      "epoch": 2.851660270315368,
      "grad_norm": 6.110743045806885,
      "learning_rate": 1.4660020449897752e-05,
      "loss": 0.5965,
      "step": 17090
    },
    {
      "epoch": 2.8533288836976474,
      "grad_norm": 4.35173225402832,
      "learning_rate": 1.4638718473074303e-05,
      "loss": 0.2705,
      "step": 17100
    },
    {
      "epoch": 2.8549974970799266,
      "grad_norm": 9.966796875,
      "learning_rate": 1.4617416496250853e-05,
      "loss": 0.6858,
      "step": 17110
    },
    {
      "epoch": 2.8566661104622058,
      "grad_norm": 3.7422025203704834,
      "learning_rate": 1.4596114519427404e-05,
      "loss": 0.3434,
      "step": 17120
    },
    {
      "epoch": 2.8583347238444854,
      "grad_norm": 4.963431358337402,
      "learning_rate": 1.4574812542603955e-05,
      "loss": 0.5401,
      "step": 17130
    },
    {
      "epoch": 2.8600033372267646,
      "grad_norm": 5.254120349884033,
      "learning_rate": 1.4553510565780503e-05,
      "loss": 0.5567,
      "step": 17140
    },
    {
      "epoch": 2.861671950609044,
      "grad_norm": 7.877273082733154,
      "learning_rate": 1.4532208588957055e-05,
      "loss": 0.7255,
      "step": 17150
    },
    {
      "epoch": 2.863340563991323,
      "grad_norm": 3.321702480316162,
      "learning_rate": 1.4510906612133606e-05,
      "loss": 0.4664,
      "step": 17160
    },
    {
      "epoch": 2.8650091773736026,
      "grad_norm": 5.706223964691162,
      "learning_rate": 1.4489604635310159e-05,
      "loss": 0.4597,
      "step": 17170
    },
    {
      "epoch": 2.866677790755882,
      "grad_norm": 13.48611831665039,
      "learning_rate": 1.4468302658486707e-05,
      "loss": 0.5618,
      "step": 17180
    },
    {
      "epoch": 2.868346404138161,
      "grad_norm": 5.8116583824157715,
      "learning_rate": 1.4447000681663258e-05,
      "loss": 0.6206,
      "step": 17190
    },
    {
      "epoch": 2.8700150175204406,
      "grad_norm": 9.999909400939941,
      "learning_rate": 1.442569870483981e-05,
      "loss": 0.5237,
      "step": 17200
    },
    {
      "epoch": 2.87168363090272,
      "grad_norm": 7.394443035125732,
      "learning_rate": 1.4404396728016361e-05,
      "loss": 0.4951,
      "step": 17210
    },
    {
      "epoch": 2.873352244284999,
      "grad_norm": 8.595489501953125,
      "learning_rate": 1.438309475119291e-05,
      "loss": 0.6219,
      "step": 17220
    },
    {
      "epoch": 2.8750208576672787,
      "grad_norm": 9.652853012084961,
      "learning_rate": 1.4361792774369462e-05,
      "loss": 0.6599,
      "step": 17230
    },
    {
      "epoch": 2.876689471049558,
      "grad_norm": 10.225759506225586,
      "learning_rate": 1.4340490797546013e-05,
      "loss": 0.7114,
      "step": 17240
    },
    {
      "epoch": 2.878358084431837,
      "grad_norm": 2.7379674911499023,
      "learning_rate": 1.4319188820722565e-05,
      "loss": 0.3664,
      "step": 17250
    },
    {
      "epoch": 2.8800266978141167,
      "grad_norm": 10.809905052185059,
      "learning_rate": 1.4297886843899114e-05,
      "loss": 0.4712,
      "step": 17260
    },
    {
      "epoch": 2.881695311196396,
      "grad_norm": 8.397762298583984,
      "learning_rate": 1.4276584867075666e-05,
      "loss": 0.6127,
      "step": 17270
    },
    {
      "epoch": 2.883363924578675,
      "grad_norm": 7.755835056304932,
      "learning_rate": 1.4255282890252217e-05,
      "loss": 0.326,
      "step": 17280
    },
    {
      "epoch": 2.8850325379609547,
      "grad_norm": 1.2611579895019531,
      "learning_rate": 1.4233980913428765e-05,
      "loss": 0.4157,
      "step": 17290
    },
    {
      "epoch": 2.886701151343234,
      "grad_norm": 11.53216552734375,
      "learning_rate": 1.4212678936605318e-05,
      "loss": 0.5295,
      "step": 17300
    },
    {
      "epoch": 2.888369764725513,
      "grad_norm": 9.663046836853027,
      "learning_rate": 1.419137695978187e-05,
      "loss": 0.5342,
      "step": 17310
    },
    {
      "epoch": 2.8900383781077923,
      "grad_norm": 2.217892646789551,
      "learning_rate": 1.417007498295842e-05,
      "loss": 0.4152,
      "step": 17320
    },
    {
      "epoch": 2.8917069914900715,
      "grad_norm": 4.640568256378174,
      "learning_rate": 1.4148773006134968e-05,
      "loss": 0.5248,
      "step": 17330
    },
    {
      "epoch": 2.893375604872351,
      "grad_norm": 4.862830638885498,
      "learning_rate": 1.412747102931152e-05,
      "loss": 0.1792,
      "step": 17340
    },
    {
      "epoch": 2.8950442182546303,
      "grad_norm": 10.57353687286377,
      "learning_rate": 1.4106169052488071e-05,
      "loss": 0.7005,
      "step": 17350
    },
    {
      "epoch": 2.8967128316369095,
      "grad_norm": 2.465329170227051,
      "learning_rate": 1.4084867075664624e-05,
      "loss": 0.5816,
      "step": 17360
    },
    {
      "epoch": 2.898381445019189,
      "grad_norm": 6.915944576263428,
      "learning_rate": 1.4063565098841172e-05,
      "loss": 0.4775,
      "step": 17370
    },
    {
      "epoch": 2.9000500584014683,
      "grad_norm": 13.631497383117676,
      "learning_rate": 1.4042263122017723e-05,
      "loss": 0.6483,
      "step": 17380
    },
    {
      "epoch": 2.9017186717837475,
      "grad_norm": 9.797148704528809,
      "learning_rate": 1.4020961145194275e-05,
      "loss": 0.5188,
      "step": 17390
    },
    {
      "epoch": 2.903387285166027,
      "grad_norm": 10.046957969665527,
      "learning_rate": 1.3999659168370826e-05,
      "loss": 0.4539,
      "step": 17400
    },
    {
      "epoch": 2.9050558985483064,
      "grad_norm": 7.689899444580078,
      "learning_rate": 1.3978357191547376e-05,
      "loss": 0.7084,
      "step": 17410
    },
    {
      "epoch": 2.9067245119305856,
      "grad_norm": 13.239697456359863,
      "learning_rate": 1.3957055214723927e-05,
      "loss": 0.4768,
      "step": 17420
    },
    {
      "epoch": 2.908393125312865,
      "grad_norm": 14.819602966308594,
      "learning_rate": 1.3935753237900478e-05,
      "loss": 0.7058,
      "step": 17430
    },
    {
      "epoch": 2.9100617386951444,
      "grad_norm": 2.7731337547302246,
      "learning_rate": 1.3914451261077028e-05,
      "loss": 0.4864,
      "step": 17440
    },
    {
      "epoch": 2.9117303520774236,
      "grad_norm": 3.9907479286193848,
      "learning_rate": 1.389314928425358e-05,
      "loss": 0.4977,
      "step": 17450
    },
    {
      "epoch": 2.9133989654597032,
      "grad_norm": 11.421585083007812,
      "learning_rate": 1.387184730743013e-05,
      "loss": 0.4579,
      "step": 17460
    },
    {
      "epoch": 2.9150675788419824,
      "grad_norm": 6.194181442260742,
      "learning_rate": 1.3850545330606682e-05,
      "loss": 0.5353,
      "step": 17470
    },
    {
      "epoch": 2.9167361922242616,
      "grad_norm": 1.698403239250183,
      "learning_rate": 1.382924335378323e-05,
      "loss": 0.1501,
      "step": 17480
    },
    {
      "epoch": 2.918404805606541,
      "grad_norm": 7.508113384246826,
      "learning_rate": 1.3807941376959783e-05,
      "loss": 0.4145,
      "step": 17490
    },
    {
      "epoch": 2.9200734189888204,
      "grad_norm": 5.066582202911377,
      "learning_rate": 1.3786639400136334e-05,
      "loss": 0.4441,
      "step": 17500
    },
    {
      "epoch": 2.9217420323710996,
      "grad_norm": 7.211310386657715,
      "learning_rate": 1.3765337423312886e-05,
      "loss": 0.3679,
      "step": 17510
    },
    {
      "epoch": 2.923410645753379,
      "grad_norm": 10.914238929748535,
      "learning_rate": 1.3744035446489434e-05,
      "loss": 0.3014,
      "step": 17520
    },
    {
      "epoch": 2.925079259135658,
      "grad_norm": 1.231544017791748,
      "learning_rate": 1.3722733469665985e-05,
      "loss": 0.4865,
      "step": 17530
    },
    {
      "epoch": 2.9267478725179377,
      "grad_norm": 0.9703734517097473,
      "learning_rate": 1.3701431492842536e-05,
      "loss": 0.7473,
      "step": 17540
    },
    {
      "epoch": 2.928416485900217,
      "grad_norm": 7.589526176452637,
      "learning_rate": 1.368012951601909e-05,
      "loss": 0.5998,
      "step": 17550
    },
    {
      "epoch": 2.930085099282496,
      "grad_norm": 2.2601287364959717,
      "learning_rate": 1.3658827539195637e-05,
      "loss": 0.6439,
      "step": 17560
    },
    {
      "epoch": 2.9317537126647757,
      "grad_norm": 12.115068435668945,
      "learning_rate": 1.3637525562372188e-05,
      "loss": 0.5094,
      "step": 17570
    },
    {
      "epoch": 2.933422326047055,
      "grad_norm": 3.8770625591278076,
      "learning_rate": 1.361622358554874e-05,
      "loss": 0.4477,
      "step": 17580
    },
    {
      "epoch": 2.935090939429334,
      "grad_norm": 2.449061870574951,
      "learning_rate": 1.359492160872529e-05,
      "loss": 0.3923,
      "step": 17590
    },
    {
      "epoch": 2.9367595528116137,
      "grad_norm": 1.2081595659255981,
      "learning_rate": 1.357361963190184e-05,
      "loss": 0.413,
      "step": 17600
    },
    {
      "epoch": 2.938428166193893,
      "grad_norm": 1.1717002391815186,
      "learning_rate": 1.3552317655078392e-05,
      "loss": 0.4675,
      "step": 17610
    },
    {
      "epoch": 2.940096779576172,
      "grad_norm": 9.158431053161621,
      "learning_rate": 1.3531015678254943e-05,
      "loss": 0.8866,
      "step": 17620
    },
    {
      "epoch": 2.9417653929584517,
      "grad_norm": 8.890134811401367,
      "learning_rate": 1.3509713701431493e-05,
      "loss": 0.8247,
      "step": 17630
    },
    {
      "epoch": 2.943434006340731,
      "grad_norm": 8.919856071472168,
      "learning_rate": 1.3488411724608044e-05,
      "loss": 0.5317,
      "step": 17640
    },
    {
      "epoch": 2.94510261972301,
      "grad_norm": 5.693058013916016,
      "learning_rate": 1.3467109747784596e-05,
      "loss": 0.7165,
      "step": 17650
    },
    {
      "epoch": 2.9467712331052898,
      "grad_norm": 9.63416576385498,
      "learning_rate": 1.3445807770961147e-05,
      "loss": 0.5212,
      "step": 17660
    },
    {
      "epoch": 2.948439846487569,
      "grad_norm": 10.167946815490723,
      "learning_rate": 1.3424505794137695e-05,
      "loss": 0.6407,
      "step": 17670
    },
    {
      "epoch": 2.950108459869848,
      "grad_norm": 6.062876224517822,
      "learning_rate": 1.3403203817314246e-05,
      "loss": 0.3515,
      "step": 17680
    },
    {
      "epoch": 2.9517770732521273,
      "grad_norm": 6.270752429962158,
      "learning_rate": 1.33819018404908e-05,
      "loss": 0.6691,
      "step": 17690
    },
    {
      "epoch": 2.9534456866344065,
      "grad_norm": 12.520092964172363,
      "learning_rate": 1.336059986366735e-05,
      "loss": 0.6186,
      "step": 17700
    },
    {
      "epoch": 2.955114300016686,
      "grad_norm": 6.514648914337158,
      "learning_rate": 1.3339297886843899e-05,
      "loss": 0.4773,
      "step": 17710
    },
    {
      "epoch": 2.9567829133989654,
      "grad_norm": 8.500420570373535,
      "learning_rate": 1.331799591002045e-05,
      "loss": 0.5542,
      "step": 17720
    },
    {
      "epoch": 2.9584515267812446,
      "grad_norm": 12.508991241455078,
      "learning_rate": 1.3296693933197001e-05,
      "loss": 0.5217,
      "step": 17730
    },
    {
      "epoch": 2.960120140163524,
      "grad_norm": 6.788778305053711,
      "learning_rate": 1.3275391956373554e-05,
      "loss": 0.5658,
      "step": 17740
    },
    {
      "epoch": 2.9617887535458034,
      "grad_norm": 2.0525221824645996,
      "learning_rate": 1.3254089979550102e-05,
      "loss": 0.2905,
      "step": 17750
    },
    {
      "epoch": 2.9634573669280826,
      "grad_norm": 2.7688703536987305,
      "learning_rate": 1.3232788002726654e-05,
      "loss": 0.6081,
      "step": 17760
    },
    {
      "epoch": 2.9651259803103622,
      "grad_norm": 7.9444966316223145,
      "learning_rate": 1.3211486025903205e-05,
      "loss": 0.5813,
      "step": 17770
    },
    {
      "epoch": 2.9667945936926414,
      "grad_norm": 2.698883533477783,
      "learning_rate": 1.3190184049079754e-05,
      "loss": 0.3968,
      "step": 17780
    },
    {
      "epoch": 2.9684632070749206,
      "grad_norm": 13.423069953918457,
      "learning_rate": 1.3168882072256306e-05,
      "loss": 0.8016,
      "step": 17790
    },
    {
      "epoch": 2.9701318204572003,
      "grad_norm": 9.802745819091797,
      "learning_rate": 1.3147580095432857e-05,
      "loss": 0.542,
      "step": 17800
    },
    {
      "epoch": 2.9718004338394794,
      "grad_norm": 4.185088634490967,
      "learning_rate": 1.3126278118609408e-05,
      "loss": 0.5758,
      "step": 17810
    },
    {
      "epoch": 2.9734690472217586,
      "grad_norm": 7.272473335266113,
      "learning_rate": 1.3104976141785958e-05,
      "loss": 0.4652,
      "step": 17820
    },
    {
      "epoch": 2.9751376606040383,
      "grad_norm": 1.7599258422851562,
      "learning_rate": 1.308367416496251e-05,
      "loss": 0.397,
      "step": 17830
    },
    {
      "epoch": 2.9768062739863175,
      "grad_norm": 9.886456489562988,
      "learning_rate": 1.306237218813906e-05,
      "loss": 0.4978,
      "step": 17840
    },
    {
      "epoch": 2.9784748873685967,
      "grad_norm": 3.7498981952667236,
      "learning_rate": 1.3041070211315612e-05,
      "loss": 0.6552,
      "step": 17850
    },
    {
      "epoch": 2.980143500750876,
      "grad_norm": 1.584409236907959,
      "learning_rate": 1.301976823449216e-05,
      "loss": 0.2727,
      "step": 17860
    },
    {
      "epoch": 2.9818121141331555,
      "grad_norm": 4.549733638763428,
      "learning_rate": 1.2998466257668711e-05,
      "loss": 0.3539,
      "step": 17870
    },
    {
      "epoch": 2.9834807275154347,
      "grad_norm": 2.0360171794891357,
      "learning_rate": 1.2977164280845264e-05,
      "loss": 0.4962,
      "step": 17880
    },
    {
      "epoch": 2.985149340897714,
      "grad_norm": 10.569588661193848,
      "learning_rate": 1.2955862304021816e-05,
      "loss": 0.5727,
      "step": 17890
    },
    {
      "epoch": 2.986817954279993,
      "grad_norm": 0.24785248935222626,
      "learning_rate": 1.2934560327198364e-05,
      "loss": 0.4925,
      "step": 17900
    },
    {
      "epoch": 2.9884865676622727,
      "grad_norm": 7.342182636260986,
      "learning_rate": 1.2913258350374915e-05,
      "loss": 0.5465,
      "step": 17910
    },
    {
      "epoch": 2.990155181044552,
      "grad_norm": 6.249943256378174,
      "learning_rate": 1.2891956373551466e-05,
      "loss": 0.7378,
      "step": 17920
    },
    {
      "epoch": 2.991823794426831,
      "grad_norm": 6.216997146606445,
      "learning_rate": 1.2870654396728016e-05,
      "loss": 0.3691,
      "step": 17930
    },
    {
      "epoch": 2.9934924078091107,
      "grad_norm": 7.591912269592285,
      "learning_rate": 1.2849352419904567e-05,
      "loss": 0.6741,
      "step": 17940
    },
    {
      "epoch": 2.99516102119139,
      "grad_norm": 9.566725730895996,
      "learning_rate": 1.2828050443081119e-05,
      "loss": 0.779,
      "step": 17950
    },
    {
      "epoch": 2.996829634573669,
      "grad_norm": 6.660927772521973,
      "learning_rate": 1.280674846625767e-05,
      "loss": 0.2721,
      "step": 17960
    },
    {
      "epoch": 2.9984982479559488,
      "grad_norm": 6.241449356079102,
      "learning_rate": 1.278544648943422e-05,
      "loss": 0.6363,
      "step": 17970
    },
    {
      "epoch": 3.000166861338228,
      "grad_norm": 3.7319912910461426,
      "learning_rate": 1.2764144512610771e-05,
      "loss": 0.4792,
      "step": 17980
    },
    {
      "epoch": 3.001835474720507,
      "grad_norm": 13.831609725952148,
      "learning_rate": 1.2742842535787322e-05,
      "loss": 0.5068,
      "step": 17990
    },
    {
      "epoch": 3.003504088102787,
      "grad_norm": 5.385341167449951,
      "learning_rate": 1.2721540558963874e-05,
      "loss": 0.4361,
      "step": 18000
    },
    {
      "epoch": 3.005172701485066,
      "grad_norm": 1.2534297704696655,
      "learning_rate": 1.2700238582140423e-05,
      "loss": 0.4404,
      "step": 18010
    },
    {
      "epoch": 3.006841314867345,
      "grad_norm": 2.76770281791687,
      "learning_rate": 1.2678936605316975e-05,
      "loss": 0.5696,
      "step": 18020
    },
    {
      "epoch": 3.0085099282496244,
      "grad_norm": 8.435077667236328,
      "learning_rate": 1.2657634628493526e-05,
      "loss": 0.5176,
      "step": 18030
    },
    {
      "epoch": 3.010178541631904,
      "grad_norm": 5.4569807052612305,
      "learning_rate": 1.2636332651670077e-05,
      "loss": 0.4185,
      "step": 18040
    },
    {
      "epoch": 3.011847155014183,
      "grad_norm": 3.7880265712738037,
      "learning_rate": 1.2615030674846625e-05,
      "loss": 0.4108,
      "step": 18050
    },
    {
      "epoch": 3.0135157683964624,
      "grad_norm": 4.346493244171143,
      "learning_rate": 1.2593728698023176e-05,
      "loss": 0.2581,
      "step": 18060
    },
    {
      "epoch": 3.015184381778742,
      "grad_norm": 4.084135055541992,
      "learning_rate": 1.257242672119973e-05,
      "loss": 0.3993,
      "step": 18070
    },
    {
      "epoch": 3.0168529951610212,
      "grad_norm": 2.6235601902008057,
      "learning_rate": 1.2551124744376277e-05,
      "loss": 0.629,
      "step": 18080
    },
    {
      "epoch": 3.0185216085433004,
      "grad_norm": 4.292140007019043,
      "learning_rate": 1.2529822767552829e-05,
      "loss": 0.5749,
      "step": 18090
    },
    {
      "epoch": 3.0201902219255796,
      "grad_norm": 8.454764366149902,
      "learning_rate": 1.250852079072938e-05,
      "loss": 0.6641,
      "step": 18100
    },
    {
      "epoch": 3.0218588353078593,
      "grad_norm": 3.300248622894287,
      "learning_rate": 1.2487218813905931e-05,
      "loss": 0.3839,
      "step": 18110
    },
    {
      "epoch": 3.0235274486901385,
      "grad_norm": 7.582348346710205,
      "learning_rate": 1.2465916837082483e-05,
      "loss": 0.4126,
      "step": 18120
    },
    {
      "epoch": 3.0251960620724176,
      "grad_norm": 5.691225528717041,
      "learning_rate": 1.2444614860259032e-05,
      "loss": 0.5256,
      "step": 18130
    },
    {
      "epoch": 3.0268646754546973,
      "grad_norm": 3.4326331615448,
      "learning_rate": 1.2423312883435584e-05,
      "loss": 0.3309,
      "step": 18140
    },
    {
      "epoch": 3.0285332888369765,
      "grad_norm": 4.234714031219482,
      "learning_rate": 1.2402010906612133e-05,
      "loss": 0.4397,
      "step": 18150
    },
    {
      "epoch": 3.0302019022192557,
      "grad_norm": 3.2502951622009277,
      "learning_rate": 1.2380708929788686e-05,
      "loss": 0.4294,
      "step": 18160
    },
    {
      "epoch": 3.0318705156015353,
      "grad_norm": 8.674446105957031,
      "learning_rate": 1.2359406952965236e-05,
      "loss": 0.2684,
      "step": 18170
    },
    {
      "epoch": 3.0335391289838145,
      "grad_norm": 13.633523941040039,
      "learning_rate": 1.2338104976141787e-05,
      "loss": 0.5519,
      "step": 18180
    },
    {
      "epoch": 3.0352077423660937,
      "grad_norm": 4.6258544921875,
      "learning_rate": 1.2316802999318337e-05,
      "loss": 0.3735,
      "step": 18190
    },
    {
      "epoch": 3.036876355748373,
      "grad_norm": 2.7539236545562744,
      "learning_rate": 1.2295501022494888e-05,
      "loss": 0.4328,
      "step": 18200
    },
    {
      "epoch": 3.0385449691306525,
      "grad_norm": 6.656394004821777,
      "learning_rate": 1.227419904567144e-05,
      "loss": 0.6849,
      "step": 18210
    },
    {
      "epoch": 3.0402135825129317,
      "grad_norm": 10.021533966064453,
      "learning_rate": 1.225289706884799e-05,
      "loss": 0.3818,
      "step": 18220
    },
    {
      "epoch": 3.041882195895211,
      "grad_norm": 5.2366743087768555,
      "learning_rate": 1.223159509202454e-05,
      "loss": 0.5469,
      "step": 18230
    },
    {
      "epoch": 3.0435508092774906,
      "grad_norm": 9.992650985717773,
      "learning_rate": 1.221029311520109e-05,
      "loss": 0.6259,
      "step": 18240
    },
    {
      "epoch": 3.0452194226597697,
      "grad_norm": 14.71081829071045,
      "learning_rate": 1.2188991138377642e-05,
      "loss": 0.5111,
      "step": 18250
    },
    {
      "epoch": 3.046888036042049,
      "grad_norm": 2.0847959518432617,
      "learning_rate": 1.2167689161554193e-05,
      "loss": 0.3112,
      "step": 18260
    },
    {
      "epoch": 3.0485566494243286,
      "grad_norm": 7.616771697998047,
      "learning_rate": 1.2146387184730744e-05,
      "loss": 0.4777,
      "step": 18270
    },
    {
      "epoch": 3.0502252628066078,
      "grad_norm": 14.66061019897461,
      "learning_rate": 1.2125085207907294e-05,
      "loss": 0.7104,
      "step": 18280
    },
    {
      "epoch": 3.051893876188887,
      "grad_norm": 6.482998847961426,
      "learning_rate": 1.2103783231083845e-05,
      "loss": 0.7269,
      "step": 18290
    },
    {
      "epoch": 3.053562489571166,
      "grad_norm": 1.2105562686920166,
      "learning_rate": 1.2082481254260396e-05,
      "loss": 0.6916,
      "step": 18300
    },
    {
      "epoch": 3.055231102953446,
      "grad_norm": 1.539942741394043,
      "learning_rate": 1.2061179277436948e-05,
      "loss": 0.3559,
      "step": 18310
    },
    {
      "epoch": 3.056899716335725,
      "grad_norm": 9.760777473449707,
      "learning_rate": 1.2039877300613497e-05,
      "loss": 0.3867,
      "step": 18320
    },
    {
      "epoch": 3.058568329718004,
      "grad_norm": 5.33079719543457,
      "learning_rate": 1.2018575323790049e-05,
      "loss": 0.5453,
      "step": 18330
    },
    {
      "epoch": 3.060236943100284,
      "grad_norm": 11.251975059509277,
      "learning_rate": 1.1997273346966598e-05,
      "loss": 0.4791,
      "step": 18340
    },
    {
      "epoch": 3.061905556482563,
      "grad_norm": 12.620903968811035,
      "learning_rate": 1.197597137014315e-05,
      "loss": 0.6946,
      "step": 18350
    },
    {
      "epoch": 3.063574169864842,
      "grad_norm": 4.567543029785156,
      "learning_rate": 1.1954669393319701e-05,
      "loss": 0.575,
      "step": 18360
    },
    {
      "epoch": 3.065242783247122,
      "grad_norm": 1.0485655069351196,
      "learning_rate": 1.193336741649625e-05,
      "loss": 0.3871,
      "step": 18370
    },
    {
      "epoch": 3.066911396629401,
      "grad_norm": 6.796639442443848,
      "learning_rate": 1.1912065439672802e-05,
      "loss": 0.4701,
      "step": 18380
    },
    {
      "epoch": 3.0685800100116802,
      "grad_norm": 5.34151029586792,
      "learning_rate": 1.1890763462849352e-05,
      "loss": 0.61,
      "step": 18390
    },
    {
      "epoch": 3.0702486233939594,
      "grad_norm": 8.120566368103027,
      "learning_rate": 1.1869461486025905e-05,
      "loss": 0.5139,
      "step": 18400
    },
    {
      "epoch": 3.071917236776239,
      "grad_norm": 10.279892921447754,
      "learning_rate": 1.1848159509202454e-05,
      "loss": 0.4471,
      "step": 18410
    },
    {
      "epoch": 3.0735858501585183,
      "grad_norm": 0.7679576873779297,
      "learning_rate": 1.1826857532379006e-05,
      "loss": 0.4386,
      "step": 18420
    },
    {
      "epoch": 3.0752544635407975,
      "grad_norm": 1.7760995626449585,
      "learning_rate": 1.1805555555555555e-05,
      "loss": 0.5373,
      "step": 18430
    },
    {
      "epoch": 3.076923076923077,
      "grad_norm": 6.215763092041016,
      "learning_rate": 1.1784253578732107e-05,
      "loss": 0.7236,
      "step": 18440
    },
    {
      "epoch": 3.0785916903053563,
      "grad_norm": 11.10634994506836,
      "learning_rate": 1.1762951601908658e-05,
      "loss": 0.7473,
      "step": 18450
    },
    {
      "epoch": 3.0802603036876355,
      "grad_norm": 4.807831287384033,
      "learning_rate": 1.174164962508521e-05,
      "loss": 0.4516,
      "step": 18460
    },
    {
      "epoch": 3.0819289170699147,
      "grad_norm": 6.068482398986816,
      "learning_rate": 1.1720347648261759e-05,
      "loss": 0.4725,
      "step": 18470
    },
    {
      "epoch": 3.0835975304521943,
      "grad_norm": 4.445672988891602,
      "learning_rate": 1.169904567143831e-05,
      "loss": 0.7888,
      "step": 18480
    },
    {
      "epoch": 3.0852661438344735,
      "grad_norm": 1.2155346870422363,
      "learning_rate": 1.1677743694614862e-05,
      "loss": 0.5658,
      "step": 18490
    },
    {
      "epoch": 3.0869347572167527,
      "grad_norm": 17.333120346069336,
      "learning_rate": 1.1656441717791411e-05,
      "loss": 0.3008,
      "step": 18500
    },
    {
      "epoch": 3.0886033705990323,
      "grad_norm": 12.315832138061523,
      "learning_rate": 1.1635139740967963e-05,
      "loss": 0.407,
      "step": 18510
    },
    {
      "epoch": 3.0902719839813115,
      "grad_norm": 0.8245803117752075,
      "learning_rate": 1.1613837764144512e-05,
      "loss": 0.5018,
      "step": 18520
    },
    {
      "epoch": 3.0919405973635907,
      "grad_norm": 9.032975196838379,
      "learning_rate": 1.1592535787321063e-05,
      "loss": 0.5655,
      "step": 18530
    },
    {
      "epoch": 3.0936092107458704,
      "grad_norm": 7.493794918060303,
      "learning_rate": 1.1571233810497615e-05,
      "loss": 0.2861,
      "step": 18540
    },
    {
      "epoch": 3.0952778241281496,
      "grad_norm": 3.206437110900879,
      "learning_rate": 1.1549931833674166e-05,
      "loss": 0.7534,
      "step": 18550
    },
    {
      "epoch": 3.0969464375104288,
      "grad_norm": 3.384528875350952,
      "learning_rate": 1.1528629856850716e-05,
      "loss": 0.349,
      "step": 18560
    },
    {
      "epoch": 3.098615050892708,
      "grad_norm": 9.949050903320312,
      "learning_rate": 1.1507327880027267e-05,
      "loss": 0.4773,
      "step": 18570
    },
    {
      "epoch": 3.1002836642749876,
      "grad_norm": 3.5408034324645996,
      "learning_rate": 1.1486025903203817e-05,
      "loss": 0.3432,
      "step": 18580
    },
    {
      "epoch": 3.1019522776572668,
      "grad_norm": 1.8286900520324707,
      "learning_rate": 1.146472392638037e-05,
      "loss": 0.2989,
      "step": 18590
    },
    {
      "epoch": 3.103620891039546,
      "grad_norm": 6.582053184509277,
      "learning_rate": 1.144342194955692e-05,
      "loss": 0.5841,
      "step": 18600
    },
    {
      "epoch": 3.1052895044218256,
      "grad_norm": 9.348885536193848,
      "learning_rate": 1.142211997273347e-05,
      "loss": 0.669,
      "step": 18610
    },
    {
      "epoch": 3.106958117804105,
      "grad_norm": 10.235345840454102,
      "learning_rate": 1.140081799591002e-05,
      "loss": 0.3187,
      "step": 18620
    },
    {
      "epoch": 3.108626731186384,
      "grad_norm": 5.655196666717529,
      "learning_rate": 1.1379516019086572e-05,
      "loss": 0.3526,
      "step": 18630
    },
    {
      "epoch": 3.1102953445686636,
      "grad_norm": 8.670672416687012,
      "learning_rate": 1.1358214042263123e-05,
      "loss": 0.5711,
      "step": 18640
    },
    {
      "epoch": 3.111963957950943,
      "grad_norm": 5.48667049407959,
      "learning_rate": 1.1336912065439673e-05,
      "loss": 0.5326,
      "step": 18650
    },
    {
      "epoch": 3.113632571333222,
      "grad_norm": 3.8513548374176025,
      "learning_rate": 1.1315610088616224e-05,
      "loss": 0.517,
      "step": 18660
    },
    {
      "epoch": 3.115301184715501,
      "grad_norm": 1.9392753839492798,
      "learning_rate": 1.1294308111792774e-05,
      "loss": 0.1829,
      "step": 18670
    },
    {
      "epoch": 3.116969798097781,
      "grad_norm": 5.097808361053467,
      "learning_rate": 1.1273006134969327e-05,
      "loss": 0.4614,
      "step": 18680
    },
    {
      "epoch": 3.11863841148006,
      "grad_norm": 6.229975700378418,
      "learning_rate": 1.1251704158145876e-05,
      "loss": 0.596,
      "step": 18690
    },
    {
      "epoch": 3.1203070248623392,
      "grad_norm": 8.689419746398926,
      "learning_rate": 1.1230402181322428e-05,
      "loss": 0.5843,
      "step": 18700
    },
    {
      "epoch": 3.121975638244619,
      "grad_norm": 17.886335372924805,
      "learning_rate": 1.1209100204498977e-05,
      "loss": 0.9054,
      "step": 18710
    },
    {
      "epoch": 3.123644251626898,
      "grad_norm": 1.9985935688018799,
      "learning_rate": 1.1187798227675529e-05,
      "loss": 0.4802,
      "step": 18720
    },
    {
      "epoch": 3.1253128650091773,
      "grad_norm": 2.536592960357666,
      "learning_rate": 1.116649625085208e-05,
      "loss": 0.1868,
      "step": 18730
    },
    {
      "epoch": 3.126981478391457,
      "grad_norm": 2.6754794120788574,
      "learning_rate": 1.1145194274028631e-05,
      "loss": 0.3903,
      "step": 18740
    },
    {
      "epoch": 3.128650091773736,
      "grad_norm": 6.693356990814209,
      "learning_rate": 1.112389229720518e-05,
      "loss": 0.424,
      "step": 18750
    },
    {
      "epoch": 3.1303187051560153,
      "grad_norm": 23.3463077545166,
      "learning_rate": 1.1102590320381732e-05,
      "loss": 0.333,
      "step": 18760
    },
    {
      "epoch": 3.1319873185382945,
      "grad_norm": 3.0296764373779297,
      "learning_rate": 1.1081288343558282e-05,
      "loss": 0.3995,
      "step": 18770
    },
    {
      "epoch": 3.133655931920574,
      "grad_norm": 7.762906074523926,
      "learning_rate": 1.1059986366734835e-05,
      "loss": 0.6086,
      "step": 18780
    },
    {
      "epoch": 3.1353245453028533,
      "grad_norm": 3.276799201965332,
      "learning_rate": 1.1038684389911384e-05,
      "loss": 0.4661,
      "step": 18790
    },
    {
      "epoch": 3.1369931586851325,
      "grad_norm": 4.700891017913818,
      "learning_rate": 1.1017382413087934e-05,
      "loss": 0.4808,
      "step": 18800
    },
    {
      "epoch": 3.138661772067412,
      "grad_norm": 1.766147494316101,
      "learning_rate": 1.0996080436264485e-05,
      "loss": 0.3574,
      "step": 18810
    },
    {
      "epoch": 3.1403303854496913,
      "grad_norm": 6.901429653167725,
      "learning_rate": 1.0974778459441037e-05,
      "loss": 0.629,
      "step": 18820
    },
    {
      "epoch": 3.1419989988319705,
      "grad_norm": 2.338397741317749,
      "learning_rate": 1.0953476482617588e-05,
      "loss": 0.5115,
      "step": 18830
    },
    {
      "epoch": 3.1436676122142497,
      "grad_norm": 1.2265543937683105,
      "learning_rate": 1.0932174505794138e-05,
      "loss": 0.4987,
      "step": 18840
    },
    {
      "epoch": 3.1453362255965294,
      "grad_norm": 1.0522934198379517,
      "learning_rate": 1.0910872528970689e-05,
      "loss": 0.3906,
      "step": 18850
    },
    {
      "epoch": 3.1470048389788086,
      "grad_norm": 1.891143798828125,
      "learning_rate": 1.0889570552147239e-05,
      "loss": 0.2367,
      "step": 18860
    },
    {
      "epoch": 3.1486734523610878,
      "grad_norm": 10.891322135925293,
      "learning_rate": 1.0868268575323792e-05,
      "loss": 0.4863,
      "step": 18870
    },
    {
      "epoch": 3.1503420657433674,
      "grad_norm": 12.430221557617188,
      "learning_rate": 1.0846966598500341e-05,
      "loss": 0.3814,
      "step": 18880
    },
    {
      "epoch": 3.1520106791256466,
      "grad_norm": 10.277569770812988,
      "learning_rate": 1.0825664621676893e-05,
      "loss": 0.5771,
      "step": 18890
    },
    {
      "epoch": 3.153679292507926,
      "grad_norm": 4.823793888092041,
      "learning_rate": 1.0804362644853442e-05,
      "loss": 0.3911,
      "step": 18900
    },
    {
      "epoch": 3.1553479058902054,
      "grad_norm": 10.530961990356445,
      "learning_rate": 1.0783060668029994e-05,
      "loss": 0.6562,
      "step": 18910
    },
    {
      "epoch": 3.1570165192724846,
      "grad_norm": 4.148990631103516,
      "learning_rate": 1.0761758691206545e-05,
      "loss": 0.6843,
      "step": 18920
    },
    {
      "epoch": 3.158685132654764,
      "grad_norm": 6.806473255157471,
      "learning_rate": 1.0740456714383096e-05,
      "loss": 0.3289,
      "step": 18930
    },
    {
      "epoch": 3.1603537460370434,
      "grad_norm": 11.639883041381836,
      "learning_rate": 1.0719154737559646e-05,
      "loss": 0.5737,
      "step": 18940
    },
    {
      "epoch": 3.1620223594193226,
      "grad_norm": 1.892486333847046,
      "learning_rate": 1.0697852760736197e-05,
      "loss": 0.6566,
      "step": 18950
    },
    {
      "epoch": 3.163690972801602,
      "grad_norm": 0.6228060126304626,
      "learning_rate": 1.0676550783912747e-05,
      "loss": 0.3589,
      "step": 18960
    },
    {
      "epoch": 3.165359586183881,
      "grad_norm": 13.784774780273438,
      "learning_rate": 1.0655248807089298e-05,
      "loss": 0.4073,
      "step": 18970
    },
    {
      "epoch": 3.1670281995661607,
      "grad_norm": 11.488543510437012,
      "learning_rate": 1.063394683026585e-05,
      "loss": 0.3456,
      "step": 18980
    },
    {
      "epoch": 3.16869681294844,
      "grad_norm": 2.7645771503448486,
      "learning_rate": 1.06126448534424e-05,
      "loss": 0.3232,
      "step": 18990
    },
    {
      "epoch": 3.170365426330719,
      "grad_norm": 11.201484680175781,
      "learning_rate": 1.059134287661895e-05,
      "loss": 0.5757,
      "step": 19000
    },
    {
      "epoch": 3.1720340397129987,
      "grad_norm": 6.168903827667236,
      "learning_rate": 1.0570040899795502e-05,
      "loss": 0.6637,
      "step": 19010
    },
    {
      "epoch": 3.173702653095278,
      "grad_norm": 16.69882583618164,
      "learning_rate": 1.0548738922972053e-05,
      "loss": 0.4004,
      "step": 19020
    },
    {
      "epoch": 3.175371266477557,
      "grad_norm": 4.9656291007995605,
      "learning_rate": 1.0527436946148603e-05,
      "loss": 0.4972,
      "step": 19030
    },
    {
      "epoch": 3.1770398798598363,
      "grad_norm": 11.900189399719238,
      "learning_rate": 1.0506134969325154e-05,
      "loss": 0.379,
      "step": 19040
    },
    {
      "epoch": 3.178708493242116,
      "grad_norm": 8.267123222351074,
      "learning_rate": 1.0484832992501704e-05,
      "loss": 0.624,
      "step": 19050
    },
    {
      "epoch": 3.180377106624395,
      "grad_norm": 2.384697198867798,
      "learning_rate": 1.0463531015678257e-05,
      "loss": 0.4213,
      "step": 19060
    },
    {
      "epoch": 3.1820457200066743,
      "grad_norm": 15.840524673461914,
      "learning_rate": 1.0442229038854806e-05,
      "loss": 0.2648,
      "step": 19070
    },
    {
      "epoch": 3.183714333388954,
      "grad_norm": 1.0072517395019531,
      "learning_rate": 1.0420927062031358e-05,
      "loss": 0.4095,
      "step": 19080
    },
    {
      "epoch": 3.185382946771233,
      "grad_norm": 7.0250043869018555,
      "learning_rate": 1.0399625085207907e-05,
      "loss": 0.9149,
      "step": 19090
    },
    {
      "epoch": 3.1870515601535123,
      "grad_norm": 15.577765464782715,
      "learning_rate": 1.0378323108384459e-05,
      "loss": 0.5042,
      "step": 19100
    },
    {
      "epoch": 3.188720173535792,
      "grad_norm": 0.5109562277793884,
      "learning_rate": 1.035702113156101e-05,
      "loss": 0.3563,
      "step": 19110
    },
    {
      "epoch": 3.190388786918071,
      "grad_norm": 5.369447708129883,
      "learning_rate": 1.033571915473756e-05,
      "loss": 0.6149,
      "step": 19120
    },
    {
      "epoch": 3.1920574003003503,
      "grad_norm": 10.52492618560791,
      "learning_rate": 1.0314417177914111e-05,
      "loss": 0.5008,
      "step": 19130
    },
    {
      "epoch": 3.1937260136826295,
      "grad_norm": 10.884807586669922,
      "learning_rate": 1.029311520109066e-05,
      "loss": 0.3725,
      "step": 19140
    },
    {
      "epoch": 3.195394627064909,
      "grad_norm": 6.707684516906738,
      "learning_rate": 1.0271813224267212e-05,
      "loss": 0.441,
      "step": 19150
    },
    {
      "epoch": 3.1970632404471884,
      "grad_norm": 11.519969940185547,
      "learning_rate": 1.0250511247443763e-05,
      "loss": 0.4945,
      "step": 19160
    },
    {
      "epoch": 3.1987318538294676,
      "grad_norm": 12.577363014221191,
      "learning_rate": 1.0229209270620315e-05,
      "loss": 0.5444,
      "step": 19170
    },
    {
      "epoch": 3.200400467211747,
      "grad_norm": 6.666540145874023,
      "learning_rate": 1.0207907293796864e-05,
      "loss": 0.517,
      "step": 19180
    },
    {
      "epoch": 3.2020690805940264,
      "grad_norm": 17.633865356445312,
      "learning_rate": 1.0186605316973416e-05,
      "loss": 0.3478,
      "step": 19190
    },
    {
      "epoch": 3.2037376939763056,
      "grad_norm": 9.218214988708496,
      "learning_rate": 1.0165303340149967e-05,
      "loss": 0.4441,
      "step": 19200
    },
    {
      "epoch": 3.205406307358585,
      "grad_norm": 3.6328349113464355,
      "learning_rate": 1.0144001363326518e-05,
      "loss": 0.4889,
      "step": 19210
    },
    {
      "epoch": 3.2070749207408644,
      "grad_norm": 5.312924861907959,
      "learning_rate": 1.0122699386503068e-05,
      "loss": 0.6208,
      "step": 19220
    },
    {
      "epoch": 3.2087435341231436,
      "grad_norm": 7.411215782165527,
      "learning_rate": 1.010139740967962e-05,
      "loss": 0.5025,
      "step": 19230
    },
    {
      "epoch": 3.210412147505423,
      "grad_norm": 7.567404270172119,
      "learning_rate": 1.0080095432856169e-05,
      "loss": 0.4341,
      "step": 19240
    },
    {
      "epoch": 3.2120807608877024,
      "grad_norm": 6.1931681632995605,
      "learning_rate": 1.005879345603272e-05,
      "loss": 0.5613,
      "step": 19250
    },
    {
      "epoch": 3.2137493742699816,
      "grad_norm": 16.062477111816406,
      "learning_rate": 1.0037491479209271e-05,
      "loss": 0.5084,
      "step": 19260
    },
    {
      "epoch": 3.215417987652261,
      "grad_norm": 11.040671348571777,
      "learning_rate": 1.0016189502385821e-05,
      "loss": 0.5553,
      "step": 19270
    },
    {
      "epoch": 3.2170866010345405,
      "grad_norm": 8.398287773132324,
      "learning_rate": 9.994887525562372e-06,
      "loss": 0.5001,
      "step": 19280
    },
    {
      "epoch": 3.2187552144168197,
      "grad_norm": 3.7937874794006348,
      "learning_rate": 9.973585548738922e-06,
      "loss": 0.4999,
      "step": 19290
    },
    {
      "epoch": 3.220423827799099,
      "grad_norm": 5.21066427230835,
      "learning_rate": 9.952283571915475e-06,
      "loss": 0.4269,
      "step": 19300
    },
    {
      "epoch": 3.2220924411813785,
      "grad_norm": 2.8375027179718018,
      "learning_rate": 9.930981595092025e-06,
      "loss": 0.4907,
      "step": 19310
    },
    {
      "epoch": 3.2237610545636577,
      "grad_norm": 3.844062089920044,
      "learning_rate": 9.909679618268576e-06,
      "loss": 0.7233,
      "step": 19320
    },
    {
      "epoch": 3.225429667945937,
      "grad_norm": 3.162228584289551,
      "learning_rate": 9.888377641445126e-06,
      "loss": 0.3196,
      "step": 19330
    },
    {
      "epoch": 3.227098281328216,
      "grad_norm": 7.516396522521973,
      "learning_rate": 9.867075664621677e-06,
      "loss": 0.2607,
      "step": 19340
    },
    {
      "epoch": 3.2287668947104957,
      "grad_norm": 1.2351359128952026,
      "learning_rate": 9.845773687798228e-06,
      "loss": 0.5545,
      "step": 19350
    },
    {
      "epoch": 3.230435508092775,
      "grad_norm": 13.38477611541748,
      "learning_rate": 9.82447171097478e-06,
      "loss": 0.3997,
      "step": 19360
    },
    {
      "epoch": 3.232104121475054,
      "grad_norm": 1.0525535345077515,
      "learning_rate": 9.80316973415133e-06,
      "loss": 0.4182,
      "step": 19370
    },
    {
      "epoch": 3.2337727348573337,
      "grad_norm": 7.93346643447876,
      "learning_rate": 9.78186775732788e-06,
      "loss": 0.6486,
      "step": 19380
    },
    {
      "epoch": 3.235441348239613,
      "grad_norm": 15.896560668945312,
      "learning_rate": 9.760565780504432e-06,
      "loss": 0.6042,
      "step": 19390
    },
    {
      "epoch": 3.237109961621892,
      "grad_norm": 15.919732093811035,
      "learning_rate": 9.739263803680983e-06,
      "loss": 0.5242,
      "step": 19400
    },
    {
      "epoch": 3.2387785750041713,
      "grad_norm": 6.96088171005249,
      "learning_rate": 9.717961826857533e-06,
      "loss": 0.6175,
      "step": 19410
    },
    {
      "epoch": 3.240447188386451,
      "grad_norm": 3.7081427574157715,
      "learning_rate": 9.696659850034083e-06,
      "loss": 0.4009,
      "step": 19420
    },
    {
      "epoch": 3.24211580176873,
      "grad_norm": 3.5470900535583496,
      "learning_rate": 9.675357873210634e-06,
      "loss": 0.4835,
      "step": 19430
    },
    {
      "epoch": 3.2437844151510093,
      "grad_norm": 0.5693513751029968,
      "learning_rate": 9.654055896387185e-06,
      "loss": 0.4241,
      "step": 19440
    },
    {
      "epoch": 3.245453028533289,
      "grad_norm": 9.123953819274902,
      "learning_rate": 9.632753919563737e-06,
      "loss": 0.8194,
      "step": 19450
    },
    {
      "epoch": 3.247121641915568,
      "grad_norm": 7.230930805206299,
      "learning_rate": 9.611451942740286e-06,
      "loss": 0.4954,
      "step": 19460
    },
    {
      "epoch": 3.2487902552978474,
      "grad_norm": 11.628961563110352,
      "learning_rate": 9.590149965916838e-06,
      "loss": 0.2654,
      "step": 19470
    },
    {
      "epoch": 3.250458868680127,
      "grad_norm": 6.044198036193848,
      "learning_rate": 9.568847989093387e-06,
      "loss": 0.5281,
      "step": 19480
    },
    {
      "epoch": 3.252127482062406,
      "grad_norm": 12.579113960266113,
      "learning_rate": 9.54754601226994e-06,
      "loss": 0.4726,
      "step": 19490
    },
    {
      "epoch": 3.2537960954446854,
      "grad_norm": 4.393145561218262,
      "learning_rate": 9.52624403544649e-06,
      "loss": 0.5297,
      "step": 19500
    },
    {
      "epoch": 3.255464708826965,
      "grad_norm": 11.251328468322754,
      "learning_rate": 9.504942058623041e-06,
      "loss": 0.5512,
      "step": 19510
    },
    {
      "epoch": 3.2571333222092442,
      "grad_norm": 9.94466781616211,
      "learning_rate": 9.48364008179959e-06,
      "loss": 0.6454,
      "step": 19520
    },
    {
      "epoch": 3.2588019355915234,
      "grad_norm": 4.623412609100342,
      "learning_rate": 9.462338104976142e-06,
      "loss": 0.3182,
      "step": 19530
    },
    {
      "epoch": 3.2604705489738026,
      "grad_norm": 1.1582998037338257,
      "learning_rate": 9.441036128152693e-06,
      "loss": 0.4357,
      "step": 19540
    },
    {
      "epoch": 3.2621391623560823,
      "grad_norm": 8.294568061828613,
      "learning_rate": 9.419734151329245e-06,
      "loss": 0.485,
      "step": 19550
    },
    {
      "epoch": 3.2638077757383614,
      "grad_norm": 11.77527904510498,
      "learning_rate": 9.398432174505794e-06,
      "loss": 0.512,
      "step": 19560
    },
    {
      "epoch": 3.2654763891206406,
      "grad_norm": 9.942558288574219,
      "learning_rate": 9.377130197682344e-06,
      "loss": 0.6592,
      "step": 19570
    },
    {
      "epoch": 3.26714500250292,
      "grad_norm": 2.9154770374298096,
      "learning_rate": 9.355828220858897e-06,
      "loss": 0.3862,
      "step": 19580
    },
    {
      "epoch": 3.2688136158851995,
      "grad_norm": 11.433850288391113,
      "learning_rate": 9.334526244035447e-06,
      "loss": 0.7356,
      "step": 19590
    },
    {
      "epoch": 3.2704822292674787,
      "grad_norm": 7.236981391906738,
      "learning_rate": 9.313224267211998e-06,
      "loss": 0.4728,
      "step": 19600
    },
    {
      "epoch": 3.272150842649758,
      "grad_norm": 1.4062271118164062,
      "learning_rate": 9.291922290388548e-06,
      "loss": 0.3676,
      "step": 19610
    },
    {
      "epoch": 3.2738194560320375,
      "grad_norm": 1.3578187227249146,
      "learning_rate": 9.270620313565099e-06,
      "loss": 0.3002,
      "step": 19620
    },
    {
      "epoch": 3.2754880694143167,
      "grad_norm": 13.07825756072998,
      "learning_rate": 9.24931833674165e-06,
      "loss": 0.4828,
      "step": 19630
    },
    {
      "epoch": 3.277156682796596,
      "grad_norm": 3.7212138175964355,
      "learning_rate": 9.228016359918202e-06,
      "loss": 0.1761,
      "step": 19640
    },
    {
      "epoch": 3.2788252961788755,
      "grad_norm": 0.42773526906967163,
      "learning_rate": 9.206714383094751e-06,
      "loss": 0.435,
      "step": 19650
    },
    {
      "epoch": 3.2804939095611547,
      "grad_norm": 2.0816519260406494,
      "learning_rate": 9.185412406271303e-06,
      "loss": 0.707,
      "step": 19660
    },
    {
      "epoch": 3.282162522943434,
      "grad_norm": 7.279458999633789,
      "learning_rate": 9.164110429447852e-06,
      "loss": 0.4311,
      "step": 19670
    },
    {
      "epoch": 3.2838311363257136,
      "grad_norm": 0.775719404220581,
      "learning_rate": 9.142808452624405e-06,
      "loss": 0.5449,
      "step": 19680
    },
    {
      "epoch": 3.2854997497079927,
      "grad_norm": 0.9994083046913147,
      "learning_rate": 9.121506475800955e-06,
      "loss": 0.384,
      "step": 19690
    },
    {
      "epoch": 3.287168363090272,
      "grad_norm": 7.851065635681152,
      "learning_rate": 9.100204498977506e-06,
      "loss": 0.4598,
      "step": 19700
    },
    {
      "epoch": 3.288836976472551,
      "grad_norm": 11.786378860473633,
      "learning_rate": 9.078902522154056e-06,
      "loss": 0.4987,
      "step": 19710
    },
    {
      "epoch": 3.2905055898548308,
      "grad_norm": 4.102958679199219,
      "learning_rate": 9.057600545330607e-06,
      "loss": 0.4003,
      "step": 19720
    },
    {
      "epoch": 3.29217420323711,
      "grad_norm": 24.16107749938965,
      "learning_rate": 9.036298568507159e-06,
      "loss": 0.4616,
      "step": 19730
    },
    {
      "epoch": 3.293842816619389,
      "grad_norm": 13.60019588470459,
      "learning_rate": 9.014996591683708e-06,
      "loss": 0.4286,
      "step": 19740
    },
    {
      "epoch": 3.295511430001669,
      "grad_norm": 6.350454807281494,
      "learning_rate": 8.99369461486026e-06,
      "loss": 0.2666,
      "step": 19750
    },
    {
      "epoch": 3.297180043383948,
      "grad_norm": 4.399065017700195,
      "learning_rate": 8.972392638036809e-06,
      "loss": 0.4542,
      "step": 19760
    },
    {
      "epoch": 3.298848656766227,
      "grad_norm": 9.02160358428955,
      "learning_rate": 8.951090661213362e-06,
      "loss": 0.4937,
      "step": 19770
    },
    {
      "epoch": 3.3005172701485064,
      "grad_norm": 11.292675971984863,
      "learning_rate": 8.929788684389912e-06,
      "loss": 0.4894,
      "step": 19780
    },
    {
      "epoch": 3.302185883530786,
      "grad_norm": 2.309056282043457,
      "learning_rate": 8.908486707566463e-06,
      "loss": 0.4413,
      "step": 19790
    },
    {
      "epoch": 3.303854496913065,
      "grad_norm": 3.792529582977295,
      "learning_rate": 8.887184730743013e-06,
      "loss": 0.5821,
      "step": 19800
    },
    {
      "epoch": 3.3055231102953444,
      "grad_norm": 9.908549308776855,
      "learning_rate": 8.865882753919564e-06,
      "loss": 0.7987,
      "step": 19810
    },
    {
      "epoch": 3.307191723677624,
      "grad_norm": 6.892454624176025,
      "learning_rate": 8.844580777096115e-06,
      "loss": 0.4921,
      "step": 19820
    },
    {
      "epoch": 3.3088603370599032,
      "grad_norm": 6.392815113067627,
      "learning_rate": 8.823278800272667e-06,
      "loss": 0.5581,
      "step": 19830
    },
    {
      "epoch": 3.3105289504421824,
      "grad_norm": 11.673991203308105,
      "learning_rate": 8.801976823449216e-06,
      "loss": 0.6116,
      "step": 19840
    },
    {
      "epoch": 3.312197563824462,
      "grad_norm": 7.070014476776123,
      "learning_rate": 8.780674846625768e-06,
      "loss": 0.3641,
      "step": 19850
    },
    {
      "epoch": 3.3138661772067413,
      "grad_norm": 14.025489807128906,
      "learning_rate": 8.759372869802317e-06,
      "loss": 0.3909,
      "step": 19860
    },
    {
      "epoch": 3.3155347905890205,
      "grad_norm": 4.366896152496338,
      "learning_rate": 8.738070892978869e-06,
      "loss": 0.774,
      "step": 19870
    },
    {
      "epoch": 3.3172034039713,
      "grad_norm": 8.533666610717773,
      "learning_rate": 8.71676891615542e-06,
      "loss": 0.5405,
      "step": 19880
    },
    {
      "epoch": 3.3188720173535793,
      "grad_norm": 0.8106459975242615,
      "learning_rate": 8.69546693933197e-06,
      "loss": 0.473,
      "step": 19890
    },
    {
      "epoch": 3.3205406307358585,
      "grad_norm": 4.1545867919921875,
      "learning_rate": 8.674164962508521e-06,
      "loss": 0.4317,
      "step": 19900
    },
    {
      "epoch": 3.3222092441181377,
      "grad_norm": 6.912739276885986,
      "learning_rate": 8.652862985685072e-06,
      "loss": 0.5974,
      "step": 19910
    },
    {
      "epoch": 3.3238778575004173,
      "grad_norm": 13.521202087402344,
      "learning_rate": 8.631561008861624e-06,
      "loss": 0.5171,
      "step": 19920
    },
    {
      "epoch": 3.3255464708826965,
      "grad_norm": 12.088569641113281,
      "learning_rate": 8.610259032038173e-06,
      "loss": 0.4588,
      "step": 19930
    },
    {
      "epoch": 3.3272150842649757,
      "grad_norm": 5.037559509277344,
      "learning_rate": 8.588957055214725e-06,
      "loss": 0.3358,
      "step": 19940
    },
    {
      "epoch": 3.328883697647255,
      "grad_norm": 7.991013050079346,
      "learning_rate": 8.567655078391274e-06,
      "loss": 0.6313,
      "step": 19950
    },
    {
      "epoch": 3.3305523110295345,
      "grad_norm": 11.982336044311523,
      "learning_rate": 8.546353101567826e-06,
      "loss": 0.4029,
      "step": 19960
    },
    {
      "epoch": 3.3322209244118137,
      "grad_norm": 6.457071304321289,
      "learning_rate": 8.525051124744377e-06,
      "loss": 0.6879,
      "step": 19970
    },
    {
      "epoch": 3.333889537794093,
      "grad_norm": 9.678885459899902,
      "learning_rate": 8.503749147920928e-06,
      "loss": 0.6423,
      "step": 19980
    },
    {
      "epoch": 3.3355581511763726,
      "grad_norm": 6.9636921882629395,
      "learning_rate": 8.482447171097478e-06,
      "loss": 0.4473,
      "step": 19990
    },
    {
      "epoch": 3.3372267645586517,
      "grad_norm": 8.325057983398438,
      "learning_rate": 8.461145194274029e-06,
      "loss": 0.3543,
      "step": 20000
    },
    {
      "epoch": 3.338895377940931,
      "grad_norm": 9.587640762329102,
      "learning_rate": 8.43984321745058e-06,
      "loss": 0.7148,
      "step": 20010
    },
    {
      "epoch": 3.3405639913232106,
      "grad_norm": 7.602834701538086,
      "learning_rate": 8.41854124062713e-06,
      "loss": 0.3364,
      "step": 20020
    },
    {
      "epoch": 3.3422326047054898,
      "grad_norm": 8.159320831298828,
      "learning_rate": 8.397239263803681e-06,
      "loss": 0.8777,
      "step": 20030
    },
    {
      "epoch": 3.343901218087769,
      "grad_norm": 6.853409767150879,
      "learning_rate": 8.375937286980231e-06,
      "loss": 0.4888,
      "step": 20040
    },
    {
      "epoch": 3.3455698314700486,
      "grad_norm": 5.302191257476807,
      "learning_rate": 8.354635310156782e-06,
      "loss": 0.3783,
      "step": 20050
    },
    {
      "epoch": 3.347238444852328,
      "grad_norm": 8.235637664794922,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.4951,
      "step": 20060
    },
    {
      "epoch": 3.348907058234607,
      "grad_norm": 4.505258560180664,
      "learning_rate": 8.312031356509885e-06,
      "loss": 0.6505,
      "step": 20070
    },
    {
      "epoch": 3.350575671616886,
      "grad_norm": 8.21998119354248,
      "learning_rate": 8.290729379686435e-06,
      "loss": 0.3854,
      "step": 20080
    },
    {
      "epoch": 3.352244284999166,
      "grad_norm": 3.092179775238037,
      "learning_rate": 8.269427402862986e-06,
      "loss": 0.4884,
      "step": 20090
    },
    {
      "epoch": 3.353912898381445,
      "grad_norm": 5.269024848937988,
      "learning_rate": 8.248125426039537e-06,
      "loss": 0.6881,
      "step": 20100
    },
    {
      "epoch": 3.355581511763724,
      "grad_norm": 9.276659965515137,
      "learning_rate": 8.226823449216089e-06,
      "loss": 0.5216,
      "step": 20110
    },
    {
      "epoch": 3.357250125146004,
      "grad_norm": 13.353070259094238,
      "learning_rate": 8.205521472392638e-06,
      "loss": 0.5394,
      "step": 20120
    },
    {
      "epoch": 3.358918738528283,
      "grad_norm": 19.568077087402344,
      "learning_rate": 8.18421949556919e-06,
      "loss": 0.5194,
      "step": 20130
    },
    {
      "epoch": 3.3605873519105622,
      "grad_norm": 9.051192283630371,
      "learning_rate": 8.16291751874574e-06,
      "loss": 0.3138,
      "step": 20140
    },
    {
      "epoch": 3.3622559652928414,
      "grad_norm": 8.112833023071289,
      "learning_rate": 8.14161554192229e-06,
      "loss": 0.3888,
      "step": 20150
    },
    {
      "epoch": 3.363924578675121,
      "grad_norm": 1.9778501987457275,
      "learning_rate": 8.120313565098842e-06,
      "loss": 0.8043,
      "step": 20160
    },
    {
      "epoch": 3.3655931920574003,
      "grad_norm": 10.137051582336426,
      "learning_rate": 8.099011588275392e-06,
      "loss": 0.5564,
      "step": 20170
    },
    {
      "epoch": 3.3672618054396795,
      "grad_norm": 7.589290142059326,
      "learning_rate": 8.077709611451943e-06,
      "loss": 0.4631,
      "step": 20180
    },
    {
      "epoch": 3.368930418821959,
      "grad_norm": 6.906275749206543,
      "learning_rate": 8.056407634628493e-06,
      "loss": 0.7186,
      "step": 20190
    },
    {
      "epoch": 3.3705990322042383,
      "grad_norm": 7.309839725494385,
      "learning_rate": 8.035105657805046e-06,
      "loss": 0.3439,
      "step": 20200
    },
    {
      "epoch": 3.3722676455865175,
      "grad_norm": 10.972208976745605,
      "learning_rate": 8.013803680981595e-06,
      "loss": 0.5749,
      "step": 20210
    },
    {
      "epoch": 3.373936258968797,
      "grad_norm": 8.345133781433105,
      "learning_rate": 7.992501704158147e-06,
      "loss": 0.734,
      "step": 20220
    },
    {
      "epoch": 3.3756048723510763,
      "grad_norm": 8.025596618652344,
      "learning_rate": 7.971199727334696e-06,
      "loss": 0.6486,
      "step": 20230
    },
    {
      "epoch": 3.3772734857333555,
      "grad_norm": 7.102121353149414,
      "learning_rate": 7.949897750511247e-06,
      "loss": 0.5632,
      "step": 20240
    },
    {
      "epoch": 3.378942099115635,
      "grad_norm": 12.703946113586426,
      "learning_rate": 7.928595773687799e-06,
      "loss": 0.5437,
      "step": 20250
    },
    {
      "epoch": 3.3806107124979143,
      "grad_norm": 18.989023208618164,
      "learning_rate": 7.90729379686435e-06,
      "loss": 0.5727,
      "step": 20260
    },
    {
      "epoch": 3.3822793258801935,
      "grad_norm": 2.3417904376983643,
      "learning_rate": 7.8859918200409e-06,
      "loss": 0.5536,
      "step": 20270
    },
    {
      "epoch": 3.3839479392624727,
      "grad_norm": 12.218836784362793,
      "learning_rate": 7.864689843217451e-06,
      "loss": 0.7262,
      "step": 20280
    },
    {
      "epoch": 3.3856165526447524,
      "grad_norm": 5.269259452819824,
      "learning_rate": 7.843387866394002e-06,
      "loss": 0.2656,
      "step": 20290
    },
    {
      "epoch": 3.3872851660270316,
      "grad_norm": 6.50781774520874,
      "learning_rate": 7.822085889570554e-06,
      "loss": 0.4883,
      "step": 20300
    },
    {
      "epoch": 3.3889537794093108,
      "grad_norm": 6.884058952331543,
      "learning_rate": 7.800783912747103e-06,
      "loss": 0.2911,
      "step": 20310
    },
    {
      "epoch": 3.39062239279159,
      "grad_norm": 18.308387756347656,
      "learning_rate": 7.779481935923653e-06,
      "loss": 0.3936,
      "step": 20320
    },
    {
      "epoch": 3.3922910061738696,
      "grad_norm": 10.86668586730957,
      "learning_rate": 7.758179959100204e-06,
      "loss": 0.3046,
      "step": 20330
    },
    {
      "epoch": 3.3939596195561488,
      "grad_norm": 10.314988136291504,
      "learning_rate": 7.736877982276756e-06,
      "loss": 0.6518,
      "step": 20340
    },
    {
      "epoch": 3.395628232938428,
      "grad_norm": 6.898045539855957,
      "learning_rate": 7.715576005453307e-06,
      "loss": 0.6532,
      "step": 20350
    },
    {
      "epoch": 3.3972968463207076,
      "grad_norm": 7.160792827606201,
      "learning_rate": 7.694274028629857e-06,
      "loss": 0.5366,
      "step": 20360
    },
    {
      "epoch": 3.398965459702987,
      "grad_norm": 3.381019353866577,
      "learning_rate": 7.672972051806408e-06,
      "loss": 0.3668,
      "step": 20370
    },
    {
      "epoch": 3.400634073085266,
      "grad_norm": 5.768394470214844,
      "learning_rate": 7.651670074982958e-06,
      "loss": 0.3583,
      "step": 20380
    },
    {
      "epoch": 3.4023026864675456,
      "grad_norm": 0.5005230903625488,
      "learning_rate": 7.63036809815951e-06,
      "loss": 0.5497,
      "step": 20390
    },
    {
      "epoch": 3.403971299849825,
      "grad_norm": 7.085357189178467,
      "learning_rate": 7.60906612133606e-06,
      "loss": 0.5054,
      "step": 20400
    },
    {
      "epoch": 3.405639913232104,
      "grad_norm": 14.85867691040039,
      "learning_rate": 7.587764144512612e-06,
      "loss": 0.2449,
      "step": 20410
    },
    {
      "epoch": 3.4073085266143837,
      "grad_norm": 1.3758444786071777,
      "learning_rate": 7.566462167689162e-06,
      "loss": 0.4304,
      "step": 20420
    },
    {
      "epoch": 3.408977139996663,
      "grad_norm": 3.860492467880249,
      "learning_rate": 7.545160190865713e-06,
      "loss": 0.6046,
      "step": 20430
    },
    {
      "epoch": 3.410645753378942,
      "grad_norm": 5.917075157165527,
      "learning_rate": 7.523858214042263e-06,
      "loss": 0.5873,
      "step": 20440
    },
    {
      "epoch": 3.4123143667612212,
      "grad_norm": 12.034058570861816,
      "learning_rate": 7.502556237218815e-06,
      "loss": 0.3668,
      "step": 20450
    },
    {
      "epoch": 3.413982980143501,
      "grad_norm": 8.98190975189209,
      "learning_rate": 7.481254260395365e-06,
      "loss": 0.9005,
      "step": 20460
    },
    {
      "epoch": 3.41565159352578,
      "grad_norm": 4.096084117889404,
      "learning_rate": 7.459952283571915e-06,
      "loss": 0.5491,
      "step": 20470
    },
    {
      "epoch": 3.4173202069080593,
      "grad_norm": 5.603082656860352,
      "learning_rate": 7.438650306748467e-06,
      "loss": 0.6159,
      "step": 20480
    },
    {
      "epoch": 3.418988820290339,
      "grad_norm": 8.483658790588379,
      "learning_rate": 7.417348329925017e-06,
      "loss": 0.3281,
      "step": 20490
    },
    {
      "epoch": 3.420657433672618,
      "grad_norm": 2.905503749847412,
      "learning_rate": 7.3960463531015685e-06,
      "loss": 0.6247,
      "step": 20500
    },
    {
      "epoch": 3.4223260470548973,
      "grad_norm": 7.6717305183410645,
      "learning_rate": 7.374744376278118e-06,
      "loss": 0.573,
      "step": 20510
    },
    {
      "epoch": 3.4239946604371765,
      "grad_norm": 9.106402397155762,
      "learning_rate": 7.35344239945467e-06,
      "loss": 0.7608,
      "step": 20520
    },
    {
      "epoch": 3.425663273819456,
      "grad_norm": 9.2079439163208,
      "learning_rate": 7.33214042263122e-06,
      "loss": 0.4144,
      "step": 20530
    },
    {
      "epoch": 3.4273318872017353,
      "grad_norm": 6.889760494232178,
      "learning_rate": 7.310838445807771e-06,
      "loss": 0.4607,
      "step": 20540
    },
    {
      "epoch": 3.4290005005840145,
      "grad_norm": 3.285325527191162,
      "learning_rate": 7.289536468984322e-06,
      "loss": 0.6243,
      "step": 20550
    },
    {
      "epoch": 3.430669113966294,
      "grad_norm": 10.770541191101074,
      "learning_rate": 7.268234492160873e-06,
      "loss": 0.5366,
      "step": 20560
    },
    {
      "epoch": 3.4323377273485733,
      "grad_norm": 6.717711448669434,
      "learning_rate": 7.2469325153374235e-06,
      "loss": 0.5147,
      "step": 20570
    },
    {
      "epoch": 3.4340063407308525,
      "grad_norm": 3.8679230213165283,
      "learning_rate": 7.225630538513975e-06,
      "loss": 0.5406,
      "step": 20580
    },
    {
      "epoch": 3.435674954113132,
      "grad_norm": 4.813882350921631,
      "learning_rate": 7.204328561690525e-06,
      "loss": 0.6782,
      "step": 20590
    },
    {
      "epoch": 3.4373435674954114,
      "grad_norm": 19.337759017944336,
      "learning_rate": 7.183026584867077e-06,
      "loss": 0.4207,
      "step": 20600
    },
    {
      "epoch": 3.4390121808776906,
      "grad_norm": 17.784420013427734,
      "learning_rate": 7.161724608043627e-06,
      "loss": 0.5631,
      "step": 20610
    },
    {
      "epoch": 3.44068079425997,
      "grad_norm": 6.6541032791137695,
      "learning_rate": 7.140422631220177e-06,
      "loss": 0.3383,
      "step": 20620
    },
    {
      "epoch": 3.4423494076422494,
      "grad_norm": 0.2335728406906128,
      "learning_rate": 7.119120654396728e-06,
      "loss": 0.282,
      "step": 20630
    },
    {
      "epoch": 3.4440180210245286,
      "grad_norm": 6.851978302001953,
      "learning_rate": 7.097818677573279e-06,
      "loss": 0.4586,
      "step": 20640
    },
    {
      "epoch": 3.445686634406808,
      "grad_norm": 9.944047927856445,
      "learning_rate": 7.07651670074983e-06,
      "loss": 0.373,
      "step": 20650
    },
    {
      "epoch": 3.4473552477890874,
      "grad_norm": 13.17838191986084,
      "learning_rate": 7.05521472392638e-06,
      "loss": 0.5418,
      "step": 20660
    },
    {
      "epoch": 3.4490238611713666,
      "grad_norm": 10.554582595825195,
      "learning_rate": 7.033912747102932e-06,
      "loss": 0.5597,
      "step": 20670
    },
    {
      "epoch": 3.450692474553646,
      "grad_norm": 7.08922815322876,
      "learning_rate": 7.012610770279482e-06,
      "loss": 0.4524,
      "step": 20680
    },
    {
      "epoch": 3.452361087935925,
      "grad_norm": 8.339189529418945,
      "learning_rate": 6.9913087934560335e-06,
      "loss": 0.2889,
      "step": 20690
    },
    {
      "epoch": 3.4540297013182046,
      "grad_norm": 9.44381332397461,
      "learning_rate": 6.970006816632583e-06,
      "loss": 0.4716,
      "step": 20700
    },
    {
      "epoch": 3.455698314700484,
      "grad_norm": 10.483624458312988,
      "learning_rate": 6.948704839809135e-06,
      "loss": 0.665,
      "step": 20710
    },
    {
      "epoch": 3.457366928082763,
      "grad_norm": 3.7900354862213135,
      "learning_rate": 6.927402862985685e-06,
      "loss": 0.5295,
      "step": 20720
    },
    {
      "epoch": 3.4590355414650427,
      "grad_norm": 11.814669609069824,
      "learning_rate": 6.906100886162236e-06,
      "loss": 0.3287,
      "step": 20730
    },
    {
      "epoch": 3.460704154847322,
      "grad_norm": 10.034854888916016,
      "learning_rate": 6.884798909338787e-06,
      "loss": 0.6829,
      "step": 20740
    },
    {
      "epoch": 3.462372768229601,
      "grad_norm": 4.252172470092773,
      "learning_rate": 6.863496932515338e-06,
      "loss": 0.4345,
      "step": 20750
    },
    {
      "epoch": 3.4640413816118807,
      "grad_norm": 4.643159866333008,
      "learning_rate": 6.842194955691889e-06,
      "loss": 0.4721,
      "step": 20760
    },
    {
      "epoch": 3.46570999499416,
      "grad_norm": 10.664862632751465,
      "learning_rate": 6.820892978868438e-06,
      "loss": 0.4024,
      "step": 20770
    },
    {
      "epoch": 3.467378608376439,
      "grad_norm": 2.7228829860687256,
      "learning_rate": 6.79959100204499e-06,
      "loss": 0.3119,
      "step": 20780
    },
    {
      "epoch": 3.4690472217587187,
      "grad_norm": 3.7227466106414795,
      "learning_rate": 6.77828902522154e-06,
      "loss": 0.7244,
      "step": 20790
    },
    {
      "epoch": 3.470715835140998,
      "grad_norm": 1.0357016324996948,
      "learning_rate": 6.756987048398092e-06,
      "loss": 0.3209,
      "step": 20800
    },
    {
      "epoch": 3.472384448523277,
      "grad_norm": 2.1783454418182373,
      "learning_rate": 6.735685071574642e-06,
      "loss": 0.5137,
      "step": 20810
    },
    {
      "epoch": 3.4740530619055563,
      "grad_norm": 3.0678086280822754,
      "learning_rate": 6.714383094751193e-06,
      "loss": 0.5631,
      "step": 20820
    },
    {
      "epoch": 3.475721675287836,
      "grad_norm": 10.292282104492188,
      "learning_rate": 6.693081117927744e-06,
      "loss": 0.6514,
      "step": 20830
    },
    {
      "epoch": 3.477390288670115,
      "grad_norm": 8.193273544311523,
      "learning_rate": 6.671779141104295e-06,
      "loss": 0.4985,
      "step": 20840
    },
    {
      "epoch": 3.4790589020523943,
      "grad_norm": 12.963034629821777,
      "learning_rate": 6.6504771642808455e-06,
      "loss": 0.5535,
      "step": 20850
    },
    {
      "epoch": 3.480727515434674,
      "grad_norm": 7.626280307769775,
      "learning_rate": 6.629175187457397e-06,
      "loss": 0.2918,
      "step": 20860
    },
    {
      "epoch": 3.482396128816953,
      "grad_norm": 2.076474189758301,
      "learning_rate": 6.607873210633947e-06,
      "loss": 0.5908,
      "step": 20870
    },
    {
      "epoch": 3.4840647421992323,
      "grad_norm": 5.542282581329346,
      "learning_rate": 6.586571233810499e-06,
      "loss": 0.4875,
      "step": 20880
    },
    {
      "epoch": 3.4857333555815115,
      "grad_norm": 7.130640506744385,
      "learning_rate": 6.565269256987048e-06,
      "loss": 0.4901,
      "step": 20890
    },
    {
      "epoch": 3.487401968963791,
      "grad_norm": 7.652356147766113,
      "learning_rate": 6.5439672801636004e-06,
      "loss": 0.5838,
      "step": 20900
    },
    {
      "epoch": 3.4890705823460704,
      "grad_norm": 15.006949424743652,
      "learning_rate": 6.52266530334015e-06,
      "loss": 0.9342,
      "step": 20910
    },
    {
      "epoch": 3.4907391957283496,
      "grad_norm": 8.381431579589844,
      "learning_rate": 6.5013633265167006e-06,
      "loss": 0.3692,
      "step": 20920
    },
    {
      "epoch": 3.492407809110629,
      "grad_norm": 7.43379545211792,
      "learning_rate": 6.480061349693252e-06,
      "loss": 0.5585,
      "step": 20930
    },
    {
      "epoch": 3.4940764224929084,
      "grad_norm": 1.1666837930679321,
      "learning_rate": 6.458759372869802e-06,
      "loss": 0.4709,
      "step": 20940
    },
    {
      "epoch": 3.4957450358751876,
      "grad_norm": 7.733052730560303,
      "learning_rate": 6.437457396046354e-06,
      "loss": 0.4237,
      "step": 20950
    },
    {
      "epoch": 3.4974136492574672,
      "grad_norm": 4.614528656005859,
      "learning_rate": 6.416155419222903e-06,
      "loss": 0.5971,
      "step": 20960
    },
    {
      "epoch": 3.4990822626397464,
      "grad_norm": 1.8375046253204346,
      "learning_rate": 6.3948534423994555e-06,
      "loss": 0.7227,
      "step": 20970
    },
    {
      "epoch": 3.5007508760220256,
      "grad_norm": 7.764051914215088,
      "learning_rate": 6.373551465576005e-06,
      "loss": 0.5212,
      "step": 20980
    },
    {
      "epoch": 3.5024194894043053,
      "grad_norm": 11.796051979064941,
      "learning_rate": 6.3522494887525565e-06,
      "loss": 0.415,
      "step": 20990
    },
    {
      "epoch": 3.5040881027865844,
      "grad_norm": 9.118303298950195,
      "learning_rate": 6.330947511929107e-06,
      "loss": 0.3629,
      "step": 21000
    },
    {
      "epoch": 3.5057567161688636,
      "grad_norm": 9.282807350158691,
      "learning_rate": 6.309645535105658e-06,
      "loss": 0.3791,
      "step": 21010
    },
    {
      "epoch": 3.507425329551143,
      "grad_norm": 4.61968994140625,
      "learning_rate": 6.288343558282209e-06,
      "loss": 0.4666,
      "step": 21020
    },
    {
      "epoch": 3.5090939429334225,
      "grad_norm": 2.273242712020874,
      "learning_rate": 6.26704158145876e-06,
      "loss": 0.5575,
      "step": 21030
    },
    {
      "epoch": 3.5107625563157017,
      "grad_norm": 3.3154001235961914,
      "learning_rate": 6.2457396046353106e-06,
      "loss": 0.5312,
      "step": 21040
    },
    {
      "epoch": 3.512431169697981,
      "grad_norm": 1.6239644289016724,
      "learning_rate": 6.224437627811861e-06,
      "loss": 0.2966,
      "step": 21050
    },
    {
      "epoch": 3.51409978308026,
      "grad_norm": 3.923654794692993,
      "learning_rate": 6.203135650988412e-06,
      "loss": 0.451,
      "step": 21060
    },
    {
      "epoch": 3.5157683964625397,
      "grad_norm": 10.32284927368164,
      "learning_rate": 6.181833674164963e-06,
      "loss": 0.4078,
      "step": 21070
    },
    {
      "epoch": 3.517437009844819,
      "grad_norm": 1.6162347793579102,
      "learning_rate": 6.160531697341513e-06,
      "loss": 0.3721,
      "step": 21080
    },
    {
      "epoch": 3.519105623227098,
      "grad_norm": 11.32284927368164,
      "learning_rate": 6.139229720518065e-06,
      "loss": 0.5557,
      "step": 21090
    },
    {
      "epoch": 3.5207742366093777,
      "grad_norm": 15.29248046875,
      "learning_rate": 6.117927743694615e-06,
      "loss": 0.523,
      "step": 21100
    },
    {
      "epoch": 3.522442849991657,
      "grad_norm": 1.30414617061615,
      "learning_rate": 6.0966257668711665e-06,
      "loss": 0.4406,
      "step": 21110
    },
    {
      "epoch": 3.524111463373936,
      "grad_norm": 5.022902488708496,
      "learning_rate": 6.075323790047717e-06,
      "loss": 0.2881,
      "step": 21120
    },
    {
      "epoch": 3.5257800767562157,
      "grad_norm": 14.619294166564941,
      "learning_rate": 6.0540218132242674e-06,
      "loss": 0.533,
      "step": 21130
    },
    {
      "epoch": 3.527448690138495,
      "grad_norm": 1.124678134918213,
      "learning_rate": 6.032719836400819e-06,
      "loss": 0.4474,
      "step": 21140
    },
    {
      "epoch": 3.529117303520774,
      "grad_norm": 5.407876491546631,
      "learning_rate": 6.011417859577368e-06,
      "loss": 0.6852,
      "step": 21150
    },
    {
      "epoch": 3.5307859169030538,
      "grad_norm": 11.153717994689941,
      "learning_rate": 5.99011588275392e-06,
      "loss": 0.4854,
      "step": 21160
    },
    {
      "epoch": 3.532454530285333,
      "grad_norm": 2.1055479049682617,
      "learning_rate": 5.96881390593047e-06,
      "loss": 0.4323,
      "step": 21170
    },
    {
      "epoch": 3.534123143667612,
      "grad_norm": 13.96599006652832,
      "learning_rate": 5.9475119291070215e-06,
      "loss": 0.4649,
      "step": 21180
    },
    {
      "epoch": 3.535791757049892,
      "grad_norm": 3.9161217212677,
      "learning_rate": 5.926209952283572e-06,
      "loss": 0.4859,
      "step": 21190
    },
    {
      "epoch": 3.537460370432171,
      "grad_norm": 9.81066608428955,
      "learning_rate": 5.9049079754601225e-06,
      "loss": 0.4273,
      "step": 21200
    },
    {
      "epoch": 3.53912898381445,
      "grad_norm": 5.189286708831787,
      "learning_rate": 5.883605998636674e-06,
      "loss": 0.3371,
      "step": 21210
    },
    {
      "epoch": 3.5407975971967294,
      "grad_norm": 9.37335205078125,
      "learning_rate": 5.862304021813224e-06,
      "loss": 0.5693,
      "step": 21220
    },
    {
      "epoch": 3.5424662105790086,
      "grad_norm": 13.17685317993164,
      "learning_rate": 5.841002044989776e-06,
      "loss": 0.6181,
      "step": 21230
    },
    {
      "epoch": 3.544134823961288,
      "grad_norm": 6.277157306671143,
      "learning_rate": 5.819700068166326e-06,
      "loss": 0.5525,
      "step": 21240
    },
    {
      "epoch": 3.5458034373435674,
      "grad_norm": 7.086718559265137,
      "learning_rate": 5.7983980913428775e-06,
      "loss": 0.4338,
      "step": 21250
    },
    {
      "epoch": 3.5474720507258466,
      "grad_norm": 7.0606818199157715,
      "learning_rate": 5.777096114519428e-06,
      "loss": 0.5054,
      "step": 21260
    },
    {
      "epoch": 3.5491406641081262,
      "grad_norm": 12.674489974975586,
      "learning_rate": 5.755794137695978e-06,
      "loss": 0.5051,
      "step": 21270
    },
    {
      "epoch": 3.5508092774904054,
      "grad_norm": 18.249391555786133,
      "learning_rate": 5.73449216087253e-06,
      "loss": 0.7602,
      "step": 21280
    },
    {
      "epoch": 3.5524778908726846,
      "grad_norm": 7.84918212890625,
      "learning_rate": 5.71319018404908e-06,
      "loss": 0.3645,
      "step": 21290
    },
    {
      "epoch": 3.5541465042549643,
      "grad_norm": 10.394564628601074,
      "learning_rate": 5.691888207225631e-06,
      "loss": 0.7829,
      "step": 21300
    },
    {
      "epoch": 3.5558151176372434,
      "grad_norm": 4.210204124450684,
      "learning_rate": 5.670586230402181e-06,
      "loss": 0.5654,
      "step": 21310
    },
    {
      "epoch": 3.5574837310195226,
      "grad_norm": 5.549288272857666,
      "learning_rate": 5.6492842535787325e-06,
      "loss": 0.378,
      "step": 21320
    },
    {
      "epoch": 3.5591523444018023,
      "grad_norm": 4.746041297912598,
      "learning_rate": 5.627982276755283e-06,
      "loss": 0.6583,
      "step": 21330
    },
    {
      "epoch": 3.5608209577840815,
      "grad_norm": 9.413887023925781,
      "learning_rate": 5.6066802999318335e-06,
      "loss": 0.6274,
      "step": 21340
    },
    {
      "epoch": 3.5624895711663607,
      "grad_norm": 9.472704887390137,
      "learning_rate": 5.585378323108385e-06,
      "loss": 0.7121,
      "step": 21350
    },
    {
      "epoch": 3.5641581845486403,
      "grad_norm": 1.4759961366653442,
      "learning_rate": 5.564076346284935e-06,
      "loss": 0.6776,
      "step": 21360
    },
    {
      "epoch": 3.5658267979309195,
      "grad_norm": 6.738171100616455,
      "learning_rate": 5.542774369461487e-06,
      "loss": 0.5978,
      "step": 21370
    },
    {
      "epoch": 3.5674954113131987,
      "grad_norm": 12.86396312713623,
      "learning_rate": 5.521472392638037e-06,
      "loss": 0.6006,
      "step": 21380
    },
    {
      "epoch": 3.5691640246954783,
      "grad_norm": 1.4340736865997314,
      "learning_rate": 5.500170415814588e-06,
      "loss": 0.256,
      "step": 21390
    },
    {
      "epoch": 3.5708326380777575,
      "grad_norm": 0.5520016551017761,
      "learning_rate": 5.478868438991139e-06,
      "loss": 0.2799,
      "step": 21400
    },
    {
      "epoch": 3.5725012514600367,
      "grad_norm": 10.946706771850586,
      "learning_rate": 5.457566462167689e-06,
      "loss": 0.5015,
      "step": 21410
    },
    {
      "epoch": 3.574169864842316,
      "grad_norm": 19.66849136352539,
      "learning_rate": 5.436264485344241e-06,
      "loss": 0.507,
      "step": 21420
    },
    {
      "epoch": 3.575838478224595,
      "grad_norm": 9.297977447509766,
      "learning_rate": 5.414962508520791e-06,
      "loss": 0.5708,
      "step": 21430
    },
    {
      "epoch": 3.5775070916068747,
      "grad_norm": 4.525036334991455,
      "learning_rate": 5.393660531697342e-06,
      "loss": 0.514,
      "step": 21440
    },
    {
      "epoch": 3.579175704989154,
      "grad_norm": 8.339523315429688,
      "learning_rate": 5.372358554873892e-06,
      "loss": 0.4333,
      "step": 21450
    },
    {
      "epoch": 3.580844318371433,
      "grad_norm": 5.435532569885254,
      "learning_rate": 5.351056578050443e-06,
      "loss": 0.5385,
      "step": 21460
    },
    {
      "epoch": 3.5825129317537128,
      "grad_norm": 6.906679153442383,
      "learning_rate": 5.329754601226994e-06,
      "loss": 0.3841,
      "step": 21470
    },
    {
      "epoch": 3.584181545135992,
      "grad_norm": 4.931797981262207,
      "learning_rate": 5.3084526244035445e-06,
      "loss": 0.5755,
      "step": 21480
    },
    {
      "epoch": 3.585850158518271,
      "grad_norm": 6.009332180023193,
      "learning_rate": 5.287150647580096e-06,
      "loss": 0.4845,
      "step": 21490
    },
    {
      "epoch": 3.587518771900551,
      "grad_norm": 3.1099226474761963,
      "learning_rate": 5.265848670756646e-06,
      "loss": 0.3506,
      "step": 21500
    },
    {
      "epoch": 3.58918738528283,
      "grad_norm": 6.694281101226807,
      "learning_rate": 5.244546693933198e-06,
      "loss": 0.4321,
      "step": 21510
    },
    {
      "epoch": 3.590855998665109,
      "grad_norm": 0.17371946573257446,
      "learning_rate": 5.223244717109748e-06,
      "loss": 0.6039,
      "step": 21520
    },
    {
      "epoch": 3.592524612047389,
      "grad_norm": 10.119128227233887,
      "learning_rate": 5.2019427402862986e-06,
      "loss": 0.3557,
      "step": 21530
    },
    {
      "epoch": 3.594193225429668,
      "grad_norm": 1.1869553327560425,
      "learning_rate": 5.18064076346285e-06,
      "loss": 0.6543,
      "step": 21540
    },
    {
      "epoch": 3.595861838811947,
      "grad_norm": 12.343558311462402,
      "learning_rate": 5.1593387866394e-06,
      "loss": 0.481,
      "step": 21550
    },
    {
      "epoch": 3.597530452194227,
      "grad_norm": 9.840676307678223,
      "learning_rate": 5.138036809815952e-06,
      "loss": 0.7406,
      "step": 21560
    },
    {
      "epoch": 3.599199065576506,
      "grad_norm": 8.073027610778809,
      "learning_rate": 5.116734832992502e-06,
      "loss": 0.485,
      "step": 21570
    },
    {
      "epoch": 3.6008676789587852,
      "grad_norm": 13.323938369750977,
      "learning_rate": 5.095432856169053e-06,
      "loss": 0.586,
      "step": 21580
    },
    {
      "epoch": 3.6025362923410644,
      "grad_norm": 20.232892990112305,
      "learning_rate": 5.074130879345604e-06,
      "loss": 0.495,
      "step": 21590
    },
    {
      "epoch": 3.604204905723344,
      "grad_norm": 1.8446178436279297,
      "learning_rate": 5.052828902522154e-06,
      "loss": 0.5234,
      "step": 21600
    },
    {
      "epoch": 3.6058735191056233,
      "grad_norm": 2.2553234100341797,
      "learning_rate": 5.031526925698705e-06,
      "loss": 0.538,
      "step": 21610
    },
    {
      "epoch": 3.6075421324879025,
      "grad_norm": 3.56135892868042,
      "learning_rate": 5.0102249488752554e-06,
      "loss": 0.6446,
      "step": 21620
    },
    {
      "epoch": 3.6092107458701816,
      "grad_norm": 9.306978225708008,
      "learning_rate": 4.988922972051807e-06,
      "loss": 0.5167,
      "step": 21630
    },
    {
      "epoch": 3.6108793592524613,
      "grad_norm": 5.579250812530518,
      "learning_rate": 4.967620995228357e-06,
      "loss": 0.4771,
      "step": 21640
    },
    {
      "epoch": 3.6125479726347405,
      "grad_norm": 11.919994354248047,
      "learning_rate": 4.946319018404908e-06,
      "loss": 0.5436,
      "step": 21650
    },
    {
      "epoch": 3.6142165860170197,
      "grad_norm": 4.502928733825684,
      "learning_rate": 4.925017041581459e-06,
      "loss": 0.4712,
      "step": 21660
    },
    {
      "epoch": 3.6158851993992993,
      "grad_norm": 6.578351020812988,
      "learning_rate": 4.9037150647580095e-06,
      "loss": 0.4586,
      "step": 21670
    },
    {
      "epoch": 3.6175538127815785,
      "grad_norm": 6.033069610595703,
      "learning_rate": 4.882413087934561e-06,
      "loss": 0.5045,
      "step": 21680
    },
    {
      "epoch": 3.6192224261638577,
      "grad_norm": 7.8848161697387695,
      "learning_rate": 4.861111111111111e-06,
      "loss": 0.6114,
      "step": 21690
    },
    {
      "epoch": 3.6208910395461373,
      "grad_norm": 4.919172286987305,
      "learning_rate": 4.839809134287663e-06,
      "loss": 0.4342,
      "step": 21700
    },
    {
      "epoch": 3.6225596529284165,
      "grad_norm": 4.924679279327393,
      "learning_rate": 4.818507157464213e-06,
      "loss": 0.3181,
      "step": 21710
    },
    {
      "epoch": 3.6242282663106957,
      "grad_norm": 13.989401817321777,
      "learning_rate": 4.797205180640764e-06,
      "loss": 0.3934,
      "step": 21720
    },
    {
      "epoch": 3.6258968796929754,
      "grad_norm": 2.325087070465088,
      "learning_rate": 4.775903203817315e-06,
      "loss": 0.2058,
      "step": 21730
    },
    {
      "epoch": 3.6275654930752546,
      "grad_norm": 12.572270393371582,
      "learning_rate": 4.7546012269938654e-06,
      "loss": 0.4431,
      "step": 21740
    },
    {
      "epoch": 3.6292341064575337,
      "grad_norm": 12.417312622070312,
      "learning_rate": 4.733299250170416e-06,
      "loss": 0.7587,
      "step": 21750
    },
    {
      "epoch": 3.6309027198398134,
      "grad_norm": 2.466243267059326,
      "learning_rate": 4.711997273346966e-06,
      "loss": 0.2884,
      "step": 21760
    },
    {
      "epoch": 3.6325713332220926,
      "grad_norm": 7.873419284820557,
      "learning_rate": 4.690695296523518e-06,
      "loss": 0.5309,
      "step": 21770
    },
    {
      "epoch": 3.6342399466043718,
      "grad_norm": 3.0082037448883057,
      "learning_rate": 4.669393319700068e-06,
      "loss": 0.6112,
      "step": 21780
    },
    {
      "epoch": 3.635908559986651,
      "grad_norm": 5.712888717651367,
      "learning_rate": 4.648091342876619e-06,
      "loss": 0.5379,
      "step": 21790
    },
    {
      "epoch": 3.63757717336893,
      "grad_norm": 4.860551834106445,
      "learning_rate": 4.62678936605317e-06,
      "loss": 0.5047,
      "step": 21800
    },
    {
      "epoch": 3.63924578675121,
      "grad_norm": 9.580473899841309,
      "learning_rate": 4.6054873892297205e-06,
      "loss": 0.4573,
      "step": 21810
    },
    {
      "epoch": 3.640914400133489,
      "grad_norm": 8.500031471252441,
      "learning_rate": 4.584185412406272e-06,
      "loss": 0.541,
      "step": 21820
    },
    {
      "epoch": 3.642583013515768,
      "grad_norm": 6.492968559265137,
      "learning_rate": 4.562883435582822e-06,
      "loss": 0.4605,
      "step": 21830
    },
    {
      "epoch": 3.644251626898048,
      "grad_norm": 6.848036289215088,
      "learning_rate": 4.541581458759373e-06,
      "loss": 0.2907,
      "step": 21840
    },
    {
      "epoch": 3.645920240280327,
      "grad_norm": 6.999863147735596,
      "learning_rate": 4.520279481935924e-06,
      "loss": 0.2972,
      "step": 21850
    },
    {
      "epoch": 3.647588853662606,
      "grad_norm": 7.967967510223389,
      "learning_rate": 4.498977505112475e-06,
      "loss": 0.8139,
      "step": 21860
    },
    {
      "epoch": 3.649257467044886,
      "grad_norm": 6.869866371154785,
      "learning_rate": 4.477675528289026e-06,
      "loss": 0.4029,
      "step": 21870
    },
    {
      "epoch": 3.650926080427165,
      "grad_norm": 5.539418697357178,
      "learning_rate": 4.456373551465576e-06,
      "loss": 0.6193,
      "step": 21880
    },
    {
      "epoch": 3.6525946938094442,
      "grad_norm": 7.24509859085083,
      "learning_rate": 4.435071574642127e-06,
      "loss": 0.5106,
      "step": 21890
    },
    {
      "epoch": 3.654263307191724,
      "grad_norm": 6.985869407653809,
      "learning_rate": 4.413769597818678e-06,
      "loss": 0.3447,
      "step": 21900
    },
    {
      "epoch": 3.655931920574003,
      "grad_norm": 18.756696701049805,
      "learning_rate": 4.392467620995228e-06,
      "loss": 0.8067,
      "step": 21910
    },
    {
      "epoch": 3.6576005339562823,
      "grad_norm": 2.838832378387451,
      "learning_rate": 4.371165644171779e-06,
      "loss": 0.4345,
      "step": 21920
    },
    {
      "epoch": 3.659269147338562,
      "grad_norm": 5.22141170501709,
      "learning_rate": 4.34986366734833e-06,
      "loss": 0.4775,
      "step": 21930
    },
    {
      "epoch": 3.660937760720841,
      "grad_norm": 5.450654983520508,
      "learning_rate": 4.328561690524881e-06,
      "loss": 0.4047,
      "step": 21940
    },
    {
      "epoch": 3.6626063741031203,
      "grad_norm": 4.59934663772583,
      "learning_rate": 4.3072597137014315e-06,
      "loss": 0.3552,
      "step": 21950
    },
    {
      "epoch": 3.6642749874853995,
      "grad_norm": 2.7810428142547607,
      "learning_rate": 4.285957736877983e-06,
      "loss": 0.2976,
      "step": 21960
    },
    {
      "epoch": 3.665943600867679,
      "grad_norm": 0.9531851410865784,
      "learning_rate": 4.264655760054533e-06,
      "loss": 0.4363,
      "step": 21970
    },
    {
      "epoch": 3.6676122142499583,
      "grad_norm": 9.14711856842041,
      "learning_rate": 4.243353783231084e-06,
      "loss": 0.4474,
      "step": 21980
    },
    {
      "epoch": 3.6692808276322375,
      "grad_norm": 10.405994415283203,
      "learning_rate": 4.222051806407635e-06,
      "loss": 0.4529,
      "step": 21990
    },
    {
      "epoch": 3.6709494410145167,
      "grad_norm": 6.108366012573242,
      "learning_rate": 4.200749829584186e-06,
      "loss": 0.7132,
      "step": 22000
    },
    {
      "epoch": 3.6726180543967963,
      "grad_norm": 3.701308250427246,
      "learning_rate": 4.179447852760737e-06,
      "loss": 0.6394,
      "step": 22010
    },
    {
      "epoch": 3.6742866677790755,
      "grad_norm": 1.0138545036315918,
      "learning_rate": 4.158145875937287e-06,
      "loss": 0.4267,
      "step": 22020
    },
    {
      "epoch": 3.6759552811613547,
      "grad_norm": 9.049661636352539,
      "learning_rate": 4.136843899113838e-06,
      "loss": 0.5294,
      "step": 22030
    },
    {
      "epoch": 3.6776238945436344,
      "grad_norm": 0.18083572387695312,
      "learning_rate": 4.115541922290389e-06,
      "loss": 0.5687,
      "step": 22040
    },
    {
      "epoch": 3.6792925079259136,
      "grad_norm": 0.8632515072822571,
      "learning_rate": 4.09423994546694e-06,
      "loss": 0.3563,
      "step": 22050
    },
    {
      "epoch": 3.6809611213081928,
      "grad_norm": 15.848794937133789,
      "learning_rate": 4.07293796864349e-06,
      "loss": 0.3582,
      "step": 22060
    },
    {
      "epoch": 3.6826297346904724,
      "grad_norm": 3.0267815589904785,
      "learning_rate": 4.051635991820041e-06,
      "loss": 0.3433,
      "step": 22070
    },
    {
      "epoch": 3.6842983480727516,
      "grad_norm": 4.760175704956055,
      "learning_rate": 4.030334014996592e-06,
      "loss": 0.3429,
      "step": 22080
    },
    {
      "epoch": 3.6859669614550308,
      "grad_norm": 0.6855878829956055,
      "learning_rate": 4.0090320381731425e-06,
      "loss": 0.3533,
      "step": 22090
    },
    {
      "epoch": 3.6876355748373104,
      "grad_norm": 3.0878689289093018,
      "learning_rate": 3.987730061349693e-06,
      "loss": 0.4119,
      "step": 22100
    },
    {
      "epoch": 3.6893041882195896,
      "grad_norm": 9.653105735778809,
      "learning_rate": 3.966428084526244e-06,
      "loss": 0.4155,
      "step": 22110
    },
    {
      "epoch": 3.690972801601869,
      "grad_norm": 0.9403486847877502,
      "learning_rate": 3.945126107702795e-06,
      "loss": 0.4312,
      "step": 22120
    },
    {
      "epoch": 3.6926414149841484,
      "grad_norm": 3.6524770259857178,
      "learning_rate": 3.923824130879346e-06,
      "loss": 0.38,
      "step": 22130
    },
    {
      "epoch": 3.6943100283664276,
      "grad_norm": 0.5509562492370605,
      "learning_rate": 3.9025221540558966e-06,
      "loss": 0.4174,
      "step": 22140
    },
    {
      "epoch": 3.695978641748707,
      "grad_norm": 12.234053611755371,
      "learning_rate": 3.881220177232447e-06,
      "loss": 0.5614,
      "step": 22150
    },
    {
      "epoch": 3.697647255130986,
      "grad_norm": 2.878143310546875,
      "learning_rate": 3.859918200408998e-06,
      "loss": 0.5942,
      "step": 22160
    },
    {
      "epoch": 3.699315868513265,
      "grad_norm": 10.889856338500977,
      "learning_rate": 3.838616223585549e-06,
      "loss": 0.4625,
      "step": 22170
    },
    {
      "epoch": 3.700984481895545,
      "grad_norm": 3.461017370223999,
      "learning_rate": 3.8173142467621e-06,
      "loss": 0.5305,
      "step": 22180
    },
    {
      "epoch": 3.702653095277824,
      "grad_norm": 6.35054874420166,
      "learning_rate": 3.7960122699386507e-06,
      "loss": 0.3216,
      "step": 22190
    },
    {
      "epoch": 3.7043217086601032,
      "grad_norm": 4.775399684906006,
      "learning_rate": 3.7747102931152016e-06,
      "loss": 0.5534,
      "step": 22200
    },
    {
      "epoch": 3.705990322042383,
      "grad_norm": 16.860179901123047,
      "learning_rate": 3.7534083162917516e-06,
      "loss": 0.4706,
      "step": 22210
    },
    {
      "epoch": 3.707658935424662,
      "grad_norm": 5.911747932434082,
      "learning_rate": 3.7321063394683025e-06,
      "loss": 0.394,
      "step": 22220
    },
    {
      "epoch": 3.7093275488069413,
      "grad_norm": 1.7392531633377075,
      "learning_rate": 3.7108043626448534e-06,
      "loss": 0.5704,
      "step": 22230
    },
    {
      "epoch": 3.710996162189221,
      "grad_norm": 0.8077092170715332,
      "learning_rate": 3.6895023858214043e-06,
      "loss": 0.2355,
      "step": 22240
    },
    {
      "epoch": 3.7126647755715,
      "grad_norm": 12.315877914428711,
      "learning_rate": 3.6682004089979552e-06,
      "loss": 0.4296,
      "step": 22250
    },
    {
      "epoch": 3.7143333889537793,
      "grad_norm": 6.3437113761901855,
      "learning_rate": 3.6468984321745057e-06,
      "loss": 0.6085,
      "step": 22260
    },
    {
      "epoch": 3.716002002336059,
      "grad_norm": 1.8391004800796509,
      "learning_rate": 3.6255964553510566e-06,
      "loss": 0.5382,
      "step": 22270
    },
    {
      "epoch": 3.717670615718338,
      "grad_norm": 9.358746528625488,
      "learning_rate": 3.6042944785276075e-06,
      "loss": 0.4642,
      "step": 22280
    },
    {
      "epoch": 3.7193392291006173,
      "grad_norm": 11.535834312438965,
      "learning_rate": 3.5829925017041584e-06,
      "loss": 0.7747,
      "step": 22290
    },
    {
      "epoch": 3.721007842482897,
      "grad_norm": 1.2300145626068115,
      "learning_rate": 3.5616905248807093e-06,
      "loss": 0.3746,
      "step": 22300
    },
    {
      "epoch": 3.722676455865176,
      "grad_norm": 6.076180458068848,
      "learning_rate": 3.54038854805726e-06,
      "loss": 0.4726,
      "step": 22310
    },
    {
      "epoch": 3.7243450692474553,
      "grad_norm": 9.847790718078613,
      "learning_rate": 3.5190865712338107e-06,
      "loss": 0.5415,
      "step": 22320
    },
    {
      "epoch": 3.7260136826297345,
      "grad_norm": 9.270429611206055,
      "learning_rate": 3.4977845944103616e-06,
      "loss": 0.3688,
      "step": 22330
    },
    {
      "epoch": 3.727682296012014,
      "grad_norm": 1.351369023323059,
      "learning_rate": 3.4764826175869125e-06,
      "loss": 0.1882,
      "step": 22340
    },
    {
      "epoch": 3.7293509093942934,
      "grad_norm": 8.206186294555664,
      "learning_rate": 3.4551806407634635e-06,
      "loss": 0.3293,
      "step": 22350
    },
    {
      "epoch": 3.7310195227765726,
      "grad_norm": 11.685091972351074,
      "learning_rate": 3.4338786639400135e-06,
      "loss": 0.4545,
      "step": 22360
    },
    {
      "epoch": 3.7326881361588518,
      "grad_norm": 15.363067626953125,
      "learning_rate": 3.4125766871165644e-06,
      "loss": 0.4646,
      "step": 22370
    },
    {
      "epoch": 3.7343567495411314,
      "grad_norm": 24.8574161529541,
      "learning_rate": 3.3912747102931153e-06,
      "loss": 0.5562,
      "step": 22380
    },
    {
      "epoch": 3.7360253629234106,
      "grad_norm": 7.382233142852783,
      "learning_rate": 3.369972733469666e-06,
      "loss": 0.5419,
      "step": 22390
    },
    {
      "epoch": 3.73769397630569,
      "grad_norm": 6.413299083709717,
      "learning_rate": 3.3486707566462167e-06,
      "loss": 0.4283,
      "step": 22400
    },
    {
      "epoch": 3.7393625896879694,
      "grad_norm": 3.155801773071289,
      "learning_rate": 3.3273687798227676e-06,
      "loss": 0.5669,
      "step": 22410
    },
    {
      "epoch": 3.7410312030702486,
      "grad_norm": 3.421372652053833,
      "learning_rate": 3.3060668029993185e-06,
      "loss": 0.2928,
      "step": 22420
    },
    {
      "epoch": 3.742699816452528,
      "grad_norm": 12.025910377502441,
      "learning_rate": 3.2847648261758694e-06,
      "loss": 0.5434,
      "step": 22430
    },
    {
      "epoch": 3.7443684298348074,
      "grad_norm": 2.6149516105651855,
      "learning_rate": 3.26346284935242e-06,
      "loss": 0.4689,
      "step": 22440
    },
    {
      "epoch": 3.7460370432170866,
      "grad_norm": 10.340877532958984,
      "learning_rate": 3.242160872528971e-06,
      "loss": 0.647,
      "step": 22450
    },
    {
      "epoch": 3.747705656599366,
      "grad_norm": 4.174807548522949,
      "learning_rate": 3.2208588957055217e-06,
      "loss": 0.4867,
      "step": 22460
    },
    {
      "epoch": 3.7493742699816455,
      "grad_norm": 9.061724662780762,
      "learning_rate": 3.1995569188820726e-06,
      "loss": 0.8739,
      "step": 22470
    },
    {
      "epoch": 3.7510428833639247,
      "grad_norm": 4.9776482582092285,
      "learning_rate": 3.1782549420586235e-06,
      "loss": 0.3365,
      "step": 22480
    },
    {
      "epoch": 3.752711496746204,
      "grad_norm": 6.972503662109375,
      "learning_rate": 3.1569529652351744e-06,
      "loss": 0.6902,
      "step": 22490
    },
    {
      "epoch": 3.7543801101284835,
      "grad_norm": 6.087345123291016,
      "learning_rate": 3.135650988411725e-06,
      "loss": 0.7464,
      "step": 22500
    },
    {
      "epoch": 3.7560487235107627,
      "grad_norm": 0.744382381439209,
      "learning_rate": 3.1143490115882754e-06,
      "loss": 0.5205,
      "step": 22510
    },
    {
      "epoch": 3.757717336893042,
      "grad_norm": 1.6998813152313232,
      "learning_rate": 3.0930470347648263e-06,
      "loss": 0.3044,
      "step": 22520
    },
    {
      "epoch": 3.759385950275321,
      "grad_norm": 16.666820526123047,
      "learning_rate": 3.071745057941377e-06,
      "loss": 0.6172,
      "step": 22530
    },
    {
      "epoch": 3.7610545636576003,
      "grad_norm": 13.282144546508789,
      "learning_rate": 3.050443081117928e-06,
      "loss": 0.4316,
      "step": 22540
    },
    {
      "epoch": 3.76272317703988,
      "grad_norm": 6.192707538604736,
      "learning_rate": 3.0291411042944786e-06,
      "loss": 0.2123,
      "step": 22550
    },
    {
      "epoch": 3.764391790422159,
      "grad_norm": 7.784547805786133,
      "learning_rate": 3.0078391274710295e-06,
      "loss": 0.5054,
      "step": 22560
    },
    {
      "epoch": 3.7660604038044383,
      "grad_norm": 7.880535125732422,
      "learning_rate": 2.9865371506475804e-06,
      "loss": 0.5184,
      "step": 22570
    },
    {
      "epoch": 3.767729017186718,
      "grad_norm": 2.494190216064453,
      "learning_rate": 2.965235173824131e-06,
      "loss": 0.5297,
      "step": 22580
    },
    {
      "epoch": 3.769397630568997,
      "grad_norm": 7.357870578765869,
      "learning_rate": 2.9439331970006818e-06,
      "loss": 0.3707,
      "step": 22590
    },
    {
      "epoch": 3.7710662439512763,
      "grad_norm": 10.55207633972168,
      "learning_rate": 2.9226312201772327e-06,
      "loss": 0.3671,
      "step": 22600
    },
    {
      "epoch": 3.772734857333556,
      "grad_norm": 10.065739631652832,
      "learning_rate": 2.9013292433537836e-06,
      "loss": 0.6747,
      "step": 22610
    },
    {
      "epoch": 3.774403470715835,
      "grad_norm": 1.6551192998886108,
      "learning_rate": 2.880027266530334e-06,
      "loss": 0.6097,
      "step": 22620
    },
    {
      "epoch": 3.7760720840981143,
      "grad_norm": 7.325553894042969,
      "learning_rate": 2.858725289706885e-06,
      "loss": 0.3996,
      "step": 22630
    },
    {
      "epoch": 3.777740697480394,
      "grad_norm": 2.563072919845581,
      "learning_rate": 2.8374233128834355e-06,
      "loss": 0.3231,
      "step": 22640
    },
    {
      "epoch": 3.779409310862673,
      "grad_norm": 21.081104278564453,
      "learning_rate": 2.8161213360599864e-06,
      "loss": 0.4673,
      "step": 22650
    },
    {
      "epoch": 3.7810779242449524,
      "grad_norm": 29.38410186767578,
      "learning_rate": 2.7948193592365373e-06,
      "loss": 0.482,
      "step": 22660
    },
    {
      "epoch": 3.782746537627232,
      "grad_norm": 19.198955535888672,
      "learning_rate": 2.773517382413088e-06,
      "loss": 0.5989,
      "step": 22670
    },
    {
      "epoch": 3.784415151009511,
      "grad_norm": 5.783555507659912,
      "learning_rate": 2.752215405589639e-06,
      "loss": 0.5909,
      "step": 22680
    },
    {
      "epoch": 3.7860837643917904,
      "grad_norm": 4.259807109832764,
      "learning_rate": 2.73091342876619e-06,
      "loss": 0.3801,
      "step": 22690
    },
    {
      "epoch": 3.7877523777740696,
      "grad_norm": 1.073207974433899,
      "learning_rate": 2.7096114519427405e-06,
      "loss": 0.3483,
      "step": 22700
    },
    {
      "epoch": 3.7894209911563492,
      "grad_norm": 6.653528690338135,
      "learning_rate": 2.688309475119291e-06,
      "loss": 0.5141,
      "step": 22710
    },
    {
      "epoch": 3.7910896045386284,
      "grad_norm": 4.60732889175415,
      "learning_rate": 2.667007498295842e-06,
      "loss": 0.2975,
      "step": 22720
    },
    {
      "epoch": 3.7927582179209076,
      "grad_norm": 11.9327974319458,
      "learning_rate": 2.6457055214723928e-06,
      "loss": 0.4589,
      "step": 22730
    },
    {
      "epoch": 3.794426831303187,
      "grad_norm": 9.094554901123047,
      "learning_rate": 2.6244035446489437e-06,
      "loss": 0.3348,
      "step": 22740
    },
    {
      "epoch": 3.7960954446854664,
      "grad_norm": 6.103396415710449,
      "learning_rate": 2.6031015678254946e-06,
      "loss": 0.4383,
      "step": 22750
    },
    {
      "epoch": 3.7977640580677456,
      "grad_norm": 13.857566833496094,
      "learning_rate": 2.581799591002045e-06,
      "loss": 0.5185,
      "step": 22760
    },
    {
      "epoch": 3.799432671450025,
      "grad_norm": 14.622899055480957,
      "learning_rate": 2.5604976141785955e-06,
      "loss": 0.5813,
      "step": 22770
    },
    {
      "epoch": 3.8011012848323045,
      "grad_norm": 5.039196968078613,
      "learning_rate": 2.5391956373551464e-06,
      "loss": 0.543,
      "step": 22780
    },
    {
      "epoch": 3.8027698982145837,
      "grad_norm": 0.5936194658279419,
      "learning_rate": 2.5178936605316973e-06,
      "loss": 0.4248,
      "step": 22790
    },
    {
      "epoch": 3.804438511596863,
      "grad_norm": 15.162697792053223,
      "learning_rate": 2.4965916837082482e-06,
      "loss": 0.624,
      "step": 22800
    },
    {
      "epoch": 3.8061071249791425,
      "grad_norm": 3.4834647178649902,
      "learning_rate": 2.475289706884799e-06,
      "loss": 0.3612,
      "step": 22810
    },
    {
      "epoch": 3.8077757383614217,
      "grad_norm": 13.128410339355469,
      "learning_rate": 2.45398773006135e-06,
      "loss": 0.4835,
      "step": 22820
    },
    {
      "epoch": 3.809444351743701,
      "grad_norm": 11.698291778564453,
      "learning_rate": 2.4326857532379005e-06,
      "loss": 0.3344,
      "step": 22830
    },
    {
      "epoch": 3.8111129651259805,
      "grad_norm": 6.414422512054443,
      "learning_rate": 2.4113837764144514e-06,
      "loss": 0.5566,
      "step": 22840
    },
    {
      "epoch": 3.8127815785082597,
      "grad_norm": 5.967574596405029,
      "learning_rate": 2.390081799591002e-06,
      "loss": 0.5251,
      "step": 22850
    },
    {
      "epoch": 3.814450191890539,
      "grad_norm": 5.110999584197998,
      "learning_rate": 2.368779822767553e-06,
      "loss": 0.4545,
      "step": 22860
    },
    {
      "epoch": 3.8161188052728185,
      "grad_norm": 3.0105319023132324,
      "learning_rate": 2.3474778459441037e-06,
      "loss": 0.8559,
      "step": 22870
    },
    {
      "epoch": 3.8177874186550977,
      "grad_norm": 6.399892807006836,
      "learning_rate": 2.3261758691206546e-06,
      "loss": 0.555,
      "step": 22880
    },
    {
      "epoch": 3.819456032037377,
      "grad_norm": 13.418932914733887,
      "learning_rate": 2.304873892297205e-06,
      "loss": 0.4905,
      "step": 22890
    },
    {
      "epoch": 3.821124645419656,
      "grad_norm": 8.10935115814209,
      "learning_rate": 2.283571915473756e-06,
      "loss": 0.4745,
      "step": 22900
    },
    {
      "epoch": 3.8227932588019353,
      "grad_norm": 4.038782119750977,
      "learning_rate": 2.262269938650307e-06,
      "loss": 0.4977,
      "step": 22910
    },
    {
      "epoch": 3.824461872184215,
      "grad_norm": 1.4457745552062988,
      "learning_rate": 2.240967961826858e-06,
      "loss": 0.383,
      "step": 22920
    },
    {
      "epoch": 3.826130485566494,
      "grad_norm": 0.4869232177734375,
      "learning_rate": 2.2196659850034083e-06,
      "loss": 0.4125,
      "step": 22930
    },
    {
      "epoch": 3.8277990989487733,
      "grad_norm": 2.779736042022705,
      "learning_rate": 2.1983640081799592e-06,
      "loss": 0.2658,
      "step": 22940
    },
    {
      "epoch": 3.829467712331053,
      "grad_norm": 5.950074195861816,
      "learning_rate": 2.17706203135651e-06,
      "loss": 0.4096,
      "step": 22950
    },
    {
      "epoch": 3.831136325713332,
      "grad_norm": 3.0173685550689697,
      "learning_rate": 2.1557600545330606e-06,
      "loss": 0.5426,
      "step": 22960
    },
    {
      "epoch": 3.8328049390956114,
      "grad_norm": 2.268900156021118,
      "learning_rate": 2.1344580777096115e-06,
      "loss": 0.5522,
      "step": 22970
    },
    {
      "epoch": 3.834473552477891,
      "grad_norm": 6.5350236892700195,
      "learning_rate": 2.1131561008861624e-06,
      "loss": 0.501,
      "step": 22980
    },
    {
      "epoch": 3.83614216586017,
      "grad_norm": 6.109586238861084,
      "learning_rate": 2.0918541240627133e-06,
      "loss": 0.9278,
      "step": 22990
    },
    {
      "epoch": 3.8378107792424494,
      "grad_norm": 1.062117338180542,
      "learning_rate": 2.070552147239264e-06,
      "loss": 0.3075,
      "step": 23000
    },
    {
      "epoch": 3.839479392624729,
      "grad_norm": 10.580078125,
      "learning_rate": 2.0492501704158147e-06,
      "loss": 0.3588,
      "step": 23010
    },
    {
      "epoch": 3.8411480060070082,
      "grad_norm": 15.449310302734375,
      "learning_rate": 2.027948193592365e-06,
      "loss": 0.5686,
      "step": 23020
    },
    {
      "epoch": 3.8428166193892874,
      "grad_norm": 6.433161735534668,
      "learning_rate": 2.006646216768916e-06,
      "loss": 0.5321,
      "step": 23030
    },
    {
      "epoch": 3.844485232771567,
      "grad_norm": 2.3375582695007324,
      "learning_rate": 1.985344239945467e-06,
      "loss": 0.4585,
      "step": 23040
    },
    {
      "epoch": 3.8461538461538463,
      "grad_norm": 3.0774526596069336,
      "learning_rate": 1.964042263122018e-06,
      "loss": 0.4371,
      "step": 23050
    },
    {
      "epoch": 3.8478224595361255,
      "grad_norm": 19.45326805114746,
      "learning_rate": 1.942740286298569e-06,
      "loss": 0.5465,
      "step": 23060
    },
    {
      "epoch": 3.8494910729184046,
      "grad_norm": 8.46158504486084,
      "learning_rate": 1.9214383094751197e-06,
      "loss": 0.3515,
      "step": 23070
    },
    {
      "epoch": 3.8511596863006843,
      "grad_norm": 3.2479164600372314,
      "learning_rate": 1.90013633265167e-06,
      "loss": 0.6296,
      "step": 23080
    },
    {
      "epoch": 3.8528282996829635,
      "grad_norm": 4.911371231079102,
      "learning_rate": 1.8788343558282209e-06,
      "loss": 0.3869,
      "step": 23090
    },
    {
      "epoch": 3.8544969130652427,
      "grad_norm": 5.602480888366699,
      "learning_rate": 1.8575323790047716e-06,
      "loss": 0.4953,
      "step": 23100
    },
    {
      "epoch": 3.856165526447522,
      "grad_norm": 14.666951179504395,
      "learning_rate": 1.8362304021813225e-06,
      "loss": 0.5498,
      "step": 23110
    },
    {
      "epoch": 3.8578341398298015,
      "grad_norm": 10.729289054870605,
      "learning_rate": 1.8149284253578734e-06,
      "loss": 0.5639,
      "step": 23120
    },
    {
      "epoch": 3.8595027532120807,
      "grad_norm": 18.996936798095703,
      "learning_rate": 1.793626448534424e-06,
      "loss": 0.4738,
      "step": 23130
    },
    {
      "epoch": 3.86117136659436,
      "grad_norm": 10.252678871154785,
      "learning_rate": 1.772324471710975e-06,
      "loss": 0.3648,
      "step": 23140
    },
    {
      "epoch": 3.8628399799766395,
      "grad_norm": 4.088766098022461,
      "learning_rate": 1.7510224948875255e-06,
      "loss": 0.4441,
      "step": 23150
    },
    {
      "epoch": 3.8645085933589187,
      "grad_norm": 7.442615032196045,
      "learning_rate": 1.7297205180640764e-06,
      "loss": 0.3058,
      "step": 23160
    },
    {
      "epoch": 3.866177206741198,
      "grad_norm": 11.904711723327637,
      "learning_rate": 1.708418541240627e-06,
      "loss": 0.7031,
      "step": 23170
    },
    {
      "epoch": 3.8678458201234776,
      "grad_norm": 8.620036125183105,
      "learning_rate": 1.687116564417178e-06,
      "loss": 0.4989,
      "step": 23180
    },
    {
      "epoch": 3.8695144335057567,
      "grad_norm": 4.967459678649902,
      "learning_rate": 1.6658145875937289e-06,
      "loss": 0.4089,
      "step": 23190
    },
    {
      "epoch": 3.871183046888036,
      "grad_norm": 6.404784202575684,
      "learning_rate": 1.6445126107702796e-06,
      "loss": 0.535,
      "step": 23200
    },
    {
      "epoch": 3.8728516602703156,
      "grad_norm": 7.28857946395874,
      "learning_rate": 1.6232106339468305e-06,
      "loss": 0.4245,
      "step": 23210
    },
    {
      "epoch": 3.8745202736525948,
      "grad_norm": 9.176414489746094,
      "learning_rate": 1.6019086571233814e-06,
      "loss": 0.334,
      "step": 23220
    },
    {
      "epoch": 3.876188887034874,
      "grad_norm": 5.676433563232422,
      "learning_rate": 1.5806066802999319e-06,
      "loss": 0.6808,
      "step": 23230
    },
    {
      "epoch": 3.8778575004171536,
      "grad_norm": 3.912097930908203,
      "learning_rate": 1.5593047034764828e-06,
      "loss": 0.6221,
      "step": 23240
    },
    {
      "epoch": 3.879526113799433,
      "grad_norm": 14.228411674499512,
      "learning_rate": 1.5380027266530335e-06,
      "loss": 0.4542,
      "step": 23250
    },
    {
      "epoch": 3.881194727181712,
      "grad_norm": 5.4704508781433105,
      "learning_rate": 1.5167007498295842e-06,
      "loss": 0.2651,
      "step": 23260
    },
    {
      "epoch": 3.882863340563991,
      "grad_norm": 11.78941535949707,
      "learning_rate": 1.495398773006135e-06,
      "loss": 0.4602,
      "step": 23270
    },
    {
      "epoch": 3.8845319539462704,
      "grad_norm": 7.837403297424316,
      "learning_rate": 1.474096796182686e-06,
      "loss": 0.6707,
      "step": 23280
    },
    {
      "epoch": 3.88620056732855,
      "grad_norm": 14.62950611114502,
      "learning_rate": 1.4527948193592367e-06,
      "loss": 0.4818,
      "step": 23290
    },
    {
      "epoch": 3.887869180710829,
      "grad_norm": 3.229193687438965,
      "learning_rate": 1.4314928425357874e-06,
      "loss": 0.3438,
      "step": 23300
    },
    {
      "epoch": 3.8895377940931084,
      "grad_norm": 4.4304728507995605,
      "learning_rate": 1.4101908657123383e-06,
      "loss": 0.4277,
      "step": 23310
    },
    {
      "epoch": 3.891206407475388,
      "grad_norm": 4.488411903381348,
      "learning_rate": 1.388888888888889e-06,
      "loss": 0.4863,
      "step": 23320
    },
    {
      "epoch": 3.8928750208576672,
      "grad_norm": 1.9417248964309692,
      "learning_rate": 1.3675869120654396e-06,
      "loss": 0.6101,
      "step": 23330
    },
    {
      "epoch": 3.8945436342399464,
      "grad_norm": 4.151752471923828,
      "learning_rate": 1.3462849352419906e-06,
      "loss": 0.5713,
      "step": 23340
    },
    {
      "epoch": 3.896212247622226,
      "grad_norm": 4.8757429122924805,
      "learning_rate": 1.3249829584185415e-06,
      "loss": 0.4492,
      "step": 23350
    },
    {
      "epoch": 3.8978808610045053,
      "grad_norm": 3.701368808746338,
      "learning_rate": 1.303680981595092e-06,
      "loss": 0.5648,
      "step": 23360
    },
    {
      "epoch": 3.8995494743867845,
      "grad_norm": 13.978412628173828,
      "learning_rate": 1.2823790047716428e-06,
      "loss": 0.4404,
      "step": 23370
    },
    {
      "epoch": 3.901218087769064,
      "grad_norm": 8.055916786193848,
      "learning_rate": 1.2610770279481938e-06,
      "loss": 0.5153,
      "step": 23380
    },
    {
      "epoch": 3.9028867011513433,
      "grad_norm": 5.607436656951904,
      "learning_rate": 1.2397750511247444e-06,
      "loss": 0.6453,
      "step": 23390
    },
    {
      "epoch": 3.9045553145336225,
      "grad_norm": 2.0918281078338623,
      "learning_rate": 1.2184730743012951e-06,
      "loss": 0.3463,
      "step": 23400
    },
    {
      "epoch": 3.906223927915902,
      "grad_norm": 5.221753120422363,
      "learning_rate": 1.197171097477846e-06,
      "loss": 0.5244,
      "step": 23410
    },
    {
      "epoch": 3.9078925412981813,
      "grad_norm": 6.649453639984131,
      "learning_rate": 1.1758691206543967e-06,
      "loss": 0.5869,
      "step": 23420
    },
    {
      "epoch": 3.9095611546804605,
      "grad_norm": 12.020371437072754,
      "learning_rate": 1.1545671438309476e-06,
      "loss": 0.5391,
      "step": 23430
    },
    {
      "epoch": 3.9112297680627397,
      "grad_norm": 7.323075294494629,
      "learning_rate": 1.1332651670074983e-06,
      "loss": 0.3517,
      "step": 23440
    },
    {
      "epoch": 3.9128983814450193,
      "grad_norm": 5.855358600616455,
      "learning_rate": 1.111963190184049e-06,
      "loss": 0.3037,
      "step": 23450
    },
    {
      "epoch": 3.9145669948272985,
      "grad_norm": 5.613039016723633,
      "learning_rate": 1.0906612133606e-06,
      "loss": 0.4339,
      "step": 23460
    },
    {
      "epoch": 3.9162356082095777,
      "grad_norm": 7.099546909332275,
      "learning_rate": 1.0693592365371506e-06,
      "loss": 0.4897,
      "step": 23470
    },
    {
      "epoch": 3.917904221591857,
      "grad_norm": 12.116971015930176,
      "learning_rate": 1.0480572597137015e-06,
      "loss": 0.3702,
      "step": 23480
    },
    {
      "epoch": 3.9195728349741366,
      "grad_norm": 3.6637279987335205,
      "learning_rate": 1.0267552828902522e-06,
      "loss": 0.4893,
      "step": 23490
    },
    {
      "epoch": 3.9212414483564157,
      "grad_norm": 8.316913604736328,
      "learning_rate": 1.0054533060668031e-06,
      "loss": 0.4509,
      "step": 23500
    },
    {
      "epoch": 3.922910061738695,
      "grad_norm": 4.777414321899414,
      "learning_rate": 9.841513292433538e-07,
      "loss": 0.3143,
      "step": 23510
    },
    {
      "epoch": 3.9245786751209746,
      "grad_norm": 0.6855123043060303,
      "learning_rate": 9.628493524199045e-07,
      "loss": 0.5703,
      "step": 23520
    },
    {
      "epoch": 3.9262472885032538,
      "grad_norm": 6.583050727844238,
      "learning_rate": 9.415473755964554e-07,
      "loss": 0.4315,
      "step": 23530
    },
    {
      "epoch": 3.927915901885533,
      "grad_norm": 4.714400291442871,
      "learning_rate": 9.202453987730062e-07,
      "loss": 0.5946,
      "step": 23540
    },
    {
      "epoch": 3.9295845152678126,
      "grad_norm": 3.7320361137390137,
      "learning_rate": 8.989434219495569e-07,
      "loss": 0.6934,
      "step": 23550
    },
    {
      "epoch": 3.931253128650092,
      "grad_norm": 0.22204960882663727,
      "learning_rate": 8.776414451261077e-07,
      "loss": 0.4345,
      "step": 23560
    },
    {
      "epoch": 3.932921742032371,
      "grad_norm": 5.385216236114502,
      "learning_rate": 8.563394683026585e-07,
      "loss": 0.4721,
      "step": 23570
    },
    {
      "epoch": 3.9345903554146506,
      "grad_norm": 6.339442253112793,
      "learning_rate": 8.350374914792094e-07,
      "loss": 0.5168,
      "step": 23580
    },
    {
      "epoch": 3.93625896879693,
      "grad_norm": 7.6876115798950195,
      "learning_rate": 8.1373551465576e-07,
      "loss": 0.3822,
      "step": 23590
    },
    {
      "epoch": 3.937927582179209,
      "grad_norm": 18.227657318115234,
      "learning_rate": 7.924335378323109e-07,
      "loss": 0.531,
      "step": 23600
    },
    {
      "epoch": 3.9395961955614887,
      "grad_norm": 11.979262351989746,
      "learning_rate": 7.711315610088617e-07,
      "loss": 0.6212,
      "step": 23610
    },
    {
      "epoch": 3.941264808943768,
      "grad_norm": 10.80088996887207,
      "learning_rate": 7.498295841854124e-07,
      "loss": 0.3391,
      "step": 23620
    },
    {
      "epoch": 3.942933422326047,
      "grad_norm": 10.821089744567871,
      "learning_rate": 7.285276073619632e-07,
      "loss": 0.4599,
      "step": 23630
    },
    {
      "epoch": 3.9446020357083262,
      "grad_norm": 6.017627716064453,
      "learning_rate": 7.07225630538514e-07,
      "loss": 0.4305,
      "step": 23640
    },
    {
      "epoch": 3.9462706490906054,
      "grad_norm": 11.47948169708252,
      "learning_rate": 6.859236537150648e-07,
      "loss": 0.5166,
      "step": 23650
    },
    {
      "epoch": 3.947939262472885,
      "grad_norm": 8.644726753234863,
      "learning_rate": 6.646216768916156e-07,
      "loss": 0.5162,
      "step": 23660
    },
    {
      "epoch": 3.9496078758551643,
      "grad_norm": 7.352728366851807,
      "learning_rate": 6.433197000681663e-07,
      "loss": 0.6031,
      "step": 23670
    },
    {
      "epoch": 3.9512764892374435,
      "grad_norm": 17.419769287109375,
      "learning_rate": 6.220177232447172e-07,
      "loss": 0.4937,
      "step": 23680
    },
    {
      "epoch": 3.952945102619723,
      "grad_norm": 6.4023871421813965,
      "learning_rate": 6.007157464212679e-07,
      "loss": 0.5318,
      "step": 23690
    },
    {
      "epoch": 3.9546137160020023,
      "grad_norm": 4.989166736602783,
      "learning_rate": 5.794137695978187e-07,
      "loss": 0.8293,
      "step": 23700
    },
    {
      "epoch": 3.9562823293842815,
      "grad_norm": 6.455521106719971,
      "learning_rate": 5.581117927743695e-07,
      "loss": 0.6612,
      "step": 23710
    },
    {
      "epoch": 3.957950942766561,
      "grad_norm": 0.8973717093467712,
      "learning_rate": 5.368098159509203e-07,
      "loss": 0.3426,
      "step": 23720
    },
    {
      "epoch": 3.9596195561488403,
      "grad_norm": 7.7965545654296875,
      "learning_rate": 5.155078391274711e-07,
      "loss": 0.547,
      "step": 23730
    },
    {
      "epoch": 3.9612881695311195,
      "grad_norm": 1.1357848644256592,
      "learning_rate": 4.942058623040219e-07,
      "loss": 0.6368,
      "step": 23740
    },
    {
      "epoch": 3.962956782913399,
      "grad_norm": 12.417492866516113,
      "learning_rate": 4.7290388548057263e-07,
      "loss": 0.4938,
      "step": 23750
    },
    {
      "epoch": 3.9646253962956783,
      "grad_norm": 5.578211784362793,
      "learning_rate": 4.5160190865712343e-07,
      "loss": 0.6631,
      "step": 23760
    },
    {
      "epoch": 3.9662940096779575,
      "grad_norm": 0.49595654010772705,
      "learning_rate": 4.302999318336742e-07,
      "loss": 0.5639,
      "step": 23770
    },
    {
      "epoch": 3.967962623060237,
      "grad_norm": 1.1984385251998901,
      "learning_rate": 4.0899795501022503e-07,
      "loss": 0.6509,
      "step": 23780
    },
    {
      "epoch": 3.9696312364425164,
      "grad_norm": 9.633634567260742,
      "learning_rate": 3.8769597818677577e-07,
      "loss": 0.4978,
      "step": 23790
    },
    {
      "epoch": 3.9712998498247956,
      "grad_norm": 2.323621988296509,
      "learning_rate": 3.663940013633265e-07,
      "loss": 0.4601,
      "step": 23800
    },
    {
      "epoch": 3.972968463207075,
      "grad_norm": 6.613117218017578,
      "learning_rate": 3.450920245398773e-07,
      "loss": 0.5513,
      "step": 23810
    },
    {
      "epoch": 3.9746370765893544,
      "grad_norm": 4.1303205490112305,
      "learning_rate": 3.237900477164281e-07,
      "loss": 0.33,
      "step": 23820
    },
    {
      "epoch": 3.9763056899716336,
      "grad_norm": 5.160812854766846,
      "learning_rate": 3.024880708929789e-07,
      "loss": 0.4948,
      "step": 23830
    },
    {
      "epoch": 3.9779743033539128,
      "grad_norm": 11.184144973754883,
      "learning_rate": 2.8118609406952966e-07,
      "loss": 0.7681,
      "step": 23840
    },
    {
      "epoch": 3.979642916736192,
      "grad_norm": 12.076009750366211,
      "learning_rate": 2.5988411724608046e-07,
      "loss": 0.4054,
      "step": 23850
    },
    {
      "epoch": 3.9813115301184716,
      "grad_norm": 10.33713436126709,
      "learning_rate": 2.3858214042263126e-07,
      "loss": 0.4092,
      "step": 23860
    },
    {
      "epoch": 3.982980143500751,
      "grad_norm": 12.051498413085938,
      "learning_rate": 2.1728016359918203e-07,
      "loss": 0.5731,
      "step": 23870
    },
    {
      "epoch": 3.98464875688303,
      "grad_norm": 5.393688678741455,
      "learning_rate": 1.959781867757328e-07,
      "loss": 0.4146,
      "step": 23880
    },
    {
      "epoch": 3.9863173702653096,
      "grad_norm": 1.2937337160110474,
      "learning_rate": 1.7467620995228358e-07,
      "loss": 0.3106,
      "step": 23890
    },
    {
      "epoch": 3.987985983647589,
      "grad_norm": 11.882678985595703,
      "learning_rate": 1.5337423312883438e-07,
      "loss": 0.3215,
      "step": 23900
    },
    {
      "epoch": 3.989654597029868,
      "grad_norm": 7.768001079559326,
      "learning_rate": 1.3207225630538515e-07,
      "loss": 0.294,
      "step": 23910
    },
    {
      "epoch": 3.9913232104121477,
      "grad_norm": 9.439292907714844,
      "learning_rate": 1.1077027948193594e-07,
      "loss": 0.6659,
      "step": 23920
    },
    {
      "epoch": 3.992991823794427,
      "grad_norm": 3.0782318115234375,
      "learning_rate": 8.946830265848671e-08,
      "loss": 0.419,
      "step": 23930
    },
    {
      "epoch": 3.994660437176706,
      "grad_norm": 7.927463531494141,
      "learning_rate": 6.81663258350375e-08,
      "loss": 0.245,
      "step": 23940
    },
    {
      "epoch": 3.9963290505589857,
      "grad_norm": 8.364059448242188,
      "learning_rate": 4.6864349011588275e-08,
      "loss": 0.5545,
      "step": 23950
    },
    {
      "epoch": 3.997997663941265,
      "grad_norm": 12.498983383178711,
      "learning_rate": 2.5562372188139064e-08,
      "loss": 0.4779,
      "step": 23960
    },
    {
      "epoch": 3.999666277323544,
      "grad_norm": 8.733543395996094,
      "learning_rate": 4.2603953646898435e-09,
      "loss": 0.5744,
      "step": 23970
    }
  ],
  "logging_steps": 10,
  "max_steps": 23972,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 2398,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.7948991420960768e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
