model:
  task: text-generation
  system_prompt: "너는 주어진 Context에서 Question에 대한 Answer를 찾는 챗봇이야. Context에서 Answer가 될 수 있는 부분을 찾아서 그대로 적어줘. 단, Answer는 주관식이 아니라 단답형으로 적어야 해."
  path: MLP-KTLim/llama-3-Korean-Bllossom-8B
  torch_dtype: auto
  device_map: auto
  attn_implementation: sdpa
  revision: ece1eea3e0f1653eebac5016bdae1fbd37894078
dataset:
  path: jijihuny/economics_qa_eval
  name: test
  shuffle: false
  test_size: null
  include_answer: false
train:
  instruction_template: "<|start_header_id|>user<|end_header_id|>\n\n"
  response_template: "<|start_header_id|>assistant<|end_header_id|>\n\n"
  use_completion_only_data_collator: true
  quantization:
    load_in_4bit: true
    bnb_4bit_quant_type: nf4
    bnb_4bit_compute_dtype: bfloat16
    bnb_4bit_use_double_quant: true
  lora:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    bias: none
    target_modules:
      - up_proj
      - down_proj
      - gate_proj
      - k_proj
      - q_proj
      - v_proj
      - o_proj
      - lm_head
    task_type: CAUSAL_LM
  args:
    output_dir: model/cos-restart
    run_name: cos-restart
    # report_to: wandb
    # dataloader_num_workers: 4
    # torch_empty_cache_steps: 3

    max_seq_length: 2048
    eval_strategy: steps
    per_device_train_batch_size: 2
    per_device_eval_batch_size: 4
    gradient_accumulation_steps: 8
    eval_accumulation_steps: 1

    optim: paged_adamw_8bit
    lr_scheduler_type: cosine_with_restart
    lr_scheduler_kwargs:
      num_cycles: 5
      gamma: 1.0
    bf16: true
    bf16_full_eval: true
    learning_rate: 1.0e-4
    weight_decay: 0.01
    num_train_epochs: 1
    warmup_ratio: 0.003
    max_grad_norm: 1.2
    
    eval_steps: 0.2
    eval_on_start: true
    save_steps: 0.2
    logging_steps: 1

    torch_compile: true
seed: 42