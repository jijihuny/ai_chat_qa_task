model:
  task: text-generation
  system_prompt: 너는 유능한 챗봇이야
  path: meta-llama/Meta-Llama-3-8B
  torch_dtype: auto
  device_map: auto
  attn_implementation: null
dataset:
  path: null
  name: null
  shuffle: true
  test_size: 0.9
  include_answer: false
metric:
  path: null
generation:
  # 프롬프트를 포함하지 않음(false)
  return_full_text: false
  # 생성할 최대 토큰 숫자
  max_new_token: null
  # Stochastic Decoding Algorithm
  do_sample: false
  # 상위 K개의 Vocab
  top_k: 1
  # Smallest subset V' s.t \sum_{v \in V} v \geq p
  top_p: 0.95
  # softmax(x/T) 
  # T > 1        => smooth(uniform as T -> \infty) 
  # 0 <= T < 1   => sharpen(deterministic as T -> 0+)
  temperature: 1.0
    # penalty on generated token. temperature보다 높아야함
  repetition_penalty: null

  # Contrastive search
  # Degeneration penalty
  # argmax (1-alpha) * p(v, x_{<i}) - alpha * max_{j<i}(similarity(v, x_j))
  penalty_alpha: null

  # https://arxiv.org/abs/2309.03883
  dola_layers: null

train:
  instruction_template: "<|start_header_id|>user<|end_header_id|>"
  response_template: "<|start_header_id|>assistant<|end_header_id|>"
  use_completion_only_data_collator: true
  quantization:
    load_in_4bit: true
    bnb_4bit_quant_type: nf4
    bnb_4bit_compute_dtype: bfloat16
    bnb_4bit_use_double_quant: true
  lora:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    bias: none
    target_modules:
      - up_proj
      - down_proj
      - gate_proj
      - k_proj
      - q_proj
      - v_proj
      - o_proj
    task_type: CAUSAL_LM
  args:
    output_dir: output
    run_name: output
    report_to: wandb

    eval_strategy: steps
    per_device_train_batch_size: 2
    per_device_eval_batch_size: 2
    gradient_accumulation_steps: 2
    eval_accumulation_steps: 2
    torch_empty_cache_steps: 10

    learning_rate: 2e-4
    weight_decay: 0.01
    num_train_epochs: 1
    warmup_ratio: 0.05

    eval_steps: 0.2
    save_steps: 0.2

    push_to_hub: True
seed: 42
